L'explosion de la production de données textuelles, alimentée par la numérisation croissante de notre monde, a atteint des proportions phénoménales ces dernières années.
Cette abondance de données textuelles représente une mine d'informations précieuses pour la prise de décision, la recherche et le développement, mais leur véritable valeur réside dans la capacité à les traiter efficacement.
Le traitement de ces données textuelles est essentiel pour extraire des informations significatives, détecter des tendances, automatiser des tâches, et prendre des décisions éclairées, ce qui en fait un enjeu majeur dans le domaine de l'intelligence artificielle et de l'analyse de données.
C'est notamment le cas dans le domaine médical avec les résumés de cas cliniques ou la pharmacovigilance.

Dans ce contexte, le \gls{tal} est un domaine qui joue un rôle fondamental en utilisant des approches syntaxiques, sémantiques ou a base d'apprentissages, pour extraire de l'information dans des textes.
Cela regroupe notamment : la reconnaissance d'entités nommées, l'extraction de relations, les liaisons d'entités nommées avec des individus d'une base de données, l'identification de la coréférence, de la temporalité et de la causalité.
Bien que tout cela constitue une étape majeure, ce n'est pas suffisant pour structurer l'information afin de l'exploiter dans des bases de données traditionnelles.
La structuration des données textuelles présente plusieurs avantages, notamment l'amélioration des capacités de recherche, la simplification de l'analyse des données, l'amélioration de la qualité des données grâce à une vérification de cohérence, l'intégration et la réutilisation des informations, la promotion de la collaboration au sein des équipes et la visualisation simplifiée des données à l'aide de graphiques ou de diagrammes.

\paragraph{Organisation}
La section~\ref{sec:struct:related-works} présente un aperçu des diverses approches dans la littérature qui tentent de répondre à cette problématique.
La section~\ref{sec:struct:pre} définit l'ensemble des concepts nécessaire à la formalisation des travaux présentés ici.
Le chapitre continue ensuite par la définition formelle de l'extraction de schéma (section~\ref{sec:struct:meta}) et la description, étape par étape, du processus proposé pour l'extraction de schéma et la structuration automatique d'information (section~\ref{sec:struct:steps}).
Une proposition d'implémentation du processus est proposée dans la section~\ref{sec:struct:implem} et une évaluation critique des résultats obtenus est réalisée dans la section~\ref{sec:struct:eval}.
La section~\ref{sec:struct:conclusion} conclus ce chapitre.

\section{Des textes aux graphes}
\label{sec:struct:related-works}

Dans cette section, nous abordons succinctement diverses méthodes de représentation de l'information textuelle sous forme de graphe.
Nous débutons par des approches où le graphe est simplement une annotation sur le texte, puis examinons des propositions visant à généraliser des informations contenues dans le texte.

Une première façon intuitive de représenter un texte comme un graphe est de visualiser uniquement la syntaxe.
Une phrase est composée d'une séquence de mots qu'il est alors possible de représenter sous forme de graphe.
Par exemple, la phrase \textquote{Alice aime les pommes} peut être représentée par le graphe donné dans la figure~\ref{fig:struct:word-graph}.
Sur ces graphes, en cumulant plusieurs phrases, il est ensuite possible de retrouver des éléments important en utilisant des algorithmes de centralité (comme page rank \cite{pagePageRankCitationRanking1999}), ou des chemins indivisibles.
\cite{ganesanOpinosisGraphBased2010} propose notamment d'utiliser ces graphes de mots pour construire des résumés de texte.

\begin{figure}[htb]
    \begin{subfigure}{.45\textwidth}
        \centering
        \footnotesize
        \begin{adjustbox}{width=.7\linewidth}
            \begin{dependency}[theme=simple, hide label, column sep=4em]
                \begin{deptext}
                    Alice \& aime \& les \& pommes \\
                \end{deptext}
                \depedge{1}{2}{}
                \depedge{2}{3}{}
                \depedge{3}{4}{}
            \end{dependency}
        \end{adjustbox}
        \caption{Graphe de mots}
        \label{fig:struct:word-graph}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.45\textwidth}
        \centering
        \footnotesize
        \begin{adjustbox}{width=.7\linewidth}
            \begin{dependency}[theme=simple, column sep=4em]
                \begin{deptext}
                    Alice \& aime \& les \& pommes \\
                \end{deptext}
                %\deproot{2}{ROOT}
                \depedge{2}{1}{subj}
                \depedge{2}{4}{obj}
                \depedge{4}{3}{det}
            \end{dependency}
        \end{adjustbox}
        \caption{Arbre de dépendance}
        \label{fig:struct:dep-graph}
    \end{subfigure}
    \caption{Représentations syntaxiques d'un texte en graphe}
\end{figure}

Pour apporter plus d'information que la séquence de mot, le \gls{tal} apporte aussi un ensemble d'outils pour la représentation de textes sous forme de graphes.
L'arbre de syntaxe représente la structure grammaticale d'une phrase en organisant les mots en fonction de leurs rôles syntaxiques et de leurs relations hiérarchiques.
D'autre part, l'arbre de dépendance (figure~\ref{fig:struct:dep-graph}) modélise les relations de dépendance entre les mots d'une phrase, où chaque nœud représente un mot et les arêtes indiquent les relations de gouverneur-dépendant.
Il est à noté que ces structures sont différentes selon la langue étudiée.
Certains travaux, tendent vers une construction des arbres de dépendance agnostique à la langue du texte à l'aide de \gls{ud} \cite{nivreUniversalGrammarNatural2015}.

Afin d'ajouter de la sémantique dans ces graphes, il est aussi possible de s'appuyer sur les techniques d'extraction d'information (\acrshort{ie}) comme la détection d'entité nommée \cite{patel-schneiderUsingDescriptionLogics2015}.
On peut imaginer représenter un document comme un nœud connecté à un ensemble de nœuds représentants les entités identifiées.
Selon la granularité voulue, on pourrait ajouter le découpage du texte en phrase et en mots.
Cette approche permet de capturer les relations entre les entités et leur contexte textuel et permet de construire des liens entre les phrases par les entités qui y sont présentes.
La puissance des graphes réside dans leur multidimensionnalité, qui permet de représenter simultanément différente modélisation.
Cette combinaison d'approches permet une compréhension plus profonde des textes en utilisant de nombreux facteurs.

\paragraph{Ontologies}
Les ontologies sont un ensemble de concepts interconnectés par des relations permettant de représenter et de raisonner sur un domaine de connaissance spécifique et sont le fondement du web sémantique.
Elles se composent d'un ensemble de prédicats utilisé pour représenter les données dans un graphe de connaissance \acrshort{rdf} (section~\ref{sec:update:pre:graph}, page~\pageref{sec:update:pre:graph}).
Dans ce contexte, la tache d'annotation sémantique (\enquote{entity linking}) consiste à lier des parties d'un texte (en leur attribuant des \acrshortpl{uri}) à des entités dans une ontologie.
Cette opération peut être réalisée de façon indépendante ou dans la continuité de l'extraction d'entités \cite{al-moslmiNamedEntityExtraction2020} (nous étudierons différentes techniques dans le chapitre~\ref{chp:tal} pour réaliser cette tâche).
On retrouve notamment des techniques à base de similarité et de règles \cite{liuDBpediaBasedEntityLinking2018,amaviNaturalLanguageQuerying2020} et d'apprentissage automatique \cite{shenProbabilisticModelLinking2014,fanLinkingEntitiesRelations2024} dont beaucoup des approches récentes se basant sur des réseaux de neurones \cite{mondalMedicalEntityLinking2019,sakorFalconEntityRelation2020,liEfficientOnePassEndtoEnd2020,bellatrecheNewTrendsDatabase2021,navigliWordSenseDisambiguation2024}.
Le processus est souvent le même :
\begin{enumerate}
    \item Identifier les mentions (partie du texte) qui peuvent être réalisés via l'extraction d'entité nommée ;
    \item Identifier les entités candidates dans la base de connaissances ;
    \item Classement des entités.
\end{enumerate}
On retrouve des approches qui traitent les entités une à une ou des approches plus globales qui peuvent aider pour la désambiguïsation.
La littérature tend à passer des approches séquentielles plus classiques à un processus unique d'annotation via l'apprentissage profond.
Ces nouvelles approches ne s'intéressent souvent qu'à l'identification des mentions d'entité et non aux relations.
Elles décorrèlent aussi le lien sémantique important entre les entités et la base de connaissance \cite{huEntityLinkingSymmetrical2019}.
Ces systèmes sont souvent très dépendants de la qualité des bases de connaissances utilisées dans lesquels il qui manque souvent des informations et dont les données ne sont pas toujours nettoyées \cite{weichselbraunMiningLeveragingBackground2018}.
De plus, les bases de connaissances sont dynamiques et volatiles, les approches statiques à base d'apprentissage ne permettent pas toujours la mise à jour rapide des modèles.

Toutes ces techniques s'appuient sur des bases de connaissances utilisant des ontologies préétablies et relativement fixe.
Cependant, il n'est pas toujours possible de représenter l'information avec ces ontologies.
Pour surmonter la complexité et le cout de leur création manuelle, une branche de l'intelligence artificielle s'intéresse à l'apprentissage d'ontologie.
La construction d'une ontologie consiste à identifier automatiquement ou semi-automatiquement des termes et des concepts pertinents, leur hiérarchie ainsi que leurs relations.
Ces concepts peuvent ensuite être utilisés pour construire une nouvelle ontologie ou en enrichir d'autres.
L'apprentissage d'ontologie peut aussi inclure la détection de règles d'inférences.
La découverte d'ontologie nécessite souvent une intervention humaine \cite{navarro-almanzaAutomatedOntologyExtraction2020} et la construction complètement automatique reste difficile voir impossible \cite{hazmanSurveyOntologyLearning2011,wongOntologyLearningText2012,browarnikOntologyLearningText2015}.
Ici on s'intéresse principalement à l'apprentissage d'ontologie à partir de textes \cite{konysKnowledgeRepositoryOntology2019,watrobskiOntologyLearningMethods2020,al-aswadiAutomaticOntologyConstruction2020}.
Comme pour l'annotation sémantique, on trouve des approches à base de règles et d'apprentissage automatique.

Historiquement, on trouve des approches, similaires à l'annotation sémantique, à base de motifs syntaxiques (comme \enquote{$X$ is a $Y$}) \cite{morinAutomaticAcquisitionSemantic1999} mais aussi des approches basées sur l'analyse des arbres syntaxiques et de dépendances \cite{gamalloMappingSyntacticDependencies2002,nivreIncrementalityDeterministicDependency2004}.
Ces approches s'intéressent notamment à l'extraction des triplets sujet / verbe / objet.
\cite{budanitskyLexicalSemanticRelatedness1999}, utilise la fréquence de co-occurrence entre les unités lexicales pour identifier les concepts et les relations.
Ces approches peuvent ensuite être complétées par du raisonnement logique pour identifier la hiérarchie des concepts, les relations ou construire des règles d'inférences \cite{zelleLearningSemanticGrammars1993}.
\cite{al-aswadiAutomaticOntologyConstruction2020} présente une vue détaillée des différents outils disponibles.
L'approche présentée dans cette thèse s'appuie sur certaines de ces techniques.
Cependant, les approches classiques comme {Text-to-Onto} \cite{maedcheTexttoontoOntologyExtraction2001}, {Text2Onto} \cite{cimianoText2Onto2005}, et {TextStorm} \cite{alvesAutomaticReadingLearning2002} dépendent beaucoup de l'intervention humaine et ne sont finalement que des outils d'aide plus que des processus automatiques.
De plus, les techniques qui se basent sur des motifs prédéfinis offrent de bonnes performances \cite{savaryRelationExtractionClinical2022}, mais restent limitées, car ces motifs sont souvent dépendants du domaine étudié.

À partir de 2010, de nombreux travaux ont émergé, mettant l'accent sur les techniques d'apprentissage profond.
Ces systèmes offrent généralement des performances supérieures aux systèmes historiques, comme documenté par \cite{al-aswadiAutomaticOntologyConstruction2020}.
Dans cette thèse, nous ne nous intéressons pas à ces systèmes, car ils sont souvent considérés comme des boîtes noires.
Il est essentiel, en particulier dans le domaine médical, d'être capable d'expliquer les prises de décision et de fournir un système en lequel les utilisateurs peuvent avoir confiance \cite{ribeiroWhyShouldTrust2016}.
Bien que de nombreux travaux tentent de répondre à cette problématique avec l'intelligence artificielle explicative, ce sujet ne sera pas détaillé ici.
Une approche plus conventionnelle permet à un humain, même en cas d'automatisation totale, de vérifier et d'intervenir si nécessaire pour corriger les données à chaque étape du processus.
Les travaux présentés dans cette thèse ont également l'ambition de viser la construction d'un système adaptable à différents domaines (\gls{ennov} couvrant un large éventail d'utilisations différentes), évitant ainsi de dépendre de l'apprentissage pour un domaine spécifique.
De plus, pour les approches basées sur les réseaux de neurones, la demande en données d'apprentissage est souvent élevée comme leur cout financier et environnemental \cite{strubellEnergyPolicyConsiderations2019}.
Dans le cadre d'\gls{ennov}, les solutions mises en place concernent des données du domaine médical contenant des informations sensibles, telles que des données de patients ou des secrets industriels.
L'utilisation de l'apprentissage à partir de ces données présente des risques liés à la protection de la vie privée et au secret de fabrication \cite{fredriksonModelInversionAttacks2015,songPrivacyRisksSecuring2019}.
Dans ce contexte, la compartimentation des données est cruciale, mais la construction d'un modèle par source de données n'est pas envisageable.
Une pratique courante consiste donc à construire un modèle générique et à le raffiner pour chaque utilisateur/source de données \cite{ribeiroWhyShouldTrust2016}.
Cette solution permet également de réduire les couts et c'est ce qu'on retrouve avec les modèles comme \gls{bert} \cite{devlinBERTPretrainingDeep2019} ou \gls{gpt} \cite{radfordImprovingLanguageUnderstanding2018, brownLanguageModelsAre2020}. 
Cependant, cela pose les mêmes problématiques d'accès aux données énoncées précédemment et le modèle générique doit alors être construit sur des données publiques ou artificielles qui ne sont pas toujours disponibles.

Dans le cadre des textes, les ontologies peuvent être considérées comme un schéma universel d'annotation, permettant ainsi de rendre les informations extraites depuis les textes interopérables, c'est-à-dire compatibles et échangeables entre différents systèmes et applications.
Leur valeur réside dans leur capacité à connecter diverses sources de données et à exprimer la connaissance en établissant des liens sémantiques entre les différents concepts et informations.
En revanche, les bases de données offrent une structure homogène où les données sont organisées selon un schéma défini, facilitant ainsi le stockage et la manipulation efficace des données, notamment pour gérer de grands volumes.
Les bases de données sont particulièrement adaptées pour l'analyse approfondie et le raisonnement sur les données.
De plus, compte tenu de l'intervention humaine quasi systématique dans le processus d'extraction d'ontologie (en dehors de la validation), cette thèse s'intéresse uniquement à la structuration des données en vue de leur exploitation dans une base de données.

\paragraph{\textsc{Abstra}}
\textsc{Abstra} \cite{barretAbstraGenericAbstractions2022} est un système conçu pour présenter un ensemble de jeux de données de manière homogène à un utilisateur, dans le but de l'aider à mieux les comprendre et de lui permettre de choisir celui à utiliser et comment il est structuré.
L'objectif principal est de représenter les jeux de données comme un ensemble de classes de haut niveau et les relations entre elles.
Le système est capable d'analyser un grand nombre de modèles de données structurés ou semi-structurés, tels que \acrshort{rdf}, JSON, \acrshort{xml}, CSV, des bases relationnelles ou des graphes de propriétés.
Le système \textsc{Abstra} se compose de trois parties principales.
La première partie vise à représenter les jeux de données de manière homogène en introduisant une structure intermédiaire inspirée du \gls{mea} \cite{chenEntityrelationshipModelUnified1976}.
Cette structure comprend des sous-enregistrements (\enquote{sub-record}) qui représentent les données, des enregistrements (\enquote{record}) représentant des objets et des collections.
Un exemple de cette structure est illustré dans la figure~\ref{fig:struct:abstra-ex}.
Dans un deuxième temps, le système identifie les nœuds de données importants ainsi que les collections principales.
Par exemple, si un objet est composé de sous-objets, seul l'objet parent peut être important si les sous-objets ne servent qu'à structurer les informations au sein de l'objet.
Cependant, les sous-objets peuvent également être importants s'ils apparaissent ailleurs dans les données.
Enfin, \textsc{Abstra} classe les collections principales (les types d'objets) pour leur attribuer une étiquette, telle que \emph{Person}, \emph{Product}, etc., permettant ainsi de construire une vision de haut niveau du jeu de données.

\begin{figure}[htb]
    \centering
    \begin{subfigure}{.6\linewidth}
        \centering
        \begin{adjustbox}{max width=\linewidth}
            \verb|{authors: [{name: "Alice", cities: ["Paris"]}]}|
        \end{adjustbox}
        \caption{Extrait JSON}
    \end{subfigure}
    \begin{subfigure}{.6\linewidth}
        \centering
        \begin{adjustbox}{max width=\linewidth}
            \begin{tikzpicture}[shorten >=3pt, -latex, node distance=3em]
                \node (epsilon) {$\epsilon$};
                \node[right=of epsilon] (authors) {authors};
                \node[right=of authors] (authors2) {authors.};
                \node[above right=1em and 3em of authors2] (name) {"Alice"};
                \node[below right=1em and 3em of authors2] (cities) {cities.};
                \node[right=of cities] (paris) {"Paris"};

                \path
                (epsilon) edge node[above]{$\epsilon$} (authors)
                (authors) edge node[above]{$\epsilon$} (authors2)
                (authors2) edge node[above left]{name} (name)
                (authors2) edge node[below left]{cities} (cities)
                (cities) edge node[above]{$\epsilon$} (paris)
                ;
            \end{tikzpicture}
        \end{adjustbox}
        \caption{Représentation graphe dans \textsc{Abstra}}
    \end{subfigure}
    \caption[Exemple de la structure de graphe utilisée par \textsc{Abstra}]{Exemple de la structure de graphe utilisée par \textsc{Abstra} \cite{barretAbstraGenericAbstractions2022}}
    \label{fig:struct:abstra-ex}
\end{figure}

Dans \textsc{Abstra} et dans nos travaux, on s'intéresse à l'extraction de schéma.
Toutefois, contrairement à l'approche présentée dans \cite{barretAbstraGenericAbstractions2022}, qui vise à identifier un schéma résumé de données structurées ou semi-structurées, notre objectif est de structurer automatiquement des données non-structurées issues du texte de manière à ce qu'elles soient conformes à un schéma valide et ainsi pouvoir les enregistrer dans une base de données.
Cette tâche consiste alors, non seulement à identifier le schéma, mais aussi à construire l'instance associée.
Bien que les travaux de \cite{barretAbstraGenericAbstractions2022} diffèrent de ceux présentés dans cette thèse, nous constaterons dans la section~\ref{sec:struct:meta} que la structure de données intermédiaire utilisée dans \textsc{Abstra} est similaire à celle proposée dans notre approche.
Cette ressemblance témoigne de la nécessité actuelle d'envisager des méta-modèles en tant qu'abstraction pour différents modèles de données.
Ce concept apparaît aussi dans \cite{gonzalezlopezdemurillasConnectingDatabasesProcess2019} et \cite{maliFACTDMFrameworkAutomated2024}.
