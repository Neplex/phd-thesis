@article{abdul-kaderSurveyChatbotDesign2015,
  title = {Survey on {{Chatbot Design Techniques}} in {{Speech Conversation Systems}}},
  author = {{Abdul-Kader}, S. A. and Woods, J. C.},
  year = {2015},
  journal = {International Journal of Advanced Computer Science and Applications},
  volume = {6},
  number = {7},
  publisher = {{The Science and Information (SAI) Organization}},
  issn = {2156-5570},
  url = {http://dx.doi.org/10.14569/IJACSA.2015.060712},
  urldate = {2021-03-11},
  abstract = {Human-Computer Speech is gaining momentum as a technique of computer interaction. There has been a recent upsurge in speech based search engines and assistants such as Siri, Google Chrome and Cortana. Natural Language Processing (NLP) techniques such as NLTK for Python can be applied to analyse speech, and intelligent responses can be found by designing an engine to provide appropriate human like responses. This type of programme is called a Chatbot, which is the focus of this study. This paper presents a survey on the techniques used to design Chatbots and a comparison is made between different design techniques from nine carefully selected papers according to the main methods adopted. These papers are representative of the significant improvements in Chatbots in the last decade. The paper discusses the similarities and differences in the techniques and examines in particular the Loebner prize-winning Chatbots.},
  copyright = {cc\_by},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2015\Abdul-Kader et Woods - 2015 - Survey on Chatbot Design Techniques in Speech Conv.pdf}
}

@book{abelloAdvancesDatabasesInformation2023,
  title = {Advances in {{Databases}} and {{Information Systems}}: 27th {{European Conference}}, {{ADBIS}} 2023, {{Barcelona}}, {{Spain}}, {{September}} 4--7, 2023, {{Proceedings}}},
  shorttitle = {Advances in {{Databases}} and {{Information Systems}}},
  editor = {Abell{\'o}, Alberto and Vassiliadis, Panos and Romero, Oscar and Wrembel, Robert},
  year = {2023},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {13985},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-42914-9},
  urldate = {2023-09-11},
  isbn = {978-3-031-42913-2 978-3-031-42914-9},
  langid = {english},
  keywords = {data exploration,data management systems,data quality,data science,fairness,information systems,metadata,query processing},
  file = {C:\Users\nhiot\OneDrive\zotero\2023\Abelló et al. - 2023 - Advances in Databases and Information Systems 27t.pdf}
}

@book{abelloNewTrendsDatabase2023,
  title = {New {{Trends}} in {{Database}} and {{Information Systems}}: {{ADBIS}} 2023 {{Short Papers}}, {{Doctoral Consortium}} and {{Workshops}}: {{AIDMA}}, {{DOING}}, {{K-Gals}}, {{MADEISD}}, {{PeRS}}, {{Barcelona}}, {{Spain}}, {{September}} 4--7, 2023, {{Proceedings}}},
  shorttitle = {New {{Trends}} in {{Database}} and {{Information Systems}}},
  editor = {Abell{\'o}, Alberto and Vassiliadis, Panos and Romero, Oscar and Wrembel, Robert and Bugiotti, Francesca and Gamper, Johann and Vargas Solar, Genoveva and Zumpano, Ester},
  year = {2023},
  series = {Communications in {{Computer}} and {{Information Science}}},
  volume = {1850},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-42941-5},
  urldate = {2023-09-11},
  isbn = {978-3-031-42940-8 978-3-031-42941-5},
  langid = {english},
  keywords = {artificial intelligence,computational linguistics,computer networks,data mining,databases,embedded systems,graph theory,information retrieval,knowledge-based system,machine learning,Natural Language Processing (NLP),nosource,query languages,software architecture,software design,software engineering}
}

@inproceedings{AbG85,
  title = {Mise-{\`a}-Jour Des {{Bases}} de {{Donn{\'e}es}} Contenant de l'information Incompl{\`e}te},
  booktitle = {Journ{\'e}es Bases de Donn{\'e}es Avanc{\'e}s, 6-8 Mars 1985, St. {{Pierre}} de Chartreuse (Informal Proceedings).},
  author = {Abiteboul, Serge and Grahne, G{\"o}sta},
  year = {1985},
  optbibsource = {dblp computer science bibliography, http://dblp.org},
  optcrossref = {DBLP:conf/bda/1985},
  opttimestamp = {Tue, 31 Oct 2006 14:01:45 +0100},
  keywords = {⛔ No DOI found}
}

@book{abiteboulFoundationsDatabases1995,
  title = {Foundations of {{Databases}}},
  shorttitle = {Foundations of {{Databases}}},
  author = {Abiteboul, Serge and Hull, Richard and Vianu, Victor},
  year = {1995},
  edition = {1st},
  volume = {8},
  publisher = {Addison-Wesley Longman Publishing Co., Inc.},
  address = {USA},
  abstract = {This work presents comprehensive coverage of the foundations and theory of database systems. It is a reference to both classical material and advanced topics, bringing together many subjects including up-to-date coverage of object-oriented and logic databases. Numerous exercises are provided at three levels of difficulty. The book is intended for use by database professionals at all levels of experience, and graduate and senior level students in Advanced Theory of Databases.},
  isbn = {978-0-201-53771-0},
  langid = {english},
  keywords = {nosource},
  file = {C:\Users\nhiot\Zotero\storage\KKV3MZ2R\abiteboul95.html}
}

@inproceedings{abiteboulUpdateSemanticsIncomplete1985,
  title = {Update Semantics for Incomplete Databases},
  booktitle = {Proceedings of the 11th International Conference on {{Very Large Data Bases}} - {{Volume}} 11},
  author = {Abiteboul, Serge and Grahne, G{\"o}sta},
  editor = {Pirotte, Alain and Vassiliou, Yannis},
  year = {1985},
  month = aug,
  series = {{{VLDB}} '85},
  pages = {1--12},
  publisher = {VLDB Endowment},
  address = {Stockholm, Sweden},
  url = {https://dblp.org/rec/conf/vldb/AbiteboulG85},
  abstract = {A database containing some incomplete information is viewed as a set of possible states of the real world. The semantics of updates is given based on simple set operations on the set of states. Some basic results concerning the capabilities of known models of incomplete databases to handle updates are exhibited.},
  isbn = {0-934613-17-6},
  keywords = {⛔ No DOI found,nosource},
  file = {C:\Users\nhiot\Zotero\storage\3SJCDNV4\1286760.html}
}

@inproceedings{abraoIncrementalConstraintChecking2004,
  title = {Incremental {{Constraint Checking}} for {{XML Documents}}},
  booktitle = {Database and {{XML Technologies}}},
  author = {Abr{\~a}o, Maria Adriana and Bouchou, B{\'e}atrice and {Halfeld-Ferrari}, Mirian and Laurent, Dominique and Musicante, Martin A.},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Bellahs{\`e}ne, Zohra and Milo, Tova and Rys, Michael and Suciu, Dan and Unland, Rainer},
  year = {2004},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {3186},
  pages = {112--127},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-30081-6_9},
  urldate = {2024-01-03},
  abstract = {We introduce a method for building an XML constraint validator from a given set of schema, key and foreign key constraints. The XML constraint validator obtained by our method is a bottom-up tree transducer that is used not only for checking, in only one pass, the correctness of an XML document but also for incrementally validating updates over this document. In this way, both the verification from scratch and the update verification are based on regular (finite and tree) automata, making the whole process efficient.},
  isbn = {978-3-540-22969-8 978-3-540-30081-6},
  langid = {english},
  keywords = {Data Node,Incremental Validation,Integrity Constraint,nosource,Output Function,Target Node},
  file = {C:\Users\nhiot\OneDrive\zotero\2004\Abrão et al. - 2004 - Incremental Constraint Checking for XML Documents.pdf}
}

@misc{ACELinguisticData2008,
  title = {{{ACE}} {\textbar} {{Linguistic Data Consortium}}},
  year = {2008},
  url = {https://www.ldc.upenn.edu/collaborations/past-projects/ace},
  urldate = {2020-01-27},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2008\\2008 - ACE  Linguistic Data Consortium.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\V39J4BPV\\ace.html}
}

@book{AcquiringSemanticLexicon2021,
  title = {Acquiring a {{Semantic Lexicon}} for {{Natural Language Processing}}},
  year = {2021},
  month = mar,
  journal = {Lexical Acquisition: Exploiting On-Line Resources to Build a Lexicon},
  pages = {341--367},
  publisher = {Psychology Press},
  doi = {10.4324/9781315785387-17},
  urldate = {2021-05-03},
  abstract = {This chapter explores the possibility of acquiring semantic information for natural language processing using on-line corpora. Our viewpoint is a very},
  isbn = {978-1-315-78538-7},
  langid = {english},
  keywords = {nosource}
}

@misc{adminSeriousAdverseEvent,
  ids = {adminSeriousAdverseEventa},
  title = {Serious {{Adverse Event}} Narratives},
  author = {{admin}},
  journal = {RIAT Support Center},
  url = {https://restoringtrials.org/glossary/serious-adverse-event-narratives/},
  urldate = {2020-01-24},
  abstract = {Clinical Study Reports contain individual participant narratives of serious adverse events (ICH E3 section 12.3.2). They consist of unstructured free text and summarize information relevant to the serious adverse event.~ Each individual narrative is typically a paragraph to a page long. Example: See PDF p.276 onwards of paroxetine study 329},
  langid = {american},
  keywords = {nosource}
}

@misc{AdverseDrugReaction,
  title = {Adverse {{Drug Reaction Extraction}} from {{Drug Labels}}},
  url = {https://bionlp.nlm.nih.gov/tac2017adversereactions/},
  urldate = {2021-05-06},
  file = {C:\Users\nhiot\Zotero\storage\BWYZSLZ7\tac2017adversereactions.html}
}

@article{affolterComparativeSurveyRecent2019,
  title = {A Comparative Survey of Recent Natural Language Interfaces for Databases},
  author = {Affolter, Katrin and Stockinger, Kurt and Bernstein, Abraham},
  year = {2019},
  month = oct,
  journal = {The VLDB Journal},
  volume = {28},
  number = {5},
  pages = {793--819},
  publisher = {Springer},
  issn = {0949-877X},
  doi = {10/ggt69b},
  urldate = {2020-05-07},
  abstract = {Over the last few years, natural language interfaces (NLI) for databases have gained significant traction both in academia and industry. These systems use very different approaches as described in recent survey papers. However, these systems have not been systematically compared against a set of benchmark questions in order to rigorously evaluate their functionalities and expressive power. In this paper, we give an overview over 24 recently developed NLIs for databases. Each of the systems is evaluated using a curated list of ten sample questions to show their strengths and weaknesses. We categorize the NLIs into four groups based on the methodology they are using: keyword-, pattern-, parsing- and grammar-based NLI. Overall, we learned that keyword-based systems are enough to answer simple questions. To solve more complex questions involving subqueries, the system needs to apply some sort of parsing to identify structural dependencies. Grammar-based systems are overall the most powerful ones, but are highly dependent on their manually designed rules. In addition to providing a systematic analysis of the major systems, we derive lessons learned that are vital for designing NLIs that can answer a wide range of user questions.},
  langid = {english},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2019\\Affolter et al. - 2019 - A comparative survey of recent natural language in.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\RLH8M7AQ\\s00778-019-00567-8.html}
}

@inproceedings{agrawalMiningAssociationRules1993,
  title = {Mining {{Association Rules Between Sets}} of {{Items}} in {{Large Databases}}},
  booktitle = {Proceedings of the 1993 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Agrawal, Rakesh and Imieli{\'n}ski, Tomasz and Swami, Arun},
  year = {1993},
  series = {{{SIGMOD}} '93},
  pages = {207--216},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10/fn4bpx},
  urldate = {2019-01-06},
  abstract = {We are given a large database of customer transactions. Each transaction consists of items purchased by a customer in a visit. We present an efficient algorithm that generates all significant association rules between items in the database. The algorithm incorporates buffer management and novel estimation and pruning techniques. We also present results of applying this algorithm to sales data obtained from a large retailing company, which shows the effectiveness of the algorithm.},
  isbn = {978-0-89791-592-2},
  annotation = {00000},
  file = {C:\Users\nhiot\OneDrive\zotero\1993\Agrawal et al. - 1993 - Mining Association Rules Between Sets of Items in .pdf}
}

@inproceedings{ahmadCRONOBenchmarkSuite2015,
  title = {{{CRONO}}: {{A Benchmark Suite}} for {{Multithreaded Graph Algorithms Executing}} on {{Futuristic Multicores}}},
  shorttitle = {{{CRONO}}},
  booktitle = {2015 {{IEEE International Symposium}} on {{Workload Characterization}}},
  author = {Ahmad, M. and Hijaz, F. and Shi, Q. and Khan, O.},
  year = {2015},
  month = oct,
  pages = {44--55},
  doi = {10/gjhfrt},
  abstract = {Algorithms operating on a graph setting are known to be highly irregular and unstructured. This leads to workload imbalance and data locality challenge when these algorithms are parallelized and executed on the evolving multicore processors. Previous parallel benchmark suites for shared memory multicores have focused on various workload domains, such as scientific, graphics, vision, financial and media processing. However, these suites lack graph applications that must be evaluated in the context of architectural design space exploration for futuristic multicores. This paper presents CRONO, a benchmark suite composed of multi-threaded graph algorithms for shared memory multicore processors. We analyze and characterize these benchmarks using a multicore simulator, as well as a real multicore machine setup. CRONO uses both synthetic and real world graphs. Our characterization shows that graph benchmarks are diverse and challenging in the context of scaling efficiency. They exhibit low locality due to unstructured memory access patterns, and incur fine-grain communication between threads. Energy overheads also occur due to nondeterministic memory and synchronization patterns on network connections. Our characterization reveals that these challenges remain in state-of-the-art graph algorithms, and in this context CRONO can be used to identify, analyze and develop novel architectural methods to mitigate their efficiency bottlenecks in futuristic multicore processors.},
  keywords = {Benchmark testing,CRONO,Data structures,graph theory,Instruction sets,multi-threading,Multicore processing,multithreaded graph algorithms,nosource,parallel benchmark suites,Scalability,shared memory multicore processors,shared memory systems,unstructured memory access patterns}
}

@article{ahoEfficientOptimizationClass1979,
  ids = {ASU79},
  title = {Efficient Optimization of a Class of Relational Expressions},
  author = {Aho, Alfred V. and Sagiv, Yehoshua and Ullman, Jeffrey D.},
  year = {1979},
  journal = {ACM Transactions on Database Systems (TODS)},
  volume = {4},
  number = {4},
  pages = {435--454},
  publisher = {ACM New York, NY, USA},
  issn = {0362-5915},
  doi = {10.1145/320107.320112},
  abstract = {The design of several database query languages has been influenced by Codd's relational algebra. This paper discusses the difficulty of optimizing queries based on the relational algebra operations select, project, and join. A matrix, called a tableau, is proposed as a useful device for representing the value of a query, and optimization of queries is couched in terms of finding a minimal tableau equivalent to a given one. Functional dependencies can be used to imply additional equivalences among tableaux. Although the optimization problem is NP-complete, a polynomial time algorithm exists to optimize tableaux that correspond to an important subclass of queries.},
  optbibsource = {dblp computer science bibliography, https://dblp.org},
  optdoi = {10.1145/320107.320112},
  opttimestamp = {Tue, 06 Nov 2018 12:51:47 +0100},
  keywords = {equivalence of queries,NP-completeness,query optimization,relational algebra,relational database,tableaux},
  file = {C:\Users\nhiot\OneDrive\zotero\1979\Aho et al. - 1979 - Efficient optimization of a class of relational ex.pdf}
}

@article{ahoTheoryJoinsRelational1979,
  title = {The Theory of Joins in Relational Databases},
  author = {Aho, Alfred V. and Beeri, Catriel and Ullman, Jeffrey D.},
  year = {1979},
  journal = {ACM Transactions on Database Systems (TODS)},
  volume = {4},
  number = {3},
  pages = {297--314},
  publisher = {ACM New York, NY, USA},
  doi = {10.1145/320083.320091},
  abstract = {Answering queries in a relational database often requires that the natural join of two or more relations be computed. However, not all joins are semantically meaningful. This paper gives an efficient algorithm to determine whether the join of several relations is semantically meaningful (lossless) and an efficient algorithm to determine whether a set of relations has a subset with a lossy join. These algorithms assume that all data dependencies are functional. Similar techniques also apply to the case where data dependencies are multivalued.},
  file = {C:\Users\nhiot\OneDrive\zotero\1979\Aho et al. - 1979 - The theory of joins in relational databases.pdf}
}

@inproceedings{airolaGraphKernelProteinProtein2008,
  ids = {airolaGraphKernelProteinprotein2008},
  title = {A {{Graph Kernel}} for {{Protein-Protein Interaction Extraction}}},
  booktitle = {Proceedings of the {{Workshop}} on {{Current Trends}} in {{Biomedical Natural Language Processing}}},
  author = {Airola, Antti and Pyysalo, Sampo and Bj{\"o}rne, Jari and Pahikkala, Tapio and Ginter, Filip and Salakoski, Tapio},
  year = {2008},
  month = jun,
  series = {{{BioNLP}} '08},
  pages = {1--9},
  publisher = {Association for Computational Linguistics},
  address = {Columbus, Ohio},
  url = {https://www.aclweb.org/anthology/W08-0601},
  urldate = {2020-11-26},
  abstract = {In this paper, we propose a graph kernel based approach for the automated extraction of protein-protein interactions (PPI) from scientific literature. In contrast to earlier approaches to PPI extraction, the introduced all-dependency-paths kernel has the capability to consider full, general dependency graphs. We evaluate the proposed method across five publicly available PPI corpora providing the most comprehensive evaluation done for a machine learning based PPI-extraction system. Our method is shown to achieve state-of-the-art performance with respect to comparable evaluations, achieving 56.4 F-score and 84.8 AUC on the AImed corpus. Further, we identify several pitfalls that can make evaluations of PPI-extraction systems incomparable, or even invalid. These include incorrect cross-validation strategies and problems related to comparing F-score results achieved on different evaluation resources.},
  isbn = {978-1-932432-11-4},
  file = {C:\Users\nhiot\OneDrive\zotero\2008\Airola et al. - 2008 - A Graph Kernel for Protein-Protein Interaction Ext.pdf}
}

@inproceedings{akhtarConstraintsRDF2011,
  title = {Constraints in {{RDF}}},
  booktitle = {Semantics in {{Data}} and {{Knowledge Bases}}},
  author = {Akhtar, Waseem and {Cort{\'e}s-Calabuig}, {\'A}lvaro and Paredaens, Jan},
  editor = {Schewe, Klaus-Dieter and Thalheim, Bernhard},
  year = {2011},
  volume = {6834},
  pages = {23--39},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-23441-5_2},
  urldate = {2023-12-27},
  isbn = {978-3-642-23440-8 978-3-642-23441-5}
}

@article{al-aswadiAutomaticOntologyConstruction2020,
  title = {Automatic Ontology Construction from Text: A Review from Shallow to Deep Learning Trend},
  shorttitle = {Automatic Ontology Construction from Text},
  author = {{Al-Aswadi}, Fatima N. and Chan, Huah Yong and Gan, Keng Hoon},
  year = {2020},
  month = aug,
  journal = {Artificial Intelligence Review},
  volume = {53},
  number = {6},
  pages = {3901--3928},
  issn = {0269-2821, 1573-7462},
  doi = {10.1007/s10462-019-09782-9},
  urldate = {2024-02-29},
  abstract = {The explosive growth of textual data on the web coupled with the increase on demand for ontologies to promote the semantic web, have made the automatic ontology construction from the text a very promising research area. Ontology learning (OL) from text is a process that aims to (semi-) automatically extract and represent the knowledge from text in machine-readable form. Ontology is considered one of the main cornerstones of representing the knowledge in a more meaningful way on the semantic web. Usage of ontologies has proven to be beneficial and efficient in different applications (e.g. information retrieval, information extraction, and question answering). Nevertheless, manually construction of ontologies is time-consuming as well extremely laborious and costly process. In recent years, many approaches and systems that try to automate the construction of ontologies have been developed. This paper reviews various approaches, systems, and challenges of automatic ontology construction from the text. In addition, it also discusses ways the ontology construction process could be enhanced in the future by presenting techniques from shallow learning to deep learning (DL).},
  langid = {english},
  keywords = {Concept classification,Deep learning,Ontology construction,Ontology learning,Semantic relation},
  file = {C:\Users\nhiot\OneDrive\zotero\2020\Al-Aswadi et al. - 2020 - Automatic ontology construction from text a revie.pdf}
}

@misc{alashqurLanguageManipulatingObjectoriented1998,
  title = {Language for {{Manipulating Object-oriented Databases}}},
  author = {Alashqur, A. and Lam, S. H.},
  year = {1998},
  url = {/paper/Language-for-Manipulating-Object-oriented-Databases-Alashqur-Lam/5af812fafd4938dd2e72b545dac953be1beea422},
  urldate = {2020-10-15},
  abstract = {An essential property which is desirable in a query language designed for a certain data model is that queries issued in that language must produce results that are structured and modeled using the same data msdel. A consequence of maintaining this property in a query language is that the result of a query can be used as an operand in sc+ne other query (or queries) or can bs saved as a user\&\#39;s view. Existing query languages that have been designed for the class of object-oriented data models do not posses this property. In this paper, we introduce the object-oriented query language (CQL), which maintains this property. An CQL query is considered as a function, which when applied to a database, returns a s\&amp;database whose structure consists of sane selected object classes and their associations. Ihe objects that satisfy the search conditions and participate in the patterns of object associations specified in the query constitute the eXtenSiOn of the resulting subdatabase. A subdatabase folms a \&quot;context\&quot; under which systemdefined and/or user defined operations can be specified and performsd. Several advanced features such as branching association patterns and set operations on subdatabases are also presented.},
  langid = {english},
  file = {C:\Users\nhiot\Zotero\storage\XKG9MSEC\5af812fafd4938dd2e72b545dac953be1beea422.html}
}

@inproceedings{alashqurOQLQueryLanguage1989,
  ids = {alashqurOQLQueryLanguage1989a},
  title = {{{OQL}}: A Query Language for Manipulating Object-Oriented Databases},
  shorttitle = {{{OQL}}},
  booktitle = {Proceedings of the 15th International Conference on {{Very}} Large Data Bases},
  author = {Alashqur, A. M. and Su, S. Y. W. and Lam, H.},
  year = {1989},
  month = jul,
  series = {{{VLDB}} '89},
  pages = {433--442},
  publisher = {Morgan Kaufmann Publishers Inc.},
  address = {San Francisco, CA, USA},
  urldate = {2020-10-14},
  isbn = {1-55860-101-5},
  keywords = {nosource}
}

@article{alexanderNaturalLanguageWeb2013,
  title = {Natural {{Language Web Interface}} for {{Database}} ({{NLWIDB}})},
  author = {Alexander, Rukshan and Rukshan, Prashanthi},
  year = {2013},
  journal = {Sri Lanka},
  pages = {8},
  abstract = {It is a long term desire of the computer users to minimize the communication gap between the computer and a human. On the other hand, almost all ICT applications store information in to databases and retrieve from them. Retrieving information from the database requires knowledge of technical languages such as Structured Query Language. However majority of the computer users who interact with the databases do not have a technical background and are intimidated by the idea of using languages such as SQL. For above reasons, a Natural Language Web Interface for Database (NLWIDB) has been developed. The NLWIDB allows the user to query the database in a language more like English, through a convenient interface over the Internet.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2013\Alexander et Rukshan - 2013 - Natural Language Web Interface for Database (NLWID.pdf}
}

@inproceedings{alexRecognisingNestedNamed2007,
  ids = {alex-etal-2007-recognising,alexRecognisingNestedNamed2007a},
  title = {Recognising {{Nested Named Entities}} in {{Biomedical Text}}},
  booktitle = {Proceedings of the {{Workshop}} on {{BioNLP}} 2007: {{Biological}}, {{Translational}}, and {{Clinical Language Processing}}},
  author = {Alex, Beatrice and Haddow, Barry and Grover, Claire},
  year = {2007},
  month = jun,
  series = {{{BioNLP}} '07},
  pages = {65--72},
  publisher = {Association for Computational Linguistics},
  address = {USA},
  url = {https://aclanthology.org/W07-1009.pdf},
  urldate = {2024-03-21},
  abstract = {Although recent named entity (NE) annotation efforts involve the markup of nested entities, there has been limited focus on recognising such nested structures. This paper introduces and compares three techniques for modelling and recognising nested entities by means of a conventional sequence tagger. The methods are tested and evaluated on two biomedical data sets that contain entity nesting. All methods yield an improvement over the baseline tagger that is only trained on flat annotation.},
  langid = {english},
  keywords = {⛔ No DOI found,nosource},
  file = {C:\Users\nhiot\OneDrive\zotero\2007\Alex et al. - 2007 - Recognising Nested Named Entities in Biomedical Te2.pdf}
}

@book{alfredvahoCompilersPrinciplesTechniques2007,
  title = {Compilers {{Principles}}, {{Techniques}}, {{And Tools}}},
  author = {Alfred V Aho, Monica S. Lam},
  year = {2007},
  url = {http://archive.org/details/aho-compilers-principles-techniques-and-tools-2e\_202203},
  urldate = {2023-11-06},
  abstract = {Compilers - Principles, Techniques and ToolsSecond Edition Alfred V Aho, Monica S Lam, Ravi Sethi, Jeffrey D Ullman Pearson Education, Inc.2007 1 Introduction~ 1.1 Language Processors1.1.1 Exercises for Section 1.1 1.2 The Structure of a Compiler1.2.1 Lexical Analysis~~1.2.2 Syntax Analysis1.2.3 Semantic Analysis1.2.4 Intermediate Code Generation1.2.5 Code Optimization~1.2.6 Code Generation1.2.7 Symbol-Table Management1.2.8 The Grouping of Phases into Passes1.2.9 Compiler-Construction Tools 1.3 The Evolution of Programming Languages1.3.1 The Move to Higher-level Languages1.3.2 Impacts on Compilers1.3.3 Exercises for Section 1.3 1.4 The Science of Building a Compiler1.4.1 Modeling in Compiler Design and Implementation1.4.2 The Science of Code Optimization 1.5 Applications of Compiler Technology~1.5.1 Implement at ion of High-Level Programming Languages1.5.2 Optimizations for Computer Architectures~1.5.3 Design of New Computer Architectures1.5.4 Program Translations~1.5.5 Software Productivity Tools~ 1.6 Programming Language Basics~1.6.1 The Static/Dynamic Distinction~1.6.2 Environments and States~1.6.3 Static Scope and Block Structure1.6.4 Explicit Access Control1.6.5 Dynamic Scope~1.6.6 Parameter Passing Mechanisms~1.6.7 Aliasing~1.6.8 Exercises for Section 1.6~ 1.7 Summary of Chapter 1 1.8 References for Chapter 1~ 2 A Simple Syntax-Directed Translator 2.1 Introduction~ 2.2 Syntax Definition2.2.1 Definition of Grammars~2.2.2 Derivations~2.2.3 Parse Trees2.2.4 Ambiguity~2.2.5 Associativity of Operators2.2.6 Precedence of Operators~2.2.7 Exercises for Section 2.2~ 2.3 Syntax-Directed Translation~2.3.1 Postfix Notation~2.3.2 Synthesized Attributes~2.3.3 Simple Syntax-Directed Definitions2.3.4 Tree Traversals~2.3.5 Translation Schemes~2.3.6 Exercises for Section 2.3~ 2.4 Parsing2.4.1 Top-Down Parsing2.4.2 Predictive Parsing2.4.3 When to Use 6-Productions~2.4.4 Designing a Predictive Parser~2.4.5 Left Recursion~2.4.6 Exercises for Section 2.4~ 2.5 A Translator for Simple Expressions2.5.1 Abstract and Concrete Syntax~2.5.2 Adapting the Translation Scheme~2.5.3 Procedures for the Nonterminals~2.5.4 Simplifying the Translator~2.5.5 The Complete Program~ 2.6 Lexical Analysis~2.6.1 Removal of White Space and Comments~2.6.2 Reading Ahead~2.6.3 Constants~2.6.4 Recognizing Keywords and Identifiers2.6.5 A Lexical Analyzer~2.6.6 Exercises for Section 2.6~ 2.7 Symbol Tables2.7.1 Symbol Table Per Scope2.7.2 The Use of Symbol Tables~ 2.8 Intermediate Code Generation2.8.1 Two Kinds of Intermediate Representations2.8.2 Construction of Syntax Trees2.8.3 Static Checking~2.8.4 Three-Address Code~2.8.5 Exercises for Section 2.8~ 2.9 Summary of Chapter 2 3 Lexical Analysis 3.1 The Role of the Lexical Analyzer3.1.1 Lexical Analysis Versus Parsing~3.1.2 Tokens, Patterns, and Lexemes~3.1.3 Attributes for Tokens3.1.4 Lexical Errors~3.1.5 Exercises for Section 3.1~ 3.2 Input Buffering 153.2.1 Buffer Pairs3.2.2 Sentinels 3.3 Specification of Tokens3.3.1 Strings and Languages3.3.2 Operations on Languages3.3.3 Regular Expressions3.3.4 Regular Definitions~3.3.5 Extensions of Regular Expressions3.3.6 Exercises for Section 3.3 3.4 Recognition of Tokens~3.4.1 Transition Diagrams3.4.2 Recognition of Reserved Words and Identifiers~3.4.3 Completion of the Running Example3.4.4 Architecture of a Transition-Diagram-Based Lexical Analyzer3.4.5 Exercises for Section 3.4~ 3.5 The Lexical-Analyzer Generator Lex3.5.1 Use of Lex~3.5.2 Structure of Lex Programs~3.5.3 Conflict Resolution in Lex3.5.4 The Lookahead Operator3.5.5 Exercises for Section 3.5~ 3.6 Finite Automata~3.6.1 Nondeterministic Finite Automata~3.6.2 Transition Tables3.6.3 Acceptance of Input Strings by Automata3.6.4 Deterministic Finite Automata3.6.5 Exercises for Section 3.6~ 3.7 From Regular Expressions to Automata~3.7.1 Conversion of an NFA to a DFA3.7.2 Simulation of an NFA~3.7.3 Efficiency of NFA Simulation3.7.4 Construction of an NFA from a Regular Expression3.7.5 Efficiency of String-Processing Algorithms~3.7.6 Exercises for Section 3.7 3.8 Design of a Lexical-Analyzer Generator~3.8.1 The Structure of the Generated Analyzer~3.8.2 Pattern Matching Based on NFA's~3.8.3 DFA's for Lexical Analyzers~3.8.4 Implementing the Lookahead Operator~3.8.5 Exercises for Section 3.8~ 3.9 Optimization of DFA-Based Pattern Matchers~3.9.1 Important States of an NFA~3.9.2 Functions Computed From the Syntax Tree~3.9.3 Computing nullable, firstpos, and lastpos~3.9.4 Computing followpos~3.9.5 Converting a Regular Expression Directly to a DFA~3.9.6 Minimizing the Number of States of a DFA~3.9.7 State Minimization in Lexical Analyzers~3.9.8 Trading Time for Space in DFA Simulation~3.9.9 Exercises for Section 3.9~ 3.10 Summary of Chapter 3~ 3.11 References for Chapter 3~ 4 Syntax Analysis 4.1 Introduction~4.1.1 The Role of the Parser4.1.2 Representative Grammars~4.1.3 Syntax Error Handling~4.1.4 Error-Recovery Strategies~ 4.2 Context-Free Grammars~4.2.1 The Formal Definition of a Context-Free Grammar~4.2.2 Notational Conventions~4.2.3 Derivations~4.2.4 Parse Trees and Derivations~4.2.5 Ambiguity4.2.6 Verifying the Language Generated by a Grammar~4.2.7 Context-Free Grammars Versus Regular Expressions4.2.8 Exercises for Section 4.2 4.3 Writing a Grammar4.3.1 Lexical Versus Syntactic Analysis4.3.2 Eliminating Ambiguity4.3.3 Elimination of Left Recursion~4.3.4 Left Factoring4.3.5 Non-Context-Free Language Constructs4.3.6 Exercises for Section 4.3 4.4 Top-Down Parsing4.4.1 Recursive-Descent Parsing4.4.2 FIRST and FOLLOW~4.4.3 LL(1) Grammars4.4.4 Nonrecursive Predictive Parsing4.4.5 Error Recovery in Predictive Parsing4.4.6 Exercises for Section 4.4~ 4.5 Bottom-Up Parsing~4.5.1 Reductions~4.5.2 Handle Pruning~4.5.3 Shift-Reduce Parsing~4.5.4 Conflicts During Shift-Reduce Parsing4.5.5 Exercises for Section 4.5~ 4.6 Introduction to LR Parsing: Simple LR~4.6.1 Why LR Parsers?~4.6.2 Items and the LR(0) Automaton~4.6.3 The LR-Parsing Algorithm~4.6.4 Constructing SLR-Parsing Tables4.6.5 Viable Prefixes~4.6.6 Exercisesfor Section 4.6~ 4.7 More Powerful LR Parsers~4.7.1 Canonical LR(1) Items~4.7.2 Constructing LR(1) Sets of Items~4.7.3 Canonical LR(1) Parsing Tables~4.7.4 Constructing LALR Parsing Tables~4.7.5 Efficient Construction of LALR Parsing Tables~4.7.6 Compaction of LR Parsing Tables~4.7.7 Exercises for Section 4.7~ 4.8 Using Ambiguous Grammars~4.8.1 Precedence and Associativity to Resolve Conflicts~4.8.2 The "Dangling-Else" Ambiguity~4.8.3 Error Recovery in LR Parsing~4.8.4 Exercises for Section 4.8~ 4.9 Parser Generators~4.9.1 The Parser Generator Yacc~4.9.2 Using Yacc with Ambiguous Grammars~4.9.3 Creating Yacc Lexical Analyzers with Lex~4.9.4 Error Recovery in Yacc~4.9.5 Exercises for Section 4.9~ 4.10 Summary of Chapter 4~4.11 References for Chapter 4~ 5 Syntax-Directed Translation~ 5.1 Syntax-Directed Definitions~5.1.1 Inherited and Synthesized Attributes~5.1.2 Evaluating an SDD at the Nodes of a Parse Tree~5.1.3 Exercises for Section 5.1~ 5.2 Evaluation Orders for SDD's5.2.1 Dependency Graphs~5.2.2 Ordering the Evaluation of Attributes5.2.3 S-Attributed Definitions~5.2.4 L-Attributed Definitions~5.2.5 Semantic Rules with Controlled Side Effects5.2.6 Exercises for Section 5.2~ 5.3 Applications of Synt ax-Directed Translation~5.3.1 Construction of Syntax Trees~5.3.2 The Structure of a Type~5.3.3 Exercises for Section 5.3~ 5.4 Syntax-Directed Translation Schemes5.4.1 Postfix Translation Schemes~5.4.2 Parser-Stack Implementation of Postfix SDT's5.4.3 SDT's With Actions Inside Productions~5.4.4 Eliminating Left Recursion From SDT's5.4.5 SDT's for L-Attributed Definitions~5.4.6 Exercises for Section 5.4~ 5.5 Implementing L- Attributed SDD's~5.5.1 Translation During Recursive-Descent Parsing~5.5.2 On-The-Fly Code Generation~5.5.3 L-Attributed SDD's and LL Parsing~5.5.4 Bottom-Up Parsing of L-Attributed SDD's~5.5.5 Exercises for Section 5.5~ 5.6 Summary of Chapter 5~5.7 References for Chapter 5~ 6 Intermediate-Code Generation~ 6.1 Variants of Syntax Trees~6.1.1 Directed Acyclic Graphs for Expressions6.1.2 The Value-Number Method for Constructing DAG's~6.1.3 Exercises for Section 6.1~ 6.2 Three-Address Code~6.2.1 Addresses and Instructions~6.2.2 Quadruples~6.2.3 Triples~6.2.4 Static Single-A ssignment Form~6.2.5 Exercises for Section 6.2~ 6.3 Types and Declarations~6.3.1 Type Expressions~6.3.2 Type Equivalence~6.3.3 Declarations~6.3.4 Storage Layout for Local Names~6.3.5 Sequences of Declarations~6.3.6 Fields in Records and Classes~6.3.7 Exercises for Section 6.3~ 6.4 Translation of Expressions~6.4.1 Operations Within Expressions~6.4.2 Incremental Translation~6.4.3 Addressing Array Elements~6.4.4 Translation of Array References6.4.5 Exercises for Section 6.4~ 6.5 Type Checking~6.5.1 Rules for Type Checking6.5.2 Type Conversions6.5.3 Overloading of Functions and Operators6.5.4 Type Inference and Polymorphic Functions~6.5.5 An Algorithm for Unification~6.5.6 Exercises for Section 6.5~ 6.6 Control Flow~6.6.1 Boolean Expressions6.6.2 Short-circuit Code~6.6.3 Flow-of- Control Statements6.6.4 Control-Flow Translation of Boolean Expressions6.6.5 Avoiding Redundant Gotos~6.6.6 Boolean Values and Jumping Code~6.6.7 Exercises for Section 6.6~ 6.7 Backpatching~6.7.1 One-Pass Code Generation Using Backpatching6.7.2 Backpatching for Boolean Expressions~6.7.3 Flow-of-Control Statements~6.7.4 Break-, Continue-, and Goto-Statements~6.7.5 Exercises for Section 6.7~ 6.8 Switch-Statements~6.8.1 Translationof Switch-Statements~6.8.2 Syntax-Directed Translation of Switch-Statements6.8.3 Exercises for Section 6.8~ 6.9 Intermediate Code for Procedures6.10 Summary of Chapter 6~6.11 References for Chapter 6~ 7 Run-Time Environments~7.1 Storage Organization~7.1.1 Static Versus Dynamic Storage Allocation~ 7.2 Stack Allocation of Space~7.2.1 Activation Trees~7.2.2 Activation Records~7.2.3 Calling Sequences~7.2.4 Variable-Length Data on the Stack~7.2.5 Exercises for Section 7.2 7.3 Access to Nonlocal Data on the Stack~7.3.1 Data Access Without Nested Procedures~7.3.2 Issues With Nested Procedures~7.3.3 A Language With Nested Procedure Declarations~7.3.4 Nesting Depth7.3.5 Access Links7.3.6 Manipulating Access Links7.3.7 Access Links for Procedure Parameters~7.3.8 Displays7.3.9 Exercises for Section 7.3 7.4 Heap Management~7.4.1 The Memory Manager~7.4.2 The Memory Hierarchy of a Computer7.4.3 Locality in Programs7.4.4 Reducing Fragmentation~7.4.5 Manual Deallocation Requests7.4.6 Exercises for Section 7.4~ 7.5 Introduction to Garbage Collection7.5.1 Design Goals for Garbage Collectors7.5.2 Reachability~7.5.3 Reference Counting Garbage Collectors~7.5.4 Exercises for Section 7.5 7.6 Introduction to Trace-Based Collection7.6.1 A Basic Mark-and-Sweep Collector~7.6.2 Basic Abstraction7.6.3 Optimizing Mark-and-Sweep7.6.4 Mark-and-Compact Garbage Collectors~7.6.5 Copying collectors~7.6.6 Comparing Costs7.6.7 Exercises for Section 7.6~ 7.7 Short-Pause Garbage Collection~7.7.1 Incremental Garbage Collection7.7.2 Incremental Reachability Analysis~7.7.3 Partial-Collection Basics~7.7.4 Generational Garbage Collection7.7.5 The Train Algorithm~7.7.6 Exercises for Section 7.7~ 7.8 Advanced Topics in Garbage Collection~7.8.1 Parallel and Concurrent Garbage Collection~7.8.2 Partial Object Relocation~7.8.3 Conservative Collection for Unsafe Languages~7.8.4 Weak References7.8.5 Exercises for Section 7.8~ 7.9 Summary of Chapter 7 7.10 References for Chapter 7 5 02 8 Code Generation 8.1 Issues in the Design of a Code Generator8.1.1 Input to the Code Generator~8.1.2 The Target Program~8.1.3 Instruction Selection~8.1.4 Register Allocation~8.1.5 Evaluation Order 8.2 The Target Language8.2.1 A Simple Target Machine Model~8.2.2 Program and Instruction Costs~8.2.3 Exercises for Section 8.2~ 8.3 Addresses in the Target Code~8.3.1 Static Allocation~8.3.2 Stack Allocation8.3.3 Run-Time Addresses for Names8.3.4 Exercises for Section 8.3~ 8.4 Basic Blocks and Flow Graphs~8.4.1 Basic Blocks~8.4.2 Next-Use Information8.4.3 Flow Graphs~8.4.4 Representation of Flow Graphs8.4.5 Loops~8.4.6 Exercises for Section 8.4~ 8.5 Optimization of Basic Blocks~8.5.1 The DAG Representation of Basic Blocks8.5.2 Finding Local Common Subexpressions8.5.3 Dead Code Elimination~8.5.4 The Use of Algebraic Identities~8.5.5 Representation of Array References8.5.6 Pointer Assignments and Procedure Calls~8.5.7 Reassembling Basic Blocks From DAG's8.5.8 Exercises for Section 8.5~ 8.6 A Simple Code Generator8.6.1 Register and Address Descriptors8.6.2 The Code-Generation Algorithm~8.6.3 Design of the Function getReg8.6.4 Exercises for Section 8.6 8.7 Peephole Optimization8.7.1 Eliminating Redundant Loads and Stores~8.7.2 Eliminating Unreachable Code~8.7.3 Flow-of-Control Optimizations8.7.4 Algebraic Simplification and Reduction in Strength~8.7.5 Use of Machine Idioms~8.7.6 Exercises for Section 8.7 8.8 Register Allocation and Assignment~8.8.1 Global Register Allocation8.8.2 Usage Counts8.8.3 Register Assignment for Outer Loops8.8.4 Register Allocation by Graph Coloring8.8.5 Exercises for Section 8.8~ 8.9 Instruction Selection by Tree Rewriting8.9.1 Tree-Translation Schemes~8.9.2 Code Generation by Tiling an Input Tree8.9.3 Pattern Matching by Parsing8.9.4 Routines for Semantic Checking~8.9.5 General Tree Matching~8.9.6 Exercises for Section 8.9~ 8.10 Optimal Code Generation for Expressions~8.10.1 Ershov Numbers~8.10.2 Generating Code From Labeled Expression Trees8.10.3 Evaluating Expressions with an Insufficient Supply of Registers~8.10.4 Exercises for Section 8.10 8.11 Dynamic Programming Code-Generation~8.11.1 Contiguous Evaluation~8.11.2 The Dynamic Programming Algorithm~8.11.3 Exercises for Section 8.11~ 8.12 Summary of Chapter 8~8.13 References for Chapter 8~ 9 Machine-Independent Optimizations~ 9.1 The Principal Sources of Optimization~9.1.1 Causes of Redundancy~9.1.2 A Running Example: Quicksort9.1.3 Semantics-Preserving Transformations9.1.4 Global Common Subexpressions~9.1.5 Copy Propagation~9.1.6 Dead-Code Elimination~9.1.7 Code Motion~9.1.8 Induction Variables and Reduction in Strength~9.1.9 Exercises for Section 9.1 5 9.2 Introduction to Data-Flow Analysis9.2.1 The Data-Flow Abstraction~9.2.2 The Data-Flow Analysis Schema~9.2.3 Data-Flow Schemas on Basic Blocks9.2.4 Reaching Definitions9.2.5 Live-Variable Arlalysis~9.2.6 Available Expressions~9.2.7 Summary~9.2.8 Exercises for Section 9.2~ 9.3 Foundations of Data-Flow Analysis~9.3.1 Semilattices9.3.2 Transfer Functions9.3.3 The Iterative Algorithm for General Frameworks~9.3.4 Meaning of a Data-Flow Solution~9.3.5 Exercises for Section 9.3~ 9.4 Constant Propagation~9.4.1 Data-Flow Values for the Constant-Propagation Framework9.4.2 The Meet for the Constant-Propagation Framework9.4.3 Transfer Functions for the Constant-Propagation Framework9.4.4 Monotonicity of the Constant-Propagation Framework9.4.5 Nondistributivity of the Constant-Propagation Framework~9.4.6 Interpretation of the Results9.4.7 Exercises for Section 9.4 9.5 Partial-Redundancy Elimination9.5.1 The Sources of Redundancy9.5.2 Can All Redundancy Be Eliminated?9.5.3 The Lazy-Code-Motion Problem~9.5.4 Anticipation of Expressions~9.5.5 The Lazy-Code-Motion Algorithm~9.5.6 Exercises for Section 9.5~ 9.6 Loops in Flow Graphs9.6.1 Dominators~9.6.2 Depth-First Ordering~9.6.3 Edges in a Depth-First Spanning Tree~9.6.4 Back Edges and Reducibility~9.6.5 Depth of a Flow Graph~9.6.6 Natural Loops~9.6.7 Speed of Convergence of Iterative Data-Flow Algorithms~9.6.8 Exercises for Section 9.6~ 9.7 Region-Based Analysis~9.7.1 Regions~9.7.2 Region Hierarchies for Reducible Flow Graphs~9.7.3 Overview of a Region-Based Analysis9.7.4 Necessary Assumptions About Transfer Functions~9.7.5 An Algorithm for Region-Based Analysis9.7.6 Handling Nonreducible Flow Graphs9.7.7 Exercises for Section 9.7~ 9.8 Symbolic Analysis9.8.1 Affine Expressions of Reference Variables~9.8.2 Data-Flow Problem Formulation~9.8.3 Region-Based Symbolic Analysis9.8.4 Exercises for Section 9.8~ 9.9 Summary of Chapter 9~ 9.10 References for Chapter 9~ 10 Instruct ion-Level Parallelism~ 10.1 Processor Architectures~10.1.1 Instruction Pipelines and Branch Delays~10.1.2 Pipelined Execution~10.1.3 Multiple Instruction Issue~ 10.2 Code-Scheduling Constraints~10.2.1 Data Dependence10.2.2 Finding Dependences Among Memory Accesses10.2.3 Tradeoff Between Register Usage and Parallelism~10.2.4 Phase Ordering Between Register Allocation and Code Scheduling10.2.5 Control Dependence~10.2.6 Speculative Execution Support~10.2.7 A Basic Machine Model10.2.8 Exercises for Section 10.2~ 10.3 Basic-Block Scheduling~10.3.1 Data-Dependence Graphs~10.3.2 List Scheduling of Basic Blocks~10.3.3 Prioritized Topological Orders10.3.4 Exercises for Section 10.3~ 10.4 Global Code Scheduling~10.4.1 Primitive Code Motion10.4.2 Upward Code Motion10.4.3 Downward Code Motion~10.4.4 Updating Data Dependences~10.4.5 Global Scheduling Algorithms~10.4.6 Advanced Code Motion Techniques~10.4.7 Interaction with Dynamic Schedulers~10.4.8 Exercises for Section 10.4~ 10.5 Software Pipelining~10.5.1 Introduction~10.5.2 Software Pipelining of Loops~10.5.3 Register Allocation and Code Generation~10.5.4 Do-Across Loops~10.5.5 Goals and Constraints of Software Pipelining10.5.6 A Software-Pipelining Algorithm~10.5.7 Scheduling Acyclic Data-Dependence Graphs~10.5.8 Scheduling Cyclic Dependence Graphs~10.5.9 Improvements to the Pipelining Algorithms~10.5.10 Modular Variable Expansion~10.5.11 Conditional Statements10.5.12 Hardware Support for Software Pipelining~10.5.13 Exercises for Section 10.5~ 10.6 Summary of Chapter 10~ 10.7 References for Chapter 10~ 11 Optimizing for Parallelism and Locality~ 11.1 Basic Concepts~11.1.1 Multiprocessors~11.1.2 Parallelism in Applications~11.1.3 Loop-Level Parallelism~11.1.4 Data Locality~11.1.5 Introduction to Affine Transform Theory 11.2 Matrix Multiply: An In-Depth Example11.2.1 The Matrix-Multiplication Algorithm~11.2.2 Optimizations~11.2.3 Cache Interference~11.2.4 Exercises for Section 11.2~ 11.3 Iteration Spaces~11.3.1 Constructing Iteration Spaces from Loop Nests11.3.2 Execution Order for Loop Nests~11.3.3 Matrix Formulation of Inequalities~11.3.4 Incorporating Symbolic Constants~11.3.5 Controlling the Order of Execution~11.3.6 Changing Axes~11.3.7 Exercises for Section 11.3~ 11.4 Affine Array Indexes~11.4.1 Affine Accesses~11.4.2 Affine and Nonaffine Accesses in Practice~11.4.3 Exercises for Section 11.4~ 11.5 Data Reuse~11.5.1 Types of Reuse~11.5.2 Self Reuse~11.5.3 Self-spatial Reuse~11.5.4 Group Reuse~11.5.5 Exercises for Section 11.5~ 11.6 Array Data-Dependence Analysis~11.6.1 Definition of Data Dependence of Array Accesses~11.6.2 Integer Linear Programming~11.6.3 The GCD Test11.6.4 Heuristics for Solving Integer Linear Programs11.6.5 Solving General Integer Linear Programs11.6.6 Summary~11.6.7 Exercises for Section 11.6~ 11.7 Finding Synchronization-Free Parallelism~11.7.1 An Introductory Example~11.7.2 Affine Space Partitions~11.7.3 Space-Partition Constraints~11.7.4 Solving Space-Partition Constraints~11.7.5 A Simple Code-Generation Algorithm~11.7.6 Eliminating Empty Iterations~11.7.7 Eliminating Tests from Innermost Loops11.7.8 Source-Code Transforms11.7.9 Exercises for Section 11.7 11.8 Synchronization Between Parallel Loops~11.8.1 A Constant Number of Synchronizations 311.8.2 Program-Dependence Graphs~11.8.3 Hierarchical Time~11.8.4 The Parallelization Algorithm~11.8.5 Exercises for Section 11.8~ 11.9 Pipelining~11.9.1 What is Pipelining?~11.9.2 Successive Over-Relaxation (SOR): An Example~11.9.3 Fully Permutable Loops11.9.4 Pipelining Fully Permutable Loops11.9.5 General Theory~11.9.6 Time-Partition Constraints~11.9.7 Solving Time-Partition Constraints by Farkas' Lemma~11.9.8 Code Transformations~11.9.9 Parallelism With Minimum Synchronization~11.9.10 Exercises for Section 11.9~ 11.10 Locality Optimizations~11.10.1 Temporal Locality of Computed Data11.10.2A rray Contraction11.10.3 Partition Interleaving~11.10.4P utting it All Together11.10.5 Exercises for Section 11.10 11.11 Other Uses of Affine Transforms11.11.1 Distributed memory machines~11.11.2 Multi-Instruction-Issue Processors~11.11.3 Vector and SIMD Instructions11.11.4 Prefetching 11.12 Summary of Chapter 11~11.13 References for Chapter 11~ 12 Interprocedural Analysis~12.1 Basic Concepts12.1.1 Call Graphs~12.1.2 Context Sensitivity~12.1.3 Call Strings~12.1.4 Cloning-Based Context-Sensitive Analysis~12.1.5 Summary-Based Context-Sensitive Analysis~12.1.6 Exercises for Section 12.1~ 12.2 Why Interprocedural Analysis?~12.2.1 Virtual Method Invocation12.2.2 Pointer Alias Analysis~12.2.3 Parallelization~12.2.4 Detection of Software Errors and Vulnerabilities~12.2.5 SQL Injection~12.2.6 Buffer Overflow~ 12.3 A Logical Representation of Data Flow12.3.1 Introduction to Datalog~12.3.2 Datalog Rules12.3.3 Intensional and Extensional Predicates12.3.4 Execution of Datalog Programs~12.3.5 Incremental Evaluation of Datalog Programs12.3.6 Problematic Datalog Rules12.3.7 Exercises for Section 12.3 12.4 A Simple Pointer-Analysis Algorithm12.4.1 Why is Pointer Analysis Difficult12.4.2 A Model for Pointers and References~12.4.3 Flow Insensitivity~12.4.4 The Formulation in Datalog~12.4.5 Using Type Information12.4.6 Exercises for Section 12.412.5 Context-Insensitive Interprocedural Analysis12.5.1 Effects of a Method Invocation~12.5.2 Call Graph Discovery in Datalog12.5.3 Dynamic Loading and Reflection~12.5.4 Exercises for Section 12.5~ 12.6 Context-Sensitive Pointer Analysis12.6.1 Contexts and Call Strings12.6.2 Adding Context to Datalog Rules12.6.3 Additional Observations About Sensitivity12.6.4 Exercises for Section 12.6 12.7 Datalog Implementation by BDD's12.7.1 Binary Decision Diagrams12.7.2 Transformations on BDD7s~12.7.3 Representing Relations by BDD7s~12.7.4 Relational Operations as BDD Operations12.7.5 Using BDD7sf or Points-to Analysis~12.7.6 Exercises for Section 12.7~ 12.8 Summary of Chapter 12~ 12.9 References for Chapter 12~ A A Complete Front End~A.1 The Source LanguageA.2 Main~A.3 Lexical Analyzer~A.4 Symbol Tables and TypesA.5 Intermediate Code for Expressions~A.6 Jumping Code for Boolean Expressions~A.7 Intermediate Code for Statements~A.8 Parser~A.9 Creating the Front End~ B Finding Linearly Independent Solutions Index},
  langid = {english},
  keywords = {programming},
  file = {C:\Users\nhiot\OneDrive\zotero\2007\Alfred V Aho - 2007 - Compilers Principles, Techniques, And Tools.pdf}
}

@article{aliPredictingAttributesNodes2021,
  title = {Predicting Attributes of Nodes Using Network Structure},
  author = {Ali, Sarwan and Shakeel, Muhammad Haroon and Khan, Imdadullah and Faizullah, Safiullah and Khan, Muhammad Asad},
  year = {2021},
  journal = {ACM Transactions on Intelligent Systems and Technology (TIST)},
  volume = {12},
  number = {2},
  pages = {1--23},
  publisher = {ACM New York, NY, USA},
  doi = {10/gkcc4d},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2021\\Ali et al. - 2021 - Predicting attributes of nodes using network struc.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\3RD5W48W\\3442390.html}
}

@misc{AllenaiScispacy2020,
  title = {Allenai/Scispacy},
  year = {2020},
  month = jan,
  url = {https://github.com/allenai/scispacy},
  urldate = {2020-01-27},
  abstract = {A full spaCy pipeline and models for scientific/biomedical documents.},
  copyright = {Apache-2.0},
  howpublished = {AI2},
  keywords = {bioinformatics,biomedical,custom-pipes,nlp,nosource,scientific-documents,spacy}
}

@inproceedings{allenPlowCollaborativeTask2007,
  title = {Plow: {{A}} Collaborative Task Learning Agent},
  shorttitle = {Plow},
  booktitle = {{{AAAI}}},
  author = {Allen, James and Chambers, Nathanael and Ferguson, George and Galescu, Lucian and Jung, Hyuckchul and Swift, Mary and Taysom, William},
  year = {2007},
  volume = {7},
  pages = {1514--1519},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2007\Allen et al. - 2007 - Plow A collaborative task learning agent.pdf}
}

@inproceedings{alotaibiPropertyGraphSchema2021,
  title = {Property {{Graph Schema Optimization}} for {{Domain-Specific Knowledge Graphs}}},
  booktitle = {2021 {{IEEE}} 37th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {Alotaibi, Rana and Lei, Chuan and Quamar, Abdul and Efthymiou, Vasilis and {\"O}zcan, Fatma},
  year = {2021},
  month = apr,
  pages = {924--935},
  publisher = {IEEE Computer Society},
  address = {Chania, Greece},
  issn = {2375-026X},
  doi = {10.1109/ICDE51399.2021.00085},
  urldate = {2024-01-03},
  abstract = {Enterprises are creating domain-specific knowledge graphs by curating and integrating their business data from multiple sources. Ontologies provide a semantic abstraction for such knowledge graphs to describe their data in terms of the entities involved and their relationships. There has been a lot of effort to build systems that enable efficient querying over knowledge graphs, represented as property graphs. However the problem of schema optimization in the property graph setting has been largely ignored. In this work, we show that graph schema design has significant impact on query performance, and propose two algorithms to generate an optimized property graph schema from the domain ontology. To the best of our knowledge, we are the first to present an ontology-driven approach for property graph schema optimization. The rich semantic relationships in an ontology contain a variety of opportunities to reduce edge traversals and consequently improve the graph query performance. Our experimental study with two real-world knowledge graphs shows that our algorithms produce high-quality schemas, achieving up to 2 orders of magnitude speed-up compared to alternative schema designs.},
  isbn = {978-1-72819-184-3},
  keywords = {nosource},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2021\\Alotaibi et al. - 2021 - Property Graph Schema Optimization for Domain-Spec.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\WDW8PMNM\\9458757.html}
}

@article{alshaikhdeebBiomedicalNamedEntity2016,
  title = {Biomedical {{Named Entity Recognition}}: {{A Review}}},
  shorttitle = {Biomedical {{Named Entity Recognition}}},
  author = {Alshaikhdeeb, Basel and Ahmad, Kamsuriah},
  year = {2016},
  month = dec,
  journal = {International Journal on Advanced Science, Engineering and Information Technology},
  volume = {6},
  number = {6},
  pages = {889--895},
  doi = {10.18517/ijaseit.6.6.1367},
  urldate = {2024-02-13},
  abstract = {Biomedical Named Entity Recognition (BNER) is the task of identifying biomedical instances such as chemical compounds, genes, proteins, viruses, disorders, DNAs and RNAs. The key challenge behind BNER lies on the methods that would be used for extracting such entities. Most of the methods used for BNER were relying on Supervised Machine Learning (SML) techniques. In SML techniques, the features play an essential role in terms of improving the effectiveness of the recognition process. Features can be identified as a set of discriminating and distinguishing characteristics that have the ability to indicate the occurrence of an entity. In this manner, the features should be able to generalize which means to discriminate the entities correctly even on new and unseen samples. Several studies have tackled the role of features in terms of identifying named entities. However, with the surge of biomedical researches, there is a vital demand to explore biomedical features. This paper aims to accommodate a review study on the features that could be used for BNER in which various types of features will be examined including morphological features, dictionary-based features, lexical features and distance-based features.},
  file = {C:\Users\nhiot\OneDrive\zotero\2016\Alshaikhdeeb et Ahmad - 2016 - Biomedical Named Entity Recognition A Review.pdf}
}

@article{alvaroTwiMedTwitterPubMed2017,
  title = {{{TwiMed}}: {{Twitter}} and {{PubMed Comparable Corpus}} of {{Drugs}}, {{Diseases}}, {{Symptoms}}, and {{Their Relations}}},
  shorttitle = {{{TwiMed}}},
  author = {Alvaro, Nestor and Miyao, Yusuke and Collier, Nigel},
  year = {2017},
  month = may,
  journal = {JMIR Public Health and Surveillance},
  volume = {3},
  number = {2},
  pages = {e6396},
  publisher = {JMIR Publications Inc., Toronto, Canada},
  doi = {10/gjv7q6},
  urldate = {2021-05-06},
  abstract = {Background: Work on pharmacovigilance systems using texts from PubMed and Twitter typically target at different elements and use different annotation guidelines resulting in a scenario where there is no comparable set of documents from both Twitter and PubMed annotated in the same manner. Objective: This study aimed to provide a comparable corpus of texts from PubMed and Twitter that can be used to study drug reports from these two sources of information, allowing researchers in the area of pharmacovigilance using natural language processing (NLP) to perform experiments to better understand the similarities and differences between drug reports in Twitter and PubMed. Methods: We produced a corpus comprising 1000 tweets and 1000 PubMed sentences selected using the same strategy and annotated at entity level by the same experts (pharmacists) using the same set of guidelines. Results: The resulting corpus, annotated by two pharmacists, comprises semantically correct annotations for a set of drugs, diseases, and symptoms. This corpus contains the annotations for 3144 entities, 2749 relations, and 5003 attributes. Conclusions: We present a corpus that is unique in its characteristics as this is the first corpus for pharmacovigilance curated from Twitter messages and PubMed sentences using the same data selection and annotation strategies. We believe this corpus will be of particular interest for researchers willing to compare results from pharmacovigilance systems (eg, classifiers and named entity recognition systems) when using data from Twitter and from PubMed. We hope that given the comprehensive set of drug names and the annotated entities and relations, this corpus becomes a standard resource to compare results from different pharmacovigilance studies in the area of NLP.},
  copyright = {Unless stated otherwise, all articles are open-access distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/2.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work ("first published in the Journal of Medical Internet Research...") is properly cited with original URL and bibliographic citation information. The complete bibliographic information, a link to the original publication on http://www.jmir.org/, as well as this copyright and license information must be included.},
  langid = {english},
  annotation = {QID: Q33708887},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2017\\Alvaro et al. - 2017 - TwiMed Twitter and PubMed Comparable Corpus of Dr.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\NG5Q4EGS\\e24.html}
}

@inproceedings{amargerEtatArtExtraction2013,
  title = {Etat de l'art : {{Extraction}} d'information {\`a} Partir de Th{\'e}saurus Pour G{\'e}n{\'e}rer Une Ontologie},
  shorttitle = {Etat de l'art},
  booktitle = {{{INFormatique}} Des {{Organisations}} et {{Systemes}} d'{{Information}} et de {{Decision}} ({{INFORSID}})},
  author = {Amarger, Fabien and Roussey, Catherine and Chanet, Jean-Pierre and Haemmerl{\'e}, Ollivier and Hernandez, Nathalie},
  year = {2013},
  month = may,
  pages = {29--44},
  address = {Paris, France},
  url = {https://hal.archives-ouvertes.fr/hal-01135098},
  urldate = {2020-01-11},
  abstract = {Afin de participer au Web de donn{\'e}es pour l'agriculture, nous voulons r{\'e}utiliser AGRO-VOC qui est un th{\'e}saurus multilingue maintenu par la FAO comportant plus de 40.000 termes. Nous pr{\'e}sentons ici un {\'e}tat de l'art des techniques de transformation de th{\'e}saurus pour obtenir une  ontologie  de  domaine.  Pour  cela,  nous  avons  {\'e}tudi{\'e}  dix  approches  suivant  trois  axes : l'extraction  de  classes,  l'extraction  de  la  hi{\'e}rarchie  et  l'extraction  de  relations.  Ainsi,  nous avons mis en {\'e}vidence certaines difficult{\'e}s li{\'e}es {\`a} la transformation de th{\'e}saurus comme la d{\'e}sambigu{\"i}sation des relations ou la validation des r{\'e}sultats. Nous constatons que les derni{\`e}res approches mises en oeuvre sont fond{\'e}es sur des techniques manuelles pour r{\'e}pondre en partie {\`a} ces difficult{\'e}s.},
  optabstract = {Afin de participer au Web de donn{\'e}es pour l'agriculture, nous voulons r{\'e}utiliser AGRO-VOC qui est un th{\'e}saurus multilingue maintenu par la FAO comportant plus de 40.000 termes. Nous pr{\'e}sentons ici un {\'e}tat de l'art des techniques de transformation de th{\'e}saurus pour obtenir une ontologie de domaine. Pour cela, nous avons {\'e}tudi{\'e} dix approches suivant trois axes : l'extraction de classes, l'extraction de la hi{\'e}rarchie et l'extraction de relations. Ainsi, nous avons mis en {\'e}vidence certaines difficult{\'e}s li{\'e}es {\`a} la transformation de th{\'e}saurus comme la d{\'e}sambigu{\"i}sation des relations ou la validation des r{\'e}sultats. Nous constatons que les derni{\`e}res approches mises en oeuvre sont fond{\'e}es sur des techniques manuelles pour r{\'e}pondre en partie {\`a} ces difficult{\'e}s.},
  optkeywords = {Enrichissement, Extraction d'informations, Ontologie, Thesaurus},
  optmonth = {05},
  optshorttitle = {Etat de l'art},
  opturl = {https://hal.archives-ouvertes.fr/hal-01135098},
  opturldate = {2020-01-11},
  keywords = {⛔ No DOI found,Enrichissement,Extraction d'informations,Ontologie,Thesaurus},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2013\\Amarger et al. - 2013 - Etat de l'art  Extraction d'information à partir .pdf;C\:\\Users\\nhiot\\Zotero\\storage\\AE8R5C2S\\hal-01135098.html}
}

@misc{amatriainNLPHealthcareUnderstanding2020,
  title = {{{NLP}} \& {{Healthcare}}: {{Understanding}} the {{Language}} of {{Medicine}}},
  shorttitle = {{{NLP}} \& {{Healthcare}}},
  author = {Amatriain, Xavier},
  year = {2020},
  month = feb,
  journal = {Medium},
  url = {https://medium.com/curai-tech/nlp-healthcare-understanding-the-language-of-medicine-e9917bbf49e7},
  urldate = {2021-03-16},
  abstract = {At Curai we have a mission to scale the world's best healthcare for every human being. We are building an Augmented Intelligence{\dots}},
  langid = {english},
  keywords = {nosource}
}

@phdthesis{amaviComparaisonEvolutionSchemas2014,
  type = {These de Doctorat},
  ids = {amaviComparaisonEvolutionSchemas2014a,amaviComparaisonEvolutionSchemas2014c},
  title = {Comparaison et {\'E}volution de Sch{\'e}mas {{XML}}},
  author = {Amavi, Joshua},
  year = {2014},
  month = nov,
  number = {2014ORLE2053},
  url = {https://www.theses.fr/2014ORLE2053},
  urldate = {2021-09-10},
  abstract = {XML est devenu le format standard d'{\'e}change de donn{\'e}es. Nous souhaitons construire un environnement multi-syst{\`e}me o{\`u} des syst{\`e}mes locaux travaillent en harmonie avec un syst{\`e}me global, qui est une {\'e}volution conservatrice des syst{\`e}mes locaux. Dans cet environnement, l'{\'e}change de donn{\'e}es se fait dans les deux sens. Pour y parvenir nous avons besoin d'un mapping entre les sch{\'e}mas des syst{\`e}mes. Le but du mapping est d'assurer l'{\'e}volution des sch{\'e}mas et de guider l'adaptation des documents entre les sch{\'e}mas concern{\'e}s. Nous proposons des outils pour faciliter l'{\'e}volution de base de donn{\'e}es XML. Ces outils permettent de : (i) calculer un mapping entre le sch{\'e}ma global et les sch{\'e}mas locaux, et d'adapter les documents ; (ii) calculer les contraintes d'int{\'e}grit{\'e} du syst{\`e}me global {\`a} partir de celles des syst{\`e}mes locaux ; (iii) comparer les sch{\'e}mas de deux syst{\`e}mes pour pouvoir remplacer un syst{\`e}me par celui qui le contient ; (iv) corriger un nouveau document qui est invalide par rapport au sch{\'e}ma d'un syst{\`e}me, afin de l'ajouter au syst{\`e}me. Des exp{\'e}riences ont {\'e}t{\'e} men{\'e}es sur des donn{\'e}es synth{\'e}tiques et r{\'e}elles pour montrer l'efficacit{\'e} de nos m{\'e}thodes.},
  collaborator = {{Halfeld-Ferrari}, Mirian and R{\'e}ty, Pierre},
  copyright = {Licence Etalab},
  school = {Universit{\'e} d'Orl{\'e}ans},
  keywords = {005.71,Approximative inclusion,Correction de documents en fonction d'un sch{\'e}ma,D{\'e}pendances fonctionnelles,Data exchange,Distance d'{\'e}dition d'arbres,Document-to-schema correction,Donn{\'e}es -- Transmission,Echanges de donn{\'e}es,Functional dependency,Grammaire de haies,Inclusion rel{\^a}ch{\'e}e,Informatique,Interop{\'e}rabilit{\'e},Interoperability,Mapping de sch{\'e}mas,Regular unranked-tree grammar,Schema mapping,Structures de donn{\'e}es (informatique),Tree edit distance,XML,XML (langage de balisage)},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2014\\Amavi - 2014 - Comparaison et évolution de schémas XML.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\KJXJNZ7N\\2014ORLE2053.html}
}

@inproceedings{amaviNaturalLanguageQuerying2020,
  title = {Natural {{Language Querying System Through Entity Enrichment}}},
  booktitle = {{{ADBIS}}, {{TPDL}} and {{EDA}} 2020 {{Common Workshops}} and {{Doctoral Consortium}}: {{International Workshops}}: {{DOING}}, {{MADEISD}}, {{SKG}}, {{BBIGAP}}, {{SIMPDA}}, {{AIMinScience}} 2020 and {{Doctoral Consortium}}, {{Lyon}}, {{France}}, {{August}} 25--27, 2020, {{Proceedings}} 24},
  author = {Amavi, Joshua and {Halfeld-Ferrari}, Mirian and Hiot, Nicolas},
  editor = {Bellatreche, Ladjel and Bielikov{\'a}, M{\'a}ria and Boussa{\"i}d, Omar and Catania, Barbara and Darmont, J{\'e}r{\^o}me and Demidova, Elena and Duchateau, Fabien and Hall, Mark and Mer{\v c}un, Tanja and Novikov, Boris and Papatheodorou, Christos and Risse, Thomas and Romero, Oscar and Sautot, Lucile and Talens, Guilaine and Wrembel, Robert and {\v Z}umer, Maja},
  year = {2020},
  month = aug,
  series = {Communications in {{Computer}} and {{Information Science}}},
  volume = {1260},
  pages = {36--48},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-55814-7_3},
  abstract = {This paper focuses on a domain expert querying system over databases. It presents a solution designed for a French enterprise interested in offering a natural language interface for its clients. The approach, based on entity enrichment, aims at translating natural language queries into database queries. In this paper, the database is treated through a logical paradigm, suggesting the adaptability of our approach to different database models. The good precision of our method is shown through some preliminary experiments.},
  copyright = {All rights reserved},
  hal_id = {hal-02959502},
  hal_version = {v1},
  isbn = {978-3-030-55814-7},
  langid = {english},
  keywords = {Database query,me,NLI,NLP,nosource,Question answering},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2020\\Amavi et al. - 2020 - Natural Language Querying System Through Entity En.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\XGTK6R7N\\DOING13.mp4;C\:\\Users\\nhiot\\Zotero\\storage\\MT5B2A5X\\978-3-030-55814-7_3.html}
}

@inproceedings{amaviToolBoxConservativeXML2014,
  title = {A {{ToolBox}} for {{Conservative XML Schema Evolution}} and {{Document Adaptation}}},
  booktitle = {Database and {{Expert Systems Applications}} - 25th {{International Conference}}. {{Proceedings}}, {{Part I}}},
  author = {Amavi, Joshua and Chabin, Jacques and {Halfeld-Ferrari}, Mirian and R{\'e}ty, Pierre},
  editor = {Decker, Hendrik and Lhotsk{\'a}, Lenka and Link, Sebastian and Spies, Marcus and Wagner, Roland R.},
  year = {2014},
  series = {{{DEXA}} 2014},
  volume = {8644},
  pages = {299--307},
  publisher = {Springer International Publishing},
  address = {Munich, Germany},
  doi = {10/gf4bb6},
  abstract = {We propose an algorithm that computes a mapping to obtain a conservative extension of original local schemas. This mapping ensures schema evolution and guides the construction of a document translator.},
  isbn = {978-3-319-10072-2 978-3-319-10073-9},
  langid = {english},
  keywords = {Edit Distance,Edit Operation,Production Rule,Regular Expression,Schema Mapping},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2014\\Amavi et al. - 2014 - A ToolBox for Conservative XML Schema Evolution an.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\WI2SRYJ5\\978-3-319-10073-9_24.html}
}

@misc{AmazingUserExperiences,
  title = {Amazing {{User Experiences}} with {{Redis}} and {{RediSearch}}},
  url = {https://www.youtube.com/watch?v=B\_BVmJ90X8Q},
  urldate = {2019-12-11},
  abstract = {Stefano Fratini and Leo Wei from Siteminder talk about user experience with Redis and RediSearch and dealing with structured data. https://redislabs.com},
  keywords = {nosource}
}

@inproceedings{ammarWGBUniversalGraph2014,
  title = {{{WGB}}: {{Towards}} a {{Universal Graph Benchmark}}},
  shorttitle = {{{WGB}}},
  booktitle = {Advancing {{Big Data Benchmarks}}},
  author = {Ammar, Khaled and {\"O}zsu, M. Tamer},
  editor = {Rabl, Tilmann and Raghunath, Nambiar and Poess, Meikel and Bhandarkar, Milind and Jacobsen, Hans-Arno and Baru, Chaitanya},
  year = {2014},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {58--72},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10/gjhfr6},
  abstract = {Graph data are of growing importance in many recent applications. There are many systems proposed in the last decade for graph processing and analysis. Unfortunately, with the exception of RDF stores, every system uses different datasets and queries to assess its scalability and efficiency. This makes it challenging (and sometimes impossible) to conduct a meaningful comparison. Our aim is to close this gap by introducing Waterloo Graph Benchmark (WGB), a benchmark for graph processing systems that offers an efficient generator that creates dynamic graphs with properties similar to real-life ones. WGB includes the basic graph queries which are used for building graph applications.},
  isbn = {978-3-319-10596-3},
  langid = {english},
  keywords = {Graph Query,Graph System,Online Query,Reachability Query,Separator Character},
  file = {C:\Users\nhiot\OneDrive\zotero\2014\Ammar et Özsu - 2014 - WGB Towards a Universal Graph Benchmark.pdf}
}

@book{AMRRS11,
  title = {Web Data Management},
  author = {Abiteboul, Serge and Manolescu, Ioana and Rigaux, Philippe and Rousset, Marie-Christine and Senellart, Pierre},
  year = {2011},
  publisher = {Cambridge University Press},
  keywords = {nosource}
}

@incollection{amsiliActesTALN19991999,
  title = {Actes de {{TALN}} 1999 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 1999 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Amsili, Pascal},
  year = {1999},
  month = jul,
  publisher = {ATALA},
  address = {Carg{\`e}se},
  keywords = {nosource}
}

@article{amsterdamerNaturalLanguageInterface2015,
  title = {A Natural Language Interface for Querying General and Individual Knowledge},
  author = {Amsterdamer, Yael and Kukliansky, Anna and Milo, Tova},
  year = {2015},
  month = aug,
  journal = {Proceedings of the VLDB Endowment},
  volume = {8},
  number = {12},
  pages = {1430--1441},
  issn = {2150-8097},
  doi = {10.14778/2824032.2824042},
  urldate = {2024-03-29},
  abstract = {Many real-life scenarios require the joint analysis of               general knowledge               , which includes facts about the world, with               individual knowledge               , which relates to the opinions or habits of individuals. Recently developed crowd mining platforms, which were designed for such tasks, are a major step towards the solution. However, these platforms require users to specify their information needs in a formal, declarative language, which may be too complicated for na{\"i}ve users. To make the joint analysis of general and individual knowledge accessible to the public, it is desirable to provide an interface that translates the user questions, posed in natural language (NL), into the formal query languages that crowd mining platforms support.                          While the translation of NL questions to queries over conventional databases has been studied in previous work, a setting with mixed individual and general knowledge raises unique challenges. In particular, to support the distinct query constructs associated with these two types of knowledge, the NL question must be partitioned and translated using different means; yet eventually all the translated parts should be seamlessly combined to a well-formed query. To account for these challenges, we design and implement a modular translation framework that employs new solutions along with state-of-the art NL parsing tools. The results of our experimental study, involving real user questions on various topics, demonstrate that our framework provides a high-quality translation for many questions that are not handled by previous translation tools.},
  langid = {english},
  optbiburl = {https://dblp.org/rec/journals/pvldb/AmsterdamerKM15.bib},
  optdoi = {10.14778/2824032.2824042},
  opttimestamp = {Sat, 25 Apr 2020 13:59:36 +0200},
  opturl = {http://www.vldb.org/pvldb/vol8/p1430-amsterdamer.pdf},
  keywords = {nosource}
}

@misc{Analytics2021,
  title = {Analytics},
  year = {2021},
  month = feb,
  url = {https://www.ibm.com/analytics},
  urldate = {2021-03-01},
  abstract = {Learn how IBM Data and AI can collect, organize and analyze your data and accelerate your journey to AI.},
  langid = {american},
  keywords = {nosource}
}

@article{androutsopoulosNaturalLanguageInterfaces1995,
  title = {Natural Language Interfaces to Databases -- an Introduction},
  author = {Androutsopoulos, I. and Ritchie, G. D. and Thanisch, P.},
  year = {1995},
  month = mar,
  journal = {Natural Language Engineering},
  volume = {1},
  number = {1},
  pages = {29--81},
  publisher = {Cambridge University Press},
  issn = {1469-8110, 1351-3249},
  doi = {10/fjkxr6},
  urldate = {2020-04-04},
  abstract = {This paper is an introduction to natural language interfaces to databases (NLIDBS). A brief overview of the history of NLIDBS is first given. Some advantages and disadvantages of NLIDBS are then discussed, comparing NLIDBS to formal query languages, form-based interfaces, and graphical interfaces. An introduction to some of the linguistic problems NLIDBS have to confront follows, for the benefit of readers less familiar with computational linguistics. The discussion then moves on to NLIDB architectures, portability issues, restricted natural language input systems (including menu-based NLIDBS), and NLIDBS with reasoning capabilities. Some less explored areas of NLIDB research are then presented, namely database updates, meta-knowledge questions, temporal questions, and multi-modal NLIDBS. The paper ends with reflections on the current state of the art.},
  langid = {english},
  optabstract = {This paper is an introduction to natural language interfaces to databases (NLIDBS). A brief overview of the history of NLIDBS is first given. Some advantages and disadvantages of NLIDBS are then discussed, comparing NLIDBS to formal query languages, form-based interfaces, and graphical interfaces. An introduction to some of the linguistic problems NLIDBS have to confront follows, for the benefit of readers less familiar with computational linguistics. The discussion then moves on to NLIDB architectures, portability issues, restricted natural language input systems (including menu-based NLIDBS), and NLIDBS with reasoning capabilities. Some less explored areas of NLIDB research are then presented, namely database updates, meta-knowledge questions, temporal questions, and multi-modal NLIDBS. The paper ends with reflections on the current state of the art.},
  optanguage = {en},
  optdoi = {10/fjkxr6},
  optissn = {1469-8110, 1351-3249},
  optmonth = {03},
  opturl = {https://www.cambridge.org/core/journals/natural-language-engineering/article/natural-language-interfaces-to-databases-an-introduction/21C30448C70DD4988E6DA0D54205FB56},
  opturldate = {2020-04-04},
  annotation = {cites: androutsopoulosNaturalLanguageInterfaces1995},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1995\\Androutsopoulos et al. - 1995 - Natural language interfaces to databases – an intr.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\ZL9R2XAX\\21C30448C70DD4988E6DA0D54205FB56.html}
}

@inproceedings{anglesPGKeysKeysProperty2021,
  ids = {BDFHHLLLM21},
  title = {{{PG-Keys}}: {{Keys}} for Property Graphs},
  shorttitle = {Pg-Keys},
  booktitle = {Proceedings of the 2021 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Angles, Renzo and Bonifati, Angela and Dumbrava, Stefania and Fletcher, George and Hare, Keith W. and Hidders, Jan and Lee, Victor E. and Li, Bei and Libkin, Leonid and Martens, Wim and Murlak, Filip and Perryman, Josh and Savkovic, Ognjen and Schmidt, Michael and Sequeda, Juan F. and Staworko, Slawek and Tomaszuk, Dominik},
  year = {2021},
  pages = {2423--2436},
  publisher = {ACM},
  doi = {10.1145/3448016.3457561},
  keywords = {nosource},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2021\\Angles et al. - 2021 - PG-Keys Keys for property graphs.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\7YULBJER\\3448016.html}
}

@inproceedings{anglesPropertyGraphDatabase2018,
  title = {The {{Property Graph Database Model}}.},
  booktitle = {{{AMW}}},
  author = {Angles, Renzo},
  year = {2018},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2018\Angles - 2018 - The Property Graph Database Model..pdf}
}

@inproceedings{anzumR2GSyncEdgeViews2021,
  title = {{{R2GSync}} and Edge Views: Practical {{RDBMS}} to {{GDBMS}} Synchronization},
  shorttitle = {{{R2GSync}} and Edge Views},
  booktitle = {Proceedings of the 4th {{ACM SIGMOD Joint International Workshop}} on {{Graph Data Management Experiences}} \& {{Systems}} ({{GRADES}}) and {{Network Data Analytics}} ({{NDA}})},
  author = {Anzum, Nafisa and Salihoglu, Semih},
  year = {2021},
  month = jun,
  series = {{{GRADES-NDA}} '21},
  pages = {1--9},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10/gk4b4r},
  urldate = {2021-07-05},
  abstract = {Graph databases that are used in enterprises are primarily extracted from a main transactional store that is often an RDBMS. This data infrastructure set up raises the challenge of keeping the extracted graph in a graph database management system (GDBMS) in sync with the source RDBMS. When the extracted graphs contain edge types that are results of join queries, this synchronization requires incrementally maintaining these join queries. In this paper, we investigate an alternative design where we can map the individual relations in these joins to virtual nodes and edges to keep the synchronization very efficient and instead support view-based querying in the GDBMS. We present a system called R2GSync, that synchronizes an RDBMS with a GDBMS and our accompanying edge view design for a GDBMS. We describe our implementation of edge views in GraphflowDB and query optimization techniques for improving the performance of queries that involve edge views.},
  isbn = {978-1-4503-8477-3},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2021\\Anzum et Salihoglu - 2021 - R2GSync and edge views practical RDBMS to GDBMS s.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\YSKWWZ9Q\\3461837.html}
}

@misc{ApacheSolr,
  ids = {ApacheSolra},
  title = {Apache {{SolR}} {\copyright}},
  author = {{Apache Software Foundation}},
  year = {2006},
  url = {https://lucene.apache.org/solr/},
  urldate = {2020-04-08},
  file = {C:\Users\nhiot\Zotero\storage\XB9W3H55\solr.html}
}

@article{appeltIntroductionInformationExtraction1999,
  title = {Introduction to Information Extraction},
  author = {Appelt, Douglas E.},
  year = {1999},
  month = jan,
  journal = {AI Communications},
  volume = {12},
  number = {3},
  pages = {161--172},
  issn = {0921-7126},
  url = {https://content.iospress.com/articles/ai-communications/aic184},
  urldate = {2019-05-24},
  abstract = {In recent years, analysts have been confronted with the increasing availability of on-line sources of information in the form of natural-language texts. This increased accessibility of textual information has led to a corresponding interest in techno},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\Zotero\storage\YLMCAVKT\aic184.html}
}

@inproceedings{araniScoredSemanticCache2020,
  ids = {araniScoredSemanticCache2020a},
  title = {A {{Scored Semantic Cache Replacement Strategy}} for {{Mobile Cloud Database Systems}}},
  booktitle = {{{ADBIS}}, {{TPDL}} and {{EDA}} 2020 {{Common Workshops}} and {{Doctoral Consortium}}},
  author = {Arani, Zachary and Chapman, Drake and Wang, Chenxiao and Gruenwald, Le and {d'Orazio}, Laurent and Basiuk, Taras},
  editor = {Bellatreche, Ladjel and Bielikov{\'a}, M{\'a}ria and Boussa{\"i}d, Omar and Catania, Barbara and Darmont, J{\'e}r{\^o}me and Demidova, Elena and Duchateau, Fabien and Hall, Mark and Mer{\v c}un, Tanja and Novikov, Boris and Papatheodorou, Christos and Risse, Thomas and Romero, Oscar and Sautot, Lucile and Talens, Guilaine and Wrembel, Robert and {\v Z}umer, Maja},
  year = {2020},
  series = {Communications in {{Computer}} and {{Information Science}}},
  pages = {237--248},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10/gg868g},
  abstract = {Current mobile cloud database systems are widespread and require special considerations for mobile devices. Although many systems rely on numerous metrics for use and optimization, few systems leverage metrics for decisional cache replacement on the mobile device. In this paper we introduce the Lowest Scored Replacement (LSR) policy---a novel cache replacement policy based on a predefined score which leverages contextual mobile data and user preferences for decisional replacement. We show an implementation of the policy using our previously proposed MOCCAD-Cache as our decisional semantic cache and our Normalized Weighted Sum Algorithm (NWSA) as a score basis. Our score normalization is based on the factors of query response time, energy spent on mobile device, and monetary cost to be paid to a cloud provider. We then demonstrate a relevant scenario for LSR, where it excels in comparison to the Least Recently Used (LRU) and Least Frequently Used (LFU) cache replacement policies.},
  isbn = {978-3-030-55814-7},
  langid = {english},
  keywords = {Big data,Caching,Cloud computing},
  file = {C:\Users\nhiot\OneDrive\zotero\2020\Arani et al. - 2020 - A Scored Semantic Cache Replacement Strategy for M.pdf}
}

@article{arenasDiscoveringXSDKeys2015,
  title = {Discovering {{XSD Keys}} from {{XML Data}}},
  author = {Arenas, Marcelo and Daenen, Jonny and Neven, Frank and Ugarte, Martin and Bussche, Jan Van Den and Vansummeren, Stijn},
  year = {2015},
  month = dec,
  journal = {ACM Transactions on Database Systems},
  volume = {39},
  number = {4},
  pages = {28:1--28:49},
  issn = {0362-5915},
  doi = {10/f6v648},
  urldate = {2021-11-30},
  abstract = {A great deal of research into the learning of schemas from XML data has been conducted in recent years to enable the automatic discovery of XML schemas from XML documents when no schema or only a low-quality one is available. Unfortunately, and in strong contrast to, for instance, the relational model, the automatic discovery of even the simplest of XML constraints, namely XML keys, has been left largely unexplored in this context. A major obstacle here is the unavailability of a theory on reasoning about XML keys in the presence of XML schemas, which is needed to validate the quality of candidate keys. The present article embarks on a fundamental study of such a theory and classifies the complexity of several crucial properties concerning XML keys in the presence of an XSD, like, for instance, testing for consistency, boundedness, satisfiability, universality, and equivalence. Of independent interest, novel results are obtained related to cardinality estimation of XPath result sets. A mining algorithm is then developed within the framework of levelwise search. The algorithm leverages known discovery algorithms for functional dependencies in the relational model, but incorporates the properties mentioned before to assess and refine the quality of derived keys. An experimental study on an extensive body of real-world XML data evaluating the effectiveness of the proposed algorithm is provided.},
  keywords = {XML key},
  file = {C:\Users\nhiot\OneDrive\zotero\2015\Arenas et al. - 2015 - Discovering XSD Keys from XML Data.pdf}
}

@inproceedings{armstrongLinkBenchDatabaseBenchmark2013,
  title = {{{LinkBench}}: A Database Benchmark Based on the {{Facebook}} Social Graph},
  shorttitle = {{{LinkBench}}},
  booktitle = {Proceedings of the 2013 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Armstrong, Timothy G. and Ponnekanti, Vamsi and Borthakur, Dhruba and Callaghan, Mark},
  year = {2013},
  month = jun,
  series = {{{SIGMOD}} '13},
  pages = {1185--1196},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10/gjhfrq},
  urldate = {2021-03-19},
  abstract = {Database benchmarks are an important tool for database researchers and practitioners that ease the process of making informed comparisons between different database hardware, software and configurations. Large scale web services such as social networks are a major and growing database application area, but currently there are few benchmarks that accurately model web service workloads. In this paper we present a new synthetic benchmark called LinkBench. LinkBench is based on traces from production databases that store "social graph" data at Facebook, a major social network. We characterize the data and query workload in many dimensions, and use the insights gained to construct a realistic synthetic benchmark. LinkBench provides a realistic and challenging test for persistent storage of social and web service data, filling a gap in the available tools for researchers, developers and administrators.},
  isbn = {978-1-4503-2037-5},
  keywords = {database benchmarks,database workload analysis,hbase,mysql,social networks},
  file = {C:\Users\nhiot\OneDrive\zotero\2013\Armstrong et al. - 2013 - LinkBench a database benchmark based on the Faceb.pdf}
}

@misc{AskDataSimplifying,
  title = {Ask {{Data}}: {{Simplifying}} Analytics with Natural Language},
  shorttitle = {Ask {{Data}}},
  journal = {Tableau Software},
  url = {https://www.tableau.com/about/blog/2018/11/ask-data-simplifying-analytics-natural-language-98655},
  langid = {english},
  keywords = {nosource}
}

@inproceedings{aurisanoShowMeData2015,
  ids = {aurisanoShowMeData,aurisanoShowMeDataa},
  title = {``{{Show Me Data}}.'' {{Observational Study}} of a {{Conversational Interface}} in {{Visual Data Exploration}}},
  shorttitle = {Show Me Data''},
  booktitle = {{{IEEE VIS}}},
  author = {Aurisano, Jillian and Kumar, Abhinav and Gonzales, Alberto and Reda, Khairi and Leigh, Jason and Di Eugenio, Barbara and Johnson, Andrew},
  year = {2015},
  volume = {15},
  pages = {1},
  abstract = {Visual data exploration poses challenges for 'InfoVis Novices'. A 'conversational interface' which would enable users to generate and interact with visualizations through natural language and gestures, while maintaining a history of the data exploration, has the potential to ameliorate many of these challenges. We performed an exploratory, observational study designed to examine the role of such a conversational interface in visual data exploration. We simulated a conversational interface, using a remote human mediator, with multiple cycles of visualization construction. We believe analysis of this data will yield concrete design goals for conversational interfaces in information visualization.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2015\Aurisano et al. - 2015 - “Show Me Data.” Observational Study of a Conversat.pdf}
}

@misc{AvaDataInsights,
  title = {Ava: {{From Data}} to {{Insights Through Conversations}} - {{Google Scholar}}},
  url = {https://scholar.google.fr/scholar?hl=fr\&as\_sdt=0\%2C5\&q=Ava\%3A+From+Data+to+Insights+Through+Conversations\&btnG=},
  urldate = {2021-03-11},
  file = {C:\Users\nhiot\Zotero\storage\VKVLYLAY\scholar.html}
}

@article{bachReviewRelationExtraction2011,
  ids = {bachReviewRelationExtraction,bachReviewRelationExtraction2007},
  title = {A {{Review}} of {{Relation Extraction}}},
  author = {Bach, Nguyen and Badaskar, Sameer},
  year = {2011},
  month = may,
  journal = {Literature review for Language and Statistics II},
  volume = {2},
  pages = {1--15},
  abstract = {Many applications in information extraction, natural language understanding, in-formation retrieval require an understanding of the semantic relations between entities. We present a comprehensive review of various aspects of the entity rela-tion extraction task. Some of the most important supervised and semi-supervised classification approaches to the relation extraction task are covered in sufficient detail along with critical analyses. We also discuss extensions to higher-order re-lations. Evaluation methodologies for both supervised and semi-supervised meth-ods are described along with pointers to the commonly used performance evalua-tion datasets. Finally, we also give short descriptions of two important applications of relation extraction, namely question answering and biotext mining.},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2011\Bach et Badaskar - 2011 - A Review of Relation Extraction.pdf}
}

@book{baderGraphPartitioningGraph2013,
  title = {Graph {{Partitioning}} and {{Graph Clustering}}},
  author = {Bader, David and Meyerhenke, Henning and Sanders, Peter and Wagner, Dorothea},
  year = {2013},
  month = jan,
  series = {Contemporary {{Mathematics}}},
  volume = {588},
  publisher = {American Mathematical Society},
  issn = {0271-4132, 1098-3627},
  doi = {10.1090/conm/588},
  urldate = {2023-10-31},
  abstract = {Advancing research. Creating connections.},
  isbn = {978-0-8218-9038-7 978-0-8218-9868-0 978-0-8218-9869-7},
  langid = {english}
}

@inproceedings{baganGMarkGenerationGraphes2016,
  ids = {baganGMarkGenerationGraphes2016a},
  title = {{gMark : G{\'e}n{\'e}ration de Graphes et de Requ{\^e}tes Dirig{\'e}e par le Sch{\'e}ma}},
  shorttitle = {{gMark}},
  booktitle = {{BDA 2016 Gestion de Donn{\'e}es--Principes, Technologies et Applications 32 e anniversaire 15-18 novembre 2016, Poitiers, Futuroscope}},
  author = {Bagan, Guillaume and Bonifati, Angela and Ciucanu, Radu and Fletcher, George and Lemay, Aur{\'e}lien and Advokaat, Nicky},
  year = {2016},
  month = nov,
  pages = {17},
  url = {https://hal.inria.fr/hal-01402580},
  urldate = {2021-03-11},
  abstract = {Les jeux de donn{\'e}es repr{\'e}sent{\'e}s par des graphes de grande taille sont omnipr{\'e}sents dans les domaines applicatifs actuels. C'est pourquoi les bases de donn{\'e}es orient{\'e}es graphes jouent un r{\^o}le de plus en plus important. Dans l'{\'e}tude de ces syst{\`e}mes, il est vital que la communaut{\'e} scientifique ait {\`a} sa disposition des solutions pour g{\'e}n{\'e}rer des jeux de donn{\'e}es de r{\'e}f{\'e}rence comprenant des instances de base de donn{\'e}es et des requ{\^e}tes ayant des propri{\'e}t{\'e}s pr{\'e}visibles et contr{\^o}lables. Dans cet article, nous pr{\'e}sentons les principes {\`a} la fois th{\'e}oriques et d'ing{\'e}nierie de gMark, un syst{\`e}me g{\'e}n{\'e}rique de g{\'e}n{\'e}ration de graphes et de requ{\^e}tes bas{\'e} sur une gestion flexible des sch{\'e}mas et des requ{\^e}tes. Une contribution centrale de gMark est sa capacit{\'e} {\`a} viser et {\`a} contr{\^o}ler la diversit{\'e} des propri{\'e}t{\'e}s {\`a} la fois des instances de graphes g{\'e}n{\'e}r{\'e}s et des requ{\^e}tes correspondantes {\`a} celles ci. Une autre innovation est la capacit{\'e} {\`a} g{\'e}n{\'e}rer des requ{\^e}tes r{\'e}cursives bas{\'e}es sur des expressions r{\'e}guli{\`e}res de chemin, un paradigme important dans les requ{\^e}tes sur les graphes. Nous illustrons {\`a} la fois la flexibilit{\'e} et l'applicabilit{\'e} de gMark en montrant ses capacit{\'e}s {\`a} g{\'e}n{\'e}rer des graphes et des requ{\^e}tes de haute qualit{\'e}, et sa capacit{\'e} {\`a} exploiter des sch{\'e}mas d{\'e}finis par l'utilisateur dans plusieurs domaines applicatifs diff{\'e}rents.},
  langid = {french},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2016\\Bagan et al. - 2016 - gMark  Génération de Graphes et de Requêtes Dirig.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\6TUYF6Z3\\hal-01402580.html}
}

@article{baganGMarkSchemaDrivenGeneration2017,
  title = {{{gMark}}: {{Schema-Driven Generation}} of {{Graphs}} and {{Queries}}},
  shorttitle = {{{gMark}}},
  author = {Bagan, G. and Bonifati, A. and Ciucanu, R. and Fletcher, G. H. and Lemay, A. and Advokaat, N.},
  year = {2017},
  month = apr,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {29},
  number = {4},
  pages = {856--869},
  issn = {1558-2191},
  doi = {10/f9299m},
  abstract = {Massive graph data sets are pervasive in contemporary application domains. Hence, graph database systems are becoming increasingly important. In the experimental study of these systems, it is vital that the research community has shared solutions for the generation of database instances and query workloads having predictable and controllable properties. In this paper, we present the design and engineering principles of gMark, a domainand query language-independent graph instance and query workload generator. A core contribution of gMark is its ability to target and control the diversity of properties of both the generated instances and the generated workloads coupled to these instances. Further novelties include support for regular path queries, a fundamental graph query paradigm, and schema-driven selectivity estimation of queries, a key feature in controlling workload chokepoints. We illustrate the flexibility and practical usability of gMark by showcasing the framework's capabilities in generating high quality graphs and workloads, and its ability to encode user-defined schemas across a variety of application domains.},
  keywords = {application domains,Benchmark testing,benchmarking,contemporary application domains,data analysis,database instances,Database languages,database management systems,design principles,domain-language-independent graph instance,engineering principles,Estimation,fundamental graph query paradigm,generated workloads,Generators,gMark,graph database systems,Graph databases,graph theory,massive graph data sets,query language-independent graph instance,query languages,query processing,Query processing,query workloads,recursive queries,regular path queries,research community,schema-driven selectivity estimation,selectivity estimation,user-defined schemas,workload chokepoints},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2017\\Bagan et al. - 2017 - gMark Schema-Driven Generation of Graphs and Quer.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\UY2RWNZ5\\7762945.html}
}

@article{balalauGraphIntegrationStructured2020,
  ids = {balalauGraphIntegrationStructured2020a},
  title = {Graph Integration of Structured, Semistructured and Unstructured Data for Data Journalism},
  author = {Balalau, Oana and Concei{\c c}\{{\~a}\}o, Catarina and Galhardas, Helena and Manolescu, Ioana and Merabti, Tayeb and You, Jingmao and Youssef, Youssr},
  year = {2020},
  month = jul,
  journal = {arXiv preprint arXiv:2007.12488},
  eprint = {2007.12488},
  url = {http://arxiv.org/abs/2007.12488},
  urldate = {2020-10-28},
  abstract = {Nowadays, journalism is facilitated by the existence of large amounts of digital data sources, including many Open Data ones. Such data sources are extremely heterogeneous, ranging from highly struc-tured (relational databases), semi-structured (JSON, XML, HTML), graphs (e.g., RDF), and text. Journalists (and other classes of users lacking advanced IT expertise, such as most non-governmental-organizations, or small public administrations) need to be able to make sense of such heterogeneous corpora, even if they lack the ability to de ne and deploy custom extract-transform-load work ows. These are di cult to set up not only for arbitrary heterogeneous inputs , but also given that users may want to add (or remove) datasets to (from) the corpus. We describe a complete approach for integrating dynamic sets of heterogeneous data sources along the lines described above: the challenges we faced to make such graphs useful, allow their integration to scale, and the solutions we proposed for these problems. Our approach is implemented within the ConnectionLens system; we validate it through a set of experiments.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Databases},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2020\\Balalau et al. - 2020 - Graph integration of structured, semistructured an.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\IY8LG3N2\\2007.html}
}

@article{Balke:2012:intro-IE,
  title = {Introduction to Information Extraction: {{Basic}} Notions and Current Trends},
  author = {Balke, Wolf-Tilo},
  year = {2012},
  month = jul,
  journal = {Datenbank-Spektrum},
  volume = {12},
  doi = {10/ggt69n},
  keywords = {nosource}
}

@article{balminIncrementalValidationXML2004,
  ids = {balminIncrementalValidationXML2004a},
  title = {Incremental Validation of {{XML}} Documents},
  author = {Balmin, Andrey and Papakonstantinou, Yannis and Vianu, Victor},
  year = {2004},
  month = dec,
  journal = {ACM Transactions on Database Systems},
  volume = {29},
  number = {4},
  pages = {710--751},
  publisher = {ACM Press},
  issn = {0362-5915, 1557-4644},
  doi = {10.1145/1042046.1042050},
  urldate = {2024-01-04},
  abstract = {We investigate the incremental validation of XML documents with respect to DTDs, specialized DTDs, and XML Schemas, under updates consisting of element tag renamings, insertions, and deletions. DTDs are modeled as extended context-free grammars. "Specialized DTDs" allow the decoupling of element types from element tags. XML Schemas are abstracted as specialized DTDs with limitations on the type assignment. For DTDs and XML Schemas, we exhibit an O(m log n) incremental validation algorithm using an auxiliary structure of size O(n), where n is the size of the document and m the number of updates. The algorithm does not handle the incremental validation of XML Schema wrt renaming of internal nodes, which is handled by the specialized DTDs incremental validation algorithm. For specialized DTDs, we provide an O(m log2 n) incremental algorithm, again using an auxiliary structure of size O(n). This is a significant improvement over brute-force re-validation from scratch.We exhibit a restricted class of DTDs called local that arise commonly in practice and for which incremental validation can be done in practically constant time by maintaining only a list of counters. We present implementations of both general incremental validation and local validation on an XML database built on top of a relational database.Our experimentation includes a study of the applicability of local validation in practice, results on the calibration of parameters of the auxiliary data structure, and results on the performance comparison between the general incremental validation technique, the local validation technique, and brute-force validation from scratch.},
  langid = {english},
  keywords = {nosource,Update,validation,XML},
  file = {C:\Users\nhiot\OneDrive\zotero\2004\Balmin et al. - 2004 - Incremental validation of XML documents2.pdf}
}

@inproceedings{baoContextRepresentationSemantic2010,
  title = {Context {{Representation}} for the {{Semantic Web}}},
  booktitle = {Web {{Science Conference}}},
  author = {Bao, Jie and Tao, Jiao and McGuinness, Deborah L. and Smart, Paul},
  year = {2010},
  month = apr,
  url = {https://eprints.soton.ac.uk/270829/},
  urldate = {2019-02-15},
  abstract = {The unambiguous and effective delivery of data and knowledge on the Web relies heavily on the correct representation and understanding of the associated contexts. However, the current way of encoding data and knowledge on the Web is largely ad hoc. Contexts are often embedded in the application program or are implied by the application- or community-specific agreements. This makes the linking and reusing of data and knowledge, and thus the integration of Web applications, a difficult problem. Therefore, building the architectural support for contexts is one of the major challenges for the Web, and in particular, for the Semantic Web. In this paper, we propose a framework for contexts that provides formal and explicit representations for the usually implicit contextual assumptions of data and knowledge on the Web. This is done by supporting the description of logic institutions, relations of contexts, and provenance. Our framework is able to tackle some critical issues for extending Web as a "Social Machine", such as, permitting different views on the same data, faithful knowledge integration and situation awareness.},
  collaborator = {Bao, Jie and Tao, Jiao and McGuinness, Deborah L. and Smart, Paul},
  langid = {english},
  annotation = {00000},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2010\\Bao et al. - 2010 - Context Representation for the Semantic Web.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\3JZFRLQE\\270829.html}
}

@misc{barqueDeepSequoiaCorpus2020,
  type = {{corpus, text}},
  title = {{Deep Sequoia corpus - PARSEME-FR corpus - FrSemCor}},
  author = {Barque, Lucie and Candito, Marie and Constant, Matthieu and Cordeiro, Silvio Ricardo and Crabb{\'e}, Beno{\^i}t and Fort, Kar{\"e}n and Guillaume, Bruno and Haas, Pauline and Huyghe, Richard and Perrier, Guy and Ramisch, Carlos and Ribeyre, Corentin and Savary, Agata and Seddah, Djam{\'e} and Segonne, Vincent and Tribout, Delphine and {Villemonte de la Clergerie}, Eric and Parmentier, Yannick and Pasquer, Caroline and Antoine, Jean-Yves},
  year = {2020},
  month = mar,
  publisher = {ANR},
  address = {https://github.com/UniversalDependencies/UD\_French-Sequoia},
  url = {https://deep-sequoia.inria.fr/},
  urldate = {2024-03-21},
  abstract = {The Sequoia corpus is a set of 3,099 linguistically-annotated French sentences, originating from four sources (Europarl, European Agency Reports, French regional journal L'Est R{\'e}publicain, and French wikipedia).    Several types of annotations were added over the years.  The current release comprises:    - parts-of-speech (SEQUOIA ANR-08-EMER-013 project)    - syntactic dependency trees    - deep syntactic dependency graphs (Deep sequoia project)    - multi-word expressions and named entities (PARSEME COST project and PARSEME-FR ANR-14-CERA-0001 project)    - coarse semantic tags for nouns (FrSemCor project)    See the deep sequoia page for a detailed description: https://deep-sequoia.inria.fr/},
  copyright = {Deep Sequoia Licence},
  langid = {fra},
  keywords = {⛔ No DOI found},
  annotation = {Accepted: 2021-01-21T10:36:00Z}
}

@book{barrasaBuildingKnowledgeGraphs2023,
  title = {Building {{Knowledge Graphs}}: {{A Practitioner}}'s {{Guide}}},
  author = {Barrasa, Jesus and Webber, Jim},
  year = {2023},
  month = jun,
  publisher = {"O'Reilly Media, Inc."},
  abstract = {Incredibly useful, knowledge graphs help organizations keep track of medical research, cybersecurity threat intelligence, GDPR compliance, web user engagement, and much more. They do so by storing interlinked descriptions of entities---objects, events, situations, or abstract concepts---and encoding the underlying information. How do you create a knowledge graph? And how do you move it from theory into production?Using hands-on examples, this practical book shows data scientists and data engineers how to build their own knowledge graphs. Authors Jes{\'u}s Barrasa and Jim Webber from Neo4j illustrate common patterns for building knowledge graphs that solve many of today's pressing knowledge management problems. You'll quickly discover how these graphs become increasingly useful as you add data and augment them with algorithms and machine learning.Learn the organizing principles necessary to build a knowledge graphExplore how graph databases serve as a foundation for knowledge graphsUnderstand how to import structured and unstructured data into your graphFollow examples to build integration-and-search knowledge graphsLearn what pattern detection knowledge graphs help you accomplishExplore dependency knowledge graphs through examplesUse examples of natural language knowledge graphs and chatbotsUse graph algorithms and ML to gain insight into connected data},
  googlebooks = {6MTGEAAAQBAJ},
  isbn = {978-1-09-812706-0},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / Natural Language Processing,Computers / Data Science / Data Modeling \& Design,Computers / Data Science / Data Visualization,Computers / Data Science / General,Computers / Data Science / Machine Learning},
  file = {C:\Users\nhiot\OneDrive\zotero\2023\Barrasa et Webber - 2023 - Building Knowledge Graphs A Practitioner's Guide.pdf}
}

@inproceedings{barretGenericAbstractionsData2021,
  ids = {barretGenericAbstractionsData2021a},
  title = {Toward {{Generic Abstractions}} for {{Data}} of {{Any Model}}},
  booktitle = {{{BDA}} 2021-{{Informal}} Publication Only},
  author = {Barret, Nelly and Manolescu, Ioana and Upadhyay, Prajna},
  year = {2021},
  month = oct,
  url = {https://hal.inria.fr/hal-03344041},
  urldate = {2021-10-26},
  abstract = {Digital data sharing leads to unprecedented opportunities to develop data-driven systems for supporting economic activities, the social and political life, and science. Many open-access datasets are RDF graphs, but others are CSV files, Neo4J property graphs, JSON or XML documents, etc. Potential users need to understand a dataset in order to decide if it is useful for their goal. While some datasets come with a schema and/or documentation, this is not always the case. Data summarization or schema inference tools have been proposed, specializing in XML, or JSON, or the RDF data models. In this work, we present a dataset abstraction approach, which () applies on relational, CSV, XML, JSON, RDF or Property Graph data; () computes an abstraction meant for humans (as opposed to a schema meant for a parser); () integrates Information Extraction data profiling, to also classify dataset content among a set of categories of interest to the user. Our abstractions are conceptually close to an Entity-Relationship diagram, if one allows nested and possibly heterogeneous structure within entities.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2021\\Barret et al. - 2021 - Toward Generic Abstractions for Data of Any Model.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\NWGV8NRL\\hal-03344041.html}
}

@article{batetOntologybasedMeasureCompute2011,
  title = {An Ontology-Based Measure to Compute Semantic Similarity in Biomedicine},
  author = {Batet, Montserrat and S{\'a}nchez, David and Valls, Aida},
  year = {2011},
  month = feb,
  journal = {Journal of Biomedical Informatics},
  series = {Ontologies for {{Clinical}} and {{Translational Research}}},
  volume = {44},
  number = {1},
  pages = {118--125},
  issn = {1532-0464},
  doi = {10/dfhkjv},
  urldate = {2019-04-05},
  abstract = {Proper understanding of textual data requires the exploitation and integration of unstructured and heterogeneous clinical sources, healthcare records or scientific literature, which are fundamental aspects in clinical and translational research. The determination of semantic similarity between word pairs is an important component of text understanding that enables the processing, classification and structuring of textual resources. In the past, several approaches for assessing word similarity by exploiting different knowledge sources (ontologies, thesauri, domain corpora, etc.) have been proposed. Some of these measures have been adapted to the biomedical field by incorporating domain information extracted from clinical data or from medical ontologies (such as MeSH or SNOMED CT). In this paper, these approaches are introduced and analyzed in order to determine their advantages and limitations with respect to the considered knowledge bases. After that, a new measure based on the exploitation of the taxonomical structure of a biomedical ontology is proposed. Using SNOMED CT as the input ontology, the accuracy of our proposal is evaluated and compared against other approaches according to a standard benchmark of manually ranked medical terms. The correlation between the results of the evaluated measures and the human experts' ratings shows that our proposal outperforms most of the previous measures avoiding, at the same time, some of their limitations.},
  keywords = {Biomedicine,Data mining,Ontologies,Semantic similarity,SNOMED CT},
  annotation = {00000 QID: Q38500061},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2011\\Batet et al. - 2011 - An ontology-based measure to compute semantic simi.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\Z3NBR397\\S1532046410001346.html}
}

@article{beauguitteBlockmodelingEquivalences2011,
  ids = {beauguitteBlockmodelingEquivalence},
  title = {{Blockmodeling et {\'e}quivalences}},
  author = {Beauguitte, Laurent},
  year = {2011},
  month = feb,
  pages = {10},
  url = {https://halshs.archives-ouvertes.fr/halshs-00566474},
  urldate = {2021-11-30},
  abstract = {Ce document du groupe fmr (flux, matrices, r{\'e}seaux) pr{\'e}sente la m{\'e}thode du blockmodeling utilis{\'e}e en analyse des r{\'e}seaux sociaux. Bas{\'e}e sur le concept d'{\'e}quivalence, cette m{\'e}thode permet de regrouper les sommets d'un graphe en fonction de leurs propri{\'e}t{\'e}s relationnelles.},
  langid = {french},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2011\\Beauguitte - 2011 - Blockmodeling et équivalences.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\7W7PA5UJ\\halshs-00566474.html}
}

@inproceedings{bedarideDeepSemanticsDependency2011,
  title = {Deep Semantics for Dependency Structures},
  booktitle = {International {{Conference}} on {{Intelligent Text Processing}} and {{Computational Linguistics}}},
  author = {B{\'e}daride, Paul and Gardent, Claire},
  year = {2011},
  pages = {277--288},
  publisher = {Springer},
  doi = {10/bx2ktp},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2011\\Bédaride et Gardent - 2011 - Deep semantics for dependency structures.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\WKM3AKM5\\978-3-642-19400-9_22.html}
}

@book{bellatrecheADBISTPDLEDA2020,
  ids = {ADBISTPDLEDA,bellatrecheADBISTPDLEDA2020a},
  title = {{{ADBIS}}, {{TPDL}} and {{EDA}} 2020 {{Common Workshops}} and {{Doctoral Consortium}}: {{International Workshops}}: {{DOING}}, {{MADEISD}}, {{SKG}}, {{BBIGAP}}, {{SIMPDA}}, {{AIMinScience}} 2020 and {{Doctoral Consortium}}, {{Lyon}}, {{France}}, {{August}} 25--27, 2020, {{Proceedings}}},
  shorttitle = {{{ADBIS}}, {{TPDL}} and {{EDA}} 2020 {{Common Workshops}} and {{Doctoral Consortium}}},
  author = {Bellatreche, Ladjel and Bielikov{\'a}, M{\'a}ria and Boussa{\"i}d, Omar and Catania, Barbara and Darmont, J{\'e}r{\^o}me and Demidova, Elena and Duchateau, Fabien and Hall, Mark and Mer{\v c}un, Tanja and Novikov, Boris and Papatheodorou, Christos and Risse, Thomas and Romero, Oscar and Sautot, Lucile and Talens, Guilaine and Wrembel, Robert and {\v Z}umer, Maja},
  year = {2020},
  month = aug,
  series = {Communications in {{Computer}} and {{Information Science}}},
  volume = {1260},
  publisher = {Springer Nature},
  address = {Lyon, France},
  doi = {10.1007/978-3-030-55814-7},
  urldate = {2023-08-07},
  abstract = {This book constitutes thoroughly reviewed and selected papers presented at Workshops and Doctoral Consortium of the 24th East-European Conference on Advances in Databases and Information Systems, ADBIS 2020, the 24th International Conference on Theory and Practice of Digital Libraries, TPDL 2020, and the 16th Workshop on Business Intelligence and Big Data, EDA 2020, held in August 2020. Due to the COVID-19 the joint conference and satellite events were held online. The 26 full papers and 5 short papers were carefully reviewed and selected from 56 submissions. This volume presents the papers that have been accepted for the following satellite events: Workshop on Intelligent Data - From Data to Knowledge, DOING 2020; Workshop on Modern Approaches in Data Engineering and Information System Design, MADEISD 2020; Workshop on Scientic Knowledge Graphs, SKG 2020; Workshop of BI \& Big Data Applications, BBIGAP 2020; International Symposium on Data-Driven Process Discovery and Analysis, SIMPDA 2020; International Workshop on Assessing Impact and Merit in Science, AIMinScience 2020; Doctoral Consortium.},
  googlebooks = {Vif4DwAAQBAJ},
  isbn = {978-3-030-55814-7},
  langid = {english},
  keywords = {Big Data,Business Intelligence,Computers / Artificial Intelligence / General,Computers / Computer Science,Computers / Data Science / General,Computers / Database Administration \& Management,Computers / General,Computers / Information Technology,Computers / Information Theory,Computers / Programming / Algorithms,Computers / System Administration / Storage \& Retrieval,Database,Digital Libraries,Information Systems},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2020\\Bellatreche et al. - 2020 - ADBIS, TPDL and EDA 2020 Common Workshops and Doct.pdf;C\:\\Users\\nhiot\\OneDrive\\zotero\\2020\\Bellatreche et al. - 2020 - ADBIS, TPDL and EDA 2020 Common Workshops and Doct2.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\73MKNIA8\\hal-02928842.html;C\:\\Users\\nhiot\\Zotero\\storage\\V4JB33X7\\978-3-030-55814-7.html}
}

@book{bellatrecheNewTrendsDatabase2021,
  ids = {bellatrecheNewTrendsDatabase2021a},
  title = {New {{Trends}} in {{Database}} and {{Information Systems}}: {{ADBIS}} 2021 {{Short Papers}}, {{Doctoral Consortium}} and {{Workshops}}: {{DOING}}, {{SIMPDA}}, {{MADEISD}}, {{MegaData}}, {{CAoNS}}, {{Tartu}}, {{Estonia}}, {{August}} 24-26, 2021, {{Proceedings}}},
  shorttitle = {New {{Trends}} in {{Database}} and {{Information Systems}}},
  author = {Bellatreche, Ladjel and Dumas, Marlon and Karras, Panagiotis and Matulevi{\v c}ius, Raimundas and Awad, Ahmed and Weidlich, Matthias and Ivanovi{\'c}, Mirjana and Hartig, Olaf},
  year = {2021},
  series = {Communications in {{Computer}} and {{Information Science}}},
  volume = {1450},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-85082-1},
  isbn = {978-3-030-85081-4 978-3-030-85082-1},
  langid = {english},
  keywords = {artificial intelligence,computer hardware,computer networks,computer security,computer systems,computer vision,cryptography,data integration,data mining,databases,image analysis,image processing,information retrieval,machine learning,network protocols,query languages,search engines,semantics,software engineering,World Wide Web},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2021\\Bellatreche et al. - 2021 - New Trends in Database and Information Systems AD.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\7DHUEQKT\\978-3-030-85082-1.html}
}

@article{belousovMedNormCorpusEmbeddings2019,
  title = {{{MedNorm}}: {{A Corpus}} and {{Embeddings}} for {{Cross-terminology Medical Concept Normalisation}}},
  shorttitle = {{{MedNorm}}},
  author = {Belousov, Maksim and Dixon, William G. and Nenadic, Goran},
  year = {2019},
  month = jun,
  volume = {1},
  publisher = {Mendeley},
  doi = {10/gjv6vr},
  urldate = {2021-05-06},
  abstract = {MedNorm is a corpus of 27,979 textual descriptions simultaneously mapped to both MedDRA and SNOMED-CT, sourced from five publicly available datasets across biomedical and social media domains. The cross-terminology medical concept embeddings are 64-dimensional vectors for UMLS, MedDRA and SNOMED-CT concepts that are able to capture semantic similarities between concepts from different medical terminologies. For more details see paper entitled "MedNorm: A Corpus and Embeddings for Cross-terminology Medical Concept Normalisation"},
  langid = {english},
  file = {C:\Users\nhiot\Zotero\storage\IMV5CRJW\1.html}
}

@article{benabachaAutomaticExtractionSemantic2011,
  title = {Automatic Extraction of Semantic Relations between Medical Entities: A Rule Based Approach},
  shorttitle = {Automatic Extraction of Semantic Relations between Medical Entities},
  author = {Ben Abacha, Asma and Zweigenbaum, Pierre},
  year = {2011},
  month = oct,
  journal = {Journal of Biomedical Semantics},
  volume = {2},
  number = {5},
  pages = {S4},
  issn = {2041-1480},
  doi = {10/dvjk2b},
  urldate = {2020-07-21},
  abstract = {Information extraction is a complex task which is necessary to develop high-precision information retrieval tools. In this paper, we present the platform MeTAE (Medical Texts Annotation and Exploration). MeTAE allows (i) to extract and annotate medical entities and relationships from medical texts and (ii) to explore semantically the produced RDF annotations.},
  langid = {english},
  annotation = {QID: Q38426005},
  file = {C:\Users\nhiot\OneDrive\zotero\2011\Ben Abacha et Zweigenbaum - 2011 - Automatic extraction of semantic relations between.pdf}
}

@inproceedings{benabachaHybridApproachExtraction2011,
  ids = {abachaHybridApproachExtraction2011},
  title = {A {{Hybrid Approach}} for the {{Extraction}} of {{Semantic Relations}} from {{MEDLINE Abstracts}}},
  booktitle = {International Conference on Intelligent Text Processing and Computational Linguistics},
  author = {Ben Abacha, Asma and Zweigenbaum, Pierre},
  editor = {Gelbukh, Alexander},
  year = {2011},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {139--150},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10/ckv8tc},
  abstract = {With the continuous digitisation of medical knowledge, information extraction tools become more and more important for practitioners of the medical domain. In this paper we tackle semantic relationships extraction from medical texts. We focus on the relations that may occur between diseases and treatments. We propose an approach relying on two different techniques to extract the target relations: (i) relation patterns based on human expertise and (ii) machine learning based on SVM classification. The presented approach takes advantage of the two techniques, relying more on manual patterns when few relation examples are available and more on feature values when a sufficient number of examples are available. Our approach obtains an overall 94.07\% F-measure for the extraction of cure, prevent and side effect relations.},
  isbn = {978-3-642-19437-5},
  langid = {english},
  keywords = {Hybrid Approach,Machine Learning Approach,Noun Phrase,Relation Type,Semantic Relation},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2011\\Ben Abacha et Zweigenbaum - 2011 - A Hybrid Approach for the Extraction of Semantic R.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\U84BPKY3\\978-3-642-19437-5_11.html}
}

@incollection{benamaraActesTALN20072007,
  title = {Actes de {{TALN}} 2007 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 2007 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Benamara, Farah and Hatout, Nabil and Muller, Philippe and Ozdowska, Sylwia},
  year = {2007},
  month = jun,
  publisher = {IRIT / ATALA},
  address = {Toulouse},
  keywords = {nosource}
}

@inproceedings{benediktBenchmarkingChase2017,
  title = {Benchmarking the {{Chase}}},
  booktitle = {Proceedings of the 36th {{ACM SIGMOD-SIGACT-SIGAI Symposium}} on {{Principles}} of {{Database Systems}}},
  author = {Benedikt, Michael and Konstantinidis, Georgios and Mecca, Giansalvatore and Motik, Boris and Papotti, Paolo and Santoro, Donatello and Tsamoura, Efthymia},
  year = {2017},
  month = may,
  series = {{{PODS}} '17},
  pages = {37--52},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3034786.3034796},
  urldate = {2024-01-03},
  abstract = {The chase is a family of algorithms used in a number of data management tasks, such as data exchange, answering queries under dependencies, query reformulation with constraints, and data cleaning. It is well established as a theoretical tool for understanding these tasks, and in addition a number of prototype systems have been developed. While individual chase-based systems and particular optimizations of the chase have been experimentally evaluated in the past, we provide the first comprehensive and publicly available benchmark - test infrastructure and a set of test scenarios - for evaluating chase implementations across a wide range of assumptions about the dependencies and the data. We used our benchmark to compare chase-based systems on data exchange and query answering tasks with one another, as well as with systems that can solve similar tasks developed in closely related communities. Our evaluation provided us with a number of new insights concerning the factors that impact the performance of chase implementations.},
  collaborator = {Benedikt, Michael and Konstantinidis, Georgios and Mecca, Giansalvatore and Motik, Boris and Papotti, Paolo and Santoro, Donatello and Tsamoura, Efthymia},
  copyright = {accepted\_manuscript},
  isbn = {978-1-4503-4198-1},
  langid = {english},
  keywords = {chase,nosource},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2017\\Benedikt et al. - 2017 - Benchmarking the Chase.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\T3GVYJVT\\423202.html}
}

@incollection{benferhatArgumentativeInferenceUncertain1993,
  title = {Argumentative Inference in Uncertain and Inconsistent Knowledge Bases},
  booktitle = {Uncertainty in {{Artificial Intelligence}}},
  author = {Benferhat, Salem and Dubois, Didier and Prade, Henri},
  editor = {Heckerman, David and Mamdani, Abe},
  year = {1993},
  month = jan,
  pages = {411--419},
  publisher = {Morgan Kaufmann},
  doi = {10.1016/B978-1-4832-1451-1.50054-8},
  urldate = {2019-02-14},
  abstract = {This paper presents and discusses several methods for reasoning from inconsistent knowledge bases. A so-called argumentative-consequence relation, taking into account the existence of consistent arguments in favor of a conclusion and the absence of consistent arguments in favor of its contrary, is particularly investigated. Flat knowledge bases, i.e. without any priority between their elements, as well as prioritized ones where some elements are considered as more strongly entrenched than others are studied under different consequence relations. Lastly a paraconsistent-like treatment of prioritized knowledge bases is proposed, where both the level of entrenchment and the level of paraconsistency attached to a formula are propagated. The priority levels are handled in the framework of possibility theory.},
  isbn = {978-1-4832-1451-1},
  annotation = {00000},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1993\\Benferhat et al. - 1993 - Argumentative inference in uncertain and inconsist.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\XZ6T4PVV\\B9781483214511500548.html}
}

@inproceedings{bernhardApprentissageNonSupervise2007,
  title = {{Apprentissage non supervis{\'e} de familles morphologiques par classification ascendante hi{\'e}rarchique}},
  booktitle = {{Actes de la 14{\`e}me conf{\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs}},
  author = {Bernhard, Delphine},
  year = {2007},
  month = jun,
  pages = {345--354},
  publisher = {IRIT / ATALA},
  address = {Toulouse, France},
  url = {https://aclanthology.org/2007.jeptalnrecital-long.34},
  urldate = {2023-10-23},
  abstract = {Cet article pr{\'e}sente un syst{\`e}me d'acquisition de familles morphologiques qui proc{\`e}de par apprentissage non supervis{\'e} {\`a} partir de listes de mots extraites de corpus de textes. L'approche consiste {\`a} former des familles par groupements successifs, similairement aux m{\'e}thodes de classification ascendante hi{\'e}rarchique. Les crit{\`e}res de regroupement reposent sur la similarit{\'e} graphique des mots ainsi que sur des listes de pr{\'e}fixes et de paires de suffixes acquises automatiquement {\`a} partir des corpus trait{\'e}s. Les r{\'e}sultats obtenus pour des corpus de textes de sp{\'e}cialit{\'e} en fran{\c c}ais et en anglais sont {\'e}valu{\'e}s {\`a} l'aide de la base CELEX et de listes de r{\'e}f{\'e}rence construites manuellement. L'{\'e}valuation d{\'e}montre les bonnes performances du syst{\`e}me, ind{\'e}pendamment de la langue, et ce malgr{\'e} la technicit{\'e} et la complexit{\'e} morphologique du vocabulaire trait{\'e}.},
  hal_id = {hal-00800342},
  langid = {french},
  keywords = {nosource},
  file = {C:\Users\nhiot\OneDrive\zotero\2007\Bernhard - 2007 - Apprentissage non supervisé de familles morphologi.pdf}
}

@article{beveridgeNetworkThrones2016,
  title = {Network of {{Thrones}}},
  author = {Beveridge, Andrew and Shan, Jie},
  year = {2016},
  month = apr,
  journal = {Math Horizons},
  volume = {23},
  number = {4},
  pages = {18--22},
  issn = {1072-4117, 1947-6213},
  doi = {10.4169/mathhorizons.23.4.18},
  urldate = {2024-03-18},
  langid = {english},
  file = {C:\Users\nhiot\Zotero\storage\YL67X5H9\Beveridge et Shan - 2016 - Network of Thrones.pdf}
}

@article{bezdekPatternRecognitionFuzzy1981,
  title = {Pattern {{Recognition}} with {{Fuzzy Objective Function Algorithms}}},
  author = {Bezdek, J. C.},
  year = {1981},
  doi = {10/gfvn23},
  keywords = {nosource},
  annotation = {00000}
}

@article{bharatSearchPadExplicitCapture2000,
  title = {{{SearchPad}}: {{Explicit}} Capture of Search Context to Support Web Search},
  shorttitle = {{{SearchPad}}},
  author = {Bharat, Krishna},
  year = {2000},
  month = jun,
  journal = {Computer Networks},
  volume = {33},
  number = {1-6},
  pages = {493--501},
  issn = {1389-1286},
  doi = {10/cgd42g},
  urldate = {2019-06-26},
  abstract = {Experienced users who query search engines have a complex behavior. They explore many topics in parallel, experiment with query variations, consult multiple search engines, and gather information over many sessions. In the process they need to keep track of search context --- namely useful queries and promising result links, which can be hard. We present an extension to search engines called SearchPad that makes it possible to keep track of `search context' explicitly. We describe an efficient implementation of this idea deployed on four search engines: AltaVista, Excite, Google and Hotbot. Our design of SearchPad has several desirable properties: (i) portability across all major platforms and browsers; (ii) instant start requiring no code download or special actions on the part of the user; (iii) no server side storage; and (iv) no added client--server communication overhead. An added benefit is that it allows search services to collect valuable relevance information about the results shown to the user. In the context of each query SearchPad can log the actions taken by the user, and in particular record the links that were considered relevant by the user in the context of the query. The service was tested in a multi-platform environment with over 150 users for 4 months and found to be usable and helpful. We discovered that the ability to maintain search context explicitly seems to affect the way people search. Repeat SearchPad users looked at more search results than is typical on the Web, suggesting that availability of search context may partially compensate for non-relevant pages in the ranking.},
  langid = {english},
  keywords = {Bookmarking,Cookies,Data collection,JavaScript,Queries,Relevance information,Search context,Search engines},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2000\\Bharat - 2000 - SearchPad Explicit capture of search context to s.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\PQN3BG7U\\S1389128600000475.html}
}

@inproceedings{biemannOntologyLearningText2005,
  title = {Ontology Learning from Text: {{A}} Survey of Methods.},
  shorttitle = {Ontology Learning from Text},
  booktitle = {{{LDV}} Forum},
  author = {Biemann, Chris},
  year = {2005},
  volume = {20},
  pages = {75--93},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2005\Biemann - 2005 - Ontology learning from text A survey of methods..pdf}
}

@techreport{biettiLatentDirichletAllocation2012,
  title = {Latent {{Dirichlet Allocation}}},
  author = {Bietti, Alberto},
  year = {2012},
  institution = {mai 2012, working paper, http://alberto. bietti. me/files/rapport-lda. pdf},
  file = {C:\Users\nhiot\OneDrive\zotero\2012\Bietti - 2012 - Latent Dirichlet Allocation.pdf}
}

@misc{big-oBigoTransvec2021,
  title = {Big-o/Transvec},
  author = {{big-o}},
  year = {2021},
  month = mar,
  url = {https://github.com/big-o/transvec},
  urldate = {2021-03-23},
  abstract = {Translate word embeddings across models. Contribute to big-o/transvec development by creating an account on GitHub.},
  copyright = {GPL-3.0 License         ,                 GPL-3.0 License},
  keywords = {nosource}
}

@incollection{bigiActesTALN20142014,
  title = {Actes de {{TALN}} 2014 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 2014 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Bigi, Brigitte},
  year = {2014},
  month = jul,
  publisher = {LPL / ATALA},
  address = {Marseille},
  keywords = {nosource}
}

@incollection{blacheActesTALN20042004,
  title = {Actes de {{TALN}} 2004 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 2004 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Blache, Philippe},
  year = {2004},
  month = apr,
  publisher = {LPL / ATALA},
  address = {F{\`e}s, Maroc},
  keywords = {nosource}
}

@inproceedings{blancoIssuesDetectingNegation2011,
  title = {Some {{Issues}} on {{Detecting Negation}} from {{Text}}},
  shorttitle = {Some Issues on Detecting Negation from Text},
  booktitle = {Proceedings of the 24th {{International Florida Artificial Intelligence Research Society}}, {{FLAIRS}} - 24},
  author = {Blanco, Eduardo and Moldovan, Dan},
  year = {2011},
  series = {Proceedings of the 24th {{International Florida Artificial Intelligence Research Society}}, {{FLAIRS}} - 24},
  pages = {228--233},
  issn = {9781577355014},
  urldate = {2024-03-22},
  abstract = {Negation is present in all human languages and it is used to reverse the polarity of parts of a statement. It is a complex phenomenon that interacts with many other aspects of language. Besides the direct meaning, negated statements often carry a latent positive meaning. Negation can be interpreted in terms of its scope and focus. This paper explores the importance of both scope and focus to capture the meaning of negated statements. Some issues on detecting negation from text are outlined, the forms in which negation occurs are depicted and heuristics to detect its scope and focus are proposed.},
  isbn = {978-1-57735-501-4},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2011\Blanco et Moldovan - 2011 - Some Issues on Detecting Negation from Text.pdf}
}

@article{blockeelDataMiningProcedural2015,
  title = {Data {{Mining}}: {{From Procedural}} to {{Declarative Approaches}}},
  shorttitle = {Data {{Mining}}},
  author = {Blockeel, Hendrik},
  year = {2015},
  month = apr,
  journal = {New Generation Computing},
  volume = {33},
  number = {2},
  pages = {115--135},
  issn = {1882-7055},
  doi = {10/f68pg4},
  urldate = {2020-01-13},
  abstract = {This article provides a viewpoint on the past and possible future development of data mining technology. On an introductory level, it provides some historical background to the development of data mining, sketches its relationship to other disciplines, and introduces a number of tasks that are typically considered data mining tasks. It next focuses on one particular aspect that may play a larger role in data mining, namely, declarativeness. Despite the fact that many different data mining tools have been developed, this variety still offers less flexibility to the user than desired. It also creates a problem of choice: which tool is most suitable for a given problem? Declarative data mining may provide a solution for this. In other domains of computer science, declarative languages have led to major leaps forward in technology. Early results show that in data mining, too, declarative approaches are feasible and may make the process easier, more flexible, more efficient, and more correct.},
  langid = {english},
  keywords = {Constraint Solving,Data Mining,Declarative Modeling Languages,Inductive Databases,Inductive Query Languages},
  annotation = {QID: Q58012222},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2015\\Blockeel - 2015 - Data Mining From Procedural to Declarative Approa.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\3ZXE2F8X\\s00354-015-0202-x.html}
}

@article{bockClusteringAlgorithmsKohonen2003,
  title = {Clustering Algorithms and Kohonen Maps for Symbolic Data (Symbolic Data Analysis)},
  author = {Bock, H.-H.},
  year = {2003},
  journal = {Journal of the Japanese Society of Computational Statistics},
  pages = {217--229},
  doi = {10/gfvn2x},
  keywords = {nosource},
  annotation = {00000},
  file = {C:\Users\nhiot\OneDrive\zotero\2003\Bock - 2003 - Clustering algorithms and kohonen maps for symboli.pdf}
}

@article{bocklischRasaOpenSource2017,
  title = {Rasa: {{Open Source Language Understanding}} and {{Dialogue Management}}},
  shorttitle = {Rasa},
  author = {Bocklisch, Tom and Faulkner, Joey and Pawlowski, Nick and Nichol, Alan},
  year = {2017},
  month = dec,
  journal = {arXiv:1712.05181 [cs]},
  eprint = {1712.05181},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1712.05181},
  urldate = {2019-12-04},
  abstract = {We introduce a pair of tools, Rasa NLU and Rasa Core, which are open source python libraries for building conversational software. Their purpose is to make machine-learning based dialogue management and language understanding accessible to non-specialist software developers. In terms of design philosophy, we aim for ease of use, and bootstrapping from minimal (or no) initial training data. Both packages are extensively documented and ship with a comprehensive suite of tests. The code is available at https://github.com/RasaHQ/},
  archiveprefix = {arxiv},
  optabstract = {We introduce a pair of tools, Rasa NLU and Rasa Core, which are open source python libraries for building conversational software. Their purpose is to make machine-learning based dialogue management and language understanding accessible to non-specialist software developers. In terms of design philosophy, we aim for ease of use, and bootstrapping from minimal (or no) initial training data. Both packages are extensively documented and ship with a comprehensive suite of tests. The code is available at https://github.com/RasaHQ/},
  optkeywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  optmonth = {12},
  optshorttitle = {Rasa},
  opturl = {http://arxiv.org/abs/1712.05181},
  opturldate = {2019-12-04},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {QID: Q113549428},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2017\\Bocklisch et al. - 2017 - Rasa Open Source Language Understanding and Dialo.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\YG4IDBUF\\1712.html}
}

@article{bohusRavenClawDialogManagement2009a,
  ids = {bohusRavenClawDialogManagement2009},
  title = {The {{RavenClaw}} Dialog Management Framework: {{Architecture}} and Systems},
  shorttitle = {The {{RavenClaw}} Dialog Management Framework},
  author = {Bohus, Dan and Rudnicky, Alexander I.},
  year = {2009},
  month = jul,
  journal = {Computer Speech \& Language},
  volume = {23},
  number = {3},
  pages = {332--361},
  publisher = {Elsevier},
  issn = {0885-2308},
  doi = {10/cnwj88},
  urldate = {2021-03-11},
  abstract = {In this paper, we describe RavenClaw, a plan-based, task-independent dialog management framework. RavenClaw isolates the domain-specific aspects of the dialog control logic from domain-independent conversational skills, and in the process facilitates rapid development of mixed-initiative systems operating in complex, task-oriented domains. System developers can focus exclusively on describing the dialog task control logic, while a large number of domain-independent conversational skills such as error handling, timing and turn-taking are transparently supported and enforced by the RavenClaw dialog engine. To date, RavenClaw has been used to construct and deploy a large number of systems, spanning different domains and interaction styles, such as information access, guidance through procedures, command-and-control, medical diagnosis, etc. The framework has easily adapted to all of these domains, indicating a high degree of versatility and scalability.},
  langid = {english},
  keywords = {Dialog management,Error handling,Focus shifting,Mixed-initiative,Spoken dialog systems},
  file = {C:\Users\nhiot\Zotero\storage\BMS5RHJK\S0885230808000545.html}
}

@inproceedings{boiretPrivacyOperatorsSemantic2022,
  title = {Privacy {{Operators}} for~{{Semantic Graph Databases}} as~{{Graph Rewriting}}},
  booktitle = {New {{Trends}} in {{Database}} and {{Information Systems}}},
  author = {Boiret, Adrien and Eichler, C{\'e}dric and Nguyen, Benjamin},
  editor = {Chiusano, Silvia and Cerquitelli, Tania and Wrembel, Robert and N{\o}rv{\aa}g, Kjetil and Catania, Barbara and {Vargas-Solar}, Genoveva and Zumpano, Ester},
  year = {2022},
  series = {Communications in {{Computer}} and {{Information Science}}},
  pages = {366--377},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-15743-1_34},
  abstract = {Database sanitization allows to share and publish open (linked) data without jeopardizing privacy. During their sanitization, graph databases are transformed following graph transformations that are usually described informally or through ad-hoc processes.},
  isbn = {978-3-031-15743-1},
  langid = {english}
}

@article{bonevaRegularMatchingInclusion2021,
  title = {Regular Matching and Inclusion on Compressed Tree Patterns with Constrained Context Variables},
  author = {Boneva, Iovka and Niehren, Joachim and Sakho, Momar},
  year = {2021},
  month = jun,
  journal = {Information and Computation},
  pages = {104776},
  issn = {0890-5401},
  doi = {10/gnnch4},
  urldate = {2021-11-30},
  abstract = {We study the complexity of regular matching and inclusion for compressed tree patterns with context variables subject to regular constraints. Context variables with regular constraints permit to properly generalize on unranked tree patterns with hedge variables. Regular inclusion on unranked tree patterns is relevant to certain query answering on Xml streams with references. We show that regular matching and inclusion with regular constraints can be reduced in polynomial time to the corresponding problem without regular constraints.},
  langid = {english},
  keywords = {Computational complexity,Grammar compression,Streams,Tree automata,Tree patterns},
  file = {C:\Users\nhiot\OneDrive\zotero\2021\Boneva et al. - 2021 - Regular matching and inclusion on compressed tree .pdf}
}

@book{bonifatiQueryingGraphs2018,
  title = {Querying {{Graphs}}},
  author = {Bonifati, Angela and Fletcher, George and Voigt, Hannes and Yakovets, Nikolay and Jagadish, H. V.},
  year = {2018},
  series = {Synthesis {{Lectures}} on {{Data Management}}},
  volume = {10},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-01864-0},
  abstract = {Graph data modeling and querying arises in many practical application domains such as social and biological networks where the primary focus is on concepts and their relationships and the rich patterns in these complex webs of interconnectivity. In this book, we present a concise unified view on the basic challenges which arise over the complete life cycle of formulating and processing queries on graph databases. To that purpose, we present all major concepts relevant to this life cycle, formulated in terms of a common and unifying ground: the property graph data model---the pre-dominant data model adopted by modern graph database systems. We aim especially to give a coherent and in-depth perspective on current graph querying and an outlook for future developments. Our presentation is self-contained, covering the relevant topics from: graph data models, graph query languages and graph query specification, graph constraints, and graph query processing. We conclude by indicating major open research challenges towards the next generation of graph data management systems.},
  isbn = {978-3-031-00736-1 978-3-031-01864-0},
  langid = {english},
  keywords = {Computers / Information Technology,Computers / Information Theory,Computers / Internet / General,Computers / Networking / General,Computers / Online Services,Computers / Programming / Algorithms,nosource},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2018\\Bonifati et al. - 2018 - Querying Graphs.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\9CCKMBG8\\978-3-031-01864-0.html}
}

@inproceedings{bonifatiSchemaValidationEvolution2019,
  title = {Schema {{Validation}} and {{Evolution}} for {{Graph Databases}}},
  booktitle = {Conceptual {{Modeling}}},
  author = {Bonifati, Angela and Furniss, Peter and Green, Alastair and Harmer, Russ and Oshurko, Eugenia and Voigt, Hannes},
  editor = {Laender, Alberto H. F. and Pernici, Barbara and Lim, Ee-Peng and {de Oliveira}, Jos{\'e} Palazzo M.},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {448--456},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10/gmnv7f},
  abstract = {Despite the maturity of commercial graph databases, little consensus has been reached so far on the standardization of data definition languages (DDLs) for property graphs (PG). Discussion on the characteristics of PG schemas is ongoing in many standardization and community groups. Although some basic aspects of a schema are already present in most commercial graph databases, full support is missing allowing to constraint property graphs with more or less flexibility.In this paper, we show how schema validation can be enforced through homomorphisms between PG schemas and PG instances by leveraging a concise schema DDL inspired by Cypher syntax. We also briefly discuss PG schema evolution that relies on graph rewriting operations allowing to consider both prescriptive and descriptive schemas.},
  isbn = {978-3-030-33223-5},
  langid = {english},
  keywords = {Graph databases,Graph schema modelling,Graph schema validation},
  file = {C:\Users\nhiot\OneDrive\zotero\2019\Bonifati et al. - 2019 - Schema Validation and Evolution for Graph Database.pdf}
}

@article{borgattiUCINETIVVersion1992,
  title = {{{UCINET IV Version}} 1.0 {{User}}'s {{Guide}}},
  author = {Borgatti, Steven and Everett, Martin and Freeman, Linton},
  year = {1992},
  journal = {Analytic Technologies, Columbia, SC},
  keywords = {⛔ No DOI found,nosource}
}

@inproceedings{borgeltCanonicalFormsFrequent2007,
  title = {Canonical {{Forms}} for {{Frequent Graph Mining}}},
  booktitle = {Advances in {{Data Analysis}}},
  author = {Borgelt, Christian},
  editor = {Decker, Reinhold and Lenz, Hans -J.},
  year = {2007},
  series = {Studies in {{Classification}}, {{Data Analysis}}, and {{Knowledge Organization}}},
  pages = {337--349},
  publisher = {Springer Berlin Heidelberg},
  abstract = {A core problem of approaches to frequent graph mining, which are based on growing subgraphs into a set of graphs, is how to avoid redundant search. A powerful technique for this is a canonical description of a graph, which uniquely identifies it, and a corresponding test. I introduce a family of canonical forms based on systematic ways to construct spanning trees. I show that the canonical form used in gSpan ([Yan and Han (2002)]) is a member of this family, and that MoSS/MoFa ([Borgelt and Berthold (2002), Borgelt et al. (2005)]) is implicitly based on a different member, which I make explicit and exploit in the same way.},
  isbn = {978-3-540-70981-7},
  langid = {english},
  keywords = {Canonical Form,Code Word,Destination Node,Search Tree,Span Tree},
  annotation = {00000},
  file = {C:\Users\nhiot\OneDrive\zotero\2007\Borgelt - 2007 - Canonical Forms for Frequent Graph Mining.pdf}
}

@incollection{bottaQueryLanguagesSupporting2004,
  title = {Query {{Languages Supporting Descriptive Rule Mining}}: {{A Comparative Study}}},
  shorttitle = {Query {{Languages Supporting Descriptive Rule Mining}}},
  booktitle = {Database {{Support}} for {{Data Mining Applications}}: {{Discovering Knowledge}} with {{Inductive Queries}}},
  author = {Botta, Marco and Boulicaut, Jean-Fran{\c c}ois and Masson, Cyrille and Meo, Rosa},
  editor = {Meo, Rosa and Lanzi, Pier Luca and Klemettinen, Mika},
  year = {2004},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {24--51},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-44497-8_2},
  urldate = {2020-01-13},
  abstract = {Recently, inductive databases (IDBs) have been proposed to tackle the problem of knowledge discovery from huge databases. With an IDB, the user/analyst performs a set of very different operations on data using a query language, powerful enough to support all the required manipulations, such as data preprocessing, pattern discovery and pattern post-processing. We provide a comparison between three query languages (MSQL, DMQL and MINE RULE) that have been proposed for descriptive rule mining and discuss their common features and differences. These query languages look like extensions of SQL. We present them using a set of examples, taken from the real practice of rule mining. In the paper we discuss also OLE DB for Data Mining and Predictive Model Markup Language, two recent proposals that like the first three query languages respectively provide native support to data mining primitives and provide a description in a standard language of statistical and data mining models.},
  isbn = {978-3-540-44497-8},
  langid = {english},
  keywords = {Association Rule,Association Rule Mining,Mine Rule,Query Language,Rule Extraction},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2004\\Botta et al. - 2004 - Query Languages Supporting Descriptive Rule Mining.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\Y9DDSN3I\\978-3-540-44497-8_2.html}
}

@inproceedings{bouchouUpdatesIncrementalValidation2003,
  ids = {bouchouUpdatesIncrementalValidation2004},
  title = {Updates and {{Incremental Validation}} of {{XML Documents}}},
  booktitle = {The 9th International Workshop on Data Base Programming Languages ({{DBPL}})},
  author = {Bouchou, B{\'e}atrice and {Halfeld-Ferrari}, Mirian},
  editor = {Lausen, Georg and Suciu, Dan},
  year = {2003},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {216--232},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-24607-7_14},
  abstract = {We consider the incremental validation of updates on XML documents. When a valid XML document (i.e., one satisfying some constraints) is updated, it has to be verified that the new document still conforms to the imposed constraints. Incremental validation of updates leads to significant savings on computing time when compared to brute-force validation of an updated document from scratch.},
  isbn = {978-3-540-20896-9 978-3-540-24607-7},
  langid = {english},
  keywords = {Incremental Validation,Label Tree,nosource,Transition Rule,Tree Automaton,Type Check},
  file = {C:\Users\nhiot\OneDrive\zotero\2003\Bouchou et Alves - 2003 - Updates and Incremental Validation of XML Document2.pdf}
}

@article{bourigaultUPERYOutilAnalyse2002,
  title = {{{UPERY}}: Un Outil d'analyse Distributionnelle {\'E}tendue Pour La Construction d'ontologies {\`a} Partir de Corpus},
  shorttitle = {{{UPERY}}},
  author = {Bourigault, Didier},
  year = {2002},
  journal = {Actes de TALN, Nancy},
  pages = {75--84},
  keywords = {⛔ No DOI found},
  annotation = {00165},
  file = {C:\Users\nhiot\OneDrive\zotero\2002\Bourigault - 2002 - UPERY un outil d’analyse distributionnelle étendu.pdf}
}

@article{bousquetPERTOMedProjectExploiting2010,
  ids = {bousquetPERTOMedProjectExploiting2010a},
  title = {The {{PERTOMed Project}}: {{Exploiting}} and Validating Terminological Resources of Comparable {{Russian-French-English}} Corpora within Pharmacovigilance.},
  shorttitle = {The {{PERTOMed Project}}},
  author = {Bousquet, Cedric and {Zimina-Poirot}, Maria},
  year = {2010},
  month = feb,
  journal = {Terminology in Everyday Life},
  volume = {13},
  pages = {213--232},
  publisher = {John Benjamins Publishing},
  doi = {10.1075/tlrp.13.15bou},
  urldate = {2023-09-25},
  abstract = {The PERTOMed project is a pluri-disciplinary research initiative undertaken by several institutions in France. Applications considered within the part of the project described in this article concern pharmacovigilance and adverse drug reactions. We had multiple objectives: to create a specialized Russian Internet corpus; to test new tools and methods for term extraction from comparable multilingual texts and to build terminological resources including Russian. A trilingual Russian-French-English lexicon resulting form this work is freely available from the PERTOMed server.},
  langid = {english},
  file = {C:\Users\nhiot\OneDrive\zotero\2010\Bousquet et Zimina-Poirot - 2010 - The PERTOMed Project Exploiting and validating te.pdf}
}

@misc{bradleyJSONWebSignature,
  title = {{{JSON Web Signature}} ({{JWS}})},
  author = {Bradley, John and Sakimura, Nat and Jones, Michael},
  url = {https://tools.ietf.org/html/rfc7515},
  urldate = {2020-10-02},
  langid = {english},
  file = {C:\Users\nhiot\Zotero\storage\V382TNFH\rfc7515.html}
}

@misc{bradleyJSONWebToken,
  title = {{{JSON Web Token}} ({{JWT}})},
  author = {Bradley, John and Sakimura, Nat and Jones, Michael},
  url = {https://tools.ietf.org/html/rfc7519},
  urldate = {2020-10-02},
  langid = {english},
  file = {C:\Users\nhiot\Zotero\storage\UV6KNJWQ\rfc7519.html}
}

@inproceedings{bravoSemanticallyCorrectQuery2006,
  title = {Semantically {{Correct Query Answers}} in the {{Presence}} of {{Null Values}}},
  booktitle = {Current {{Trends}} in {{Database Technology}} -- {{EDBT}} 2006},
  author = {Bravo, Loreto and Bertossi, Leopoldo},
  editor = {Grust, Torsten and H{\"o}pfner, Hagen and Illarramendi, Arantza and Jablonski, Stefan and Mesiti, Marco and M{\"u}ller, Sascha and Patranjan, Paula-Lavinia and Sattler, Kai-Uwe and Spiliopoulou, Myra and Wijsen, Jef},
  year = {2006},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {336--357},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/11896548_27},
  abstract = {For several reasons a database may not satisfy a given set of integrity constraints (ICs), but most likely most of the information in it is still consistent with those ICs; and could be retrieved when queries are answered. Consistent answers to queries wrt a set of ICs have been characterized as answers that can be obtained from every possible minimally repaired consistent version of the original database. In this paper we consider databases that contain null values and are also repaired, if necessary, using null values. For this purpose, we propose first a precise semantics for IC satisfaction in a database with null values that is compatible with the way null values are treated in commercial database management systems. Next, a precise notion of repair is introduced that privileges the introduction of null values when repairing foreign key constraints, in such a way that these new values do not create an infinite cycle of new inconsistencies. Finally, we analyze how to specify this kind of repairs of a database that contains null values using disjunctive logic programs with stable model semantics.},
  isbn = {978-3-540-46790-8},
  langid = {english},
  keywords = {Database Instance,Disjunctive Logic Program,Integrity Constraint,Query Answer,Stable Model Semantic},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2006\\Bravo et Bertossi - 2006 - Semantically Correct Query Answers in the Presence2.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\CMVWCZF5\\semantically-correct-query-answers-in-z9VL6b.html}
}

@article{brinAnatomyLargescaleHypertextual1998,
  title = {The Anatomy of a Large-Scale Hypertextual {{Web}} Search Engine},
  author = {Brin, Sergey and Page, Lawrence},
  year = {1998},
  month = apr,
  journal = {Computer Networks and ISDN Systems},
  series = {Proceedings of the {{Seventh International World Wide Web Conference}}},
  volume = {30},
  number = {1-7},
  pages = {107--117},
  publisher = {Elsevier},
  issn = {0169-7552},
  doi = {10.1016/s0169-7552(98)00110-x},
  urldate = {2023-09-22},
  abstract = {In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/ To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of Web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the Web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and Web proliferation, creating a Web search engine today is very different from three years ago. This paper provides an in-depth description of our large-scale Web search engine --- the first such detailed public description we know of to date. Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want.},
  keywords = {Google,Information retrieval,PageRank,Search engines,World Wide Web},
  annotation = {QID: Q55970547},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1998\\Brin et Page - 1998 - The anatomy of a large-scale hypertextual Web sear.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\5FZIELHS\\S016975529800110X.html}
}

@inproceedings{briscoeRobustAccurateStatistical2002,
  title = {Robust Accurate Statistical Annotation of General Text},
  booktitle = {Proceedings of the Third International Conference on Language Resources and Evaluation ({{LREC}}'02)},
  author = {Briscoe, Ted and Carroll, John},
  editor = {Gonz{\'a}lez Rodr{\'i}guez, Manuel and Suarez Araujo, Carmen Paz},
  year = {2002},
  month = may,
  publisher = {European Language Resources Association (ELRA)},
  address = {Las Palmas, Canary Islands - Spain},
  url = {http://www.lrec-conf.org/proceedings/lrec2002/pdf/250.pdf},
  keywords = {⛔ No DOI found,nosource}
}

@inproceedings{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M.F. and Lin, H.},
  year = {2020},
  month = jul,
  volume = {33},
  eprint = {2005.14165},
  primaryclass = {cs},
  pages = {1877--1901},
  publisher = {Curran Associates, Inc.},
  url = {http://arxiv.org/abs/2005.14165},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2020\\Brown et al. - 2020 - Language Models are Few-Shot Learners2.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\Y9KQQ8SU\\2005.html}
}

@article{brownMedicalDictionaryRegulatory1999,
  title = {The {{Medical Dictionary}} for {{Regulatory Activities}} ({{MedDRA}})},
  author = {Brown, Elliot G. and Wood, Louise and Wood, Sue},
  year = {1999},
  month = feb,
  journal = {Drug Safety},
  volume = {20},
  number = {2},
  pages = {109--117},
  publisher = {Springer},
  issn = {1179-1942},
  doi = {10.2165/00002018-199920020-00002},
  abstract = {The International Conference on Harmonisation has agreed upon the structure and content of the Medical Dictionary for Regulatory Activities (MedDRA) version 2.0 which should become available in the early part of 1999.},
  langid = {english},
  file = {C:\Users\nhiot\Zotero\storage\FJ536H56\00002018-199920020-00002.html}
}

@inproceedings{bunescuShortestPathDependency2005,
  ids = {bunescuShortestPathDependency2005a},
  title = {A Shortest Path Dependency Kernel for Relation Extraction},
  booktitle = {Proceedings of {{Human Language Technology Conference}} and {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Bunescu, Razvan C. and Mooney, Raymond J.},
  year = {2005},
  month = oct,
  series = {{{HLT}} '05},
  pages = {724--731},
  publisher = {Association for Computational Linguistics},
  address = {USA},
  doi = {10/cwq4w7},
  urldate = {2020-11-18},
  abstract = {We present a novel approach to relation extraction, based on the observation that the information required to assert a relationship between two named entities in the same sentence is typically captured by the shortest path between the two entities in the dependency graph. Experiments on extracting top-level relations from the ACE (Automated Content Extraction) newspaper corpus show that the new shortest path dependency kernel outperforms a recent approach based on dependency tree kernels.},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2005\\Bunescu et Mooney - 2005 - A shortest path dependency kernel for relation ext.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\7Q3E92GV\\1220575.html}
}

@misc{BusinessIntelligenceBI,
  title = {Business {{Intelligence}} ({{BI}}) \& {{AI Analytics Tools}}},
  journal = {ThoughtSpot},
  url = {https://www.thoughtspot.com/},
  abstract = {ThoughtSpot is a business intelligence and big data analytics platform that helps you explore, analyze and share real-time business analytics data easily.},
  file = {C:\Users\nhiot\Zotero\storage\QYPSNI8D\www.thoughtspot.com.html}
}

@inproceedings{cailliauRedisgraphGraphblasEnabled2019,
  title = {Redisgraph Graphblas Enabled Graph Database},
  booktitle = {2019 {{IEEE International Parallel}} and {{Distributed Processing Symposium Workshops}} ({{IPDPSW}})},
  author = {Cailliau, Pieter and Davis, Tim and Gadepally, Vijay and Kepner, Jeremy and Lipman, Roi and Lovitz, Jeffrey and Ouaknine, Keren},
  year = {2019},
  pages = {285--286},
  publisher = {IEEE},
  doi = {10/gmrmw8},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2019\\Cailliau et al. - 2019 - Redisgraph graphblas enabled graph database.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\ZMKNJCD3\\8778293.html}
}

@inproceedings{camposBiomedicalNamedEntity2012,
  title = {Biomedical {{Named Entity Recognition}}: {{A Survey}} of {{Machine-Learning Tools}}},
  shorttitle = {Biomedical {{Named Entity Recognition}}},
  booktitle = {Theory and Applications for Advanced Text Mining},
  author = {Campos, David and Matos, S{\'e}rgio and Oliveira, Jos{\'e} Lu{\'i}s},
  year = {2012},
  volume = {11},
  pages = {175--195},
  publisher = {IntechOpen},
  address = {Croatia},
  doi = {10.5772/51066},
  urldate = {2024-03-20},
  keywords = {nosource},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2012\\Campos et al. - 2012 - Biomedical Named Entity Recognition A Survey of M2.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\57VLBTUM\\38735.html}
}

@inproceedings{canditoCorpusSequoiaAnnotation2012,
  ids = {canditoCorpusSequoiaAnnotation2012a},
  title = {{Le corpus Sequoia : annotation syntaxique et exploitation pour l'adaptation d'analyseur par pont lexical}},
  shorttitle = {{Le corpus Sequoia}},
  booktitle = {{TALN 2012-19e conf{\'e}rence sur le Traitement Automatique des Langues Naturelles}},
  author = {Candito, Marie and Seddah, Djam{\'e}},
  year = {2012},
  url = {https://inria.hal.science/hal-00698938},
  urldate = {2024-03-21},
  abstract = {Nous pr{\'e}sentons dans cet article la m{\'e}thodologie de constitution et les caract{\'e}ristiques du corpus Sequoia, un corpus en fran{\c c}ais, syntaxiquement annot{\'e} d'apr{\`e}s un sch{\'e}ma d'annotation tr{\`e}s proche de celui du French Treebank (Abeill{\'e} et Barrier, 2004), et librement disponible, en constituants et en d{\'e}pendances. Le corpus comporte des phrases de quatre origines : Europarl fran{\c c}ais, le journal l'Est R{\'e}publicain, Wikip{\'e}dia Fr et des documents de l'Agence Europ{\'e}enne du M{\'e}dicament, pour un total de 3204 phrases et 69246 tokens. En outre, nous pr{\'e}sentons une application de ce corpus : l'{\'e}valuation d'une technique d'adaptation d'analyseurs syntaxiques probabilistes {\`a} des domaines et/ou genres autres que ceux du corpus sur lequel ces analyseurs sont entra{\^i}n{\'e}s. Cette technique utilise des clusters de mots obtenus d'abord par regroupement morphologique {\`a} l'aide d'un lexique, puis par regroupement non supervis{\'e}, et permet une nette am{\'e}lioration de l'analyse des domaines cibles (le corpus Sequoia), tout en pr{\'e}servant le m{\^e}me niveau de performance sur le domaine source (le FTB), ce qui fournit un analyseur multi-domaines, {\`a} la diff{\'e}rence d'autres techniques d'adaptation comme le self-training.},
  langid = {french},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2012\Candito et Seddah - 2012 - Le corpus Sequoia  annotation syntaxique et explo.pdf}
}

@inproceedings{canRicherbutSmarterShortestDependency2019,
  title = {A {{Richer-but-Smarter Shortest Dependency Path}} with {{Attentive Augmentation}} for {{Relation Extraction}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Can, Duy-Cat and Le, Hoang-Quynh and Ha, Quang-Thuy and Collier, Nigel},
  year = {2019},
  month = jun,
  pages = {2902--2912},
  publisher = {Association for Computational Linguistics},
  address = {Minneapolis, Minnesota},
  doi = {10/ggn9rx},
  urldate = {2020-03-19},
  abstract = {To extract the relationship between two entities in a sentence, two common approaches are (1) using their shortest dependency path (SDP) and (2) using an attention model to capture a context-based representation of the sentence. Each approach suffers from its own disadvantage of either missing or redundant information. In this work, we propose a novel model that combines the advantages of these two approaches. This is based on the basic information in the SDP enhanced with information selected by several attention mechanisms with kernel filters, namely RbSP (Richer-but-Smarter SDP). To exploit the representation behind the RbSP structure effectively, we develop a combined deep neural model with a LSTM network on word sequences and a CNN on RbSP. Experimental results on the SemEval-2010 dataset demonstrate improved performance over competitive baselines. The data and source code are available at https://github.com/catcd/RbSP.},
  file = {C:\Users\nhiot\OneDrive\zotero\2019\Can et al. - 2019 - A Richer-but-Smarter Shortest Dependency Path with.pdf}
}

@inproceedings{cardonPresentationCampagneEvaluation2020,
  ids = {DEFT2020},
  title = {Pr{\'e}sentation de La Campagne d'{\'e}valuation {{DEFT}} 2020 : Similarit{\'e} Textuelle En Domaine Ouvert et Extraction d'information Pr{\'e}cise Dans Des Cas Cliniques},
  booktitle = {6e Conf{\'e}rence Conjointe Journ{\'e}es d'{{{\'E}tudes}} Sur La Parole ({{JEP}}, 33e {\'E}dition), Traitement Automatique Des Langues Naturelles ({{TALN}}, 27e {\'E}dition), Rencontre Des {\'E}tudiants Chercheurs En Informatique Pour Le Traitement Automatique Des Langues ({{R{\'E}CITAL}}, 22e {\'E}dition). {{Atelier D{\'E}fi}} Fouille de Textes},
  author = {Cardon, R{\'e}mi and Grabar, Natalia and Grouin, Cyril and Hamon, Thierry},
  editor = {Cardon, R{\'e}mi and Grabar, Natalia and Grouin, Cyril and Hamon, Thierry},
  year = {2020},
  pages = {1--13},
  publisher = {ATALA},
  address = {Nancy, France},
  url = {https://hal.archives-ouvertes.fr/hal-02784737},
  keywords = {⛔ No DOI found,Cas cliniques,extraction d'information,nosource,similarit{\'e} textuelle.},
  file = {C:\Users\nhiot\OneDrive\zotero\2020\Cardon et al. - 2020 - Présentation de la campagne d'évaluation DEFT 2020.pdf}
}

@article{carlssonCharacterizationStabilityConvergence2010,
  title = {Characterization, {{Stability}} and {{Convergence}} of {{Hierarchical Clustering Methods}}},
  author = {Carlsson, Gunnar and M{\'e}moli, Facundo},
  editor = {{Microtome Publishing}},
  year = {2010},
  month = aug,
  journal = {Journal of Machine Learning Research},
  volume = {11},
  number = {Apr},
  pages = {1425--1470},
  issn = {1533-7928},
  url = {https://www.jmlr.org/papers/volume11/carlsson10a/carlsson10a.pdf},
  abstract = {We study hierarchical clustering schemes under an axiomatic view. We show that within this framework, one can prove a theorem analogous to one of Kleinberg (2002), in which one obtains an existence and uniqueness theorem instead of a non-existence result. We explore further properties of this unique scheme: stability and convergence are established. We represent dendrograms as ultrametric spaces and use tools from metric geometry, namely the Gromov-Hausdorff distance, to quantify the degree to which perturbations in the input metric space affect the result of hierarchical methods.},
  keywords = {{$\warning$}️ Invalid DOI,⛔ No DOI found,Cluster Analysis,Statistics},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2010\\Carlsson et Mémoli - 2010 - Characterization, Stability and Convergence of Hie3.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\MESX564R\\2010-16571-001.html}
}

@inproceedings{caropreseDealingIncompletenessInconsistency2014,
  title = {Dealing with Incompleteness and Inconsistency in {{P2P}} Deductive Databases},
  booktitle = {Proceedings of the 18th {{International Database Engineering}} \& {{Applications Symposium}}},
  author = {Caroprese, Luciano and Zumpano, Ester},
  year = {2014},
  month = jul,
  series = {{{IDEAS}} '14},
  pages = {124--131},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2628194.2628236},
  urldate = {2023-08-08},
  abstract = {This paper proposes a logic framework for modeling the interaction among incomplete and inconsistent deductive databases in a P2P environment. Each peer joining a P2P system provides or imports data from its neighbors by using a set of mapping rules, i.e. a set of semantic correspondences to a set of peers belonging to the same environment. By using mapping rules, as soon as it enters the system, a peer can participate and access all data available in its neighborhood, and through its neighborhood it becomes accessible to all the other peers in the system. Two different types of mapping rules are defined: a first type allowing to import maximal sets of atoms and a second type allowing to import minimal sets of atoms from source peers to target peers. In the proposed setting, each peer can be thought of as a resource used either to enrich (integrate) the knowledge or to fix (repair) the knowledge. The declarative semantics of a P2P system is defined in terms of preferred weak models. An equivalent and alternative characterization of preferred weak model semantics, in terms of prioritized logic programs, is also introduced. The paper also presents preliminary results about complexity of P2P logic queries.},
  isbn = {978-1-4503-2627-8},
  keywords = {nosource}
}

@article{castro-schezHighlyAdaptiveRecommender2011,
  title = {A Highly Adaptive Recommender System Based on Fuzzy Logic for {{B2C}} E-Commerce Portals},
  author = {{Castro-Schez}, Jose Jesus and Miguel, Raul and Vallejo, David and {L{\'o}pez-L{\'o}pez}, Lorenzo Manuel},
  year = {2011},
  month = mar,
  journal = {Expert Systems with Applications},
  volume = {38},
  number = {3},
  pages = {2441--2454},
  issn = {0957-4174},
  doi = {10/d5bz4n},
  urldate = {2019-02-15},
  abstract = {Past years have witnessed a growing interest in e-commerce as a strategy for improving business. Several paradigms have arisen from the e-commerce field in recent years which try to support different business activities, such as B2C and C2C. This paper introduces a prototype of e-commerce portal, called e-Zoco, of which main features are: (i) a catalogue service intended to arrange product categories hierarchically and describe them through sets of attributes, (ii) a product selection service able to deal with imprecise and vague search preferences which returns a set of results clustered in accordance with their potential relevance to the user, and (iii) a rule-based knowledge learning service to provide the users with knowledge about the existing relationships among the attributes that describe a given product category. The portal prototype is supported by a multi-agent infrastructure composed of a set of agents responsible for providing these and other services.},
  keywords = {B2C,C2C,e-Commerce,Fuzzy logic,Product selection,Recommender system,Rule-based knowledge learning,Supervised learning},
  annotation = {00085},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2011\\Castro-Schez et al. - 2011 - A highly adaptive recommender system based on fuzz.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\ZDYGYB9U\\S095741741000816X.html}
}

@article{cebiricSummarizingSemanticGraphs2019,
  title = {Summarizing Semantic Graphs: A Survey},
  shorttitle = {Summarizing Semantic Graphs},
  author = {{\v C}ebiri{\'c}, {\v S}ejla and Goasdou{\'e}, Fran{\c c}ois and Kondylakis, Haridimos and Kotzinos, Dimitris and Manolescu, Ioana and Troullinou, Georgia and Zneika, Mussab},
  year = {2019},
  month = jun,
  journal = {The VLDB Journal},
  volume = {28},
  number = {3},
  pages = {295--327},
  publisher = {Springer},
  issn = {0949-877X},
  doi = {10/gh3fqw},
  urldate = {2021-02-16},
  abstract = {The explosion in the amount of the available RDF data has lead to the need to explore, query and understand such data sources. Due to the complex structure of RDF graphs and their heterogeneity, the exploration and understanding tasks are significantly harder than in relational databases, where the schema can serve as a first step toward understanding the structure. Summarization has been applied to RDF data to facilitate these tasks. Its purpose is to extract concise and meaningful information from RDF knowledge bases, representing their content as faithfully as possible. There is no single concept of RDF summary, and not a single but many approaches to build such summaries; each is better suited for some uses, and each presents specific challenges with respect to its construction. This survey is the first to provide a comprehensive survey of summarization method for semantic RDF graphs. We propose a taxonomy of existing works in this area, including also some closely related works developed prior to the adoption of RDF in the data management community; we present the concepts at the core of each approach and outline their main technical aspects and implementation. We hope the survey will help readers understand this scientifically rich area and identify the most pertinent summarization method for a variety of usage scenarios.},
  langid = {english},
  annotation = {QID: Q99208374},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2019\\Čebirić et al. - 2019 - Summarizing semantic graphs a survey.pdf;C\:\\Users\\nhiot\\OneDrive\\zotero\\2019\\Čebirić et al. - 2019 - Summarizing semantic graphs a survey2.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\94MJ3HZL\\s00778-018-0528-3.html}
}

@inproceedings{chabinConservativeTypeExtensions2013,
  title = {Conservative {{Type Extensions}} for {{XML Data}}},
  booktitle = {Transactions on {{Large-Scale Data-and Knowledge-Centered Systems IX}}},
  author = {Chabin, Jacques and {Halfeld-Ferrari}, Mirian and Musicante, Martin A. and R{\'e}ty, Pierre},
  editor = {Hameurlain, Abdelkader and K{\"u}ng, Josef and Wagner, Roland},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {7980},
  pages = {65--94},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10/gf4bcf},
  urldate = {2019-06-26},
  abstract = {We introduce a method for building a minimal XML type (belonging to standard class of regular tree grammars) as an extension of other given types. Not only do we propose an easy-to-handle XML type evolution method, but we prove that this method computes the smallest extension of a given tree grammar, respecting pre-established constraints. We also adapt our technique to an interactive context, where an advised user is guided to build a new XML type from existing ones. A basic prototype of our tool is implemented.},
  isbn = {978-3-642-40068-1 978-3-642-40069-8},
  langid = {english},
  keywords = {Dependency Graph,Integrity Constraint,Production Rule,Regular Expression,Tree Language},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2013\\Chabin et al. - 2013 - Conservative Type Extensions for XML Data.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\E7SQR2V6\\978-3-642-40069-8_4.html}
}

@article{chabinConsistentUpdatingDatabases2020,
  ids = {CHL19,CHL20},
  title = {Consistent {{Updating}} of {{Databases}} with {{Marked Nulls}}},
  author = {Chabin, Jacques and {Halfeld-Ferrari}, Mirian and Laurent, Dominique},
  year = {2020},
  month = apr,
  journal = {Knowledge and Information Systems (KAIS)},
  volume = {62},
  number = {4},
  pages = {1571--1609},
  issn = {0219-3116},
  doi = {10.1007/s10115-019-01402-w},
  urldate = {2023-07-05},
  abstract = {This paper revisits the problem of consistency maintenance when insertions or deletions are performed on a valid database containing marked nulls. This problem comes back to light in real-world linked data or RDF databases when blank nodes are associated with null values. This paper proposes solutions for the main problems one has to face when dealing with updates and constraints, namely update determinism, minimal change and leanness of an RDF graph instance. The update semantics is formally introduced and the notion of core is used to ensure a database as small as possible (i.e.~~ the RDF graph leanness). Our algorithms allow the use of constraints such as tuple-generating dependencies, offering a way for solving many practical problems.},
  langid = {english},
  optbibsource = {dblp computer science bibliography, https://dblp.org},
  optdoi = {10.1007/s10115-019-01402-w},
  opttimestamp = {Mon, 04 May 2020 13:22:49 +0200},
  keywords = {Constraints,Logical database,nosource,Null values,RDF,TGD,Updates},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2020\\Chabin et al. - 2020 - Consistent Updating of Databases with Marked Nulls.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\4YE6ECDH\\s10115-019-01402-w.html}
}

@inproceedings{chabinContextdrivenQueryingSystem2018,
  title = {A {{Context-driven Querying System}} for {{Urban Graph Analysis}}},
  booktitle = {Proceedings of the 22nd {{International Database Engineering}} \& {{Applications Symposium}} on - {{IDEAS}} 2018},
  author = {Chabin, Jacques and {Gomes-Jr.}, Luiz and {Halfeld-Ferrari}, Mirian},
  year = {2018},
  series = {{{IDEAS}} 2018},
  pages = {297--301},
  publisher = {ACM Press},
  address = {Villa San Giovanni, Italy},
  doi = {10.1145/3216122.3216148},
  urldate = {2018-12-29},
  abstract = {This paper presents a context-driven query system for urban computing where users are responsible for defining their own restrictions over which datalog-like queries are built. Instead of imposing constraints on databases, our goal is to filter consistent data during the query process. Our query language is able to express aggregates in recursive rules, allowing it to capture network properties typical of graph analysis. This paper presents our query system and analyzes its capabilities using use cases in Urban Computing.},
  isbn = {978-1-4503-6527-7},
  langid = {english},
  keywords = {constraints,data graph,data quality,Query language,smart city},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2018\\Chabin et al. - 2018 - A Context-driven Querying System for Urban Graph A.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\MGXJHLF7\\citation.html;C\:\\Users\\nhiot\\Zotero\\storage\\QEF3INHL\\hal-01837921.html}
}

@inproceedings{chabinGraphRewritingRules2020,
  title = {Graph {{Rewriting Rules}} for {{RDF Database Evolution Management}}},
  booktitle = {Proceedings of the 22nd {{International Conference}} on {{Information Integration}} and {{Web-based Applications}} \& {{Services}}},
  author = {Chabin, Jacques and Eichler, C{\'e}dric and {Halfeld-Ferrari}, Mirian and Hiot, Nicolas},
  year = {2020},
  month = nov,
  series = {{{iiWAS}} '20},
  pages = {134--143},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3428757.3429126},
  urldate = {2022-05-13},
  abstract = {This paper introduces SetUp, a theoretical and applied framework for the management of RDF/S database evolution on the basis of graph rewriting rules. Rewriting rules formalize instance or schema changes, ensuring graph's consistency with respect to given constraints. Constraints considered in this paper are a well known variant of RDF/S semantic, but the approach can be adapted to user-defined constraints. Furthermore, SetUp manages updates by ensuring rule applicability through the generation of side-effects: new updates which guarantee that rule application conditions hold. We provide herein formal validation and experimental evaluation of SetUp.},
  isbn = {978-1-4503-8922-8},
  keywords = {Constraints,Database Management,Graph rewriting,me,Update},
  file = {C:\Users\nhiot\OneDrive\zotero\2020\Chabin et al. - 2020 - Graph Rewriting Rules for RDF Database Evolution M.pdf}
}

@article{chabinGraphRewritingRules2021,
  title = {Graph Rewriting Rules for {{RDF}} Database Evolution: Optimizing Side-Effect Processing},
  shorttitle = {Graph Rewriting Rules for {{RDF}} Database Evolution},
  author = {Chabin, Jacques and Eichler, C{\'e}dric and {Halfeld-Ferrari}, Mirian and Hiot, Nicolas},
  year = {2021},
  month = aug,
  journal = {International Journal of Web Information Systems},
  volume = {17},
  number = {6},
  pages = {622--644},
  publisher = {Emerald Publishing Limited},
  doi = {10.1108/IJWIS-03-2021-0033},
  keywords = {me},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2021\\Chabin et al. - 2021 - Graph rewriting rules for RDF database evolution .pdf;C\:\\Users\\nhiot\\Zotero\\storage\\GNKTXUMA\\html.html;C\:\\Users\\nhiot\\Zotero\\storage\\UP65BUJK\\hal-03329965v1.html}
}

@techreport{chabinGraphRewritingSystem2020,
  type = {Research {{Report}}},
  ids = {chabinGraphRewritingSystem2020a},
  title = {Graph {{Rewriting System}} for {{Consistent Evolution}} of {{RDF}}/{{S}} Databases},
  author = {Chabin, Jacques and Eichler, C{\'e}dric and {Halfeld-Ferrari}, Mirian and Hiot, Nicolas},
  year = {2020},
  institution = {LIFO, Universit{\'e} d'Orl{\'e}ans, INSA Centre Val de Loire},
  url = {https://hal.science/hal-02560325},
  urldate = {2023-08-03},
  abstract = {This paper investigates the use of graph rewriting rules to model updates-instance or schema changes-on RDF/S databases which are expected to satisfy RDF intrinsic semantic constraints. Such databases being modeled as knowledge graphs, we propose graph rewriting rules formalizing atomic updates whose application transforms the graph and necessarily preserves its consistency. If an update has to be applied when the application conditions of the corresponding rule do not hold, side-effects are generated: they engender new updates in order to ensure the rule applicability. Our system, SetUp, implements our updating approach for RDF/S data and offers a theoretical and applied framework for ensuring consistency when a RDF knowledge graph evolves.},
  copyright = {All rights reserved},
  keywords = {me},
  file = {C:\Users\nhiot\OneDrive\zotero\2020\Chabin et al. - 2020 - Graph Rewriting System for Consistent Evolution of.pdf}
}

@misc{chabinIncrementalConsistentUpdating2023,
  title = {Incremental {{Consistent Updating}} of {{Incomplete Databases}}},
  author = {Chabin, Jacques and {Halfeld-Ferrari}, Mirian and Hiot, Nicolas and Laurent, Dominique},
  year = {2023},
  month = feb,
  number = {arXiv:2302.06246},
  eprint = {2302.06246},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2302.06246},
  urldate = {2023-08-07},
  abstract = {Efficient consistency maintenance of incomplete and dynamic real-life databases is a quality label for further data analysis. In prior work, we tackled the generic problem of database updating in the presence of tuple generating constraints from a theoretical viewpoint. The current paper considers the usability of our approach by (a) introducing incremental update routines (instead of the previous from-scratch versions) and (b) removing the restriction that limits the contents of the database to fit in the main memory. In doing so, this paper offers new algorithms, proposes queries and data models inviting discussions on the representation of incompleteness on databases. We also propose implementations under a graph database model and the traditional relational database model. Our experiments show that computation times are similar globally but point to discrepancies in some steps.},
  archiveprefix = {arxiv},
  keywords = {⛔ No DOI found,Computer Science - Databases,me,nosource},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2023\\Chabin et al. - 2023 - Incremental Consistent Updating of Incomplete Data.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\WNDR3UKH\\2302.html}
}

@inproceedings{chabinManagingLinkedNulls2023,
  title = {Managing {{Linked Nulls}} in~{{Property Graphs}}: {{Tools}} to~{{Ensure Consistency}} and~{{Reduce Redundancy}}},
  shorttitle = {Managing {{Linked Nulls}} in~{{Property Graphs}}},
  booktitle = {Advances in {{Databases}} and {{Information Systems}}},
  author = {Chabin, Jacques and {Halfeld-Ferrari}, Mirian and Hiot, Nicolas and Laurent, Dominique},
  editor = {Abell{\'o}, Alberto and Vassiliadis, Panos and Romero, Oscar and Wrembel, Robert},
  year = {2023},
  pages = {180--194},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-42914-9_13},
  abstract = {Ensuring the provision of consistent and irredundant data sets remains essential to minimize bugs, promote maintainable application code and obtain dependable results in data analytics. A major challenge in achieving consistency is handling incomplete data, i.e., missing information that may be provided later, comes from the fact that, the use of marked (or linked) nulls is required in many applications to express unknown but connected information. In this context, it is well known that maintaining the data consistent and irredundant is not an easy task. This paper proposes a query-driven incremental maintenance approach for consistent and irredundant incomplete databases. Can graph databases improve the efficiency of this operation? How can graph databases manipulate linked nulls? What is the impact of using graph databases on other essential maintenance operations? This paper presents an innovative approach to answering these questions, highlighting the proposal's strengths and weaknesses and offering avenues for further research.},
  isbn = {978-3-031-42914-9},
  langid = {english},
  keywords = {constraints,graph databases,incomplete data,incremental maintenance,tuple generating dependencies,updates}
}

@article{chabinMinimalExtensionsTree2010,
  title = {Minimal {{Extensions}} of {{Tree Languages}}: {{Application}} to {{XML Schema Evolution}}},
  shorttitle = {Minimal {{Extensions}} of {{Tree Languages}}},
  author = {Chabin, Jacques and {Halfeld-Ferrari}, Mirian and R{\'e}ty, Pierre and Rapport, Orl{\'e}ans and Rr, O},
  year = {2010},
  month = jan,
  abstract = {Rapport de recherche LIFO},
  keywords = {⛔ No DOI found}
}

@inproceedings{chabinMinimalTreeLanguage2010,
  ids = {chabinMinimalTreeLanguage2010a},
  title = {Minimal {{Tree Language Extensions}}: {{A Keystone}} of {{XML Type Compatibility}} and {{Evolution}}},
  shorttitle = {Minimal {{Tree Language Extensions}}},
  booktitle = {Theoretical {{Aspects}} of {{Computing}} -- {{ICTAC}} 2010},
  author = {Chabin, Jacques and {Halfeld-Ferrari}, Mirian and Musicante, Martin A. and R{\'e}ty, Pierre},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Cavalcanti, Ana and Deharbe, David and Gaudel, Marie-Claude and Woodcock, Jim},
  year = {2010},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {6255},
  pages = {60--75},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10/b6hzxg},
  urldate = {2021-09-27},
  abstract = {In this paper, we propose algorithms that extend a given regular tree grammar G0 to a new grammar G respecting the following two properties: (i) G belongs to the sub-class of local or single-type tree grammars and (ii) G is the least grammar (in the sense of language inclusion) that contains the language of G0. Our algorithms give rise to important tools in the context of web service composition or XML schema evolution. We are particularly interested in applying them in order to reconcile different XML type messages among services. The algorithms are proven correct and some of their applications are discussed.},
  isbn = {978-3-642-14807-1 978-3-642-14808-8},
  langid = {english},
  keywords = {Normal Form,Production Rule,Regular Expression,Regular Language,Tree Language},
  file = {C:\Users\nhiot\OneDrive\zotero\2010\Chabin et al. - 2010 - Minimal Tree Language Extensions A Keystone of XM.pdf}
}

@techreport{chabinSpecificationSideeffectManagement2020,
  type = {Research {{Report}}},
  title = {Specification of Side-Effect Management Techniques for Semantic Graph Sanitization},
  author = {Chabin, Jacques and Eichler, C{\'e}dric and {Halfeld-Ferrari}, Mirian and Hiot, Nicolas},
  year = {2020},
  number = {D6},
  institution = {LIFO, Universit{\'e} d'Orl{\'e}ans, INSA Centre Val de Loire},
  url = {https://hal.science/hal-02957974},
  abstract = {The goal of the SENDUP project is to propose anonymisation mechanisms for data organized as graphs with an underlying semantic. Such mechanisms trig- gers updates on the database. This deliverable presents the update approach and side-effect management techniques defined in SENDUP. We focus on updates -instance or schema changes- on RDF/S databases which are expected to satisfy RDF intrinsic semantic constraints. We model RDF/S databases as type graphs and use graph rewriting rules to formalize updates. Such rules define both the effect of a graph transformation and its applicability conditions. We propose 19 rules modelling atomic updates and prove that their application necessarily preserves the database's consistency. If an update has to be applied when the application conditions of the corre- sponding rule do not hold, side-effects are generated: they engender new updates in order to ensure the rule applicability. These techniques are implemented in a dedicated software module S1 called SetUp. This deliverable also presents a preliminary experimental validation and evaluation of SetUp.},
  keywords = {me,nosource},
  file = {C:\Users\nhiot\OneDrive\zotero\2020\Chabin et al. - 2020 - Specification of side-effect management techniques.pdf}
}

@misc{chabinUpdateChase2023,
  title = {{{UpdateChase}}},
  author = {Chabin, Jacques and {Halfeld-Ferrari}, Mirian and Hiot, Dominique and Laurent, Dominique and {Moret-Bailly}, Lucas},
  year = {2023},
  month = jan,
  url = {https://gitlab.com/jacques-chabin/UpdateChase},
  urldate = {2023-07-21},
  abstract = {Impl{\'e}mentation et benchmarks des algorithmes incr{\'e}mentaux pour la mise {\`a} jour coh{\'e}rente d'une base de donn{\'e}es graphe},
  keywords = {me},
  file = {C:\Users\nhiot\Zotero\storage\WPJMI7RA\UpdateChase.html}
}

@techreport{chabinUrbanGraphAnalysis2018,
  type = {Research {{Report}}},
  ids = {chabinUrbanGraphAnalysis},
  title = {Urban {{Graph Analysis}} on a {{Context-driven Querying System}}},
  author = {Chabin, Jacques and {Gomes-Jr}, Luiz and {Halfeld-Ferrari}, Mirian},
  year = {2018},
  pages = {18},
  institution = {LIFO, Universit{\'e} d'Orl{\'e}ans},
  url = {https://hal.archives-ouvertes.fr/hal-01916725},
  urldate = {2018-12-27},
  abstract = {Urban computing intends to provide guidance to solve problems in big cities such as pollution, energy consumption, and human mobility. These are, however, difficult problems with several variables interacting in complex patterns. Such complex systems are often represented as graphs, taking advantage of the flexibility of the model and the availability of network science tools for analysis. In this context, expressive query languages capable of tackling graph analysis problems on a customized context are essential. This paper presents a context-driven query system for urban computing where users are responsible for defining their own restrictions over which datalog-like queries are built. Instead of imposing constraints on databases, our goal is to filter consistent data during the query process. Our query language is able to express aggregates in recursive rules, allowing to it capture network properties typical of graph analysis. This paper presents our query system and analyzes its capabilities using use cases in Urban Computing.},
  langid = {english},
  annotation = {00000},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2018\\Chabin et al. - 2018 - Urban Graph Analysis on a Context-driven Querying .pdf;C\:\\Users\\nhiot\\OneDrive\\zotero\\2018\\Chabin et al. - 2018 - Urban Graph Analysis on a Context-driven Querying 2.pdf}
}

@unpublished{chabinUsingGraphGrammar2019,
  title = {Using a Graph Grammar to Update a {{RDF}}/{{S}} Document},
  author = {Chabin, Jacques and Eichler, C{\'e}dric and {Halfeld-Ferrari}, Mirian},
  year = {2019},
  langid = {english},
  keywords = {nosource}
}

@inproceedings{chandraOptimalImplementationConjunctive1977,
  ids = {CM77},
  title = {Optimal Implementation of Conjunctive Queries in Relational Data Bases},
  booktitle = {Proceedings of the Ninth Annual {{ACM}} Symposium on {{Theory}} of Computing},
  author = {Chandra, Ashok K. and Merlin, Philip M.},
  year = {1977},
  month = may,
  series = {{{STOC}} '77},
  pages = {77--90},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/800105.803397},
  abstract = {We define the class of conjunctive queries in relational data bases, and the generalized join operator on relations. The generalized join plays an important part in answering conjunctive queries, and it can be implemented using matrix multiplication. It is shown that while answering conjunctive queries is NP complete (general queries are PSPACE complete), one can find an implementation that is within a constant of optimal. The main lemma used to show this is that each conjunctive query has a unique minimal equivalent query (much like minimal finite automata).},
  isbn = {978-1-4503-7409-5},
  annotation = {QID: Q56813555},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1977\\Chandra et Merlin - 1977 - Optimal implementation of conjunctive queries in r.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\6D3X624J\\800105.html}
}

@inproceedings{charikarBetterStreamingAlgorithms2003,
  title = {Better {{Streaming Algorithms}} for {{Clustering Problems}}},
  booktitle = {Proceedings of the {{Thirty-fifth Annual ACM Symposium}} on {{Theory}} of {{Computing}}},
  author = {Charikar, Moses and O'Callaghan, Liadan and Panigrahy, Rina},
  year = {2003},
  series = {{{STOC}} '03},
  pages = {30--39},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10/bg89wh},
  urldate = {2019-01-22},
  abstract = {We study clustering problems in the streaming model, where the goal is to cluster a set of points by making one pass (or a few passes) over the data using a small amount of storage space. Our main result is a randomized algorithm for the k--Median problem which produces a constant factor approximation in one pass using storage space O(k poly log n). This is a significant improvement of the previous best algorithm which yielded a 2O(1/{$\varepsilon$}) approximation using O(n{$\varepsilon$}) space. Next we give a streaming algorithm for the k--Median problem with an arbitrary distance function. We also study algorithms for clustering problems with outliers in the streaming model. Here, we give bicriterion guarantees, producing constant factor approximations by increasing the allowed fraction of outliers slightly.},
  isbn = {978-1-58113-674-6},
  keywords = {clustering,k-median,nosource,streaming algorithm},
  annotation = {00000}
}

@misc{ChatetteDatasetGenerator2024,
  ids = {simgusSimGusChatette2024},
  title = {Chatette: {{A}} Dataset Generator for {{Rasa NLU}}},
  shorttitle = {Chatette},
  year = {2024},
  month = mar,
  url = {https://github.com/SimGus/Chatette},
  urldate = {2024-03-25},
  abstract = {A powerful dataset generator for Rasa NLU, inspired by Chatito},
  copyright = {MIT License},
  keywords = {botkit,chatbot,chatbots,chatito,cli,dataset-generation,nlg,nlp,nlu,parsing,python,rasa,rasa-nlu,sentence},
  file = {C:\Users\nhiot\Zotero\storage\DYKEDZD8\1.1.5.html}
}

@article{chaventTroisNouvellesMethodes2003,
  title = {Trois Nouvelles M{\'e}thodes de Classification Automatique de Donn{\'e}es Symboliques de Type Intervalle},
  author = {Chavent, M. and {de A.T. De Carvalho}, F. and Levhevallier, Y. and Verde, R.},
  year = {2003},
  journal = {Revue de Statistique Appliqu{\'e}e},
  pages = {5--29},
  keywords = {⛔ No DOI found,nosource},
  annotation = {00000}
}

@article{chenEntityrelationshipModelUnified1976,
  title = {The Entity-Relationship Model---toward a Unified View of Data},
  author = {Chen, Peter Pin-Shan},
  year = {1976},
  month = mar,
  journal = {ACM Transactions on Database Systems},
  volume = {1},
  number = {1},
  pages = {9--36},
  issn = {0362-5915, 1557-4644},
  doi = {10.1145/320434.320440},
  urldate = {2023-10-27},
  abstract = {A data model, called the entity-relationship model, is proposed. This model incorporates some of the important semantic information about the real world. A special diagrammatic technique is introduced as a tool for database design. An example of database design and description using the model and the diagrammatic technique is given. Some implications for data integrity, information retrieval, and data manipulation are discussed. The entity-relationship model can be used as a basis for unification of different views of data: the network model, the relational model, and the entity set model. Semantic ambiguities in these models are analyzed. Possible ways to derive their views of data from the entity-relationship model are presented.},
  langid = {english},
  keywords = {Data Base Task Group,data definition and manipulation,data integrity and consistency,data models,database design,entity set model,entity-relationship model,logigcal view of data,network model,relational model,semantics of data},
  annotation = {QID: Q54151498},
  file = {C:\Users\nhiot\OneDrive\zotero\1976\Chen - 1976 - The entity-relationship model—toward a unified vie.pdf}
}

@inproceedings{chenKnowledgeExpansionProbabilistic2014,
  title = {Knowledge {{Expansion}} over {{Probabilistic Knowledge Bases}}},
  booktitle = {Proceedings of the 2014 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Chen, Yang and Wang, Daisy Zhe},
  year = {2014},
  series = {{{SIGMOD}} '14},
  pages = {649--660},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10/gfvn2r},
  urldate = {2019-02-14},
  abstract = {Information extraction and human collaboration techniques are widely applied in the construction of web-scale knowledge bases. However, these knowledge bases are often incomplete or uncertain. In this paper, we present ProbKB, a probabilistic knowledge base designed to infer missing facts in a scalable, probabilistic, and principled manner using a relational DBMS. The novel contributions we make to achieve scalability and high quality are: 1) We present a formal definition and a novel relational model for probabilistic knowledge bases. This model allows an efficient SQL-based inference algorithm for knowledge expansion that applies inference rules in batches; 2) We implement ProbKB on massive parallel processing databases to achieve further scalability; and 3) We combine several quality control methods that identify erroneous rules, facts, and ambiguous entities to improve the precision of inferred facts. Our experiments show that ProbKB system outperforms the state-of-the-art inference engine in terms of both performance and quality.},
  isbn = {978-1-4503-2376-5},
  keywords = {databases,knowledge bases,probabilistic reasoning},
  annotation = {00000},
  file = {C:\Users\nhiot\OneDrive\zotero\2014\Chen et Wang - 2014 - Knowledge Expansion over Probabilistic Knowledge B.pdf}
}

@article{chenMiningPatientsNarratives2018,
  title = {Mining {{Patients}}' {{Narratives}} in {{Social Media}} for {{Pharmacovigilance}}: {{Adverse Effects}} and {{Misuse}} of {{Methylphenidate}}},
  shorttitle = {Mining {{Patients}}' {{Narratives}} in {{Social Media}} for {{Pharmacovigilance}}},
  author = {Chen, Xiaoyi and Faviez, Carole and Schuck, St{\'e}phane and {Lillo-Le-Lou{\"e}t}, Agn{\`e}s and Texier, Nathalie and Dahamna, Badisse and Huot, Charles and Foulqui{\'e}, Pierre and Pereira, Suzanne and Leroux, Vincent and Karapetiantz, Pierre and {Guenegou-Arnoux}, Armelle and Katsahian, Sandrine and Bousquet, C{\'e}dric and Burgun, Anita},
  year = {2018},
  month = may,
  journal = {Frontiers in Pharmacology},
  volume = {9},
  pages = {541},
  issn = {1663-9812},
  doi = {10/gdpqt9},
  urldate = {2020-01-24},
  abstract = {Background: The Food and Drug Administration (FDA) in the United States and the European Medicines Agency (EMA) have recognized social media as a new data source to strengthen their activities regarding drug safety. Objective: Our objective in the ADR-PRISM project was to provide text mining and visualization tools to explore a corpus of posts extracted from social media. We evaluated this approach on a corpus of 21 million posts from five patient forums, and conducted a qualitative analysis of the data available on methylphenidate in this corpus. Methods: We applied text mining methods based on named entity recognition and relation extraction in the corpus, followed by signal detection using proportional reporting ratio (PRR). We also used topic modelling based on the Correlated Topic Model to obtain the list of thematics in the corpus and classify the messages based on their topics. Results: We automatically identified 3443 posts about methylphenidate published between 2007 and 2016, among which 61 adverse drug reactions (ADR) were automatically detected. Two pharmacovigilance experts evaluated manually the quality of automatic identification, and a f-measure of 0.57 was reached. Patient's reports were mainly neuro-psychiatric effects. Applying PRR, 67\% of the ADRs were signals, including most of the neuro-psychiatric symptoms but also palpitations. Topic modelling showed that the most represented topics were related to Childhood and Treatment initiation, but also Side effects. Cases of misuse were also identified in this corpus, including recreational use and abuse. Conclusion: Named entity recognition combined with signal detection and topic modelling have demonstrated their complementarity in mining social media data. An in-depth analysis focused on methylphenidate showed that this approach was able to detect potential signals and to provide better understanding of patients' behaviors regarding drugs, including misuse.},
  langid = {english},
  pmid = {29881351},
  keywords = {data mining,Data Mining,drug misuse,drug-related side effects and adverse reactions,Drug-Related Side Effects and Adverse Reactions,methylphenidate,Methylphenidate,natural language processing,Natural Language Processing,pharmacovigilance,Pharmacovigilance,social media,Social Media},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2018\\Chen et al. - 2018 - Mining Patients' Narratives in Social Media for Ph.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\AF6NXPH5\\hal-02103689.html}
}

@techreport{CHHL23,
  title = {Incremental Consistent Updating of Incomplete Databases (Extended Version - Technical Report)},
  author = {Chabin, Jacques and {Halfeld-Ferrari}, Mirian and Hiot, Nicolas and Laurent, Dominique},
  year = {2023},
  institution = {LIFO- Universit{\'e} d'Orl{\'e}ans,},
  url = {https://hal.science/hal-03982841},
  keywords = {nosource},
  file = {C:\Users\nhiot\OneDrive\zotero\2023\Chabin et al. - 2023 - Incremental consistent updating of incomplete data2.pdf}
}

@inproceedings{chiCMTreeMinerMiningBoth2004,
  title = {{{CMTreeMiner}}: {{Mining Both Closed}} and {{Maximal Frequent Subtrees}}},
  shorttitle = {{{CMTreeMiner}}},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Chi, Yun and Yang, Yirong and Xia, Yi and Muntz, Richard R.},
  editor = {Dai, Honghua and Srikant, Ramakrishnan and Zhang, Chengqi},
  year = {2004},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {63--73},
  publisher = {Springer Berlin Heidelberg},
  abstract = {Tree structures are used extensively in domains such as computational biology, pattern recognition, XML databases, computer networks, and so on. One important problem in mining databases of trees is to find frequently occurring subtrees. However, because of the combinatorial explosion, the number of frequent subtrees usually grows exponentially with the size of the subtrees. In this paper, we present CMTreeMiner, a computationally efficient algorithm that discovers all closed and maximal frequent subtrees in a database of rooted unordered trees. The algorithm mines both closed and maximal frequent subtrees by traversing an enumeration tree that systematically enumerates all subtrees, while using an enumeration DAG to prune the branches of the enumeration tree that do not correspond to closed or maximal frequent subtrees. The enumeration tree and the enumeration DAG are defined based on a canonical form for rooted unordered trees--the depth-first canonical form (DFCF). We compare the performance of our algorithm with that of PathJoin, a recently published algorithm that mines maximal frequent subtrees.},
  isbn = {978-3-540-24775-3},
  langid = {english},
  keywords = {closed subtree,enumeration tree,Frequent subtree,maximal subtree,rooted unordered tree},
  annotation = {00000},
  file = {C:\Users\nhiot\OneDrive\zotero\2004\Chi et al. - 2004 - CMTreeMiner Mining Both Closed and Maximal Freque.pdf}
}

@book{chiusanoNewTrendsDatabase2022,
  title = {New {{Trends}} in {{Database}} and {{Information Systems}}: {{ADBIS}} 2022 {{Short Papers}}, {{Doctoral Consortium}} and {{Workshops}}: {{DOING}}, {{K-GALS}}, {{MADEISD}}, {{MegaData}}, {{SWODCH}}, {{Turin}}, {{Italy}}, {{September}} 5--8, 2022, {{Proceedings}}},
  shorttitle = {New {{Trends}} in {{Database}} and {{Information Systems}}},
  editor = {Chiusano, Silvia and Cerquitelli, Tania and Wrembel, Robert and N{\o}rv{\aa}g, Kjetil and Catania, Barbara and {Vargas-Solar}, Genoveva and Zumpano, Ester},
  year = {2022},
  series = {Communications in {{Computer}} and {{Information Science}}},
  volume = {1652},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-15743-1},
  urldate = {2023-09-11},
  isbn = {978-3-031-15742-4 978-3-031-15743-1},
  langid = {english},
  keywords = {artificial intelligence,computational linguistics,computer networks,data mining,databases,embedded systems,graph theory,information retrieval,knowledge-based system,machine learning,Natural Language Processing (NLP),network protocols,query languages,semantics,signal processing,software architecture,software design,software engineering},
  file = {C:\Users\nhiot\OneDrive\zotero\2022\Chiusano et al. - 2022 - New Trends in Database and Information Systems AD.pdf}
}

@article{chomskyAlgebraicTheoryContextfree1963,
  title = {The Algebraic Theory of Context-Free Languages},
  author = {Chomsky, Noam and Sch{\"u}tzenberger, Marcel-Paul},
  year = {1963},
  journal = {Studies in Logic and the Foundations of Mathematics},
  volume = {35},
  pages = {118--161},
  doi = {10.1016/S0049-237X(08)72023-8},
  keywords = {nosource}
}

@article{christiansenSurveyAdaptableGrammars1990,
  title = {A Survey of Adaptable Grammars},
  author = {Christiansen, H.},
  year = {1990},
  month = nov,
  journal = {ACM SIGPLAN Notices},
  volume = {25},
  number = {11},
  pages = {35--44},
  issn = {0362-1340, 1558-1160},
  doi = {10.1145/101356.101357},
  urldate = {2023-09-28},
  langid = {english},
  file = {C:\Users\nhiot\OneDrive\zotero\1990\Christiansen - 1990 - A survey of adaptable grammars.pdf}
}

@inproceedings{chuMapReduceMachineLearning2007,
  title = {Map-{{Reduce}} for {{Machine Learning}} on {{Multicore}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 19},
  author = {Chu, Cheng-tao and Kim, Sang K. and Lin, Yi-an and Yu, Yuanyuan and Bradski, Gary and Olukotun, Kunle and Ng, Andrew Y.},
  editor = {Sch{\"o}lkopf, B. and Platt, J. C. and Hoffman, T.},
  year = {2007},
  pages = {281--288},
  publisher = {MIT Press},
  doi = {10.7551/mitpress/7503.003.0040},
  urldate = {2020-02-18},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2007\\Chu et al. - 2007 - Map-Reduce for Machine Learning on Multicore.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\NLGIQ43L\\3150-map-reduce-for-machine-learning-on-multicore.html}
}

@inproceedings{chuRelationalApproachIncrementally2007,
  title = {A {{Relational Approach}} to {{Incrementally Extracting}} and {{Querying Structure}} in {{Unstructured Data}}},
  booktitle = {Proceedings of the 33rd International Conference on {{Very}} Large Data Bases},
  author = {Chu, Eric and Chen, Akanksha Baid Ting and Naughton, AnHai Doan Jeffrey},
  year = {2007},
  pages = {1045--1056},
  abstract = {There is a growing consensus that it is desirable to query over the structure implicit in unstructured documents, and that ideally this capability should be provided incrementally. However, there is no consensus about what kind of system should be used to support this kind of incremental capability. We explore using a relational system as the basis for a workbench for extracting and querying structure from unstructured data. As a proof of concept, we applied our relational approach to support structured queries over Wikipedia. We show that the data set is always available for some form of querying, and that as it is processed, users can pose a richer set of structured queries. We also provide examples of how we can incrementally evolve our understanding of the data in the context of the relational workbench.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2007\Chu et al. - 2007 - A Relational Approach to Incrementally Extracting .pdf}
}

@inproceedings{ciaramellaCombiningFuzzyLogic2010,
  title = {Combining {{Fuzzy Logic}} and {{Semantic Web}} to {{Enable Situation-Awareness}} in {{Service Recommendation}}},
  booktitle = {Database and {{Expert Systems Applications}}},
  author = {Ciaramella, Alessandro and Cimino, Mario G. C. A. and Marcelloni, Francesco and Straccia, Umberto},
  editor = {Bringas, Pablo Garc{\'i}a and Hameurlain, Abdelkader and Quirchmayr, Gerald},
  year = {2010},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {31--45},
  publisher = {Springer Berlin Heidelberg},
  abstract = {Mobile Internet is rapidly growing and an enormous quantity of resources are currently available. Thus, the common mechanisms used up to now to locate resources, such as browsing and searching, do not look anymore to be effective in helping users in mobility. Indeed, the user's personal information space can be very large, with respect to the limited interaction capabilities of mobile devices. This paper proposes a situation-aware framework for providing personalized resources in a proactive manner. Current situations of the user are inferred by exploiting domain knowledge expressed in terms of ontologies and semantic rules, which are represented in the well-known Web Ontology Language (OWL) and Semantic Web Rule Language (SWRL), respectively. Uncertainty in some contextual rule conditions is handled by defining appropriate linguistic variables through the Fuzzy Control Language (FCL), a standard representation of fuzzy systems for data exchange among different implementations, and adopting a purposely-adapted coding of ontologies and rules. Uncertain conditions bring to infer more than one situation with different certainty degrees: these degrees are used to assign a rank to concurrent situations. Finally, situations are connected to a set of related resources to be recommended to the user.},
  isbn = {978-3-642-15364-8},
  langid = {english},
  keywords = {Context-awareness,Fuzzy Inference System,Mobile Service Recommender,Semantic Rules,Web Ontology},
  annotation = {00000},
  file = {C:\Users\nhiot\OneDrive\zotero\2010\Ciaramella et al. - 2010 - Combining Fuzzy Logic and Semantic Web to Enable S.pdf}
}

@incollection{cimianoOntologyLearningText2006,
  title = {Ontology {{Learning}} from {{Text}}},
  booktitle = {Ontology {{Learning}} and {{Population}} from {{Text}}: {{Algorithms}}, {{Evaluation}} and {{Applications}}},
  editor = {Cimiano, Philipp},
  year = {2006},
  pages = {19--34},
  publisher = {Springer US},
  address = {Boston, MA},
  doi = {10.1007/978-0-387-39252-3_3},
  urldate = {2019-10-22},
  isbn = {978-0-387-39252-3},
  langid = {english},
  keywords = {Formal Concept Analysis,Inductive Logic Programming,Latent Semantic Indexing,Ontology Model,Reverse Engineering},
  file = {C:\Users\nhiot\Zotero\storage\3TDSRW5M\10.html}
}

@inproceedings{cimianoText2Onto2005,
  title = {{{Text2Onto}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Cimiano, Philipp and V{\"o}lker, Johanna},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Montoyo, Andr{\'e}s and Mu{\'n}oz, Rafael and M{\'e}tais, Elisabeth},
  year = {2005},
  month = jun,
  volume = {3513},
  pages = {227--238},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/11428817_21},
  urldate = {2024-02-29},
  abstract = {In this paper we present Text2Onto, a framework for ontology learning from textual resources. Three main features distinguish Text2Onto from our earlier framework TextToOnto as well as other state-of-the-art ontology learning frameworks. First, by representing the learned knowledge at a meta-level in the form of instantiated modeling primitives within a so called Probabilistic Ontology Model (POM), we remain independent of a concrete target language while being able to translate the instantiated primitives into any (reasonably expressive) knowledge representation formalism. Second, user interaction is a core aspect of Text2Onto and the fact that the system calculates a confidence for each learned object allows to design sophisticated visualizations of the POM. Third, by incorporating strategies for data-driven change discovery, we avoid processing the whole corpus from scratch each time it changes, only selectively updating the POM according to the corpus changes instead. Besides increasing efficiency in this way, it also allows a user to trace the evolution of the ontology with respect to the changes in the underlying corpus.},
  isbn = {978-3-540-26031-8 978-3-540-32110-1},
  file = {C:\Users\nhiot\OneDrive\zotero\2005\Cimiano et Völker - 2005 - Text2Onto2.pdf}
}

@misc{cismefHeTOP,
  type = {{texte.portail}},
  title = {{HeTOP}},
  author = {CISMeF},
  url = {https://www.hetop.eu/hetop/fr/?q=\&home},
  urldate = {2019-01-29},
  abstract = {HeTOP a pour objectif de mettre {\`a} disposition des humains et des machines les principales terminologies, ontologies, nomenclatures, dictionnaires, th{\'e}saurus et classifications en sant{\'e}},
  copyright = {CHU de Rouen, toute utilisation partielle ou totale de ce document doit mentionner la source},
  langid = {ar,bg,br,bs,ca,cs,da,de,el,en,es,et,fi,fr,he,hr,hu,is,it,iu,ja,ka,ko,la,lt,lv,mt,nl,no,pl,pt,ro,ru,sk,sl,sr,sv,tr,uk,vi,zh},
  annotation = {00000},
  file = {C:\Users\nhiot\Zotero\storage\3QHWSR82\fr.html}
}

@inproceedings{clancyScalableKnowledgeGraph2019,
  title = {Scalable {{Knowledge Graph Construction}} from {{Text Collections}}},
  booktitle = {Proceedings of the {{Second Workshop}} on {{Fact Extraction}} and {{VERification}} ({{FEVER}})},
  author = {Clancy, Ryan and Ilyas, Ihab F. and Lin, Jimmy},
  year = {2019},
  month = nov,
  pages = {39--46},
  publisher = {Association for Computational Linguistics},
  address = {Hong Kong, China},
  doi = {10/ggwjzk},
  urldate = {2020-05-18},
  abstract = {We present a scalable, open-source platform that ``distills'' a potentially large text collection into a knowledge graph. Our platform takes documents stored in Apache Solr and scales out the Stanford CoreNLP toolkit via Apache Spark integration to extract mentions and relations that are then ingested into the Neo4j graph database. The raw knowledge graph is then enriched with facts extracted from an external knowledge graph. The complete product can be manipulated by various applications using Neo4j's native Cypher query language: We present a subgraph-matching approach to align extracted relations with external facts and show that fact verification, locating textual support for asserted facts, detecting inconsistent and missing facts, and extracting distantly-supervised training data can all be performed within the same framework.},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2019\\Clancy et al. - 2019 - Scalable Knowledge Graph Construction from Text Co.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\ETBKWK4G\\D19-6607.html}
}

@inproceedings{clarkOntologybasedTemporalAnalysis2013,
  title = {Ontology-Based {{Temporal Analysis}} for {{Medical Device Adverse Event-A Use Case Study}} on {{Late Stent Thrombosis}}.},
  booktitle = {{{SWAT4LS}}},
  author = {Clark, Kim and Sharma, Deepak and Qin, Rui and Jiang, Guoqian and Chute, Christopher G},
  year = {2013},
  pages = {12},
  abstract = {In this paper, we show how we have applied the Clinical Narrative Temporal Relation Ontology (CNTRO) and its associated temporal reasoning system (the CNTRO Timeline Library) for automatically identifying, ordering, and calculating the duration of temporal events within adverse event report narratives. The Objective of this research is to evaluate the feasibility of the CNTRO Timeline Library using a real clinical use case application (late stent thrombosis adverse events). Narratives from late stent thrombosis adverse events documented within the Food and Drug Administration's (FDA) Manufacturing and User Facility Device Experience (MAUDE) database were used as a test case. 238 annotated narratives were evaluated using the CNTRO Timeline Library. The CNTRO Timeline Library had a 95.38\% accuracy in correctly ordering events within the narratives. The duration function of the CNTRO Timeline Library was also evaluated and found to have 80\% accuracy in correctly determining the duration of an event across 41 narratives, and 76.6\% accuracy in determining the duration between two given events across 77 narratives. Within this paper is an example of how the durations calculated by the CNTRO Timeline Library can be used to examine therapeutic guidelines. Complaint narratives were separated into two groups based on a long (greater than 6 months) or short (6 months or less) duration of antiplatelet therapy administration. The duration of antiplatelet administration was then compared to the duration between stent implantation and occurrence of late stent thrombosis. The goal of this analysis was to show how the CNTRO ontology and is associated Timeline Library could be used to examine recommendations for length of drug administration. In this use case, the result supports guidance for use of longer antiplatelet therapy. This example validates the CNTRO System's ability to confirm known temporal trends.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2013\Clark et al. - 2013 - Ontology-based Temporal Analysis for Medical Devic.pdf}
}

@article{clarkUseCaseStudy2014,
  title = {A Use Case Study on Late Stent Thrombosis for Ontology-Based Temporal Reasoning and Analysis},
  author = {Clark, Kim and Sharma, Deepak and Qin, Rui and Chute, Christopher G and Tao, Cui},
  year = {2014},
  month = dec,
  journal = {Journal of Biomedical Semantics},
  volume = {5},
  number = {1},
  pages = {49},
  issn = {2041-1480},
  doi = {10/ggjdqg},
  urldate = {2020-01-24},
  abstract = {In this paper, we show how we have applied the Clinical Narrative Temporal Relation Ontology (CNTRO) and its associated temporal reasoning system (the CNTRO Timeline Library) to trend temporal information within medical device adverse event report narratives. 238 narratives documenting occurrences of late stent thrombosis adverse events from the Food and Drug Administration's (FDA) Manufacturing and User Facility Device Experience (MAUDE) database were annotated and evaluated using the CNTRO Timeline Library to identify, order, and calculate the duration of temporal events. The CNTRO Timeline Library had a 95\% accuracy in correctly ordering events within the 238 narratives. 41 narratives included an event in which the duration was documented, and the CNTRO Timeline Library had an 80\% accuracy in correctly determining these durations. 77 narratives included documentation of a duration between events, and the CNTRO Timeline Library had a 76\% accuracy in determining these durations. This paper also includes an example of how this temporal output from the CNTRO ontology can be used to verify recommendations for length of drug administration, and proposes that these same tools could be applied to other medical device adverse event narratives in order to identify currently unknown temporal trends.},
  langid = {english},
  pmcid = {PMC4275934},
  pmid = {25540680},
  keywords = {Antiplatelet Therapy,Resource Description Framework,Stent Implantation,Temporal Information,Temporal Relation},
  annotation = {QID: Q34768875},
  file = {C:\Users\nhiot\OneDrive\zotero\2014\Clark et al. - 2014 - A use case study on late stent thrombosis for onto.pdf}
}

@article{cleuziouOKMExtensionKmoyennes2007,
  title = {{{OKM}} : Une Extension Des k-Moyennes Pour La Recherche de Classes Recouvrantes},
  author = {Cleuziou, G.},
  year = {2007},
  keywords = {⛔ No DOI found,nosource},
  annotation = {00000}
}

@phdthesis{cleuziouStructurationDonneesPar2015,
  type = {Habilitation {\`a} {{Diriger}} Les {{Recherches}}},
  title = {Structuration de Donn{\'e}es Par Apprentissage Non-Supervis{\'e} : Applications Aux Donn{\'e}es Textuelles},
  author = {Cleuziou, G.},
  year = {2015},
  school = {Laboratoire d'Informatique Fondamentale d'Orl{\'e}ans},
  keywords = {nosource},
  annotation = {00000}
}

@article{cluetDesigningOQLAllowing1998,
  ids = {cluetDesigningOQLAllowing1998a},
  title = {Designing {{OQL}}: {{Allowing}} Objects to Be Queried},
  shorttitle = {Designing {{OQL}}},
  author = {Cluet, Sophie},
  year = {1998},
  month = jul,
  journal = {Information Systems},
  volume = {23},
  number = {5},
  pages = {279--305},
  issn = {0306-4379},
  doi = {10/bv4f6m},
  urldate = {2020-10-15},
  abstract = {This paper tells the story of OQL, the standard query language of the Object Database Management Group (ODMG) [30]. The story starts in 1988, at INRIA in the Alta{\"i}r Group.{\ddag} The objective of that group was to develop an object-oriented database system [41]. This objective was reached: in September 1991 the O2 database system started its commercial career as the main product of a company called O2Technology [6]. As opposed to its competitors, O2 featured a full-fledged query language named O2SQL [22]. The story goes on with the creation of the ODMG in 1991 and the adoption of O2SQL as the standard object query language under its new and final name: OQL. During the following years, OQL went through some modifications, the most important of which resulted in OQL 1.2 that offers some level of compliance with SQL92. On top of providing the expressive power of the SQL92 query language [54], OQL allows objects to be queried. This is a claim also supported by the upcoming SQL3. However, due to its adequacy to the object oriented type system and its functional nature, OQL is much simpler to learn, use and implement. A goal of this paper is to demonstrate this. This paper tells about the mistakes and pertinent choices we made while designing and implementing OQL. I hope it also conveys the great pleasure I had to be part of this adventure.},
  langid = {english},
  keywords = {Object-Oriented Database,Query Language},
  file = {C:\Users\nhiot\Zotero\storage\UQZQAIF4\S0306437998000131.html}
}

@article{coddRelationalModelData1970,
  title = {A Relational Model of Data for Large Shared Data Banks},
  author = {Codd, Edgar F.},
  year = {1970},
  journal = {Communications of the ACM},
  volume = {13},
  number = {6},
  pages = {377--387},
  publisher = {ACM New York, NY, USA},
  doi = {10.1145/362384.362685},
  annotation = {QID: Q32061744},
  file = {C:\Users\nhiot\OneDrive\zotero\1970\Codd - 1970 - A relational model of data for large shared data b.pdf}
}

@article{ComputationGreatestRegular2016,
  title = {Computation of the {{Greatest Regular Equivalence}}},
  year = {2016},
  month = jan,
  journal = {Filomat},
  volume = {30},
  number = {1},
  pages = {179--190},
  publisher = {National Library of Serbia},
  issn = {0354-5180},
  doi = {10.2298/FIL1601179S},
  urldate = {2023-10-31},
  abstract = {The notion of social roles is a centerpiece of most sociological theoretical considerations. Regular equivalences were introduced by White and Reitz in [15] as the least restrictive among the most commonly used definitions of equivalence in social network analysis. In this paper we consider a generalisation of this notion to a bipartite case. We define a pair of regular equivalences on a two-mode social network and we provide an algorithm for computing the greatest pair of regular equivalences.},
  langid = {english},
  file = {C:\Users\nhiot\OneDrive\zotero\2016\2016 - Computation of the Greatest Regular Equivalence.pdf}
}

@inproceedings{consoleCopingIncompleteData2020,
  ids = {CGLT20},
  title = {Coping with {{Incomplete Data}}: {{Recent Advances}}},
  shorttitle = {Coping with {{Incomplete Data}}},
  booktitle = {Proceedings of the 39th {{ACM SIGMOD-SIGACT-SIGAI Symposium}} on {{Principles}} of {{Database Systems}}},
  author = {Console, Marco and Guagliardo, Paolo and Libkin, Leonid and Toussaint, Etienne},
  year = {2020},
  month = jun,
  series = {{{PODS}}'20},
  pages = {33--47},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3375395.3387970},
  urldate = {2023-08-03},
  abstract = {Handling incomplete data in a correct manner is a notoriously hard problem in databases. Theoretical approaches rely on the computationally hard notion of certain answers, while practical solutions rely on ad hoc query evaluation techniques based on three-valued logic. Can we find a middle ground, and produce correct answers efficiently? The paper surveys results of the last few years motivated by this question. We re-examine the notion of certainty itself, and show that it is much more varied than previously thought. We identify cases when certain answers can be computed efficiently and, short of that, provide deterministic and probabilistic approximation schemes for them. We look at the role of three-valued logic as used in SQL query evaluation, and discuss the correctness of the choice, as well as the necessity of such a logic for producing query answers.},
  isbn = {978-1-4503-7108-7},
  keywords = {approximate query answering,certain answers,incomplete information,many-valued logics,naive evaluation,nosource,relational databases},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2020\\Console et al. - 2020 - Coping with Incomplete Data Recent Advances.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\KJF3RWAC\\3375395.html}
}

@article{correaExactHeuristicAlgorithms2005,
  title = {Exact and {{Heuristic Algorithms}} for {{Dynamic Tree Simplification}}},
  author = {Correa, Carlos and Marsic, Ivan and Sun, Xiaodong},
  year = {2005},
  month = dec,
  journal = {J. Math. Model. Algorithms},
  volume = {4},
  pages = {331--353},
  doi = {10/fm5v45},
  abstract = {The Tree Knapsack Problem (TKP) is a 0???1 integer programming problem where hierarchy constraints are enforced. If a node is selected for packing into the knapsack, all the ancestor nodes on the path from the root to the selected node are packed as well. One apparent application of this problem is the simplification of computer graphics models. Real applications also use alternative representations of the nodes or whole subtrees, called impostors, to provide simplified trees that are visually acceptable. To account for this simplification, we introduce a generalized TKP, called Exclusive Multiple Choice Tree Knapsack Problem (EMCTKP). We present a dynamic programming algorithm to solve EMCTKP and a heuristic, called Lazy Iterative Arrangement, which reuses previous EMCTKP solutions to solve new instances of the problem. We show that this algorithm and heuristic reduce significantly the computation time of EMCTKP problems when changes in their parameters have spatial and temporal coherence. We also compare our algorithm with commercial integer programming solvers, and show that in our case the computation time grows linearly with the size of the problem tree and the available resources, while for generic IP solvers it is unpredictable and varies over a wide range of values.},
  file = {C:\Users\nhiot\OneDrive\zotero\2005\Correa et al. - 2005 - Exact and Heuristic Algorithms for Dynamic Tree Si.pdf}
}

@inproceedings{corte-realHybridMapreduceModel2014,
  title = {A Hybrid Mapreduce Model for Prolog},
  booktitle = {2014 {{International Symposium}} on {{Integrated Circuits}} ({{ISIC}})},
  author = {{C{\^o}rte-Real}, Joana and Dutra, In{\^e}s and Rocha, Ricardo},
  year = {2014},
  month = dec,
  pages = {340--343},
  publisher = {IEEE},
  issn = {2325-0631},
  doi = {10/ggk86d},
  abstract = {Interest in the Map Reduce programming model has been rekindled by Google in the past 10 years; its popularity is mostly due to the convenient abstraction for parallelization details this framework provides. State-of-the-art systems such as Google's, Hadoop or SAGA often provide added features like a distributed file system, fault tolerance mechanisms, data redundancy and portability to the basic Map Reduce framework. However, these features pose an additional overhead in terms of system performance. In this work, we present a Map Reduce design for Prolog which can potentially take advantage of hybrid parallel environments; this combination allies the easy declarative syntax of logic programming with its suitability to represent and handle multi-relational data due to its first order logic basis. Map Reduce for Prolog addresses efficiency issues by performing load balancing on data with different granularity and allowing for parallelization in shared memory, as well as across machines. In an era where multicore processors have become common, taking advantage of a cluster's full capabilities requires the hybrid use of parallelism.},
  keywords = {Computer architecture,data handling,data redundancy,declarative syntax,distributed file system,fault tolerance mechanisms,first order logic basis,Google,Hadoop,hybrid MapReduce programming model,load balancing,logic programming,Logic programming,multicore processors,multirelational data handling,Parallel processing,parallel programming,portability,Process control,Prolog,PROLOG,resource allocation,Runtime,SAGA,shared memory,shared memory systems},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2014\\Côrte-Real et al. - 2014 - A hybrid mapreduce model for prolog.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\8HHJ3LUG\\7029555.html;C\:\\Users\\nhiot\\Zotero\\storage\\RMS5KJPF\\7029555.html}
}

@inproceedings{cortesSuboptimalGraphMatching2019,
  title = {Sub-Optimal {{Graph Matching}} by {{Node-to-Node Assignment Classification}}},
  booktitle = {Graph-{{Based Representations}} in {{Pattern Recognition}}},
  author = {Cort{\'e}s, Xavier and Conte, Donatello and Serratosa, Francesc},
  editor = {Conte, Donatello and Ramel, Jean-Yves and Foggia, Pasquale},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {35--44},
  publisher = {Springer},
  address = {Cham},
  doi = {10/ghx6m7},
  abstract = {In the recent years, Graph Edit Distance has awaken interest in the scientific community and some new graph-matching algorithms that compute it have been presented. Nevertheless, these algorithms usually cannot be used in real applications due to runtime restrictions. For this reason, other graph-matching algorithms have also been used that compute an approximation of the graph correspondence with lower runtime. Clearly, in a real application, there is a tradeoff between runtime and accuracy. One of the most costly part in these algorithms is the deduction of the node-to-node mapping. We present a new graph-matching algorithm that returns a graph correspondence without the explicit computation of the assignment problem. This is done thanks to a classification of the node-to-node assignment learned in a previous training stage.},
  isbn = {978-3-030-20081-7},
  langid = {english},
  keywords = {Graph Edit Distance,Graph embedding,Graph matching,Node assignment classification},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2019\\Cortés et al. - 2019 - Sub-optimal Graph Matching by Node-to-Node Assignm.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\3426YS6R\\978-3-030-20081-7_4.html}
}

@article{cowieInformationExtraction2000,
  title = {Information Extraction},
  author = {Cowie, Jim and Wilks, Yorick},
  year = {2000},
  journal = {Handbook of Natural Language Processing},
  volume = {56},
  pages = {57},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2000\\Cowie et Wilks - 2000 - Information extraction.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\43IETTRD\\books.html}
}

@article{coxMultimodalNaturalLanguage2001,
  title = {A Multi-Modal Natural Language Interface to an Information Visualization Environment},
  author = {Cox, Kenneth and Grinter, Rebecca E. and Hibino, Stacie L. and Jagadeesan, Lalita Jategaonkar and Mantilla, David},
  year = {2001},
  journal = {International Journal of Speech Technology},
  volume = {4},
  number = {3},
  pages = {297--314},
  publisher = {Springer},
  doi = {10/dv4zq4},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2001\\Cox et al. - 2001 - A multi-modal natural language interface to an inf.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\4VUSZ2CX\\A1011368926479.html}
}

@article{cuencagrauFacetedSearchRDFbased2016,
  title = {Faceted Search over {{RDF-based}} Knowledge Graphs},
  author = {Cuenca Grau, Bernardo and Kharlamov, Evgeny and Zheleznyakov, Dmitriy},
  year = {2016},
  journal = {Journal of Web Semantics},
  volume = {37},
  url = {https://www.cs.ox.ac.uk/publications/publication10608-abstract.html},
  urldate = {2019-02-05},
  abstract = {Knowledge graphs such as Yago and Freebase have become a powerful asset for enhancing search, and are being intensively used in both academia and industry. Many existing knowledge graphs are either available as Linked Open Data, or they can be exported as RDF datasets enhanced with background knowledge in the form of an OWL 2 ontology. Faceted search is the de facto approach for exploratory search in many online applications, and has been recently proposed as a suitable paradigm for querying RDF repositories. In this paper, we provide rigorous theoretical underpinnings for faceted search in the context of RDF-based knowledge graphs enhanced with OWL 2 ontologies. We identify well-defined fragments of SPARQL that can be naturally captured using faceted search as a query paradigm, and establish the computational complexity of answering such queries. We also study the problem of updating faceted interfaces, which is critical for guiding users in the formulation of meaningful queries during exploratory search. We have implemented our approach in a fully-fledged faceted search system, SemFacet, which we have evaluated over the Yago knowledge graph.},
  langid = {british},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2016\\Cuenca Grau et al. - 2016 - Faceted search over RDF-based knowledge graphs.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\YHM4LNHS\\publication10608-abstract.html}
}

@article{cuiTexttoVizAutomaticGeneration2020,
  title = {Text-to-{{Viz}}: {{Automatic Generation}} of {{Infographics}} from {{Proportion-Related Natural Language Statements}}},
  shorttitle = {Text-to-{{Viz}}},
  author = {Cui, Weiwei and Zhang, Xiaoyu and Wang, Yun and Huang, He and Chen, Bei and Fang, Lei and Zhang, Haidong and Lou, Jian-Guan and Zhang, Dongmei},
  year = {2020},
  month = jan,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {26},
  number = {1},
  pages = {906--916},
  issn = {1077-2626, 1941-0506, 2160-9306},
  doi = {10/ggvjbx},
  urldate = {2020-05-10},
  abstract = {Combining data content with visual embellishments, infographics can effectively deliver messages in an engaging and memorable manner. Various authoring tools have been proposed to facilitate the creation of infographics. However, creating a professional infographic with these authoring tools is still not an easy task, requiring much time and design expertise. Therefore, these tools are generally not attractive to casual users, who are either unwilling to take time to learn the tools or lacking in proper design expertise to create a professional infographic. In this paper, we explore an alternative approach: to automatically generate infographics from natural language statements. We first conducted a preliminary study to explore the design space of infographics. Based on the preliminary study, we built a proof-of-concept system that automatically converts statements about simple proportionrelated statistics to a set of infographics with pre-designed styles. Finally, we demonstrated the usability and usefulness of the system through sample results, exhibits, and expert reviews.},
  langid = {english},
  annotation = {QID: Q93070810},
  file = {C:\Users\nhiot\OneDrive\zotero\2020\Cui et al. - 2020 - Text-to-Viz Automatic Generation of Infographics .pdf}
}

@inproceedings{culottaDependencyTreeKernels2004,
  ids = {culottaDependencyTreeKernels2004a,culottaDependencyTreeKernels2004b},
  title = {Dependency Tree Kernels for Relation Extraction},
  booktitle = {Proceedings of the 42nd {{Annual Meeting}} on {{Association}} for {{Computational Linguistics}}},
  author = {Culotta, Aron and Sorensen, Jeffrey},
  year = {2004},
  month = jul,
  series = {{{ACL}} '04},
  pages = {423--429},
  publisher = {Association for Computational Linguistics},
  address = {USA},
  doi = {10/ftsxfs},
  abstract = {We extend previous work on tree kernels to estimate the similarity between the dependency trees of sentences. Using this kernel within a Support Vector Machine, we detect and classify relations between entities in the Automatic Content Extraction (ACE) corpus of news articles. We examine the utility of different features such as Wordnet hypernyms, parts of speech, and entity types, and find that the dependency tree kernel achieves a 20\% F1 improvement over a "bag-of-words" kernel.},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2004\\Culotta et Sorensen - 2004 - Dependency tree kernels for relation extraction.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\CJBX6YVQ\\1218955.html}
}

@incollection{dailleActesTALN20032003,
  title = {Actes de {{TALN}} 2003 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 2003 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Daille, B{\'e}atrice},
  year = {2003},
  month = jun,
  publisher = {IRIN / ATALA},
  address = {Batz-sur-mer},
  keywords = {nosource}
}

@book{daleHandbookNaturalLanguage2000,
  title = {Handbook of {{Natural Language Processing}}},
  author = {Dale, Robert and Moisl, Hermann and Somers, Harold},
  year = {2000},
  month = jul,
  publisher = {CRC Press},
  abstract = {This study explores the design and application of natural language text-based processing systems, based on generative linguistics, empirical copus analysis, and artificial neural networks. It emphasizes the practical tools to accommodate the selected system.},
  googlebooks = {VoOLvxyX0BUC},
  isbn = {978-0-8247-9000-4},
  langid = {english},
  keywords = {Business \& Economics / Statistics,Computers / Databases / General,Computers / General,Technology \& Engineering / Automation}
}

@phdthesis{dallouxFouilleTexteExtraction2020,
  type = {Theses},
  title = {Fouille de Texte et Extraction d'informations Dans Les Donn{\'e}es Cliniques},
  author = {Dalloux, Cl{\'e}ment},
  year = {2020},
  month = dec,
  number = {2020REN1S050},
  url = {https://hal.archives-ouvertes.fr/tel-03081563},
  urldate = {2021-04-22},
  school = {Universit{\'e} Rennes 1},
  keywords = {{\'E}tiquetage de s{\'e}quence,Classification multi-{\'E}tiquette,Clinical narratives,Incertitude,Multi-Label classification,N{\'e}gation,Negation,NLP,Sequence labeling,Speculation,TALN,Textes cliniques},
  file = {C:\Users\nhiot\OneDrive\zotero\2020\Dalloux - 2020 - Fouille de texte et extraction d'informations dans.pdf}
}

@inproceedings{daltonEntityQueryFeature2014,
  title = {Entity {{Query Feature Expansion Using Knowledge Base Links}}},
  booktitle = {Proceedings of the 37th {{International ACM SIGIR Conference}} on {{Research}} \& {{Development}} in {{Information Retrieval}}},
  author = {Dalton, Jeffrey and Dietz, Laura and Allan, James},
  year = {2014},
  series = {{{SIGIR}} '14},
  pages = {365--374},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10/gfwbtv},
  urldate = {2019-03-05},
  abstract = {Recent advances in automatic entity linking and knowledge base construction have resulted in entity annotations for document and query collections. For example, annotations of entities from large general purpose knowledge bases, such as Freebase and the Google Knowledge Graph. Understanding how to leverage these entity annotations of text to improve ad hoc document retrieval is an open research area. Query expansion is a commonly used technique to improve retrieval effectiveness. Most previous query expansion approaches focus on text, mainly using unigram concepts. In this paper, we propose a new technique, called entity query feature expansion (EQFE) which enriches the query with features from entities and their links to knowledge bases, including structured attributes and text. We experiment using both explicit query entity annotations and latent entities. We evaluate our technique on TREC text collections automatically annotated with knowledge base entity links, including the Google Freebase Annotations (FACC1) data. We find that entity-based feature expansion results in significant improvements in retrieval effectiveness over state-of-the-art text expansion approaches.},
  isbn = {978-1-4503-2257-7},
  keywords = {entities,information extraction,information retrieval,ontologies},
  annotation = {00000  QID: Q115619004},
  file = {C:\Users\nhiot\OneDrive\zotero\2014\Dalton et al. - 2014 - Entity Query Feature Expansion Using Knowledge Bas.pdf}
}

@article{darmoniMLPubMedBaseDonnees,
  title = {{{MLPubMed}}: Une Base de Donn{\'e}es Bibliographique Multi-Lingue},
  shorttitle = {{{MLPubMed}}},
  author = {Darmoni, St{\'e}fan J. and Soualmia, Lina F. and Griffon, Nicolas and Grosjean, Julien and Kerdelhu{\'e}, Ga{\'e}tan and Kergourlay, Ivan and Thirion, Benoit and Dahamna, Badisse},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\_\Darmoni et al. - MLPubMed une base de données bibliographique mult.pdf}
}

@article{darwicheGraphEditDistance2020,
  ids = {darwicheGraphEditDistance2020a,darwicheGraphEditDistance2020b},
  title = {Graph Edit Distance: {{Accuracy}} of Local Branching from an Application Point of View},
  shorttitle = {Graph Edit Distance},
  author = {Darwiche, Mostafa and Conte, Donatello and Raveaux, Romain and T'Kindt, Vincent},
  year = {2020},
  month = jun,
  journal = {Pattern Recognition Letters},
  series = {Applications of {{Graph-based Techniques}} to {{Pattern Recognition}}},
  volume = {134},
  pages = {20--28},
  issn = {0167-8655},
  doi = {10/gh44pr},
  urldate = {2021-02-23},
  abstract = {In the context of graph-based representations, comparing and measuring the dissimilarities between graphs can be done by solving the Graph Edit Distance (GED) problem. It is well known and embedded in many application fields such as Computer Vision and Cheminformatics. GED is a NP-hard minimization problem, therefore the optimal solution cannot be found in reasonable time. The GED problem has been addressed by exact approaches like Mixed Integer Linear Programs (MILP) formulations and heuristic approaches like beam-search, bipartite graph matching among others. Recently, a novel heuristic, called local branching (LocBra) for the GED problem, has been proposed and shown to be efficient. In this work, the focus is on evaluating LocBra with other competitive heuristics available in the literature from an application point of view. Moreover, it tries to answer the following question: is it important to compute an accurate GED regarding the final applications? Similarity search and graph matching are considered as final applications. Three experiments are conducted to evaluate the accuracy and efficiency of the heuristics. The quality of the obtained solutions and matching w.r.t. optimal solutions and ground-truth matching is studied. The results of those experiments show that LocBra has a high correlation with the optimal solutions and the ground-truth matching.},
  langid = {english},
  keywords = {Application of graph edit distance,Graph edit distance,Graph matching,Local Branching Heuristic},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2020\\Darwiche et al. - 2020 - Graph edit distance Accuracy of local branching f.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\JW3DY84U\\S0167865518301119.html}
}

@article{DataflowProgramming2018,
  title = {Dataflow Programming},
  year = {2018},
  month = dec,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Dataflow\_programming\&oldid=875727325},
  urldate = {2019-01-23},
  abstract = {In computer programming, dataflow programming is a programming paradigm that models a program as a directed graph of the data flowing between operations, thus implementing dataflow principles and architecture. Dataflow programming languages share some features of functional languages, and were generally developed in order to bring some functional concepts to a language more suitable for numeric processing. Some authors use the term datastream instead of dataflow to avoid confusion with dataflow computing or dataflow architecture, based on an indeterministic machine paradigm. Dataflow programming was pioneered by Jack Dennis and his graduate students at MIT in the 1960s.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {00000  Page Version ID: 875727325},
  file = {C:\Users\nhiot\Zotero\storage\ERMM9BIX\index.html}
}

@article{davisAlgorithm1000SuiteSparse2019,
  title = {Algorithm 1000: {{SuiteSparse}}:{{GraphBLAS}}: {{Graph Algorithms}} in the {{Language}} of {{Sparse Linear Algebra}}},
  shorttitle = {Algorithm 1000},
  author = {Davis, Timothy A.},
  year = {2019},
  month = dec,
  journal = {ACM Transactions on Mathematical Software},
  volume = {45},
  number = {4},
  pages = {44:1--44:25},
  issn = {0098-3500},
  doi = {10/gmns68},
  urldate = {2021-09-01},
  abstract = {SuiteSparse:GraphBLAS is a full implementation of the GraphBLAS standard, which defines a set of sparse matrix operations on an extended algebra of semirings using an almost unlimited variety of operators and types. When applied to sparse adjacency matrices, these algebraic operations are equivalent to computations on graphs. GraphBLAS provides a powerful and expressive framework for creating graph algorithms based on the elegant mathematics of sparse matrix operations on a semiring. An overview of the GraphBLAS specification is given, followed by a description of the key features and performance of its implementation in the SuiteSparse:GraphBLAS package.},
  keywords = {Graph algorithms,GraphBLAS,sparse matrices},
  file = {C:\Users\nhiot\OneDrive\zotero\2019\Davis - 2019 - Algorithm 1000 SuiteSparseGraphBLAS Graph Algor.pdf}
}

@incollection{davisProductionRulesRepresentation1985,
  title = {Production {{Rules}} as a {{Representation}} for a {{Knowledge-Based Consultation Program}}},
  booktitle = {Computer-{{Assisted Medical Decision Making}}},
  author = {Davis, Randall and Buchanan, Bruce and Shortliffe, Edward},
  editor = {Reggia, James A. and Tuhrim, Stanley},
  year = {1985},
  series = {Computers and {{Medicine}}},
  pages = {3--37},
  publisher = {Springer New York},
  address = {New York, NY},
  doi = {10.1007/978-1-4612-5108-8_1},
  urldate = {2019-01-29},
  abstract = {The MYCIN system has begun to exhibit a high level of performance as a consultant on the difficult task of selecting antibiotic therapy for bacteremia. This report discusses issues of representation and design for the system. We describe the basic task and document the constraints involved in the use of a program as a consultant. The control structure and knowledge representation of the system are examined in this light, and special attention is given to the impact of production rules as a representation. The extent of the domain independence of the approach is also examined.},
  isbn = {978-1-4612-5108-8},
  langid = {english},
  keywords = {Certainty Factor,Consultation System,Explanation Capability,Knowledge Base,nosource,Production Rule},
  annotation = {00000}
}

@article{dea.t.decarvalhoFuzziCmeansClustering2005,
  title = {Fuzzi C-Means Clustering Methods for Symbolic Interval Data},
  author = {{de A.T. De Carvalho}, F.},
  year = {2005},
  keywords = {⛔ No DOI found,nosource},
  annotation = {00000}
}

@article{dea.t.decarvalhoPartitioningFuzzyClustering2012,
  title = {Partitioning Fuzzy Clustering Algorithms for Interval-Valued Data Based on {{Hausdorff}} Distances},
  author = {{de A.T. De Carvalho}, F. and T. Pimentel, J},
  year = {2012},
  keywords = {⛔ No DOI found,nosource},
  annotation = {00000}
}

@misc{DeclareActionsEmail,
  title = {Declare {{Actions}} {\textbar} {{Email Markup}}},
  journal = {Google Developers},
  url = {https://developers.google.com/gmail/markup/actions/declaring-actions},
  urldate = {2019-01-29},
  langid = {english},
  annotation = {00000},
  file = {C:\Users\nhiot\Zotero\storage\R2Y726D7\declaring-actions.html}
}

@misc{DeepLearningNLP,
  title = {Deep {{Learning}}, {{NLP}}, and {{Representations}} - Colah's Blog},
  url = {https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/},
  urldate = {2020-04-04},
  file = {C:\Users\nhiot\Zotero\storage\YHVT5H6Q\2014-07-NLP-RNNs-Representations.html}
}

@inproceedings{degiacomoDealingInconsistenciesIncompleteness2009,
  title = {Dealing with Inconsistencies and Incompleteness in Database Update (Position Paper)},
  booktitle = {Inconsistency and {{Incompleteness}} in {{Databases}}},
  author = {De Giacomo, Giuseppe and Lenzerini, Maurizio and Poggi, Antonella and Rosati, Riccardo},
  year = {2009},
  month = aug,
  pages = {97},
  url = {https://www.academia.edu/61920314/Dealing\_with\_inconsistencies\_and\_incompleteness\_in\_database\_update\_position\_paper\_},
  urldate = {2024-01-08},
  abstract = {Several areas of research and various application domains have been concerned in the last years with the problem of dealing with incomplete databases. Data integration as well as the Semantic Web are notable examples. Surprisingly, while many research efforts have been focusing on several interesting issues related to incomplete databases, as query answering, not much investigation have been done concerning updates. In this position paper we aim at highlighting some of the issues we are dealing with in our work on updates over incomplete databases. Instance level updates under constraints Our interest in this area stems mainly from the need to deal with updates in Description Logics based ontologies. Description logics (DLs) are logics for expressing the conceptual knowledge about a domain in terms of classes and associations between them [1]. Such logics are currently considered among the most promising formalisms for representing ontologies by the Semantic Web community [2]. DL-based ontologies are often used for accessing data stored in a data layer by means of query answering.},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2009\De Giacomo et al. - 2009 - Dealing with inconsistencies and incompleteness in2.pdf}
}

@inproceedings{degiacomoPracticalUpdateManagement2017,
  title = {Practical {{Update Management}} in {{Ontology-Based Data Access}}},
  booktitle = {The {{Semantic Web}} -- {{ISWC}} 2017},
  author = {De Giacomo, Giuseppe and Lembo, Domenico and Oriol, Xavier and Savo, Domenico Fabio and Teniente, Ernest},
  editor = {{d'Amato}, Claudia and Fernandez, Miriam and Tamma, Valentina and Lecue, Freddy and {Cudr{\'e}-Mauroux}, Philippe and Sequeda, Juan and Lange, Christoph and Heflin, Jeff},
  year = {2017},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {225--242},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-68288-4_14},
  abstract = {Ontology-based Data Access (OBDA) is gaining importance both scientifically and practically. However, little attention has been paid so far to the problem of updating OBDA systems. This is an essential issue if we want to be able to cope with modifications of data both at the ontology and at the source level, while maintaining the independence of the data sources. In this paper, we propose mechanisms to properly handle updates in this context. We show that updating data both at the ontology and source level is first-order rewritable. We also provide a practical implementation of such updating mechanisms based on non-recursive Datalog.},
  isbn = {978-3-319-68288-4},
  langid = {english},
  file = {C:\Users\nhiot\OneDrive\zotero\2017\De Giacomo et al. - 2017 - Practical Update Management in Ontology-Based Data.pdf}
}

@inproceedings{delliboviKnowledgeBaseUnification2015,
  title = {Knowledge {{Base Unification}} via {{Sense Embeddings}} and {{Disambiguation}}},
  booktitle = {Proceedings of the 2015 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Delli Bovi, Claudio and Espinosa Anke, Luis and Navigli, Roberto},
  year = {2015},
  month = sep,
  pages = {726--736},
  publisher = {Association for Computational Linguistics},
  address = {Lisbon, Portugal},
  doi = {10/ggh7wb},
  urldate = {2020-01-21},
  abstract = {We present KB-UNIFY, a novel approach for integrating the output of different Open Information Extraction systems into a single unified and fully disambiguated knowledge repository. KB-UNIFY consists of three main steps: (1) disambiguation of relation argument pairs via a sensebased vector representation and a large unified sense inventory; (2) ranking of semantic relations according to their degree of specificity; (3) cross-resource relation alignment and merging based on the semantic similarity of domains and ranges. We tested KB-UNIFY on a set of four heterogeneous knowledge bases, obtaining high-quality results. We discuss and provide evaluations at each stage, and release output and evaluation data for the use and scrutiny of the community1.},
  langid = {english},
  file = {C:\Users\nhiot\OneDrive\zotero\2015\Delli Bovi et al. - 2015 - Knowledge Base Unification via Sense Embeddings an.pdf}
}

@article{delliboviLargeScaleInformationExtraction2015,
  title = {Large-{{Scale Information Extraction}} from {{Textual Definitions}} through {{Deep Syntactic}} and {{Semantic Analysis}}},
  author = {Delli Bovi, Claudio and Telesca, Luca and Navigli, Roberto},
  year = {2015},
  month = dec,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {3},
  pages = {529--543},
  issn = {2307-387X},
  doi = {10/ggd39f},
  abstract = {We present DefIE, an approach to large-scale Information Extraction (IE) based on a syntactic-semantic analysis of textual definitions. Given a large corpus of definitions we leverage syntactic dependencies to reduce data sparsity, then disambiguate the arguments and content words of the relation strings, and finally exploit the resulting information to organize the acquired relations hierarchically. The output of DefIE is a high-quality knowledge base consisting of several million automatically acquired semantic relations.},
  langid = {english},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2015\\Delli Bovi et al. - 2015 - Large-Scale Information Extraction from Textual De.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\3DK69H7U\\tacl_a_00156.html;C\:\\Users\\nhiot\\Zotero\\storage\\HAWGQSKT\\63071.html}
}

@inproceedings{delpeuchNaturalLanguageRDF2014,
  title = {From {{Natural Language}} to {{RDF Graphs}} with {{Pregroups}}},
  booktitle = {{{EACL}}: {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Delpeuch, Antonin and Preller, Anne},
  year = {2014},
  month = apr,
  pages = {55--62},
  publisher = {EACL},
  doi = {10/gf4chv},
  urldate = {2019-06-27},
  abstract = {We define an algorithm translating natural language sentences to the formal syntax of RDF, an existential conjunctive logic widely used on the Semantic Web. Our translation is based on pregroup grammars, an efficient type-logical grammatical framework with a transparent syntax-semantics interface. We introduce a restricted notion of side effects in the semantic category of finitely generated free semimodules over \{0, 1\} to that end. The translation gives an intensional counterpart to previous extensional models. We establish a one-to-one correspondence between extensional models and RDF models such that satisfaction is preserved. Our translation encompasses the expressivity of the target language and supports complex linguistic constructions like relative clauses and unbounded dependencies.},
  langid = {english},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2014\\Delpeuch et Preller - 2014 - From Natural Language to RDF Graphs with Pregroups.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\YZ6M38ZR\\lirmm-00992381.html}
}

@article{delpeuchOpenTapiocaLightweightEntity2020,
  title = {{{OpenTapioca}}: {{Lightweight Entity Linking}} for {{Wikidata}}},
  shorttitle = {{{OpenTapioca}}},
  author = {Delpeuch, Antonin},
  year = {2020},
  month = nov,
  journal = {arXiv:1904.09131 [cs]},
  eprint = {1904.09131},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1904.09131},
  urldate = {2021-03-19},
  abstract = {We propose a simple Named Entity Linking system that can be trained from Wikidata only. This demonstrates the strengths and weaknesses of this data source for this task and provides an easily reproducible baseline to compare other systems against. Our model is lightweight to train, to run and to keep synchronous with Wikidata in real time.},
  archiveprefix = {arxiv},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Information Retrieval},
  annotation = {QID: Q63278360},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2020\\Delpeuch - 2020 - OpenTapioca Lightweight Entity Linking for Wikida.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\GPLDSZS2\\1904.html}
}

@inproceedings{demirGeneratingTextualSummaries2008,
  title = {Generating {{Textual Summaries}} of {{Bar Charts}}},
  booktitle = {Proceedings of the {{Fifth International Natural Language Generation Conference}}},
  author = {Demir, Seniz and Carberry, Sandra and McCoy, Kathleen},
  year = {2008},
  month = jun,
  pages = {7--15},
  publisher = {Association for Computational Linguistics},
  address = {Salt Fork, Ohio, USA},
  url = {https://www.aclweb.org/anthology/W08-1103},
  urldate = {2020-05-10},
  file = {C:\Users\nhiot\OneDrive\zotero\2008\Demir et al. - 2008 - Generating Textual Summaries of Bar Charts.pdf}
}

@article{deprilAlgorithmsAdditiveClustering2008,
  title = {Algorithms for Additive Clustering of Rectangular Data Tables},
  author = {Depril, D. and Van Mechelen, I. and Mirkin, B.},
  year = {2008},
  journal = {Computational Statistics and Data Analysis},
  pages = {4923--4938},
  doi = {10/dhgf5p},
  keywords = {nosource},
  annotation = {00020}
}

@inproceedings{deshpandeBiomedicalDataCategorization2019,
  ids = {deshpandeBiomedicalDataCategorization,deshpandeBiomedicalDataCategorizationa},
  title = {Biomedical {{Data Categorization}} and {{Integration}} Using {{Human-in-the-loop Approach}}},
  booktitle = {{{PhD}}@{{VLDB}}},
  author = {Deshpande, Priya},
  year = {2019},
  pages = {4},
  abstract = {Digitized world demands data integration systems that combine data repositories from multiple data sources. Vast amounts of existing clinical and biomedical research data are considered a primary force enabling data-driven research toward advancing health research and for introducing efficiencies in healthcare delivery. Datadriven research may have many goals, including but not limited to improved diagnostics processes, novel biomedical discoveries, epidemiology, and education. However, finding and gaining access to relevant data remains an elusive goal. We identified different data integration challenges and developed an Integrated Radiology Image Search (IRIS) framework that could be a step toward aiding data-driven research. We propose building a biomedical data categorization and integration framework using human-in-the-loop and developing data bridges to support search and retrieval of relevant documents from the integrated repository. My research focuses on biomedical data integration, indexing systems, and providing relevance-ranked document retrieval from an integrated repository. Although we currently focus on integrating biomedical data sources (for medical professionals), we believe that our proposed framework and methodologies can be used in other domains as well. PVLDB Reference Format: Priya Deshpande. Biomedical Data Categorization and Integration using Human-in-the-loop Approach.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2019\Deshpande - 2019 - Biomedical Data Categorization and Integration usi.pdf}
}

@inproceedings{devlinBERTPretrainingDeep2019,
  ids = {devlinBERTPretrainingDeep2019a},
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  booktitle = {Proceedings of {{NAACL-HLT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  eprint = {1810.04805},
  primaryclass = {cs},
  pages = {4171--4186},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1810.04805},
  urldate = {2024-03-20},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\nhiot\OneDrive\zotero\2019\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform2.pdf}
}

@techreport{dewittImplementationTechniquesMain1984,
  type = {Technical {{Report}}},
  title = {Implementation {{Techniques}} for {{Main Memory Database Systems}}},
  author = {DeWitt, David and Katz, Randy H. and Olken, Frank and Shapiro, Leonard D. and Stonebraker, Michael R. and Wood, David A.},
  year = {1984},
  institution = {University of Wisconsin-Madison Department of Computer Sciences},
  url = {https://minds.wisconsin.edu/handle/1793/58496},
  urldate = {2019-01-17},
  langid = {english},
  annotation = {00000},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1984\\DeWitt et al. - 1984 - Implementation Techniques for Main Memory Database.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\NHW2XDQV\\58496.html}
}

@inproceedings{dhamdhereAnalyzaExploringData2017,
  title = {Analyza: {{Exploring Data}} with {{Conversation}}},
  shorttitle = {Analyza},
  booktitle = {Proceedings of the 22nd {{International Conference}} on {{Intelligent User Interfaces}}},
  author = {Dhamdhere, Kedar and McCurley, Kevin S. and Nahmias, Ralfi and Sundararajan, Mukund and Yan, Qiqi},
  year = {2017},
  month = mar,
  series = {{{IUI}} '17},
  pages = {493--504},
  publisher = {Association for Computing Machinery},
  address = {Limassol, Cyprus},
  doi = {10/ggcn2f},
  urldate = {2020-05-14},
  abstract = {We describe Analyza, a system that helps lay users explore data. Analyza has been used within two large real world systems. The first is a question-and-answer feature in a spreadsheet product. The second provides convenient access to a revenue/inventory database for a large sales force. Both user bases consist of users who do not necessarily have coding skills, demonstrating Analyza's ability to democratize access to data. We discuss the key design decisions in implementing this system. For instance, how to mix structured and natural language modalities, how to use conversation to disambiguate and simplify querying, how to rely on the ``semantics' of the data to compensate for the lack of syntactic structure, and how to efficiently curate the data.},
  isbn = {978-1-4503-4348-0},
  keywords = {exploratory data analysis,natural language},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2017\\Dhamdhere et al. - 2017 - Analyza Exploring Data with Conversation.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\ZZ9AX66M\\3025171.html}
}

@incollection{diasActesTALN20152015,
  title = {Actes de {{TALN}} 2015 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 2015 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Dias, Ga{\"e}l},
  year = {2015},
  month = jun,
  publisher = {HULTECH / ATALA},
  address = {Caen},
  keywords = {nosource}
}

@misc{DiscoverNewAPIs,
  title = {Discover {{New APIs}}},
  journal = {ProgrammableWeb},
  url = {https://www.programmableweb.com/apis/directory},
  urldate = {2019-01-23},
  abstract = {View our API Directory, the largest Application Programming Interface repository on the web},
  langid = {english},
  annotation = {00000},
  file = {C:\Users\nhiot\Zotero\storage\4YKIX2DT\directory.html}
}

@inproceedings{doCOMASystemFlexible2002,
  title = {{{COMA}}: {{A System}} for {{Flexible Combination}} of {{Schema Matching Approaches}}},
  shorttitle = {{{COMA}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Very Large Data Bases}}},
  author = {Do, Hong-Hai and Rahm, Erhard},
  year = {2002},
  series = {{{VLDB}} '02},
  pages = {610--621},
  publisher = {VLDB Endowment},
  address = {Hong Kong, China},
  doi = {10/b73xbn},
  urldate = {2019-04-05},
  abstract = {Schema matching is the task of finding semantic correspondences between elements of two schemas. It is needed in many database applications, such as integration of web data sources, data warehouse loading and XML message mapping. To reduce the amount of user effort as much as possible, automatic approaches combining several match techniques are required. While such match approaches have found considerable interest recently, the problem of how to best combine different match algorithms still requires further work. We have thus developed the COMA schema matching system as a platform to combine multiple matchers in a flexible way. We provide a large spectrum of individual matchers, in particular a novel approach aiming at reusing results from previous match operations, and several mechanisms to combine the results of matcher executions. We use COMA as a framework to comprehensively evaluate the effectiveness of different matchers and their combinations for real-world schemas. The results obtained so far show the superiority of combined match approaches and indicate the high value of reuse-oriented strategies.},
  annotation = {00000},
  file = {C:\Users\nhiot\OneDrive\zotero\2002\Do et Rahm - 2002 - COMA A System for Flexible Combination of Schema .pdf}
}

@inproceedings{dominguez-salDiscussionDesignGraph2011,
  title = {A {{Discussion}} on the {{Design}} of {{Graph Database Benchmarks}}},
  booktitle = {Performance {{Evaluation}}, {{Measurement}} and {{Characterization}} of {{Complex Systems}}},
  author = {{Dominguez-Sal}, David and {Martinez-Bazan}, Norbert and {Muntes-Mulero}, Victor and Baleta, Pere and {Larriba-Pey}, Josep Lluis},
  editor = {Nambiar, Raghunath and Poess, Meikel},
  year = {2011},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {25--40},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10/fgz4sf},
  abstract = {Graph Database Management systems (GDBs) are gaining popularity. They are used to analyze huge graph datasets that are naturally appearing in many application areas to model interrelated data. The objective of this paper is to raise a new topic of discussion in the benchmarking community and allow practitioners having a set of basic guidelines for GDB benchmarking. We strongly believe that GDBs will become an important player in the market field of data analysis, and with that, their performance and capabilities will also become important. For this reason, we discuss those aspects that are important from our perspective, i.e. the characteristics of the graphs to be included in the benchmark, the characteristics of the queries that are important in graph analysis applications and the evaluation workbench.},
  isbn = {978-3-642-18206-8},
  langid = {english},
  keywords = {Betweenness Centrality,Large Graph,Resource Description Framework,Social Network Analysis,SPARQL Query},
  file = {C:\Users\nhiot\OneDrive\zotero\2011\Dominguez-Sal et al. - 2011 - A Discussion on the Design of Graph Database Bench.pdf}
}

@inproceedings{dominguez-salSurveyGraphDatabase2010,
  title = {Survey of {{Graph Database Performance}} on the {{HPC Scalable Graph Analysis Benchmark}}},
  booktitle = {Web-{{Age Information Management}}},
  author = {{Dominguez-Sal}, D. and {Urb{\'o}n-Bayes}, P. and {Gim{\'e}nez-Va{\~n}{\'o}}, A. and {G{\'o}mez-Villamor}, S. and {Mart{\'i}nez-Baz{\'a}n}, N. and {Larriba-Pey}, J. L.},
  editor = {Shen, Heng Tao and Pei, Jian and {\"O}zsu, M. Tamer and Zou, Lei and Lu, Jiaheng and Ling, Tok-Wang and Yu, Ge and Zhuang, Yi and Shao, Jie},
  year = {2010},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {37--48},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10/cbpp2z},
  abstract = {The analysis of the relationship among data entities has lead to model them as graphs. Since the size of the datasets has significantly grown in the recent years, it has become necessary to implement efficient graph databases that can load and manage these huge datasets.In this paper, we evaluate the performance of four of the most scalable native graph database projects (Neo4j, Jena, HypergraphDB and DEX). We implement the full HPC Scalable Graph Analysis Benchmark, and we test the performance of each database for different typical graph operations and graph sizes, showing that in their current development status, DEX and Neo4j are the most efficient graph databases.},
  isbn = {978-3-642-16720-1},
  langid = {english},
  keywords = {Graph Database,Large Graph,Query Execution,Resource Description Framework,Small Graph},
  file = {C:\Users\nhiot\OneDrive\zotero\2010\Dominguez-Sal et al. - 2010 - Survey of Graph Database Performance on the HPC Sc.pdf}
}

@inproceedings{dongKnowledgeVaultWebscale2014,
  title = {Knowledge {{Vault}}: {{A Web-scale Approach}} to {{Probabilistic Knowledge Fusion}}},
  shorttitle = {Knowledge {{Vault}}},
  booktitle = {Proceedings of the 20th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining - {{KDD}} '14},
  author = {Dong, Xin and Gabrilovich, Evgeniy and Heitz, Geremy and Horn, Wilko and Lao, Ni and Murphy, Kevin and Strohmann, Thomas and Sun, Shaohua and Zhang, Wei},
  year = {2014},
  series = {{{KDD}} '14},
  pages = {601--610},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10/gfvn2s},
  urldate = {2019-02-13},
  abstract = {Recent years have witnessed a proliferation of large-scale knowledge bases, including Wikipedia, Freebase, YAGO, Microsoft's Satori, and Google's Knowledge Graph. To increase the scale even further, we need to explore automatic methods for constructing knowledge bases. Previous approaches have primarily focused on text-based extraction, which can be very noisy. Here we introduce Knowledge Vault, a Web-scale probabilistic knowledge base that combines extractions from Web content (obtained via analysis of text, tabular data, page structure, and human annotations) with prior knowledge derived from existing knowledge repositories. We employ supervised machine learning methods for fusing these distinct information sources. The Knowledge Vault is substantially bigger than any previously published structured knowledge repository, and features a probabilistic inference system that computes calibrated probabilities of fact correctness. We report the results of multiple studies that explore the relative utility of the different information sources and extraction methods.},
  isbn = {978-1-4503-2956-9},
  langid = {english},
  keywords = {information extraction,knowledge bases,machine learning,probabilistic models},
  file = {C:\Users\nhiot\OneDrive\zotero\2014\Dong et al. - 2014 - Knowledge Vault A Web-scale Approach to Probabili.pdf}
}

@article{dongLanguageLogicalForm2016,
  title = {Language to {{Logical Form}} with {{Neural Attention}}},
  author = {Dong, Li and Lapata, Mirella},
  year = {2016},
  month = jan,
  journal = {arXiv:1601.01280 [cs]},
  eprint = {1601.01280},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1601.01280},
  urldate = {2019-05-31},
  abstract = {Semantic parsing aims at mapping natural language to machine interpretable meaning representations. Traditional approaches rely on high-quality lexicons, manually-built templates, and linguistic features which are either domain- or representation-specific. In this paper we present a general method based on an attention-enhanced encoder-decoder model. We encode input utterances into vector representations, and generate their logical forms by conditioning the output sequences or trees on the encoding vectors. Experimental results on four datasets show that our approach performs competitively without using hand-engineered features and is easy to adapt across domains and meaning representations.},
  archiveprefix = {arxiv},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2016\\Dong et Lapata - 2016 - Language to Logical Form with Neural Attention.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\AKQGGRNN\\1601.html}
}

@article{doreianMeasuringRegularEquivalence1987,
  title = {Measuring Regular Equivalence in Symmetric Structures},
  author = {Doreian, Patrick},
  year = {1987},
  journal = {Social Networks},
  volume = {9},
  number = {2},
  pages = {89--107},
  publisher = {Elsevier},
  issn = {0378-8733},
  doi = {10/dwzcdk},
  abstract = {A method for computing the extent to which all pairs of points is a symmetric graph are regularly equivalent is proposed. By considering the relative centralities of points connected by an edge, the symmetric graph is decomposed into two asymmetric graphs. These asymmetric graphs provide the input for the regular equivalence algorithm, REGE, of White and Reitz (1983).},
  langid = {english},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1987\\Doreian - 1987 - Measuring regular equivalence in symmetric structu.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\SVCE4GMY\\0378873387900086.html}
}

@misc{driessenSuccessfulGitBranching2010,
  title = {A Successful {{Git}} Branching Model},
  author = {Driessen, Vincent},
  year = {2010},
  month = jan,
  journal = {nvie.com},
  url = {http://nvie.com/posts/a-successful-git-branching-model/},
  urldate = {2019-01-15},
  abstract = {In this post I present a Git branching strategy for developing and releasing software as I've used it in many of my projects, and which has turned out to be very successful.},
  langid = {english},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2010\\Driessen - 2010 - A successful Git branching model.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\HZ789HWK\\a-successful-git-branching-model.html}
}

@inproceedings{duchierMetagrammarCompilerNLP2005,
  title = {The {{Metagrammar Compiler}}: {{An NLP Application}} with a {{Multi-paradigm Architecture}}},
  shorttitle = {The {{Metagrammar Compiler}}},
  booktitle = {Multiparadigm {{Programming}} in {{Mozart}}/{{Oz}}},
  author = {Duchier, Denys and Le Roux, Joseph and Parmentier, Yannick},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Van Roy, Peter},
  year = {2005},
  volume = {3389},
  pages = {175--187},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-31845-3_15},
  urldate = {2023-09-29},
  isbn = {978-3-540-25079-1 978-3-540-31845-3},
  file = {C:\Users\nhiot\OneDrive\zotero\2005\Duchier et al. - 2005 - The Metagrammar Compiler An NLP Application with .pdf}
}

@article{eckhardtInductiveModelsUser2007,
  ids = {eckhardtInductiveModelsUser},
  title = {Inductive Models of User Preferences for Semantic Web},
  author = {Eckhardt, Alan},
  year = {2007},
  journal = {proceedings of Dateso (2007)},
  pages = {103--114},
  abstract = {Abstract. User preferences became recently a hot topic. The massive use of internet shops and social webs require the presence of a user modelling, which helps users to orient them selfs on a page. There are many different approaches to model user preferences. In this paper, we will overview the current state-of-the-art in the area of acquisition of user preferences and their induction. Main focus will be on the models of user preferences and on the induction of these models, but also the process of extracting preferences from the user behaviour will be studied. We will also present our contribution to the probabilistic user models. 1},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2007\\Eckhardt - 2007 - Inductive models of user preferences for semantic 2.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\SECPWIYR\\summary.html}
}

@phdthesis{eichlerModelisationFormelleSystemes2015,
  title = {{Mod{\'e}lisation formelle de syst{\`e}mes dynamiques autonomes : graphe, r{\'e}{\'e}criture et grammaire}},
  shorttitle = {{Mod{\'e}lisation formelle de syst{\`e}mes dynamiques autonomes}},
  author = {Eichler, C{\'e}dric},
  year = {2015},
  month = jun,
  url = {https://tel.archives-ouvertes.fr/tel-01174370/document},
  urldate = {2019-03-07},
  abstract = {Les syst{\`e}mes distribu{\'e}s modernes {\`a} large-{\'e}chelle {\'e}voluent dans des contextes variables soumis {\`a} de nombreux al{\'e}as auxquels ils doivent s'adapter dynamiquement. Dans ce cadre, l'informatique autonome se propose de r{\'e}duire (voire supprimer) les interventions humaines lentes et co{\^u}teuse, en leur pr{\'e}f{\'e}rant l'auto-gestion. L'adaptabilit{\'e} autonome d'un syst{\`e}me repose avant tout sur une description ad{\'e}quate de ses composants, de leurs interactions et des diff{\'e}rents aspects ou topologies qu'il peut adopter. Diverses approches de mod{\'e}lisation ont {\'e}t{\'e}s propos{\'e}es dans la litt{\'e}rature, bas{\'e}es notamment sur des langages de descriptions sp{\'e}cifiques (e.g., les ADLs) ou des mod{\`e}les g{\'e}n{\'e}riques plus ou moins formels (e.g., profils UML, graphes). Ces repr{\'e}sentations se concentrent en g{\'e}n{\'e}ral sur certains aspects ou propri{\'e}t{\'e}s du syst{\`e}me dynamique et ne permettent ainsi pas de r{\'e}pondre {\`a} chacune des probl{\'e}matiques inh{\'e}rentes {\`a} l'auto-gestion. Cette th{\`e}se traite de la mod{\'e}lisation bas{\'e}e graphes des syst{\`e}mes dynamiques et de son ad{\'e}quation pour la mise en {\oe}uvre des quatre propri{\'e}t{\'e}s fondamentales de l'informatique autonome : l'auto-optimisation, l'auto-protection, l'auto-gu{\'e}rison et l'auto-configuration. Cette th{\`e}se propose quatre principales contributions th{\'e}oriques et appliqu{\'e}es. La premi{\`e}re est une m{\'e}thodologie pour la construction et la caract{\'e}risation g{\'e}n{\'e}rative de transformations correctes par construction dont l'application pr{\'e}serve n{\'e}cessairement la correction du syst{\`e}me. Le maintien d'une application dans un {\'e}tat acceptable peut ainsi {\^e}tre efficacement garanti lors de son adaptation. La seconde contribution consiste en une extension des syst{\`e}mes de r{\'e}{\'e}criture de graphe permettant de repr{\'e}senter, mettre {\`a} jour, {\'e}valuer et param{\'e}trer les caract{\'e}ristiques d'un syst{\`e}me ais{\'e}ment et efficacement. Ces affirmations sont soutenues par des exemples illustratifs concrets reposant sur DIET, un r{\'e}partiteur de charge distribu{\'e}. Une {\'e}tude exp{\'e}rimentale extensive r{\'e}v{\`e}le un net gain d'efficacit{\'e} vis {\`a} vis de m{\'e}thodes classiques, en particulier celles int{\'e}gr{\'e}es nativement aux outils AGG et GMTE. La troisi{\`e}me contribution s'articule autour de l'{\'e}laboration d'un module de gestion de bout en bout pour des requ{\^e}tes de traitement d'{\'e}v{\'e}nements complexes. Elle d{\'e}montre l'int{\'e}r{\^e}t des graphes en tant que repr{\'e}sentation abstraite et haut niveau dans un contexte applicatif comprenant de multiples solutions fragment{\'e}es. La quatri{\`e}me et derni{\`e}re contribution r{\'e}side dans le design d'un gestionnaire autonome apte {\`a} r{\'e}genter tout syst{\`e}me Machine-{\`a}-Machine se conformant au standard ETSI M2M. Elle illustre la m{\'e}thodologie relative {\`a} la correction par construction, mais {\'e}galement l'int{\'e}gration de la repr{\'e}sentation propos{\'e}e {\`a} des approches multi-mod{\`e}les incluant des probl{\'e}matiques de coh{\'e}rence interne. La faisabilit{\'e} de l'approche est d{\'e}montr{\'e}e exp{\'e}rimentalement en s'appuyant sur une application de compteur intelligent pour la domotique.},
  langid = {french},
  school = {Universite Toulouse III Paul Sabatier},
  annotation = {00000},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2015\\Eichler - 2015 - Modélisation formelle de systèmes dynamiques auton.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\KIIE3K28\\tel-01174370.html}
}

@article{el-kassasEdgeSummGraphbasedFramework2020,
  title = {{{EdgeSumm}}: {{Graph-based}} Framework for Automatic Text Summarization},
  shorttitle = {{{EdgeSumm}}},
  author = {{El-Kassas}, Wafaa S. and Salama, Cherif R. and Rafea, Ahmed A. and Mohamed, Hoda K.},
  year = {2020},
  month = nov,
  journal = {Information Processing \& Management},
  volume = {57},
  number = {6},
  pages = {102264},
  issn = {0306-4573},
  doi = {10/ghw98x},
  urldate = {2021-03-16},
  abstract = {Searching the Internet for a certain topic can become a daunting task because users cannot read and comprehend all the resulting texts. Automatic Text summarization (ATS) in this case is clearly beneficial because manual summarization is expensive and time-consuming. To enhance ATS for single documents, this paper proposes a novel extractive graph-based framework ``EdgeSumm'' that relies on four proposed algorithms. The first algorithm constructs a new text graph model representation from the input document. The second and third algorithms search the constructed text graph for sentences to be included in the candidate summary. When the resulting candidate summary still exceeds a user-required limit, the fourth algorithm is used to select the most important sentences. EdgeSumm combines a set of extractive ATS methods (namely graph-based, statistical-based, semantic-based, and centrality-based methods) to benefit from their advantages and overcome their individual drawbacks. EdgeSumm is general for any document genre (not limited to a specific domain) and unsupervised so it does not require any training data. The standard datasets DUC2001 and DUC2002 are used to evaluate EdgeSumm using the widely used automatic evaluation tool: Recall-Oriented Understudy for Gisting Evaluation (ROUGE). EdgeSumm gets the highest ROUGE scores on DUC2001. For DUC2002, the evaluation results show that the proposed framework outperforms the state-of-the-art ATS systems by achieving improvements of 1.2\% and 4.7\% over the highest scores in the literature for the metrics of ROUGE-1 and ROUGE-L respectively. In addition, EdgeSumm achieves very competitive results for the metrics of ROUGE-2 and ROUGE-SU4.},
  langid = {english},
  keywords = {Automatic text summarization,EdgeSumm,Extractive text summarization,Graph representation model,Single-document summarization},
  file = {C:\Users\nhiot\Zotero\storage\FDXMJI9N\S0306457320301047.html}
}

@inproceedings{elhadadSemEval2015Task142015,
  title = {{{SemEval-2015}} Task 14: {{Analysis}} of Clinical Text},
  booktitle = {Proceedings of the 9th International Workshop on Semantic Evaluation, {{SemEval}}@{{NAACL-HLT}} 2015, Denver, Colorado, {{USA}}, June 4-5, 2015},
  author = {Elhadad, No{\'e}mie and Pradhan, Sameer and Gorman, Sharon Lipsky and Manandhar, Suresh and Chapman, Wendy W. and Savova, Guergana K.},
  editor = {Cer, Daniel M. and Jurgens, David and Nakov, Preslav and Zesch, Torsten},
  year = {2015},
  pages = {303--310},
  publisher = {The Association for Computer Linguistics},
  doi = {10.18653/v1/s15-2051},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/semeval/ElhadadPGMCS15.bib},
  keywords = {nosource},
  timestamp = {Tue, 28 Jan 2020 10:29:10 +0100},
  file = {C:\Users\nhiot\OneDrive\zotero\2015\Elhadad et al. - 2015 - SemEval-2015 task 14 Analysis of clinical text.pdf}
}

@phdthesis{erbsApproachesAutomaticText2015,
  title = {Approaches to {{Automatic Text Structuring}}},
  author = {Erbs, Nicolai},
  year = {2015},
  month = sep,
  address = {Darmstadt},
  url = {http://tuprints.ulb.tu-darmstadt.de/4959/},
  urldate = {2019-04-16},
  abstract = {Structured text helps readers to better understand the content of documents. In classic newspaper texts or books, some structure already exists. In the Web 2.0, the amount of textual data, especially user-generated data, has increased dramatically. As a result, there exists a large amount of textual data which lacks structure, thus making it more difficult to understand. In this thesis, we will explore techniques for automatic text structuring to help readers to fulfill their information needs. Useful techniques for automatic text structuring are keyphrase identification, table-of-contents generation, and link identification. We improve state of the art results for approaches to text structuring on several benchmark datasets. In addition, we present new representative datasets for users' everyday tasks. We evaluate the quality of text structuring approaches with regard to these scenarios and discover that the quality of approaches highly depends on the dataset on which they are applied. In the first chapter of this thesis, we establish the theoretical foundations regarding text structuring. We describe our findings from a user survey regarding web usage from which we derive three typical scenarios of Internet users. We then proceed to the three main contributions of this thesis. We evaluate approaches to keyphrase identification both by extracting and assigning keyphrases for English and German datasets. We find that unsupervised keyphrase extraction yields stable results, but for datasets with predefined keyphrases, additional filtering of keyphrases and assignment approaches yields even higher results. We present a de- compounding extension, which further improves results for datasets with shorter texts. We construct hierarchical table-of-contents of documents for three English datasets and discover that the results for hierarchy identification are sufficient for an automatic system, but for segment title generation, user interaction based on suggestions is required. We investigate approaches to link identification, including the subtasks of identifying the mention (anchor) of the link and linking the mention to an entity (target). Approaches that make use of the Wikipedia link structure perform best, as long as there is sufficient training data available. For identifying links to sense inventories other than Wikipedia, approaches that do not make use of the link structure outperform the approaches using existing links. We further analyze the effect of senses on computing similarities. In contrast to entity linking, where most entities can be discriminated by their name, we consider cases where multiple entities with the same name exist. We discover that similarity de- pends on the selected sense inventory. To foster future evaluation of natural language processing components for text structuring, we present two prototypes of text structuring systems, which integrate techniques for automatic text structuring in a wiki setting and in an e-learning setting with eBooks.},
  copyright = {CC-BY-NC-ND 3.0 - Creative Commons, Attribution Non-commerical, No-derivatives},
  langid = {english},
  school = {Technische Universit{\"a}t},
  annotation = {00000},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2015\\Erbs - 2015 - Approaches to Automatic Text Structuring.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\HMUEALYV\\4959.html}
}

@article{erikssonDictionaryConstructionIdentification2013,
  title = {Dictionary Construction and Identification of Possible Adverse Drug Events in {{Danish}} Clinical Narrative Text},
  author = {Eriksson, Robert and Jensen, Peter Bj{\o}dstrup and Frankild, Sune and Jensen, Lars Juhl and Brunak, S{\o}ren},
  year = {2013},
  month = sep,
  journal = {Journal of the American Medical Informatics Association},
  volume = {20},
  number = {5},
  pages = {947--953},
  issn = {1067-5027},
  doi = {10/f47nf7},
  urldate = {2021-05-03},
  abstract = {Objective Drugs have tremendous potential to cure and relieve disease, but the risk of unintended effects is always present. Healthcare providers increasingly record data in electronic patient records (EPRs), in which we aim to identify possible adverse events (AEs) and, specifically, possible adverse drug events (ADEs).Materials and methods Based on the undesirable effects section from the summary of product characteristics (SPC) of 7446 drugs, we have built a Danish ADE dictionary. Starting from this dictionary we have developed a pipeline for identifying possible ADEs in unstructured clinical narrative text. We use a named entity recognition (NER) tagger to identify dictionary matches in the text and post-coordination rules to construct ADE compound terms. Finally, we apply post-processing rules and filters to handle, for example, negations and sentences about subjects other than the patient. Moreover, this method allows synonyms to be identified and anatomical location descriptions can be merged to allow appropriate grouping of effects in the same location.Results The method identified 1 970 731 (35 477 unique) possible ADEs in a large corpus of 6011 psychiatric hospital patient records. Validation was performed through manual inspection of possible ADEs, resulting in precision of 89\% and recall of 75\%.Discussion The presented dictionary-building method could be used to construct other ADE dictionaries. The complication of compound words in Germanic languages was addressed. Additionally, the synonym and anatomical location collapse improve the method.Conclusions The developed dictionary and method can be used to identify possible ADEs in Danish clinical narratives.},
  annotation = {QID: Q30002316},
  file = {C:\Users\nhiot\OneDrive\zotero\2013\Eriksson et al. - 2013 - Dictionary construction and identification of poss.pdf}
}

@inproceedings{erlingLDBCSocialNetwork2015,
  title = {The {{LDBC Social Network Benchmark}}: {{Interactive Workload}}},
  shorttitle = {The {{LDBC Social Network Benchmark}}},
  booktitle = {Proceedings of the 2015 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Erling, Orri and Averbuch, Alex and {Larriba-Pey}, Josep and Chafi, Hassan and Gubichev, Andrey and Prat, Arnau and Pham, Minh-Duc and Boncz, Peter},
  year = {2015},
  month = may,
  series = {{{SIGMOD}} '15},
  pages = {619--630},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10/ggwc3r},
  urldate = {2021-03-19},
  abstract = {The Linked Data Benchmark Council (LDBC) is now two years underway and has gathered strong industrial participation for its mission to establish benchmarks, and benchmarking practices for evaluating graph data management systems. The LDBC introduced a new choke-point driven methodology for developing benchmark workloads, which combines user input with input from expert systems architects, which we outline. This paper describes the LDBC Social Network Benchmark (SNB), and presents database benchmarking innovation in terms of graph query functionality tested, correlated graph generation techniques, as well as a scalable benchmark driver on a workload with complex graph dependencies. SNB has three query workloads under development: Interactive, Business Intelligence, and Graph Algorithms. We describe the SNB Interactive Workload in detail and illustrate the workload with some early results, as well as the goals for the two other workloads.},
  isbn = {978-1-4503-2758-9},
  keywords = {benchmarking,graph databases,rdf databases},
  file = {C:\Users\nhiot\OneDrive\zotero\2015\Erling et al. - 2015 - The LDBC Social Network Benchmark Interactive Wor.pdf}
}

@inproceedings{estivalOntologybasedNaturalLanguage2004,
  title = {Towards Ontology-Based Natural Language Processing},
  booktitle = {Proceeedings of the {{Workshop}} on {{NLP}} and {{XML}} ({{NLPXML-2004}}): {{RDF}}/{{RDFS}} and {{OWL}} in {{Language Technology}} on - {{NLPXML}} '04},
  author = {Estival, Dominique and Nowak, Chris and Zschorn, Andrew},
  year = {2004},
  pages = {59--66},
  publisher = {Association for Computational Linguistics},
  address = {Not Known},
  doi = {10/cvpj68},
  urldate = {2019-01-29},
  abstract = {Conceptualising a domain has long been recognised as a prerequisite for understanding that domain and processing information about it. Ontologies are explicit specifications of conceptualisations which are now recognised as important components of information systems and information processing. In this paper, we describe a project in which ontologies are part of the reasoning process used for information management and for the presentation of information. Both accessing and presenting information are mediated via natural language and the ontologies are coupled with the lexicon used in the natural language component.},
  langid = {english},
  annotation = {00000},
  file = {C:\Users\nhiot\OneDrive\zotero\2004\Estival et al. - 2004 - Towards ontology-based natural language processing.pdf}
}

@inproceedings{esuliEvaluatingInformationExtraction2010,
  title = {Evaluating {{Information Extraction}}},
  author = {Esuli, Andrea and Sebastiani, Fabrizio},
  year = {2010},
  month = sep,
  pages = {100--111},
  doi = {10/bs4gfh},
  abstract = {The issue of how to experimentally evaluate information extraction (IE) systems has received hardly any satisfactory solution in the literature. In this paper we propose a novel evaluation model for IE and argue that, among others, it allows (i) a correct appreciation of the degree of overlap between predicted and true segments, and (ii) a fair evaluation of the ability of a system to correctly identify segment boundaries. We describe the properties of this models, also by presenting the result of a re-evaluation of the results of the CoNLL'03 and CoNLL'02 Shared Tasks on Named Entity Extraction.},
  file = {C:\Users\nhiot\OneDrive\zotero\2010\Esuli et Sebastiani - 2010 - Evaluating Information Extraction.pdf}
}

@inproceedings{eswaranHigherOrderLabelHomogeneity2020,
  title = {Higher-{{Order Label Homogeneity}} and {{Spreading}} in {{Graphs}}},
  booktitle = {Proceedings of {{The Web Conference}} 2020},
  author = {Eswaran, Dhivya and Kumar, Srijan and Faloutsos, Christos},
  year = {2020},
  month = apr,
  series = {{{WWW}} '20},
  pages = {2493--2499},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3366423.3379997},
  urldate = {2023-08-21},
  abstract = {Do higher-order network structures aid graph semi-supervised learning? Given a graph and a few labeled vertices, labeling the remaining vertices is a high-impact problem with applications in several tasks, such as recommender systems, fraud detection and protein identification. However, traditional methods rely on edges for spreading labels, which is limited as all edges are not equal. Vertices with stronger connections participate in higher-order structures in graphs, which calls for methods that can leverage these structures in the semi-supervised learning tasks. To this end, we propose Higher-Order Label Spreading (HOLS) to spread labels using higher-order structures. HOLS has strong theoretical guarantees and reduces to standard label spreading in the base case. Via extensive experiments, we show that higher-order label spreading using triangles in addition to edges is up to 4.7\% better than label spreading using edges alone. Compared to prior traditional and state-of-the-art methods, the proposed method leads to statistically significant accuracy gains in all-but-one cases, while remaining fast and scalable to large graphs.},
  isbn = {978-1-4503-7023-3},
  file = {C:\Users\nhiot\OneDrive\zotero\2020\Eswaran et al. - 2020 - Higher-Order Label Homogeneity and Spreading in Gr.pdf}
}

@inproceedings{etzioniWebscaleInformationExtraction2004,
  title = {Web-Scale {{Information Extraction}} in {{Knowitall}}: ({{Preliminary Results}})},
  shorttitle = {Web-Scale {{Information Extraction}} in {{Knowitall}}},
  booktitle = {Proceedings of the 13th {{International Conference}} on {{World Wide Web}}},
  author = {Etzioni, Oren and Cafarella, Michael and Downey, Doug and Kok, Stanley and Popescu, Ana-Maria and Shaked, Tal and Soderland, Stephen and Weld, Daniel S. and Yates, Alexander},
  year = {2004},
  series = {{{WWW}} '04},
  pages = {100--110},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10/cgwsqz},
  urldate = {2019-05-24},
  abstract = {Manually querying search engines in order to accumulate a large bodyof factual information is a tedious, error-prone process of piecemealsearch. Search engines retrieve and rank potentially relevantdocuments for human perusal, but do not extract facts, assessconfidence, or fuse information from multiple documents. This paperintroduces KnowItAll, a system that aims to automate the tedious process ofextracting large collections of facts from the web in an autonomous,domain-independent, and scalable manner.The paper describes preliminary experiments in which an instance of KnowItAll, running for four days on a single machine, was able to automatically extract 54,753 facts. KnowItAll associates a probability with each fact enabling it to trade off precision and recall. The paper analyzes KnowItAll's architecture and reports on lessons learned for the design of large-scale information extraction systems.},
  isbn = {978-1-58113-844-3},
  keywords = {information extraction,mutual information,nosource,pmi,search}
}

@article{everettRegularEquivalenceGeneral1994,
  title = {Regular Equivalence: {{General}} Theory},
  shorttitle = {Regular Equivalence},
  author = {Everett, Martin G. and Borgatti, Stephen P.},
  year = {1994},
  month = may,
  journal = {The Journal of Mathematical Sociology},
  volume = {19},
  number = {1},
  pages = {29--52},
  publisher = {Routledge},
  issn = {0022-250X, 1545-5874},
  doi = {10.1080/0022250x.1994.9990134},
  urldate = {2024-01-12},
  abstract = {The theory of regular equivalence has advanced over the last 15 years on a number of different fronts. Notation and terminology have developed often making it difficult to obtain a coherent view of the area as a whole. This paper attempts to provide a framework in which to develop and explore the general mathematical theory of regular equivalence and to place a number of the more important results into that framework.},
  langid = {english},
  file = {C:\Users\nhiot\OneDrive\zotero\1994\Everett et Borgatti - 1994 - Regular equivalence General theory.pdf}
}

@article{ezzatAcquisitionGrammairesLocales2010,
  title = {Acquisition de Grammaires Locales Pour l'extraction de Relations Entre Entit{\'e}s Nomm{\'e}es},
  author = {Ezzat, Mani},
  year = {2010},
  journal = {Rencontre des {\'e}tudiants chercheurs en informatique pour le traitement automatique des langues (RECITAL'10)},
  keywords = {⛔ No DOI found},
  annotation = {00010},
  file = {C:\Users\nhiot\OneDrive\zotero\2010\Ezzat - 2010 - Acquisition de grammaires locales pour l’extractio.pdf}
}

@inproceedings{fabreExtractionRelationsSemantiques2006,
  title = {Extraction de Relations S{\'e}mantiques Entre Noms et Verbes Au-Del{\`a} Des Liens Morphologiques},
  booktitle = {Proceedings of the 13th {{TALN Conference}}},
  author = {Fabre, C{\'e}cile and Bourigault, Didier},
  year = {2006},
  pages = {121--129},
  keywords = {⛔ No DOI found},
  annotation = {00028},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2006\\Fabre et Bourigault - 2006 - Extraction de relations sémantiques entre noms et .pdf;C\:\\Users\\nhiot\\Zotero\\storage\\Z7VXU7UK\\books.html}
}

@inproceedings{faderOpenQuestionAnswering2014,
  title = {Open Question Answering over Curated and Extracted Knowledge Bases},
  booktitle = {Proceedings of the 20th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  author = {Fader, Anthony and Zettlemoyer, Luke and Etzioni, Oren},
  year = {2014},
  month = aug,
  pages = {1156--1165},
  publisher = {ACM},
  address = {New York New York USA},
  doi = {10.1145/2623330.2623677},
  urldate = {2024-03-29},
  isbn = {978-1-4503-2956-9},
  langid = {english},
  optbibsource = {dblp computer science bibliography, https://dblp.org},
  optbiburl = {https://dblp.org/rec/conf/kdd/FaderZE14.bib},
  opttimestamp = {Tue, 06 Nov 2018 16:59:36 +0100},
  opturl = {https://doi.org/10.1145/2623330.2623677},
  keywords = {⛔ No DOI found,nosource}
}

@article{faginDataExchangeGetting2005,
  title = {Data Exchange: Getting to the Core},
  shorttitle = {Data Exchange},
  author = {Fagin, Ronald and Kolaitis, Phokion G. and Popa, Lucian},
  year = {2005},
  month = mar,
  journal = {ACM Transactions on Database Systems (TODS)},
  volume = {30},
  number = {1},
  pages = {174--210},
  issn = {0362-5915, 1557-4644},
  doi = {10.1145/1061318.1061323},
  urldate = {2023-08-16},
  abstract = {Data exchange is the problem of taking data structured under a source schema and creating an instance of a target schema that reflects the source data as accurately as possible. Given a source instance, there may be many solutions to the data exchange problem, that is, many target instances that satisfy the constraints of the data exchange problem. In an earlier article, we identified a special class of solutions that we call universal. A universal solution has homomorphisms into every possible solution, and hence is a ``most general possible'' solution. Nonetheless, given a source instance, there may be many universal solutions. This naturally raises the question of whether there is a ``best'' universal solution, and hence a best solution for data exchange. We answer this question by considering the well-known notion of the core of a structure, a notion that was first studied in graph theory, and has also played a role in conjunctive-query processing. The core of a structure is the smallest substructure that is also a homomorphic image of the structure. All universal solutions have the same core (up to isomorphism); we show that this core is also a universal solution, and hence the smallest universal solution. The uniqueness of the core of a universal solution together with its minimality make the core an ideal solution for data exchange. We investigate the computational complexity of producing the core. Well-known results by Chandra and Merlin imply that, unless P = NP, there is no polynomial-time algorithm that, given a structure as input, returns the core of that structure as output. In contrast, in the context of data exchange, we identify natural and fairly broad conditions under which there are polynomial-time algorithms for computing the core of a universal solution. We also analyze the computational complexity of the following decision problem that underlies the computation of cores: given two graphs G and H, is H the core of G? Earlier results imply that this problem is both NP-hard and coNP-hard. Here, we pinpoint its exact complexity by establishing that it is a DP-complete problem. Finally, we show that the core is the best among all universal solutions for answering existential queries, and we propose an alternative semantics for answering queries in data exchange settings.},
  langid = {english},
  keywords = {Certain answers,chase,computational complexity,conjunctive queries,core,data exchange,data integration,dependencies,nosource,query answering,universal solutions},
  annotation = {QID: Q106466848}
}

@inproceedings{faginSemanticsUpdatesDatabases1983,
  ids = {FUV83},
  title = {On the Semantics of Updates in Databases},
  booktitle = {Proceedings of the 2nd {{ACM SIGACT-SIGMOD Symposium}} on {{Principles}} of {{Database Systems}}},
  author = {Fagin, Ronald and Ullman, Jeffrey D. and Vardi, Moshe Y.},
  year = {1983},
  month = mar,
  series = {{{PODS}} '83},
  pages = {352--365},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/588058.588100},
  urldate = {2023-08-03},
  abstract = {We suggest here a methodology for updating databases with integrity constraints and rules for deriving inexphcit information. First we consider the problem of updating arbitrary theories by inserting into them or deleting from them arbitrary sentences. The solution involves two key ideas when replacing an old theory by a new one we wish to minimize the change in the theory, and when there are several theories that involve minimal changes, we look for a new theory that reflects that ambiguity. The methodology is also adapted to updating databases, where different facts can carry different priorities, and to updating user views.},
  isbn = {978-0-89791-097-2},
  optbibsource = {dblp computer science bibliography, http://dblp.org},
  optcrossref = {DBLP:conf/pods/83},
  optdoi = {10.1145/588058.588100},
  opttimestamp = {Wed, 29 Mar 2017 16:45:25 +0200},
  keywords = {nosource},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1983\\Fagin et al. - 1983 - On the semantics of updates in databases.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\7SGAML5K\\588058.html}
}

@article{faginUpdatingLogicalDatabases1986,
  title = {Updating {{Logical Databases}}},
  author = {Fagin, Ronald and Kuper, Gabriel M. and Ullman, Jeffrey D. and Vardi, Moshe Y.},
  year = {1986},
  journal = {Advances in Computing Research},
  volume = {3},
  pages = {1--18},
  doi = {10.21236/ada144937},
  keywords = {nosource},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1986\\Fagin et al. - 1986 - Updating Logical Databases.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\RS48PZZC\\ADA144937.html}
}

@article{fanDependenciesGraphs2019,
  title = {Dependencies for {{Graphs}}},
  author = {Fan, Wenfei and Lu, Ping},
  year = {2019},
  month = jun,
  journal = {ACM Transactions on Database Systems},
  volume = {44},
  number = {2},
  pages = {1--40},
  issn = {0362-5915, 1557-4644},
  doi = {10.1145/3287285},
  urldate = {2024-01-08},
  abstract = {This article proposes a class of dependencies for graphs, referred to as graph entity dependencies (GEDs). A GED is defined as a combination of a graph pattern and an attribute dependency. In a uniform format, GEDs can express graph functional dependencies with constant literals to catch inconsistencies, and keys carrying id literals to identify entities (vertices) in a graph. We revise the chase for GEDs and prove its Church-Rosser property. We characterize GED satisfiability and implication, and establish the complexity of these problems and the validation problem for GEDs, in the presence and absence of constant literals and id literals. We also develop a sound, complete and independent axiom system for finite implication of GEDs. In addition, we extend GEDs with built-in predicates or disjunctions, to strike a balance between the expressive power and complexity. We settle the complexity of the satisfiability, implication, and validation problems for these extensions.},
  langid = {english},
  keywords = {axiom system,built-in predicates,conditional functional dependencies,disjunction,EGDs,Graph dependencies,implication,keys,satisfiability,TGDs,validation},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2019\\Fan et Lu - 2019 - Dependencies for Graphs.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\8BMXGJZI\\3287285.html}
}

@inproceedings{fanFunctionalDependenciesGraphs2016,
  title = {Functional {{Dependencies}} for {{Graphs}}},
  booktitle = {Proceedings of the 2016 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Fan, Wenfei and Wu, Yinghui and Xu, Jingbo},
  year = {2016},
  month = jun,
  series = {{{SIGMOD}} '16},
  pages = {1843--1857},
  publisher = {ACM},
  address = {San Francisco California USA},
  doi = {10.1145/2882903.2915232},
  urldate = {2023-12-07},
  abstract = {We propose a class of functional dependencies for graphs, referred to as GFDs. GFDs capture both attribute-value dependencies and topological structures of entities, and subsume conditional functional dependencies (CFDs) as a special case. We show that the satisfiability and implication problems for GFDs are coNP-complete and NP-complete, respectively, no worse than their CFD counterparts. We also show that the validation problem for GFDs is coNP-complete. Despite the intractability, we develop parallel scalable algorithms for catching violations of GFDs in large-scale graphs. Using real-life and synthetic data, we experimentally verify that GFDs provide an effective approach to detecting inconsistencies in knowledge and social graphs.},
  isbn = {978-1-4503-3531-7},
  langid = {english},
  keywords = {functional dependencies,graphs,implication,satisfiability,validation},
  file = {C:\Users\nhiot\OneDrive\zotero\2016\Fan et al. - 2016 - Functional Dependencies for Graphs.pdf}
}

@inproceedings{fanIncrementalizingGraphAlgorithms2021,
  title = {Incrementalizing {{Graph Algorithms}}},
  booktitle = {Proceedings of the 2021 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Fan, Wenfei and Tian, Chao and Xu, Ruiqi and Yin, Qiang and Yu, Wenyuan and Zhou, Jingren},
  editor = {Li, Guoliang and Li, Zhanhuai and Idreos, Stratos and Srivastava, Divesh},
  year = {2021},
  month = jun,
  series = {{{SIGMOD}} '21},
  pages = {459--471},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3448016.3452796},
  abstract = {Incremental algorithms are important to dynamic graph analyses, but are hard to write and analyze. Few incremental graph algorithms are in place, and even fewer offer performance guarantees. This paper approaches this by proposing to incrementalize existing batch algorithms. We identify a class of incrementalizable algorithms abstracted in a fixpoint model. We show how to deduce an incremental algorithm A{$\Delta$} from such an algorithm A. Moreover, A{$\Delta$} can be made bounded relative to A, i.e., its cost is determined by the sizes of changes to graphs and changes to the affected area that is necessarily checked by batch algorithm A. We provide generic conditions under which a deduced algorithm A{$\Delta$} warrants to be correct and relatively bounded, by adopting the same logic and data structures of A, at most using timestamps as an additional auxiliary structure. Based on these, we show that a variety of graph-centric algorithms can be incrementalized with relative boundedness. Using real-life and synthetic graphs, we experimentally verify the scalability and efficiency of the incrementalized algorithms.},
  isbn = {978-1-4503-8343-1},
  langid = {english},
  keywords = {boundedness,fixpoint algorithm,incrementalization,nosource},
  file = {C:\Users\nhiot\OneDrive\zotero\2021\Fan et al. - 2021 - Incrementalizing Graph Algorithms2.pdf}
}

@article{fanKeysGraphs2015,
  ids = {FFTD15},
  title = {Keys for Graphs},
  author = {Fan, Wenfei and Fan, Zhe and Tian, Chao and Dong, Xin Luna},
  year = {2015},
  month = aug,
  journal = {Proceedings of the VLDB Endowment},
  volume = {8},
  number = {12},
  pages = {1590--1601},
  publisher = {VLDB Endowment},
  issn = {2150-8097},
  doi = {10.14778/2824032.2824056},
  urldate = {2023-08-03},
  abstract = {Keys for graphs aim to uniquely identify entities represented by vertices in a graph. We propose a class of keys that are recursively defined in terms of graph patterns, and are interpreted with subgraph isomorphism. Extending conventional keys for relations and XML, these keys find applications in object identification, knowledge fusion and social network reconciliation. As an application, we study the entity matching problem that, given a graph G and a set {$\Sigma$} of keys, is to find all pairs of entities (vertices) in G that are identified by keys in {$\Sigma$}. We show that the problem is intractable, and cannot be parallelized in logarithmic rounds. Nonetheless, we provide two parallel scalable algorithms for entity matching, in MapReduce and a vertex-centric asynchronous model. Using real-life and synthetic data, we experimentally verify the effectiveness and scalability of the algorithms.},
  file = {C:\Users\nhiot\OneDrive\zotero\2015\Fan et al. - 2015 - Keys for graphs.pdf}
}

@inproceedings{fastIrisConversationalAgent2018,
  ids = {fastIrisConversationalAgent2018a},
  title = {Iris: {{A Conversational Agent}} for {{Complex Tasks}}},
  shorttitle = {Iris},
  booktitle = {Proceedings of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Fast, Ethan and Chen, Binbin and Mendelsohn, Julia and Bassen, Jonathan and Bernstein, Michael S.},
  year = {2018},
  month = apr,
  series = {{{CHI}} '18},
  pages = {1--12},
  publisher = {Association for Computing Machinery},
  address = {Montreal QC, Canada},
  doi = {10/ggvw67},
  urldate = {2020-05-14},
  abstract = {Today, most conversational agents are limited to simple tasks supported by standalone commands, such as getting directions or scheduling an appointment. To support more complex tasks, agents must be able to generalize from and combine the commands they already understand. This paper presents a new approach to designing conversational agents inspired by linguistic theory, where agents can execute complex requests interactively by combining commands through nested conversations. We demonstrate this approach in Iris, an agent that can perform open-ended data science tasks such as lexical analysis and predictive modeling. To power Iris, we have created a domain-specific language that transforms Python functions into combinable automata and regulates their combinations through a type system. Running a user study to examine the strengths and limitations of our approach, we find that data scientists completed a modeling task 2.6 times faster with Iris than with Jupyter Notebook.},
  isbn = {978-1-4503-5620-6},
  keywords = {conversational agents,data science},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2018\\Fast et al. - 2018 - Iris A Conversational Agent for Complex Tasks.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\PQKKT9NE\\3173574.html}
}

@inproceedings{ferreSQUALLControlledNatural2012,
  title = {{{SQUALL}}: {{A Controlled Natural Language}} for {{Querying}} and {{Updating RDF Graphs}}},
  shorttitle = {{{SQUALL}}},
  booktitle = {International {{Workshop}} on {{Controlled Natural Language}}},
  author = {Ferr{\'e}, S{\'e}bastien},
  editor = {Kuhn, Tobias and Fuchs, Norbert E.},
  year = {2012},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {11--25},
  publisher = {Springer Berlin Heidelberg},
  doi = {10/gf4chw},
  abstract = {Formal languages play a central role in the Semantic Web. An important aspect regarding their design is syntax as it plays a crucial role in the wide acceptance of the Semantic Web approach. The main advantage of controlled natural languages (CNL) is to reconcile the high-level and natural syntax of natural languages, and the precision and lack of ambiguity of formal languages. In the context of the Semantic Web and Linked Open Data, CNL could not only allow more people to contribute by abstracting from the low-level details, but also make experienced people more productive, and make the produced documents easier to share and maintain. We introduce SQUALL, a controlled natural language for querying and updating RDF graphs. It has a strong adequacy with RDF, an expressiveness close to SPARQL 1.1, and a CNL syntax that completely abstracts from low-level notions such as bindings and relational algebra. We formally define the syntax and semantics of SQUALL as a Montague grammar, and its translation to SPARQL. It features disjunction, negation, quantifiers, built-in predicates, aggregations with grouping, and n-ary relations through reification.},
  isbn = {978-3-642-32612-7},
  langid = {english},
  keywords = {Graph Pattern,Noun Phrase,Prepositional Phrase,Relational Algebra,Triple Pattern},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2012\\Ferré - 2012 - SQUALL A Controlled Natural Language for Querying.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\ABZELF92\\978-3-642-32612-7_2.html}
}

@misc{FinalOpenIDConnect,
  title = {Final: {{OpenID Connect Core}} 1.0 Incorporating Errata Set 1},
  url = {https://openid.net/specs/openid-connect-core-1\_0.html},
  urldate = {2020-10-02},
  file = {C:\Users\nhiot\Zotero\storage\TWKWS2Z8\openid-connect-core-1_0.html}
}

@inproceedings{finkel-manning-2009-nested,
  title = {Nested Named Entity Recognition},
  booktitle = {Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing},
  author = {Finkel, Jenny Rose and Manning, Christopher D.},
  year = {2009},
  month = aug,
  pages = {141--150},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  url = {https://www.aclweb.org/anthology/D09-1015},
  keywords = {nosource}
}

@article{finkelsteinPlacingSearchContext2002,
  title = {Placing {{Search}} in {{Context}}: {{The Concept Revisited}}},
  shorttitle = {Placing {{Search}} in {{Context}}},
  author = {Finkelstein, Lev and Gabrilovich, Evgeniy and Matias, Yossi and Rivlin, Ehud and Solan, Zach and Wolfman, Gadi and Ruppin, Eytan},
  year = {2002},
  month = jan,
  journal = {ACM Transactions on Information Systems},
  volume = {20},
  number = {1},
  pages = {116--131},
  issn = {1046-8188},
  doi = {10/bvqdvb},
  urldate = {2019-02-12},
  abstract = {Keyword-based search engines are in widespread use today as a popular means for Web-based information retrieval. Although such systems seem deceptively simple, a considerable amount of skill is required in order to satisfy non-trivial information needs. This paper presents a new conceptual paradigm for performing search in context, that largely automates the search process, providing even non-professional users with highly relevant results. This paradigm is implemented in practice in the IntelliZap system, where search is initiated from a text query marked by the user in a document she views, and is guided by the text surrounding the marked query in that document ("the context"). The context-driven information retrieval process involves semantic keyword extraction and clustering to automatically generate new, augmented queries. The latter are submitted to a host of general and domain-specific search engines. Search results are then semantically reranked, using context. Experimental results testify that using context to guide search, effectively offers even inexperienced users an advanced search tool on the Web.},
  keywords = {context,invisible web,Search,semantic processing,statistical natural language processing},
  annotation = {QID: Q28045598},
  file = {C:\Users\nhiot\OneDrive\zotero\2002\Finkelstein et al. - 2002 - Placing Search in Context The Concept Revisited.pdf}
}

@article{florekLiaisonDivisionPoints1951,
  title = {{Sur la liaison et la division des points d'un ensemble fini}},
  author = {Florek, K. and {\L}ukaszewicz, J. and Perkal, J. and Steinhaus, Hugo and Zubrzycki, S.},
  year = {1951},
  journal = {Colloquium Mathematicum},
  volume = {2},
  number = {3-4},
  pages = {282--285},
  issn = {0010-1354},
  doi = {10.4064/cm-2-3-4-282-285},
  urldate = {2023-04-18},
  langid = {fra},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1951\\Florek et al. - 1951 - Sur la liaison et la division des points d'un ense.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\NWJZTFEF\\209969.html}
}

@article{flourisFormalFoundationsRDF2013,
  ids = {FKAC13},
  title = {Formal Foundations for {{RDF}}/{{S KB}} Evolution},
  author = {Flouris, Giorgos and Konstantinidis, George and Antoniou, Grigoris and Christophides, Vassilis},
  year = {2013},
  month = apr,
  journal = {Knowledge and Information Systems},
  volume = {35},
  number = {1},
  pages = {153--191},
  issn = {0219-1377, 0219-3116},
  doi = {10.1007/s10115-012-0500-2},
  urldate = {2019-06-20},
  langid = {english},
  optbibsource = {dblp computer science bibliography, http://dblp.org},
  optdoi = {10.1007/s10115-012-0500-2},
  opttimestamp = {Tue, 09 Apr 2013 17:54:05 +0200},
  annotation = {QID: Q58198067},
  file = {C:\Users\nhiot\OneDrive\zotero\2013\Flouris et al. - 2013 - Formal foundations for RDFS KB evolution.pdf}
}

@article{fominParameterizedComplexityGraph2018,
  title = {On the {{Parameterized Complexity}} of {{Graph Modification}} to {{First-Order Logic Properties}}},
  author = {Fomin, Fedor V. and Golovach, Petr A. and Thilikos, Dimitrios M.},
  year = {2018},
  month = may,
  journal = {arXiv:1805.04375 [cs]},
  eprint = {1805.04375},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1805.04375},
  urldate = {2019-04-29},
  abstract = {We consider the problems of deciding whether an input graph can be modified by removing/adding at most k vertices/edges such that the result of the modification satisfies some property definable in first-order logic. We establish a number of sufficient and necessary conditions on the quantification pattern of the first-order formula {\textbackslash}phi for the problem to be fixed-parameter tractable or to admit a polynomial kernel.},
  archiveprefix = {arxiv},
  keywords = {{05C85, 68R10, 68W05},⛔ No DOI found,Computer Science - Computational Complexity,Computer Science - Data Structures and Algorithms},
  annotation = {00000},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2018\\Fomin et al. - 2018 - On the Parameterized Complexity of Graph Modificat.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\L5F6EMXB\\1805.html}
}

@incollection{forresiStreamingApproachSchema2023,
  title = {Streaming {{Approach}} to {{Schema Profiling}}},
  author = {Forresi, Chiara and Francia, Matteo and Gallinucci, Enrico and Golfarelli, Matteo},
  year = {2023},
  month = aug,
  pages = {211--220},
  doi = {10.1007/978-3-031-42941-5_19},
  abstract = {Schema profiling consists in producing key insights about the schema of data in a high-variety context. In this paper, we present a streaming approach to schema profiling, where heterogeneous data is continuously ingested from multiple sources, as is typical in many IoT applications (e.g., with multiple devices or applications dynamically logging messages). The produced profile is a clustering of the schemas extracted from the data and it is computed and evolved in real-time under the overlapping sliding window paradigm. The approach is based on two-phase k-means clustering, which entails pre-aggregating the data into a coreset and incrementally updating the previous clustering results without recomputing it in every iteration. Differently from previous proposals, the approach works in a domain where dimensionality is variable and unknown apriori, it automatically selects the optimal number of clusters, and detects cluster evolution by minimizing the need to recompute the profile. The experimental evaluation demonstrated the effectiveness and efficiency of the approach against the na{\"i}ve baseline and the state-of-the-art algorithms on stream clustering.KeywordsSchema profilingStream clusteringK-meansApproximation algorithm},
  isbn = {978-3-031-42940-8},
  file = {C:\Users\nhiot\OneDrive\zotero\2023\Forresi et al. - 2023 - Streaming Approach to Schema Profiling.pdf}
}

@article{franciscusDependencyGraphShort2019,
  title = {Dependency Graph for Short Text Extraction and Summarization},
  author = {Franciscus, Nigel and Ren, Xuguang and Stantic, Bela},
  year = {2019},
  month = oct,
  journal = {Journal of Information and Telecommunication},
  volume = {3},
  number = {4},
  pages = {413--429},
  issn = {2475-1839},
  doi = {10.1080/24751839.2019.1598771},
  urldate = {2021-07-05},
  abstract = {A sheer amount of text generated from microblogs and social media brings huge opportunities to the text mining applications. Many techniques such as sentiment analysis and opinion mining are proven effective to deliver insights from documents. However, most of these textual data are in the form of short and fragmented texts which are difficult to visually extract due to the sparsity issue and the context in the content is often unknown. Naive while widely used models, term frequency and the bag-of-words never considered the semantic relationship between the words, making the results relatively difficult to interpret. A well-known technique in text mining like topic model may provide a general `at glance' understanding but can be difficult to interpret or to understand. One alternative is to aggregate words in a semantical order and generates an output of human-understandable sentences. In this paper, we address this direction by proposing the belief graph data model that joins short texts by inducing the part-of-speech tagging to maintain the order and to preserve the context of the content. Extensive experiments showed that our approach improves the overall qualitative evaluation of text understanding compared to the previous state of the art text mining techniques.},
  keywords = {Graph-of-word,part of speech,short text,text representation},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2019\\Franciscus et al. - 2019 - Dependency graph for short text extraction and sum.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\A74NSSV8\\24751839.2019.html}
}

@inproceedings{francisCypherEvolvingQuery2018,
  title = {Cypher: {{An Evolving Query Language}} for {{Property Graphs}}},
  shorttitle = {Cypher},
  booktitle = {Proceedings of the 2018 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Francis, Nadime and Green, Alastair and Guagliardo, Paolo and Libkin, Leonid and Lindaaker, Tobias and Marsault, Victor and Plantikow, Stefan and Rydberg, Mats and Selmer, Petra and Taylor, Andr{\'e}s},
  editor = {Das, Gautam and Jermaine, Christopher M. and Bernstein, Philip A.},
  year = {2018},
  month = may,
  series = {{{SIGMOD}} '18},
  pages = {1433--1445},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3183713.3190657},
  abstract = {The Cypher property graph query language is an evolving language, originally designed and implemented as part of the Neo4j graph database, and it is currently used by several commercial database products and researchers. We describe Cypher 9, which is the first version of the language governed by the openCypher Implementers Group. We first introduce the language by example, and describe its uses in industry. We then provide a formal semantic definition of the core read-query features of Cypher, including its variant of the property graph data model, and its ASCII Art graph pattern matching mechanism for expressing subgraphs of interest to an application. We compare the features of Cypher to other property graph query languages, and describe extensions, at an advanced stage of development, which will form part of Cypher 10, turning the language into a compositional language which supports graph projections and multiple named graphs.},
  isbn = {978-1-4503-4703-7},
  keywords = {cypher,formal semantics,formal specification,graph databases,property graphs,query language},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2018\\Francis et al. - 2018 - Cypher An Evolving Query Language for Property Gr.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\ERZNBMM2\\3183713.html}
}

@book{freemanResearchMethodsSocial2017a,
  ids = {freemanResearchMethodsSocial2017},
  title = {Research {{Methods}} in {{Social Network Analysis}}},
  author = {Freeman, Linton C.},
  year = {2017},
  month = jul,
  edition = {1er {\'e}dition},
  publisher = {Routledge},
  abstract = {Since the publication of Herbert Spencer's Principles of Sociology in 1875, the use of social structure as a defining concept has produced a large body of creative speculations, insights, and intuitions about social life. However, writers in this tradition do not always provide the sorts of formal definitons and propositions that are the building blocks of modern social research. In its broad-ranging examination of the kind of data that form the basis for the systematic study of social structure, Research Methods in Social Network Analysis marks a significant methodological advance in network studies.As used in this volume, social structure refers to a bundle of intuitive natural language ideas and concepts about patterning in social relationships among people. In contrast, social networks is used to refer to a collection of precise analytic and methodological concepts and procedures that facilitate the collection of data and the systematic study of such patterning. Accordingly, the book's five sections are arranged to address analytical problems in a series of logically ordered stages or processes.The major contributors define the fundamental modes by which social structural phenomena are to be represented; how boundaries to a social structure are set; how the relations of a network are measured in terms of structure and content; the ways in which the relational structure of a network affects system actors; and how actors within a social network are clustered into cliques or groups. The chapters in the last section build on solutions to problems proposed in the previous sections. This highly unified approach to research design combined with a representative diversity of viewpoints makes Research Methods in Social Network Analysis a state-of-the-art volume.},
  googlebooks = {DisxDwAAQBAJ},
  isbn = {978-1-351-49335-2},
  langid = {english},
  keywords = {Social Science / Research}
}

@article{freemanSetMeasuresCentrality1977a,
  ids = {freemanSetMeasuresCentrality1977},
  title = {A {{Set}} of {{Measures}} of {{Centrality Based}} on {{Betweenness}}},
  author = {Freeman, Linton C.},
  year = {1977},
  month = mar,
  journal = {Sociometry},
  volume = {40},
  number = {1},
  eprint = {3033543},
  eprinttype = {jstor},
  pages = {35--41},
  publisher = {[American Sociological Association, Sage Publications, Inc.]},
  issn = {0038-0431},
  doi = {10.2307/3033543},
  urldate = {2024-01-12},
  abstract = {A Family of new measures of point and graph centrality based on early intuitions of Bavelas (1948) is introduced. These measures define centrality in terms of the degree to which a point falls on the shortest path between others and therefore has a potential for control of communication. They may be used to index centrality in any large or small network of symmetrical relations, whether connected or unconnected.},
  file = {C:\Users\nhiot\Zotero\storage\X8LFH5V5\Freeman - 1977 - A Set of Measures of Centrality Based on Betweenne.pdf}
}

@inproceedings{friburgerFiniteStateTransducerCascade2001,
  title = {Finite-{{State Transducer Cascade}} to {{Extract Proper Names}} in {{Texts}}},
  booktitle = {Implementation and Application of Automata, 6th International Conference, {{CIAA}} 2001, Pretoria, South Africa, July 23-25, 2001, Revised Papers},
  author = {Friburger, Nathalie and Maurel, Denis},
  editor = {Watson, Bruce W. and Wood, Derick},
  year = {2001},
  month = jul,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {2494},
  pages = {115--124},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-36390-4_10},
  abstract = {This article describes a finite-state cascade for the extraction of person names in texts in French. We extract these proper names in order to categorize and to cluster texts with them. After a finite-state pre-processing (division of the text in sentences, tagging with dictionaries, etc.), a series of finite-state transducers is applied one after the other to the text and locates left and right contexts that indicates the presence of a person name. An evaluation of the results of this extraction is presented.},
  isbn = {978-3-540-36390-3},
  langid = {english},
  keywords = {Compound Word,Coreference Resolution,Input Alphabet,Natural Language Processing,nosource,Output Alphabet},
  file = {C:\Users\nhiot\OneDrive\zotero\2001\Friburger et Maurel - 2001 - Finite-State Transducer Cascade to Extract Proper .pdf}
}

@inproceedings{gac,
  title = {{{AGG}} 2.0 -- New Features for Specifying and Analyzing Algebraic Graph Transformations},
  booktitle = {Applications of Graph Transformations with Industrial Relevance},
  author = {Runge, Olga and Ermel, Claudia and Taentzer, Gabriele},
  editor = {Sch{\"u}rr, Andy and Varr{\'o}, D{\'a}niel and Varr{\'o}, Gergely},
  year = {2012},
  pages = {81--88},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  abstract = {The integrated development environment AGG supports the specification of algebraic graph transformation systems based on attributed, typed graphs with node type inheritance, graph rules with application conditions, and graph constraints. It offers several analysis techniques for graph transformation systems including graph parsing, consistency checking of graphs as well as conflict and dependency detection in transformations by critical pair analysis of graph rules, an important instrument to support the confluence check of graph transformation systems. AGG 2.0 includes various new features added over the past two years. It supports the specification of complex control structures for rule application comprising the definition of control and object flow for rule sequences and nested application conditions. Furthermore, new possibilities for constructing rules from existing ones (e.g., inverse, minimal, amalgamated, and concurrent rules) and for more flexible usability of critical pair analyses have been realized.},
  isbn = {978-3-642-34176-2},
  keywords = {nosource}
}

@article{gac2,
  title = {A Visual Interpreter Semantics for Statecharts Based on Amalgamated Graph Transformation},
  author = {Golas, Ulrike and Biermann, Enrico and Ehrig, Hartmut and Ermel, Claudia},
  year = {2011},
  month = jan,
  journal = {ECEASST},
  volume = {39},
  keywords = {⛔ No DOI found,nosource}
}

@inproceedings{gaioExtendedNamedEntity2017,
  title = {Extended Named Entity Recognition Using Finite-State Transducers: {{An}} Application to Place Names},
  booktitle = {The Ninth International Conference on Advanced Geographic Information Systems, Applications, and Services ({{GEOProcessing}} 2017)},
  author = {Gaio, Mauro and Moncla, Ludovic},
  year = {2017},
  month = mar,
  address = {Nice, France},
  url = {https://hal.archives-ouvertes.fr/hal-01492994},
  hal_id = {hal-01492994},
  hal_version = {v1},
  keywords = {Geo-information processing,Geo-spatial data mining,Geo-spatial Web Ser- vices and processing},
  file = {C:\Users\nhiot\Zotero\storage\NG6IYJVY\Gaio et Moncla - 2017 - Extended named entity recognition using finite-sta.pdf}
}

@inproceedings{gaoDatatoneManagingAmbiguity2015,
  title = {Datatone: {{Managing}} Ambiguity in Natural Language Interfaces for Data Visualization},
  shorttitle = {Datatone},
  booktitle = {Proceedings of the 28th {{Annual ACM Symposium}} on {{User Interface Software}} \& {{Technology}}},
  author = {Gao, Tong and Dontcheva, Mira and Adar, Eytan and Liu, Zhicheng and Karahalios, Karrie G.},
  year = {2015},
  pages = {489--500},
  doi = {10/gh58sx},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2015\\Gao et al. - 2015 - Datatone Managing ambiguity in natural language i.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\UPCMIYJI\\2807442.html}
}

@article{gaoSurveyGraphEdit2010,
  ids = {gaoSurveyGraphEdit2010a},
  title = {A Survey of Graph Edit Distance},
  author = {Gao, Xinbo and Xiao, Bing and Tao, Dacheng and Li, Xuelong},
  year = {2010},
  month = feb,
  journal = {Pattern Analysis and Applications},
  volume = {13},
  number = {1},
  pages = {113--129},
  issn = {1433-755X},
  doi = {10/bkdspq},
  abstract = {Inexact graph matching has been one of the significant research foci in the area of pattern analysis. As an important way to measure the similarity between pairwise graphs error-tolerantly, graph edit distance (GED) is the base of inexact graph matching. The research advance of GED is surveyed in order to provide a review of the existing literatures and offer some insights into the studies of GED. Since graphs may be attributed or non-attributed and the definition of costs for edit operations is various, the existing GED algorithms are categorized according to these two factors and described in detail. After these algorithms are analyzed and their limitations are identified, several promising directions for further research are proposed.},
  langid = {english},
  annotation = {QID: Q62795295},
  file = {C:\Users\nhiot\OneDrive\zotero\2010\Gao et al. - 2010 - A survey of graph edit distance.pdf}
}

@inproceedings{garcelonEnrichissementSemantiqueAssocie2014,
  title = {Enrichissement S{\'e}mantique Associ{\'e} {\`a} La D{\'e}tection de La N{\'e}gation et Des Ant{\'e}c{\'e}dents Familiaux Dans Un Entrep{\^o}t de Donn{\'e}es Hospitalier.},
  booktitle = {{{JFIM}}},
  author = {Garcelon, Nicolas and Salomon, R{\'e}mi and Burgun, Anita},
  year = {2014},
  pages = {83--93},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2014\Garcelon et al. - 2014 - Enrichissement sémantique associé à la détection d.pdf}
}

@inproceedings{gaumeDesambiguisationParProximite2004,
  title = {D{\'e}sambigu{\"i}sation Par Proximit{\'e} Structurelle},
  booktitle = {Proceedings of {{TALN}}},
  author = {Gaume, Bruno and Hathout, Nabil and Muller, Philippe},
  year = {2004},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2004\Gaume et al. - 2004 - Désambiguïsation par proximité structurelle.pdf}
}

@misc{GENIA,
  title = {{{GENIA}}},
  url = {https://paperswithcode.com/dataset/genia},
  urldate = {2024-03-21},
  abstract = {The GENIA corpus is the primary collection of biomedical literature compiled and annotated within the scope of the GENIA project. The corpus was created to support the development and evaluation of information extraction and text mining systems for the domain of molecular biology. The corpus contains 1,999 Medline abstracts, selected using a PubMed query for the three MeSH terms ``human'', ``blood cells'', and ``transcription factors''. The corpus has been annotated with various levels of linguistic and semantic information. The primary categories of annotation in the GENIA corpus and the corresponding subcorpora are: Part-of-Speech annotation Constituency (phrase structure) syntactic annotation Term annotation Event annotation Relation annotation Coreference annotation},
  langid = {english},
  file = {C:\Users\nhiot\Zotero\storage\KXPYG8T8\genia.html}
}

@incollection{genthialActesTALN19971997,
  title = {Actes de {{TALN}} 1997 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 1997 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Genthial, Damien},
  year = {1997},
  month = jun,
  address = {Grenoble},
  keywords = {nosource}
}

@inproceedings{ghotingSystemMLDeclarativeMachine2011,
  title = {{{SystemML}}: {{Declarative}} Machine Learning on {{MapReduce}}},
  shorttitle = {{{SystemML}}},
  booktitle = {2011 {{IEEE}} 27th {{International Conference}} on {{Data Engineering}}},
  author = {Ghoting, Amol and Krishnamurthy, Rajasekar and Pednault, Edwin and Reinwald, Berthold and Sindhwani, Vikas and Tatikonda, Shirish and Tian, Yuanyuan and Vaithyanathan, Shivakumar},
  year = {2011},
  month = apr,
  pages = {231--242},
  publisher = {IEEE},
  issn = {1063-6382},
  doi = {10/cxxkhk},
  abstract = {MapReduce is emerging as a generic parallel programming paradigm for large clusters of machines. This trend combined with the growing need to run machine learning (ML) algorithms on massive datasets has led to an increased interest in implementing ML algorithms on MapReduce. However, the cost of implementing a large class of ML algorithms as low-level MapReduce jobs on varying data and machine cluster sizes can be prohibitive. In this paper, we propose SystemML in which ML algorithms are expressed in a higher-level language and are compiled and executed in a MapReduce environment. This higher-level language exposes several constructs including linear algebra primitives that constitute key building blocks for a broad class of supervised and unsupervised ML algorithms. The algorithms expressed in SystemML are compiled and optimized into a set of MapReduce jobs that can run on a cluster of machines. We describe and empirically evaluate a number of optimization strategies for efficiently executing these algorithms on Hadoop, an open-source MapReduce implementation. We report an extensive performance evaluation on three ML algorithms on varying data and cluster sizes.},
  keywords = {Clustering algorithms,Computer architecture,data analysis,data cluster,declarative machine learning,high level languages,higher level language,learning (artificial intelligence),linear algebra,machine cluster,Machine learning,Machine learning algorithms,open source MapReduce,optimisation,Optimization,optimization strategy,parallel programming,Runtime,Semantics,supervised ML algorithm,SystemML,unsupervised ML algorithm},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2011\\Ghoting et al. - 2011 - SystemML Declarative machine learning on MapReduc.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\JSSINK2U\\5767930.html;C\:\\Users\\nhiot\\Zotero\\storage\\YJJDLUND\\5767930.html}
}

@inproceedings{girgensohnDocuBrowseFacetedSearching2010,
  title = {{{DocuBrowse}}: {{Faceted Searching}}, {{Browsing}}, and {{Recommendations}} in an {{Enterprise Context}}},
  shorttitle = {{{DocuBrowse}}},
  booktitle = {Proceedings of the 15th International Conference on {{Intelligent}} User Interfaces - {{IUI}} '10},
  author = {Girgensohn, Andreas and Shipman, Frank and Chen, Francine and Wilcox, Lynn},
  year = {2010},
  series = {{{IUI}} '10},
  pages = {189--198},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10/gfvn2p},
  urldate = {2019-02-12},
  abstract = {Browsing and searching for documents in large, online enterprise document repositories are common activities. While internet search produces satisfying results for most user queries, enterprise search has not been as successful because of differences in document types and user requirements. To support users in finding the information they need in their online enterprise repository, we created DocuBrowse, a faceted document browsing and search system. Search results are presented within the user-created document hierarchy, showing only directories and documents matching selected facets and containing text query terms. In addition to file properties such as date and file size, automatically detected document types, or genres, serve as one of the search facets. Highlighting draws the user's attention to the most promising directories and documents while thumbnail images and automatically identified keyphrases help select appropriate documents. DocuBrowse utilizes document similarities, browsing histories, and recommender system techniques to suggest additional promising documents for the current facet and content filters.},
  isbn = {978-1-60558-515-4},
  langid = {english},
  keywords = {document management,document recommendation,document retrieval,document visualization,faceted search},
  file = {C:\Users\nhiot\OneDrive\zotero\2010\Girgensohn et al. - 2010 - DocuBrowse Faceted Searching, Browsing, and Recom.pdf}
}

@inproceedings{goasdoueEfficientQueryAnswering2013,
  title = {Efficient Query Answering against Dynamic {{RDF}} Databases},
  booktitle = {Proceedings of the 16th {{International Conference}} on {{Extending Database Technology}}},
  author = {Goasdou{\'e}, Fran{\c c}ois and Manolescu, Ioana and Roati{\c s}, Alexandra},
  year = {2013},
  month = mar,
  series = {{{EDBT}} '13},
  pages = {299--310},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2452376.2452412},
  urldate = {2023-08-03},
  abstract = {A promising method for efficiently querying RDF data consists of translating SPARQL queries into efficient RDBMS-style operations. However, answering SPARQL queries requires handling RDF reasoning, which must be implemented outside the relational engines that do not support it. We introduce the database (DB) fragment of RDF, going beyond the expressive power of previously studied RDF fragments. We devise novel sound and complete techniques for answering Basic Graph Pattern (BGP) queries within the DB fragment of RDF, exploring the two established approaches for handling RDF semantics, namely reformulation and saturation. In particular, we focus on handling database updates within each approach and propose a method for incrementally maintaining the saturation; updates raise specific difficulties due to the rich RDF semantics. Our techniques are designed to be deployed on top of any RDBMS(-style) engine, and we experimentally study their performance trade-offs.},
  isbn = {978-1-4503-1597-5},
  keywords = {query answering,RDF fragments,reasoning},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2013\\Goasdoué et al. - 2013 - Efficient query answering against dynamic RDF data.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\ZEREIB62\\2452376.html}
}

@inproceedings{goasdoueIncrementalStructuralSummarization2019,
  title = {Incremental Structural Summarization of {{RDF}} Graphs},
  author = {Goasdou{\'e}, Fran{\c c}ois and Guzewicz, Pawe{\l} and Manolescu, Ioana},
  year = {2019},
  month = mar,
  url = {https://hal.inria.fr/hal-01978784},
  urldate = {2019-05-23},
  abstract = {Realizing the full potential of Linked Open Data sharing and reuse is currently limited by the difficulty users have when trying to understand the data modeled within an RDF graph, in order to determine whether or not it may be useful for their need. We demonstrate our RDFQuotient tool, which builds compact summaries of heterogeneous RDF graphs for the purpose of first-sight visualizations. An RDFQuotient summary provides an overview of the complete structure of an RDF graph, while being typically many orders of magnitude smaller, thus can be easily grasped by new users. Our summarization algorithms are time linear in the size of the input graph and incremental: they incrementally update a summary upon addition of new data. For the demo, we plan to show the visualizations of our summaries obtained from well-known synthetic and real data sets. Further, attendees will be able to add data to the summarized RDF graphs and visually witness the incurred changes.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2019\\Goasdoué et al. - 2019 - Incremental structural summarization of RDF graphs.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\CIKDLEK7\\hal-01978784.html}
}

@inproceedings{gottlobComputingCoresData2005,
  title = {Computing Cores for Data Exchange: New Algorithms and Practical Solutions},
  shorttitle = {Computing Cores for Data Exchange},
  booktitle = {Proceedings of the Twenty-Fourth {{ACM SIGMOD-SIGACT-SIGART}} Symposium on {{Principles}} of Database Systems},
  author = {Gottlob, Georg},
  year = {2005},
  month = jun,
  series = {{{PODS}} '05},
  pages = {148--159},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1065167.1065187},
  urldate = {2023-08-16},
  abstract = {Data Exchange is the problem of inserting data structured under a source schema into a target schema of different structure (possibly with integrity constraints), while reflecting the source data as accurately as possible. We study computational issues related to data exchange in the setting of Fagin, Kolaitis, and Popa(PODS'03). We use the technique of hypertree decompositions to derive improved algorithms for computing the core of a relational instance with labeled nulls, a problem we show to be fixed-parameter intractable with respect to the block size of the input instances. We show that computing the core of a data exchange problem is tractable for two large and useful classes of target constraints. The first class includes functional dependencies and weakly acyclic inclusion dependencies. The second class consists of full tuple generating dependencies and arbitrary equation generating dependencies. Finally, we show that computing cores is NP-hard in presence of a system-predicate NULL(x), which is true iff x is a null value.},
  isbn = {978-1-59593-062-0},
  keywords = {nosource}
}

@inproceedings{gottlobOntologicalQueriesRewriting2011,
  title = {Ontological Queries: {{Rewriting}} and Optimization},
  shorttitle = {Ontological Queries},
  booktitle = {Proceedings of the 27th International Conference on Data Engineering, {{ICDE}}, Germany},
  author = {Gottlob, Georg and Orsi, Giorgio and Pieris, Andreas},
  year = {2011},
  month = apr,
  pages = {2--13},
  publisher = {IEEE},
  issn = {2375-026X},
  doi = {10.1109/ICDE.2011.5767965},
  urldate = {2023-12-24},
  abstract = {Ontological queries are evaluated against an enterprise ontology rather than directly on a database. The evaluation and optimization of such queries is an intriguing new problem for database research. In this paper we discuss two important aspects of this problem: query rewriting and query optimization. Query rewriting consists of the compilation of an ontological query into an equivalent query against the underlying relational database. The focus here is on soundness and completeness. We review previous results and present a new rewriting algorithm for rather general types of ontological constraints (description logics). In particular, we show how a conjunctive query (CQ) against an enterprise ontology can be compiled into a union of conjunctive queries (UCQ) against the underlying database. Ontological query optimization, in this context, attempts to improve this process so to produce possibly small and cost-effective output UCQ. We review existing optimization methods, and propose an effective new method that works for Linear Datalog{\textpm}, a description logic that encompasses well-known description logics of the DL-Lite family.},
  keywords = {nosource},
  file = {C:\Users\nhiot\OneDrive\zotero\2011\Gottlob et al. - 2011 - Ontological queries Rewriting and optimization.pdf}
}

@article{gottlobSchemaMappingDiscovery2010,
  title = {Schema Mapping Discovery from Data Instances},
  author = {Gottlob, Georg and Senellart, Pierre},
  year = {2010},
  month = feb,
  journal = {Journal of the ACM},
  volume = {57},
  number = {2},
  pages = {6:1--6:37},
  issn = {0004-5411},
  doi = {10/bk9tbh},
  urldate = {2021-09-01},
  abstract = {We introduce a theoretical framework for discovering relationships between two database instances over distinct and unknown schemata. This framework is grounded in the context of data exchange. We formalize the problem of understanding the relationship between two instances as that of obtaining a schema mapping so that a minimum repair of this mapping provides a perfect description of the target instance given the source instance. We show that this definition yields ``intuitive'' results when applied on database instances derived from each other by basic operations. We study the complexity of decision problems related to this optimality notion in the context of different logical languages and show that, even in very restricted cases, the problem is of high complexity.},
  keywords = {complexity,data exchange,instance,match,Schema mapping},
  annotation = {QID: Q59259577},
  file = {C:\Users\nhiot\OneDrive\zotero\2010\Gottlob et Senellart - 2010 - Schema mapping discovery from data instances.pdf}
}

@inproceedings{goyalAutomaticallyProducingPlot2010,
  title = {Automatically Producing Plot Unit Representations for Narrative Text},
  booktitle = {Proceedings of the 2010 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Goyal, Amit and Riloff, Ellen and Daum{\'e}, Hal},
  year = {2010},
  month = oct,
  series = {{{EMNLP}} '10},
  pages = {77--86},
  publisher = {Association for Computational Linguistics},
  address = {Cambridge, Massachusetts},
  urldate = {2020-05-10},
  abstract = {In the 1980s, plot units were proposed as a conceptual knowledge structure for representing and summarizing narrative stories. Our research explores whether current NLP technology can be used to automatically produce plot unit representations for narrative text. We create a system called AESOP that exploits a variety of existing resources to identify affect states and applies "projection rules" to map the affect states onto the characters in a story. We also use corpus-based techniques to generate a new type of affect knowledge base: verbs that impart positive or negative states onto their patients (e.g., being eaten is an undesirable state, but being fed is a desirable state). We harvest these "patient polarity verbs" from a Web corpus using two techniques: co-occurrence with Evil/Kind Agent patterns, and bootstrapping over conjunctions of verbs. We evaluate the plot unit representations produced by our system on a small collection of Aesop's fables.},
  file = {C:\Users\nhiot\OneDrive\zotero\2010\Goyal et al. - 2010 - Automatically producing plot unit representations .pdf}
}

@article{goyalRecentNamedEntity2018,
  title = {Recent {{Named Entity Recognition}} and {{Classification}} Techniques: {{A}} Systematic Review},
  shorttitle = {Recent {{Named Entity Recognition}} and {{Classification}} Techniques},
  author = {Goyal, Archana and Gupta, Vishal and Kumar, Manish},
  year = {2018},
  month = aug,
  journal = {Computer Science Review},
  volume = {29},
  pages = {21--43},
  publisher = {Elsevier},
  issn = {1574-0137},
  doi = {10.1016/j.cosrev.2018.06.001},
  urldate = {2024-02-13},
  abstract = {Textual information is becoming available in abundance on the web, arising the requirement of techniques and tools to extract the meaningful information. One of such an important information extraction task is Named Entity Recognition and Classification. It is the problem of finding the members of various predetermined classes, such as person, organization, location, date/time, quantities, numbers etc. The concept of named entity extraction was first proposed in Sixth Message Understanding Conference in 1996. Since then, a number of techniques have been developed by many researchers for extracting diversity of entities from different languages and genres of text. Still, there is a growing interest among research community to develop more new approaches to extract diverse named entities which are helpful in various natural language applications. Here we present a survey of developments and progresses made in Named Entity Recognition and Classification research.},
  langid = {english},
  file = {C:\Users\nhiot\Zotero\storage\DETWMZLZ\S1574013717302782.html}
}

@inproceedings{grabarCASFrenchCorpus2018,
  title = {{{CAS}}: {{French Corpus}} with {{Clinical Cases}}},
  shorttitle = {{{CAS}}},
  booktitle = {Proceedings of the {{Ninth International Workshop}} on {{Health Text Mining}} and {{Information Analysis}}},
  author = {Grabar, Natalia and Claveau, Vincent and Dalloux, Cl{\'e}ment},
  year = {2018},
  month = oct,
  pages = {122--128},
  publisher = {Association for Computational Linguistics},
  address = {Brussels, Belgium},
  doi = {10.18653/v1/w18-5614},
  urldate = {2021-05-25},
  abstract = {Textual corpora are extremely important for various NLP applications as they provide information necessary for creating, setting and testing these applications and the corresponding tools. They are also crucial for designing reliable methods and reproducible results. Yet, in some areas, such as the medical area, due to confidentiality or to ethical reasons, it is complicated and even impossible to access textual data representative of those produced in these areas. We propose the CAS corpus built with clinical cases, such as they are reported in the published scientific literature in French. We describe this corpus, currently containing over 397,000 word occurrences, and the existing linguistic and semantic annotations.},
  file = {C:\Users\nhiot\OneDrive\zotero\2018\Grabar et al. - 2018 - CAS French Corpus with Clinical Cases.pdf}
}

@inproceedings{grabarCorpusAnnoteCas2019,
  title = {Corpus Annot{\'e} de Cas Cliniques En Fran{\c c}ais},
  booktitle = {{{TALN}} 2019 - 26e Conference on Traitement Automatique Des Langues Naturelles},
  author = {Grabar, Natalia and Grouin, Cyril and Hamon, Thierry and Claveau, Vincent},
  year = {2019},
  month = jul,
  pages = {1--14},
  address = {Toulouse, France},
  url = {https://hal.archives-ouvertes.fr/hal-02391878},
  hal_id = {hal-02391878},
  hal_version = {v1},
  keywords = {annotations,cas clinique,cat{\'e}gorisation,categorization,clinical case,Clinical corpus,Corpus clinique,extraction d'information,information extraction},
  file = {C:\Users\nhiot\OneDrive\zotero\2019\Grabar et al. - 2019 - Corpus annoté de cas cliniques en français.pdf}
}

@book{grahneProblemIncompleteInformation1991,
  title = {The {{Problem}} of {{Incomplete Information}} in {{Relational Databases}}},
  author = {Grahne, G{\"o}sta},
  year = {1991},
  month = nov,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {554},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-54919-6},
  abstract = {In a relational database the information is recorded as rows in tables. However, in many practical situations the available information is incomplete and the values for some columns are missing. Yet few existing database management systems allow the user to enter null values in the database. This monograph analyses the problems raised by allowing null values in relational databases. The analysis covers semantical, syntactical, and computational aspects. Algorithms for query evaluation, dependency enforcement and updates in the presence of null values are also given. The analysis of the computational complexity of the algorithms suggests that from a practical point of view the database should be stored as Horn tables, which are generalizations of ordinary relations, allowing null values and Horn clause-like restrictions on these null values. Horn tables efficiently support a large class of queries, dependencies and updates.},
  isbn = {978-3-540-46507-2},
  langid = {english},
  lccn = {QA76.9.D3 G69 1991},
  keywords = {⛔ No DOI found,Computers / Artificial Intelligence / General,Computers / Database Administration \& Management,Computers / Information Technology,Computers / Information Theory,Computers / Programming / Algorithms,nosource,Relational databases},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1991\\Grahne - 1991 - The Problem of Incomplete Information in Relationa.pdf;C\:\\Users\\nhiot\\OneDrive\\zotero\\1991\\Grahne - 1991 - The Problem of Incomplete Information in Relationa2.pdf;C\:\\Users\\nhiot\\OneDrive\\zotero\\1991\\Grahne - 1991 - The Problem of Incomplete Information in Relationa3.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\R23IRBKN\\3-540-54919-6_10.html}
}

@article{grauxSPARUBSPARQLUPDATE2017,
  title = {{{SPARUB}}: {{SPARQL UPDATE Benchmark}}},
  shorttitle = {{{SPARUB}}},
  author = {Graux, Damien and Genev{\`e}s, Pierre and Laya{\"i}da, Nabil},
  year = {2017},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2017\\Graux et al. - 2017 - SPARUB SPARQL UPDATE Benchmark.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\7QW7N86J\\hal-01523496.html}
}

@article{greenUpdatingGraphDatabases2019,
  ids = {greenUpdatingGraphDatabases2019a},
  title = {Updating Graph Databases with {{Cypher}}},
  author = {Green, Alastair and Guagliardo, Paolo and Libkin, Leonid and Lindaaker, Tobias and Marsault, Victor and Plantikow, Stefan and Schuster, Martin and Selmer, Petra and Voigt, Hannes},
  year = {2019},
  month = aug,
  journal = {Proceedings of the VLDB Endowment},
  volume = {12},
  number = {12},
  pages = {2242--2254},
  publisher = {VLDB Endowment},
  issn = {2150-8097},
  doi = {10/ggwfzh},
  urldate = {2020-05-17},
  abstract = {The paper describes the present and the future of graph updates in Cypher, the language of the Neo4j property graph database and several other products. Update features include those with clear analogs in relational databases, as well as those that do not correspond to any relational operators. Moreover, unlike SQL, Cypher updates can be arbitrarily intertwined with querying clauses. After presenting the current state of update features, we point out their shortcomings, most notably violations of atomicity and non-deterministic behavior of updates. These have not been previously known in the Cypher community. We then describe the industry-academia collaboration on designing a revised set of Cypher update operations. Based on discovered shortcomings of update features, a number of possible solutions were devised. They were presented to key Cypher users, who were given the opportunity to comment on how update features are used in real life, and on their preferences for proposed fixes. As the result of the consultation, a new set of update operations for Cypher were designed. Those led to a streamlined syntax, and eliminated the unexpected and problematic behavior that original Cypher updates exhibited.},
  langid = {english},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2019\\Green et al. - 2019 - Updating graph databases with Cypher.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\8FQEVTPH\\3352063.html}
}

@inproceedings{grishmanInformationExtractionTechniques1997,
  title = {Information Extraction: {{Techniques}} and Challenges},
  shorttitle = {Information Extraction},
  booktitle = {International Summer School on Information Extraction},
  author = {Grishman, Ralph},
  year = {1997},
  volume = {1299},
  pages = {10--27},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-63438-x_2},
  isbn = {978-3-540-63438-6 978-3-540-69548-6},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1997\\Grishman - 1997 - Information extraction Techniques and challenges.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\PYHSUCCK\\10.html}
}

@inproceedings{grossUseFiniteAutomata1989,
  ids = {grossUseFiniteAutomata1987},
  title = {The Use of Finite Automata in the Lexical Representation of Natural Language},
  booktitle = {Electronic {{Dictionaries}} and {{Automata}} in {{Computational Linguistics}}},
  author = {Gross, Maurice},
  editor = {Gross, Maurice and Perrin, Dominique},
  year = {1989},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {34--50},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-51465-1_3},
  isbn = {978-3-540-48140-9},
  langid = {english},
  keywords = {Compound Word,Finite Automaton,Flight Simulator,Oxford English Dictionary,Simple Word},
  file = {C:\Users\nhiot\Zotero\storage\W3UCPU6M\10.html}
}

@inproceedings{grouinClassificationCasCliniques2021,
  ids = {grouinClassificationCasCliniques2021a},
  title = {{Classification de cas cliniques et {\'e}valuation automatique de r{\'e}ponses d'{\'e}tudiants : pr{\'e}sentation de la campagne DEFT 2021 (Clinical cases classification and automatic evaluation of student answers : Presentation of the DEFT 2021 Challenge)}},
  shorttitle = {{Classification de cas cliniques et {\'e}valuation automatique de r{\'e}ponses d'{\'e}tudiants}},
  booktitle = {{Actes de la 28e Conf{\'e}rence sur le Traitement Automatique des Langues Naturelles. Atelier D{\'E}fi Fouille de Textes (DEFT)}},
  author = {Grouin, Cyril and Grabar, Natalia and Illouz, Gabriel},
  year = {2021},
  month = jun,
  pages = {1--13},
  publisher = {ATALA},
  address = {Lille, France},
  url = {https://aclanthology.org/2021.jeptalnrecital-deft.1},
  urldate = {2023-09-22},
  abstract = {Le d{\'e}fi fouille de textes (DEFT) est une campagne d'{\'e}valuation annuelle francophone. Nous pr{\'e}sentons les corpus et baselines {\'e}labor{\'e}es pour trois t{\^a}ches : (i) identifier le profil clinique de patients d{\'e}crits dans des cas cliniques, (ii) {\'e}valuer automatiquement les r{\'e}ponses d'{\'e}tudiants sur des questionnaires en ligne (Moodle) {\`a} partir de la correction de l'enseignant, et (iii) poursuivre une {\'e}valuation de r{\'e}ponses d'{\'e}tudiants {\`a} partir de r{\'e}ponses d{\'e}j{\`a} {\'e}valu{\'e}es par l'enseignant. Les r{\'e}sultats varient de 0,394 {\`a} 0,814 de F-mesure sur la premi{\`e}re t{\^a}che (7 {\'e}quipes), de 0,448 {\`a} 0,682 de pr{\'e}cision sur la deuxi{\`e}me (3 {\'e}quipes), et de 0,133 {\`a} 0,510 de pr{\'e}cision sur la derni{\`e}re (3 {\'e}quipes).},
  langid = {french},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2021\\Grouin et al. - 2021 - Classification de cas cliniques et évaluation auto.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\X22H9BDQ\\2021.jeptalnrecital-deft.1.html}
}

@article{gTemporalRelationDiscovery2009,
  ids = {savovaTemporalRelationDiscovery2009,savovaTemporalRelationDiscovery2009a},
  title = {Towards {{Temporal Relation Discovery}} from the {{Clinical Narrative}}},
  author = {G, Savova and S, Bethard and W, Styler and J, Martin and M, Palmer and J, Masanz and W, Ward},
  year = {2009},
  month = nov,
  journal = {AMIA Annual Symposium Proceedings},
  volume = {2009},
  pages = {568--572},
  issn = {1942-597X},
  url = {https://europepmc.org/article/med/20351919},
  urldate = {2020-11-20},
  abstract = {Disease progression and understanding relies on temporal concepts. Discovery of automated temporal relations and timelines from the clinical narrative allows for mining large data sets of clinical text to uncover patterns at the disease and patient level. Our overall goal is the complex task of building a system for automated temporal relation discovery. As a first step, we evaluate enabling methods from the general natural language processing domain - deep parsing and semantic role labeling in predicate-argument structures - to explore their portability to the clinical domain. As a second step, we develop an annotation schema for temporal relations based on TimeML. In this paper we report results and findings from these first steps. Our next efforts will scale up the data collection to develop domain-specific modules for the enabling technologies within Mayo's open-source clinical Text Analysis and Knowledge Extraction System.},
  langid = {english},
  pmcid = {PMC2815499},
  pmid = {20351919},
  keywords = {Disease Progression,Humans,Methods,Narration,Natural Language Processing,Semantics,Time},
  annotation = {QID: Q38384557},
  file = {C:\Users\nhiot\OneDrive\zotero\2009\G et al. - 2009 - Towards Temporal Relation Discovery from the Clini.pdf}
}

@article{guagliardoCorrectnessSQLQueries2017,
  title = {Correctness of {{SQL Queries}} on {{Databases}} with {{Nulls}}},
  author = {Guagliardo, Paolo and Libkin, Leonid},
  year = {2017},
  month = oct,
  journal = {ACM SIGMOD Record},
  volume = {46},
  number = {3},
  pages = {5--16},
  issn = {0163-5808},
  doi = {10.1145/3156655.3156657},
  urldate = {2023-08-08},
  abstract = {Multiple issues with SQL's handling of nulls have been well documented. Having efficiency as its main goal, SQL disregards the standard notion of correctness on incomplete databases -- certain answers -- due to its high complexity. As a result, the evaluation of SQL queries on databases with nulls may produce answers that are just plain wrong. However, SQL evaluation can be modified, at least for relational algebra queries, to approximate certain answers, i.e., return only correct answers. We examine recently proposed approximation schemes for certain answers and analyze their complexity, both theoretical bounds and real-life behavior},
  file = {C:\Users\nhiot\OneDrive\zotero\2017\Guagliardo et Libkin - 2017 - Correctness of SQL Queries on Databases with Nulls.pdf}
}

@article{gutierrezHybridOntologybasedInformation2016,
  title = {A Hybrid Ontology-Based Information Extraction System},
  author = {Gutierrez, Fernando and Dou, Dejing and Fickas, Stephen and Wimalasuriya, Daya and Zong, Hui},
  year = {2016},
  month = dec,
  journal = {Journal of Information Science},
  volume = {42},
  number = {6},
  pages = {798--820},
  issn = {0165-5515, 1741-6485},
  doi = {10/f9d8c5},
  urldate = {2019-01-31},
  abstract = {Information Extraction is the process of automatically obtaining knowledge from plain text. Because of the ambiguity of written natural language, Information Extraction is a difficult task. Ontology-based Information Extraction (OBIE) reduces this complexity by including contextual information in the form of a domain ontology. The ontology provides guidance to the extraction process by providing concepts and relationships about the domain. However, OBIE systems have not been widely adopted because of the difficulties in deployment and maintenance. The Ontology-based Components for Information Extraction (OBCIE) architecture has been proposed as a form to encourage the adoption of OBIE by promoting reusability through modularity. In this paper, we propose two orthogonal extensions to OBCIE that allow the construction of hybrid OBIE systems with higher extraction accuracy and a new functionality. The first extension utilizes OBCIE modularity to integrate different types of implementation into one extraction system, producing a more accurate extraction. For each concept or relationship in the ontology, we can select the best implementation for extraction, or we can combine both implementations under an ensemble learning schema. The second extension is a novel ontology-based error detection mechanism. Following a heuristic approach, we can identify sentences that are logically inconsistent with the domain ontology. Because the implementation strategy for the extraction of a concept is independent of the functionality of the extraction, we can design a hybrid OBIE system with concepts utilizing different implementation strategies for extracting correct or incorrect sentences. Our evaluation shows that, in the implementation extension, our proposed method is more accurate in terms of correctness and completeness of the extraction. Moreover, our error detection method can identify incorrect statements with a high accuracy.},
  langid = {english},
  file = {C:\Users\nhiot\OneDrive\zotero\2016\Gutierrez et al. - 2016 - A hybrid ontology-based information extraction sys.pdf}
}

@article{Habel:1996:GGN:2379538.2379542,
  title = {{{GRAPH GRAMMARS WITH NEGATIVE APPLICATION CONDITIONS}}},
  author = {Habel, Annegret and Heckel, Reiko and Taentzer, Gabriele},
  year = {1996},
  month = dec,
  volume = {26},
  number = {3,4},
  pages = {287--313},
  publisher = {IOS Press},
  address = {Amsterdam, The Netherlands, The Netherlands},
  issn = {0169-2968},
  doi = {10.3233/FI-1996-263404},
  acmid = {2379542},
  issue_date = {December 1996},
  keywords = {application conditions,contextual conditions,graph grammars,graph transformation systems,nosource}
}

@article{Habel:2009:CHT:1552068.1552070,
  title = {Correctness of High-Level Transformation Systems Relative to Nested Conditions},
  author = {Habel, Annegret and Pennemann, Karl-heinz},
  year = {2009},
  month = apr,
  journal = {Mathematical. Structures in Comp. Sci.},
  volume = {19},
  number = {2},
  pages = {245--296},
  publisher = {Cambridge University Press},
  address = {New York, NY, USA},
  issn = {0960-1295},
  doi = {10.1017/S0960129508007202},
  acmid = {1552070},
  issue_date = {April 2009},
  keywords = {nosource}
}

@article{hajjiAdaptationText2OntoSupporting2020,
  title = {An Adaptation of {{Text2Onto}} for Supporting the {{French}} Language},
  author = {Hajji, Morad and Qbadou, Mohammed and Mansouri, Khalifa},
  year = {2020},
  month = aug,
  journal = {International Journal of Electrical and Computer Engineering (IJECE)},
  volume = {10},
  number = {4},
  pages = {3743},
  issn = {2722-2578, 2088-8708},
  doi = {10.11591/ijece.v10i4.pp3743-3750},
  urldate = {2024-02-29},
  abstract = {The ontologies are progressively imposing themselves in the field of knowledge management. While the manual construction of an ontology is by far the most reliable, this task has proved to be too tedious and expensive. To assist humans in the process of building an ontology, several tools have emerged proposing the automatic or semi-automatic construction of ontologies. In this context, Text2Onto has become one of the most recognized ontology learning tools. The performance of this tool is confirmed by several research works. However, the development of this tool is based on Princeton WordNet (PWN) for English. As a result, it is limited to the processing of textual resources written in English. In this paper, we present our approach based on JWOLF, a Java API to access the free WordNet for French that we have developed to adapt this tool for the construction of ontologies from corpus in French. To evaluate the usefulness of our approach, we assessed the performance of the improved version of Text2Onto on a simplistic corpus of French language documents. The results of this experiment have shown that the improved version of Text2Onto according to our approach is effective for the construction of an ontology from textual documents in the French language.},
  file = {C:\Users\nhiot\OneDrive\zotero\2020\Hajji et al. - 2020 - An adaptation of Text2Onto for supporting the Fren.pdf}
}

@inproceedings{halfeld-ferrariRDFUpdatesConstraints2017,
  title = {{{RDF Updates}} with {{Constraints}}},
  booktitle = {Knowledge {{Engineering}} and {{Semantic Web}} - 8th {{International Conference}}, {{KESW}}, {{Szczecin}}, {{Poland}}, {{Proceedings}}},
  author = {{Halfeld-Ferrari}, Mirian and Hara, Carmem S. and Uber, Flavio R.},
  editor = {R{\'o}{\.z}ewski, Przemys{\l}aw and Lange, Christoph},
  year = {2017},
  series = {Communications in {{Computer}} and {{Information Science}}},
  volume = {786},
  pages = {229--245},
  publisher = {Springer International Publishing},
  address = {Szczecin, Poland},
  doi = {10.1007/978-3-319-69548-8_16},
  abstract = {This paper deals with the problem of updating an RDF database, expected to satisfy user-defined constraints as well as RDF intrinsic semantic constraints. As updates may violate these constraints, side-effects are generated in order to preserve consistency. We investigate the use of nulls (blank nodes) as placeholders for unknown required data as a technique to provide this consistency and to reduce the number of side-effects. Experimental results validate our goals.},
  isbn = {978-3-319-69547-1 978-3-319-69548-8},
  langid = {english},
  keywords = {Constraints,RDF,RDFS,Updates},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2017\\Halfed Ferrari et al. - 2017 - RDF Updates with Constraints.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\KT9SZGTE\\978-3-319-69548-8_16.html}
}

@article{halfeld-ferrariUpdateRulesDatalog1998,
  title = {Update {{Rules}} in {{Datalog Programs}}},
  author = {{Halfeld-Ferrari}, Mirian and Laurent, Dominique and Spyratos, Nicolas},
  year = {1998},
  month = dec,
  journal = {Journal of Logic and Computation},
  volume = {8},
  number = {6},
  pages = {745--775},
  publisher = {OUP},
  issn = {0955-792X, 1465-363X},
  doi = {10.1093/logcom/8.6.745},
  abstract = {We propose a deductive database model containing two kinds of rules: update rules of the form L0{\textleftarrow}L1, where L0 and L1 are literals, and query rules of the form of normal logic program rules. A basic feature of our approach is that new knowledge inputs are always assimilated. Moreover, updates are always deterministic and they preserve database consistency.We consider that update rules have higher priority than query rules, i.e., update rules may generate exceptions to query-driven derivations. We introduce a semantics framework for database update and query answering, based on the well-founded semantics. We also suggest an alternative approach based on extended logic programs and we show that our database model can be defined in terms of non-monotonic formalisms.},
  langid = {english},
  keywords = {Datalog,deductive database,nosource,update},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1998\\Halfeld-Ferrari et al. - 1998 - Update Rules in Datalog Programs.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\7CB956IH\\8142245.html;C\:\\Users\\nhiot\\Zotero\\storage\\B72S7SY9\\8142245.html}
}

@inproceedings{halfeld-ferrariUpdatingRDFDatabases2017,
  title = {Updating {{RDF}}/{{S Databases Under Constraints}}},
  booktitle = {Advances in Databases and Information Systems - 21st European Conference, {{ADBIS}}, Nicosia, Cyprus, Proceedings},
  author = {{Halfeld-Ferrari}, Mirian and Laurent, Dominique},
  editor = {Kirikova, M{\=a}r{\=i}te and N{\o}rv{\aa}g, Kjetil and Papadopoulos, George A.},
  year = {2017},
  volume = {10509},
  pages = {357--371},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-66917-5_24},
  urldate = {2023-12-24},
  isbn = {978-3-319-66916-8 978-3-319-66917-5},
  keywords = {nosource}
}

@article{halkidiTHESUSOrganizingWeb2003,
  title = {{{THESUS}}: {{Organizing Web}} Document Collections Based on Link Semantics},
  shorttitle = {{{THESUS}}},
  author = {Halkidi, Maria and Nguyen, Benjamin and Varlamis, Iraklis and Vazirgiannis, Michalis},
  year = {2003},
  month = nov,
  journal = {The VLDB Journal},
  volume = {12},
  number = {4},
  pages = {320--332},
  issn = {0949-877X},
  doi = {10/c4pqpg},
  urldate = {2020-02-13},
  abstract = {The requirements for effective search and management of the WWW are stronger than ever. Currently Web documents are classified based on their content not taking into account the fact that these documents are connected to each other by links. We claim that a page's classification is enriched by the detection of its incoming links' semantics. This would enable effective browsing and enhance the validity of search results in the WWW context. Another aspect that is underaddressed and strictly related to the tasks of browsing and searching is the similarity of documents at the semantic level. The above observations lead us to the adoption of a hierarchy of concepts (ontology) and a thesaurus to exploit links and provide a better characterization of Web documents. The enhancement of document characterization makes operations such as clustering and labeling very interesting. To this end, we devised a system called THESUS. The system deals with an initial sets of Web documents, extracts keywords from all pages' incoming links, and converts them to semantics by mapping them to a domain's ontology. Then a clustering algorithm is applied to discover groups of Web documents. The effectiveness of the clustering process is based on the use of a novel similarity measure between documents characterized by sets of terms. Web documents are organized into thematic subsets based on their semantics. The subsets are then labeled, thereby enabling easier management (browsing, searching, querying) of the Web. In this article, we detail the process of this system and give an experimental analysis of its results.},
  langid = {english},
  keywords = {Document clustering,Link analysis,Link management,Semantics,Similarity measure,World Wide Web},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2003\\Halkidi et al. - 2003 - THESUS Organizing Web document collections based .pdf;C\:\\Users\\nhiot\\Zotero\\storage\\LTJF3CSJ\\s00778-003-0100-6.html}
}

@inproceedings{haoPatMatDistributedPattern2019,
  title = {{{PatMat}}: {{A Distributed Pattern Matching Engine}} with {{Cypher}}},
  shorttitle = {{{PatMat}}},
  booktitle = {Proceedings of the 28th {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Hao, Kongzhang and Yang, Zhengyi and Lai, Longbin and Lai, Zhengmin and Jin, Xin and Lin, Xuemin},
  year = {2019},
  month = nov,
  series = {{{CIKM}} '19},
  pages = {2921--2924},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3357384.3357840},
  urldate = {2022-05-23},
  abstract = {Graph pattern matching is one of the most fundamental problems in graph database and is associated with a wide spectrum of applications. Due to its computational intensiveness, researchers have primarily devoted their efforts to improving the performance of the algorithm while constraining the graphs to have singular labels on vertices (edges) or no label. Whereas in practice graphs are typically associated with rich properties, thus the main focus in the industry is instead on powerful query languages that can express a sufficient number of pattern matching scenarios. We demo PatMat in this work to glue together the academic efforts on performance and the industrial efforts on expressiveness. To do so, we leverage the state-of-the-art join-based algorithms in the distributed contexts and Cypher query language - the most widely-adopted declarative language for graph pattern matching. The experiments demonstrate how we are capable of turning complex Cypher semantics into a distributed solution with high performance.},
  isbn = {978-1-4503-6976-3},
  keywords = {cypher,distributed processing,graph database,graph pattern matching,join optimization},
  file = {C:\Users\nhiot\OneDrive\zotero\2019\Hao et al. - 2019 - PatMat A Distributed Pattern Matching Engine with.pdf}
}

@misc{hardtOAuthAuthorizationFramework,
  title = {The {{OAuth}} 2.0 {{Authorization Framework}}},
  author = {Hardt {$<$}dick.hardt@gmail.com{$>$}, Dick},
  url = {https://tools.ietf.org/html/rfc6749},
  urldate = {2020-10-02},
  langid = {english},
  file = {C:\Users\nhiot\Zotero\storage\LNB48BA2\rfc6749.html}
}

@inproceedings{hassanGRFusionGraphsFirstClass2018,
  ids = {hassanGRFusionGraphsFirstClass2018a},
  title = {{{GRFusion}}: {{Graphs}} as {{First-Class Citizens}} in {{Main-Memory Relational Database Systems}}},
  shorttitle = {{{GRFusion}}},
  booktitle = {Proceedings of the 2018 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Hassan, Mohamed S. and Kuznetsova, Tatiana and Jeong, Hyun Chai and Aref, Walid G. and Sadoghi, Mohammad},
  year = {2018},
  month = may,
  series = {{{SIGMOD}} '18},
  pages = {1789--1792},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3183713.3193541},
  urldate = {2024-01-11},
  abstract = {The maturity of RDBMSs has motivated academia and industry to invest efforts in leveraging RDBMSs for graph processing, where efficiency is proven for vital graph queries. However, none of these efforts process graphs natively inside the RDBMS, which is particularly challenging due to the impedance mismatch between the relational and the graph models. In this demonstration, we present GRFusion, an in-memory relational database system, where graphs are managed as first-class citizens. GRFusion is realized inside VoltDB. The SQL and query engines of VoltDB are empowered to declaratively define graphs and execute cross-data-model query plans that consist of relational operators and newly-introduced graph operators. Using a social network and a real continental-sized road network covering the entire U.S., we demonstrate the functionality and the performance of GRFusion in evaluating queries that reference both relational tables and graphs seamlessly in the same query execution pipeline. GRFusion shows up to four orders-of-magnitude speed-up in query-time w.r.t. state-of-the-art approaches.},
  isbn = {978-1-4503-4703-7},
  langid = {english},
  keywords = {cross-data-model query plans,graph queries,main-memory databases,nosource,path traversals},
  file = {C:\Users\nhiot\OneDrive\zotero\2018\Hassan et al. - 2018 - GRFusion Graphs as First-Class Citizens in Main-M3.pdf}
}

@inproceedings{hauswaldSiriusOpenEndtoend2015,
  title = {Sirius: {{An}} Open End-to-End Voice and Vision Personal Assistant and Its Implications for Future Warehouse Scale Computers},
  shorttitle = {Sirius},
  booktitle = {Proceedings of the {{Twentieth International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}},
  author = {Hauswald, Johann and Laurenzano, Michael A. and Zhang, Yunqi and Li, Cheng and Rovinski, Austin and Khurana, Arjun and Dreslinski, Ronald G. and Mudge, Trevor and Petrucci, Vinicius and Tang, Lingjia},
  year = {2015},
  pages = {223--238},
  doi = {10/gjbtj3},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2015\\Hauswald et al. - 2015 - Sirius An open end-to-end voice and vision person.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\7CXHHSP3\\2694344.html}
}

@article{heChEMU2020Natural2021,
  title = {{{ChEMU}} 2020: {{Natural Language Processing Methods Are Effective}} for {{Information Extraction From Chemical Patents}}},
  shorttitle = {{{ChEMU}} 2020},
  author = {He, Jiayuan and Nguyen, Dat Quoc and Akhondi, Saber A. and Druckenbrodt, Christian and Thorne, Camilo and Hoessel, Ralph and Afzal, Zubair and Zhai, Zenan and Fang, Biaoyan and Yoshikawa, Hiyori and Albahem, Ameer and Cavedon, Lawrence and Cohn, Trevor and Baldwin, Timothy and Verspoor, Karin},
  year = {2021},
  journal = {Frontiers in Research Metrics and Analytics},
  volume = {6},
  issn = {2504-0537},
  doi = {10/gpjd9k},
  urldate = {2022-02-23},
  abstract = {Chemical patents represent a valuable source of information about new chemical compounds, which is critical to the drug discovery process. Automated information extraction over chemical patents is, however, a challenging task due to the large volume of existing patents and the complex linguistic properties of chemical patents. The Cheminformatics Elsevier Melbourne University (ChEMU) evaluation lab 2020, part of the Conference and Labs of the Evaluation Forum 2020 (CLEF2020), was introduced to support the development of advanced text mining techniques for chemical patents. The ChEMU 2020 lab proposed two fundamental information extraction tasks focusing on chemical reaction processes described in chemical patents: (1) chemical named entity recognition, requiring identification of essential chemical entities and their roles in chemical reactions, as well as reaction conditions; and (2) event extraction, which aims at identification of event steps relating the entities involved in chemical reactions. The ChEMU 2020 lab received 37 team registrations and 46 runs. Overall, the performance of submissions for these tasks exceeded our expectations, with the top systems outperforming strong baselines. We further show the methods to be robust to variations in sampling of the test data. We provide a detailed overview of the ChEMU 2020 corpus and its annotation, showing that inter-annotator agreement is very strong. We also present the methods adopted by participants, provide a detailed analysis of their performance, and carefully consider the potential impact of data leakage on interpretation of the results. The ChEMU 2020 Lab has shown the viability of automated methods to support information extraction of key information in chemical patents.},
  annotation = {QID: Q112063325},
  file = {C:\Users\nhiot\OneDrive\zotero\2021\He et al. - 2021 - ChEMU 2020 Natural Language Processing Methods Ar.pdf}
}

@article{hellCoreGraph1992,
  title = {The Core of a Graph},
  author = {Hell, Pavol and Ne{\v s}et{\v r}il, Jaroslav},
  year = {1992},
  month = nov,
  journal = {Discrete Mathematics},
  volume = {109},
  number = {1-3},
  pages = {117--126},
  issn = {0012365X},
  doi = {10.1016/0012-365X(92)90282-K},
  urldate = {2024-01-03},
  langid = {english},
  keywords = {nosource}
}

@misc{HelpPropertyConstraints,
  title = {Help:{{Property}} Constraints Portal - {{Wikidata}}},
  url = {https://www.wikidata.org/wiki/Help:Property\_constraints\_portal},
  urldate = {2021-03-19},
  file = {C:\Users\nhiot\Zotero\storage\C8L8679Y\HelpProperty_constraints_portal.html}
}

@misc{hendrickxBabelEnteEntityExtractioN,
  title = {{{BabelEnte}}: {{Entity extractioN}}, {{Translation}} and {{Evaluation}} Using {{BabelFy}}},
  shorttitle = {{{BabelEnte}}},
  author = {Hendrickx, Iris, Maarten van Gompel},
  url = {https://github.com/proycon/babelente},
  urldate = {2020-01-21},
  copyright = {GNU General Public License v3},
  keywords = {{computational\_linguistics,},{dbpedia,},{entities,},{linguistics,},{nlp,},{wikipedia,},Text Processing - Linguistic,tramooc},
  file = {C:\Users\nhiot\Zotero\storage\7FEFKE4W\BabelEnte.html}
}

@inproceedings{HHKU18,
  title = {Urban Data Consistency in {{RDF}}: {{A}} Case Study of {{Curitiba}} Transportation System},
  booktitle = {{{LADaS}}@{{VLDB}}},
  author = {{Halfeld-Ferrari}, Mirian and Hara, Carmem S. and Kozievitch, N{\'a}dia P. and Uber, Flavio R.},
  year = {2018},
  series = {{{CEUR}} Workshop Proceedings},
  volume = {2170},
  pages = {33--40},
  publisher = {CEUR-WS.org},
  keywords = {⛔ No DOI found,nosource}
}

@misc{HighLevelGrammarInteractive,
  title = {A {{High-Level Grammar}} of {{Interactive Graphics}}},
  journal = {Vega-Lite},
  url = {https://vega.github.io/vega-lite/},
  urldate = {2021-03-02},
  abstract = {Vega-Lite - a high-level grammar for statistical graphics. Vega-Lite provides a higher-level grammar for visual analysis, comparable to ggplot or Tableau, that generates complete Vega specifications. Vega-Lite specifications consist of simple mappings of variables in a data set to visual encoding channels such as x, y, color, and size. These mappings are then translated into detailed visualization specifications in the form of Vega specification language. Vega-Lite produces default values for visualization components (e.g., scales, axes, and legends) in the output Vega specification using a rule-based approach, but users can explicit specify these properties to override default values.},
  file = {C:\Users\nhiot\Zotero\storage\A8GAMZZQ\vega-lite.html}
}

@misc{hildebrandJSONWebEncryption,
  title = {{{JSON Web Encryption}} ({{JWE}})},
  author = {Hildebrand, Joe and Jones, Michael},
  url = {https://tools.ietf.org/html/rfc7516},
  urldate = {2020-10-02},
  langid = {english},
  file = {C:\Users\nhiot\Zotero\storage\PRLVX49K\rfc7516.html}
}

@book{hinzenOxfordHandbookCompositionality2012,
  title = {The Oxford Handbook of Compositionality},
  editor = {Hinzen, Wolfram and Machery, Edouard and Werning, Markus},
  year = {2012},
  publisher = {Oxford},
  doi = {10.1093/oxfordhb/9780199541072.001.0001},
  keywords = {nosource}
}

@inproceedings{hiotDOINGDEFTUtilisation2021,
  title = {{DOING@DEFT : utilisation de lexiques pour une classification efficace de cas cliniques}},
  shorttitle = {{DOING@DEFT}},
  booktitle = {{Traitement Automatique des Langues Naturelles}},
  author = {Hiot, Nicolas and Minard, Anne-Lyse and Badin, Flora},
  editor = {Denis, Pascal and Grabar, Natalia and Fraisse, Amel and Cardon, R{\'e}mi and Jacquemin, Bernard and Kergosien, Eric and Balvet, Antonio},
  year = {2021},
  pages = {41--53},
  publisher = {ATALA},
  address = {Lille, France},
  url = {https://hal.science/hal-03265924},
  urldate = {2023-09-22},
  abstract = {Nous pr{\'e}sentons dans cet article notre participation {\`a} la t{\^a}che 1 de la campagne d'{\'e}valuation francophone DEFT 2021, sur l'identification du profil clinique du patient. Nous proposons une m{\'e}thode {\'e}volutive et efficace en temps et en ressources pour la classification de documents m{\'e}dicaux pouvant {\^e}tre facilement adapt{\'e}e {\`a} d'autres domaines de recherche. Notre syst{\`e}me a obtenu les meilleures performances sur cette t{\^a}che avec une F-mesure de 0,814.},
  copyright = {All rights reserved},
  hal_id = {hal-03265924},
  hal_version = {v1},
  langid = {french},
  pdf = {https://hal.archives-ouvertes.fr/hal-03265924/file/75.pdf},
  keywords = {⛔ No DOI found,cas clinique,classification.,lexique,me,transducteur fini},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2021\\Hiot et al. - 2021 - DOING@DEFT  utilisation de lexiques pour une clas.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\FHMF4CQI\\hal-03265924.html}
}

@techreport{HL17,
  title = {Updating {{RDF}}/{{S}} Databases under Negative and Tuple-Generating Constraints},
  author = {{Halfeld-Ferrari}, Mirian and Laurent, Dominique},
  year = {2017},
  institution = {LIFO- Universit{\'e} d'Orl{\'e}ans, RR-2017-05},
  url = {https://www.univ-orleans.fr/lifo/rapports.php?lang=en\&sub=sub3},
  keywords = {nosource}
}

@inproceedings{hoffartRobustDisambiguationNamed2011,
  title = {Robust {{Disambiguation}} of {{Named Entities}} in {{Text}}},
  booktitle = {Proceedings of the {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Hoffart, Johannes and Yosef, Mohamed Amir and Bordino, Ilaria and F{\"u}rstenau, Hagen and Pinkal, Manfred and Spaniol, Marc and Taneva, Bilyana and Thater, Stefan and Weikum, Gerhard},
  year = {2011},
  series = {{{EMNLP}} '11},
  pages = {782--792},
  publisher = {Association for Computational Linguistics},
  address = {Stroudsburg, PA, USA},
  url = {http://dl.acm.org/citation.cfm?id=2145432.2145521},
  urldate = {2019-01-23},
  abstract = {Disambiguating named entities in natural-language text maps mentions of ambiguous names onto canonical entities like people or places, registered in a knowledge base such as DBpedia or YAGO. This paper presents a robust method for collective disambiguation, by harnessing context from knowledge bases and using a new form of coherence graph. It unifies prior approaches into a comprehensive framework that combines three measures: the prior probability of an entity being mentioned, the similarity between the contexts of a mention and a candidate entity, as well as the coherence among candidate entities for all mentions together. The method builds a weighted graph of mentions and candidate entities, and computes a dense subgraph that approximates the best joint mention-entity mapping. Experiments show that the new method significantly outperforms prior methods in terms of accuracy, with robust behavior across a variety of inputs.},
  isbn = {978-1-937284-11-4},
  annotation = {00000},
  file = {C:\Users\nhiot\OneDrive\zotero\2011\Hoffart et al. - 2011 - Robust Disambiguation of Named Entities in Text.pdf}
}

@inproceedings{holschPerformanceAnalyticalPattern2017a,
  ids = {holschPerformanceAnalyticalPattern2017},
  title = {On the {{Performance}} of {{Analytical}} and {{Pattern Matching Graph Queries}} in {{Neo4j}} and a {{Relational Database}}},
  booktitle = {{{EDBT}}/{{ICDT}} 2017 {{Joint Conference}}: 6th {{International Workshop}} on {{Querying Graph Structured Data}} ({{GraphQ}})},
  author = {H{\"o}lsch, J{\"u}rgen and Schmidt, Tobias and Grossniklaus, Michael},
  year = {2017},
  pages = {8},
  abstract = {Graph databases with a custom non-relational backend promote themselves to outperform relational databases in answering queries on large graphs. Recent empirical studies show that this claim is not always true. However, these studies focus only on pattern matching queries and neglect analytical queries used in practice such as shortest path, diameter, degree centrality or closeness centrality. In addition, there is no distinction between different types of pattern matching queries. In this paper, we introduce a set of analytical and pattern matching queries, and evaluate them in Neo4j and a market-leading commercial relational database system. We show that the relational database system outperforms Neo4j for our analytical queries and that Neo4j is faster for queries that do not filter on specific edge types.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2017\\Hölsch et al. - 2017 - On the Performance of Analytical and Pattern Match2.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\YD6KGH5Y\\39842.html}
}

@misc{HomeColahBlog,
  ids = {HomeColahBloga},
  title = {Home - Colah's Blog},
  url = {https://colah.github.io/},
  urldate = {2020-04-04},
  file = {C:\Users\nhiot\Zotero\storage\YZXA5V6A\colah.github.io.html}
}

@inproceedings{honnibalImprovedNonmonotonicTransition2015,
  title = {An Improved Non-Monotonic Transition System for Dependency Parsing},
  booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  author = {Honnibal, Matthew and Johnson, Mark},
  year = {2015},
  month = sep,
  pages = {1373--1378},
  publisher = {Association for Computational Linguistics},
  address = {Lisbon, Portugal},
  url = {https://aclweb.org/anthology/D/D15/D15-1162},
  file = {C:\Users\nhiot\OneDrive\zotero\2015\Honnibal et Johnson - 2015 - An improved non-monotonic transition system for de.pdf}
}

@misc{honnibalSpaCyIndustrialstrengthNatural2020,
  title = {{{spaCy}}: {{Industrial-strength Natural Language Processing}} in {{Python}}},
  shorttitle = {{{spaCy}}},
  author = {Honnibal, Matthew and Montani, Ines and Van Landeghem, Sofie and Boyd, Adriane},
  year = {2020},
  doi = {10.5281/zenodo.1212303},
  urldate = {2024-03-21},
  abstract = {💫 Industrial-strength Natural Language Processing (NLP) in Python},
  copyright = {MIT}
}

@article{honnibalSpacyNaturalLanguage2017,
  title = {Spacy 2: {{Natural}} Language Understanding with Bloom Embeddings, Convolutional Neural Networks and Incremental Parsing},
  shorttitle = {Spacy 2},
  author = {Honnibal, Matthew and Montani, Ines},
  year = {2017},
  journal = {To appear},
  volume = {7},
  number = {1},
  optshorttitle = {spacy 2},
  keywords = {⛔ No DOI found,nosource}
}

@inproceedings{honnibalSpaCyNaturalLanguage2017a,
  title = {{{spaCy}} 2: {{Natural Language Understanding}} with {{Bloom Embeddings}}, {{Convolutional Neural Networks}} and {{Incremental Parsing}}. {{Neural Machine Translation}}},
  shorttitle = {{{spaCy}} 2},
  booktitle = {Proceedings of the {{Association}} for {{Computational Linguistics}} ({{ACL}})},
  author = {Honnibal, Matthew and Montani, Ines},
  year = {2017},
  pages = {688--697},
  keywords = {⛔ No DOI found}
}

@book{hopcroftIntroductionAutomataTheory2007,
  title = {Introduction to Automata Theory, Languages and Computation},
  author = {Hopcroft, John E. and Motwani, Rajeev and Ullman, Jeffrey D.},
  year = {2007},
  edition = {3rd},
  publisher = {Pearson/Addison Wesley},
  address = {Boston},
  isbn = {978-0-321-45536-9 978-0-321-46225-1 978-0-321-45537-6},
  langid = {english},
  lccn = {QA267 .H56 2007},
  keywords = {Computational complexity,Formal languages,Machine theory},
  annotation = {OCLC: ocm69013079 QID: Q90418603},
  file = {C:\Users\nhiot\OneDrive\zotero\2007\Hopcroft et al. - 2007 - Introduction to automata theory, languages and com.pdf}
}

@article{hoqueApplyingPragmaticsPrinciples2018,
  title = {Applying {{Pragmatics Principles}} for {{Interaction}} with {{Visual Analytics}}},
  author = {Hoque, Enamul and Setlur, Vidya and Tory, Melanie and Dykeman, Isaac},
  year = {2018},
  month = jan,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {24},
  number = {1},
  pages = {309--318},
  issn = {1077-2626},
  doi = {10/gcp7wd},
  urldate = {2021-03-02},
  abstract = {Interactive visual data analysis is most productive when users can focus on answering the questions they have about their data, rather than focusing on how to operate the interface to the analysis tool. One viable approach to engaging users in interactive conversations with their data is a natural language interface to visualizations. These interfaces have the potential to be both more expressive and more accessible than other interaction paradigms. We explore how principles from language pragmatics can be applied to the flow of visual analytical conversations, using natural language as an input modality. We evaluate the effectiveness of pragmatics support in our system Evizeon, and present design considerations for conversation interfaces to visual analytics tools.},
  langid = {english},
  annotation = {QID: Q50230412},
  file = {C:\Users\nhiot\OneDrive\zotero\2018\Hoque et al. - 2018 - Applying Pragmatics Principles for Interaction wit.pdf}
}

@misc{HordeLog,
  title = {Horde :: {{Log}} In},
  url = {https://webmailetu.univ-orleans.fr/login.php},
  urldate = {2020-04-09},
  file = {C:\Users\nhiot\Zotero\storage\XBLFK35Q\login.html}
}

@misc{HowUseAI2019,
  title = {How to Use {{AI}} Powered {{Query}} in {{Google Spreadsheet}}},
  year = {2019},
  month = jan,
  journal = {Kanoki},
  url = {https://kanoki.org/2019/01/05/how-google-sheet-uses-natural-language-processing/},
  urldate = {2020-05-10},
  abstract = {Dealing with data has always been a daunting task no matter how much data geek you are. Being a Data Scientist the moment I see a new dataset the first thing which comes to my mind is OMG! How would I clean or explore this data? Can I ever do a EDA(Exploratory Data Analysis) on...},
  langid = {american},
  file = {C:\Users\nhiot\Zotero\storage\UPBCTH2R\how-google-sheet-uses-natural-language-processing.html}
}

@article{huangEvorusCrowdpoweredConversational2018,
  title = {Evorus: {{A Crowd-powered Conversational Assistant Built}} to {{Automate Itself Over Time}}},
  shorttitle = {Evorus},
  author = {Huang, Ting-Hao 'Kenneth' and Chang, Joseph Chee and Bigham, Jeffrey P.},
  year = {2018},
  month = apr,
  journal = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
  eprint = {1801.02668},
  pages = {1--13},
  doi = {10/gjbss2},
  urldate = {2021-03-11},
  abstract = {Crowd-powered conversational assistants have been shown to be more robust than automated systems, but do so at the cost of higher response latency and monetary costs. A promising direction is to combine the two approaches for high quality, low latency, and low cost solutions. In this paper, we introduce Evorus, a crowd-powered conversational assistant built to automate itself over time by (i) allowing new chatbots to be easily integrated to automate more scenarios, (ii) reusing prior crowd answers, and (iii) learning to automatically approve response candidates. Our 5-month-long deployment with 80 participants and 281 conversations shows that Evorus can automate itself without compromising conversation quality. Crowd-AI architectures have long been proposed as a way to reduce cost and latency for crowd-powered systems; Evorus demonstrates how automation can be introduced successfully in a deployed system. Its architecture allows future researchers to make further innovation on the underlying automated components in the context of a deployed open domain dialog system.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,H.5.m},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2018\\Huang et al. - 2018 - Evorus A Crowd-powered Conversational Assistant B.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\R94DI89G\\1801.html}
}

@article{huNaturalLanguageAggregate2018,
  title = {Natural Language Aggregate Query over {{RDF}} Data},
  author = {Hu, Xin and Dang, Depeng and Yao, Yingting and Ye, Luting},
  year = {2018},
  month = jul,
  journal = {Information Sciences},
  volume = {454--455},
  pages = {363--381},
  issn = {00200255},
  doi = {10.1016/j.ins.2018.04.042},
  urldate = {2024-03-29},
  langid = {english},
  optbibsource = {dblp computer science bibliography, https://dblp.org},
  optbiburl = {https://dblp.org/rec/journals/isci/HuDYY18.bib},
  opttimestamp = {Thu, 16 Apr 2020 14:52:54 +0200},
  opturl = {https://doi.org/10.1016/j.ins.2018.04.042},
  keywords = {nosource},
  file = {C:\Users\nhiot\OneDrive\zotero\2018\Hu et al. - 2018 - Natural language aggregate query over RDF data.pdf}
}

@article{hunterBiomedicalLanguageProcessing2006,
  title = {Biomedical {{Language Processing}}: {{What}}'s {{Beyond PubMed}}?},
  shorttitle = {Biomedical {{Language Processing}}},
  author = {Hunter, Lawrence and Cohen, K. Bretonnel},
  year = {2006},
  month = mar,
  journal = {Molecular Cell},
  volume = {21},
  number = {5},
  pages = {589--594},
  issn = {1097-2765},
  doi = {10/dh952g},
  urldate = {2019-01-23},
  langid = {english},
  pmid = {16507357},
  keywords = {Language,Periodicals as Topic,PubMed,Software},
  annotation = {00000},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2006\\Hunter et Cohen - 2006 - Biomedical Language Processing What's Beyond PubM.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\Y3MZPWAF\\Hunter et Cohen - 2006 - Biomedical Language Processing What's Beyond PubM.html}
}

@inproceedings{hussainSurveyConversationalAgents2019,
  title = {A {{Survey}} on {{Conversational Agents}}/{{Chatbots Classification}} and {{Design Techniques}}},
  booktitle = {Web, {{Artificial Intelligence}} and {{Network Applications}}},
  author = {Hussain, Shafquat and Ameri Sianaki, Omid and Ababneh, Nedal},
  editor = {Barolli, Leonard and Takizawa, Makoto and Xhafa, Fatos and Enokido, Tomoya},
  year = {2019},
  series = {Advances in {{Intelligent Systems}} and {{Computing}}},
  pages = {946--956},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10/gjbdm5},
  abstract = {A chatbot can be defined as a computer program, designed to interact with users using natural language or text in a way that the user thinks he is having dialogue with a human. Most of the chatbots utilise the algorithms of artificial intelligence (AI) in order to generate required response. Earlier chatbots merely created an illusion of intelligence by employing much simpler pattern matching and string processing design techniques for their interaction with users using rule-based and generative-based models. However, with the emergence of new technologies more intelligent systems have emerged using complex knowledge-based models. This paper aims to discuss chatbots classification, their design techniques used in earlier and modern chatbots and how the two main categories of chatbots handle conversation context.},
  isbn = {978-3-030-15035-8},
  langid = {english},
  keywords = {AIML,Algorithms,Chatbots,Chatscript,Classifications,Conversational agents,Design techniques conversational context,Generative based,Machine learning,Neural network,Pattern matching,Retrieval based,Rule based},
  file = {C:\Users\nhiot\OneDrive\zotero\2019\Hussain et al. - 2019 - A Survey on Conversational AgentsChatbots Classif.pdf}
}

@misc{I2b2InformaticsIntegrating,
  title = {I2b2: {{Informatics}} for {{Integrating Biology}} \& the {{Bedside}}},
  url = {https://www.i2b2.org/NLP/Medication/},
  urldate = {2021-03-16},
  file = {C:\Users\nhiot\Zotero\storage\I76CZ83D\Medication.html}
}

@misc{IBMCognosAnalytics,
  title = {{{IBM Cognos Analytics}}},
  journal = {IBM},
  url = {https://www.ibm.com/products/cognos-analytics},
  abstract = {Give users the autonomy they crave to find, explore, and share insights in the governed, trusted environment you need with IBM Cognos Analytics.},
  langid = {american},
  keywords = {nosource}
}

@article{ilievskiCommonsenseKnowledgeWikidata2020,
  title = {Commonsense {{Knowledge}} in {{Wikidata}}},
  author = {Ilievski, Filip and Szekely, Pedro and Schwabe, Daniel},
  year = {2020},
  month = oct,
  journal = {arXiv:2008.08114 [cs]},
  eprint = {2008.08114},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2008.08114},
  urldate = {2021-03-19},
  abstract = {Wikidata and Wikipedia have been proven useful for reason-ing in natural language applications, like question answering or entitylinking. Yet, no existing work has studied the potential of Wikidata for commonsense reasoning. This paper investigates whether Wikidata con-tains commonsense knowledge which is complementary to existing commonsense sources. Starting from a definition of common sense, we devise three guiding principles, and apply them to generate a commonsense subgraph of Wikidata (Wikidata-CS). Within our approach, we map the relations of Wikidata to ConceptNet, which we also leverage to integrate Wikidata-CS into an existing consolidated commonsense graph. Our experiments reveal that: 1) albeit Wikidata-CS represents a small portion of Wikidata, it is an indicator that Wikidata contains relevant commonsense knowledge, which can be mapped to 15 ConceptNet relations; 2) the overlap between Wikidata-CS and other commonsense sources is low, motivating the value of knowledge integration; 3) Wikidata-CS has been evolving over time at a slightly slower rate compared to the overall Wikidata, indicating a possible lack of focus on commonsense knowledge. Based on these findings, we propose three recommended actions to improve the coverage and quality of Wikidata-CS further.},
  archiveprefix = {arxiv},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence},
  annotation = {QID: Q100052578},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2020\\Ilievski et al. - 2020 - Commonsense Knowledge in Wikidata.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\7KXDREE4\\2008.html}
}

@article{imielinskiIncompleteInformationRelational1984,
  ids = {IL84},
  title = {Incomplete Information in Relational Databases},
  author = {Imielinski, Tomasz and Lipski Jr., Witold},
  year = {1984},
  month = sep,
  journal = {Journal of the ACM (JACM)},
  volume = {31},
  number = {4},
  pages = {761--791},
  publisher = {ACM New York, NY, USA},
  issn = {0004-5411},
  doi = {10.1145/1634.1886},
  optbibsource = {dblp computer science bibliography, http://dblp.org},
  optdoi = {10.1145/1634.1886},
  opttimestamp = {Thu, 26 Jan 2012 17:31:32 +0100},
  keywords = {nosource},
  annotation = {QID: Q56698644},
  file = {C:\Users\nhiot\OneDrive\zotero\1984\Imielinski et Lipski Jr. - 1984 - Incomplete information in relational databases.pdf}
}

@article{inokuchiCompleteMiningFrequent2003,
  title = {Complete {{Mining}} of {{Frequent Patterns}} from {{Graphs}}: {{Mining Graph Data}}},
  shorttitle = {Complete {{Mining}} of {{Frequent Patterns}} from {{Graphs}}},
  author = {Inokuchi, Akihiro and Washio, Takashi and Motoda, Hiroshi},
  year = {2003},
  month = mar,
  journal = {Machine Learning},
  volume = {50},
  number = {3},
  pages = {321--354},
  issn = {1573-0565},
  doi = {10/bbcg53},
  urldate = {2019-01-06},
  abstract = {Basket Analysis, which is a standard method for data mining, derives frequent itemsets from database. However, its mining ability is limited to transaction data consisting of items. In reality, there are many applications where data are described in a more structural way, e.g. chemical compounds and Web browsing history. There are a few approaches that can discover characteristic patterns from graph-structured data in the field of machine learning. However, almost all of them are not suitable for such applications that require a complete search for all frequent subgraph patterns in the data. In this paper, we propose a novel principle and its algorithm that derive the characteristic patterns which frequently appear in graph-structured data. Our algorithm can derive all frequent induced subgraphs from both directed and undirected graph structured data having loops (including self-loops) with labeled or unlabeled nodes and links. Its performance is evaluated through the applications to Web browsing pattern analysis and chemical carcinogenesis analysis.},
  langid = {english},
  keywords = {adjacency matrix,Apriori algorithm,chemical carcinogenesis analysis,data mining,graph data,Web browsing analysis},
  annotation = {00000},
  file = {C:\Users\nhiot\OneDrive\zotero\2003\Inokuchi et al. - 2003 - Complete Mining of Frequent Patterns from Graphs .pdf}
}

@inproceedings{jabbariFrenchCorpusAnnotation2020,
  ids = {jabbariFrenchCorpusAnnotation2020a,jabbariFrenchCorpusAnnotation2020b},
  title = {A {{French Corpus}} and {{Annotation Schema}} for {{Named Entity Recognition}} and {{Relation Extraction}} of {{Financial News}}},
  booktitle = {Proceedings of the 12th {{Language Resources}} and {{Evaluation Conference}}},
  author = {Jabbari, Ali and Sauvage, Olivier and Zeine, Hamada and Chergui, Hamza},
  year = {2020},
  month = may,
  pages = {2293--2299},
  publisher = {European Language Resources Association},
  address = {Marseille, France},
  url = {https://aclanthology.org/2020.lrec-1.279},
  urldate = {2022-02-14},
  abstract = {In financial services industry, compliance involves a series of practices and controls in order to meet key regulatory standards which aim to reduce financial risk and crime, e.g. money laundering and financing of terrorism. Faced with the growing risks, it is imperative for financial institutions to seek automated information extraction techniques for monitoring financial activities of their customers. This work describes an ontology of compliance-related concepts and relationships along with a corpus annotated according to it. The presented corpus consists of financial news articles in French and allows for training and evaluating domain-specific named entity recognition and relation extraction algorithms. We present some of our experimental results on named entity recognition and relation extraction using our annotated corpus. We aim to furthermore use the the proposed ontology towards construction of a knowledge base of financial relations.},
  isbn = {979-10-95546-34-4},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2020\\Jabbari et al. - 2020 - A French Corpus and Annotation Schema for Named En.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\R8WKG9XD\\2020.lrec-1.279.html}
}

@article{jaccardDistributionFloreAlpine1901,
  title = {Distribution de La {{Flore Alpine}} Dans Le {{Bassin}} Des {{Dranses}} et Dans Quelques R{\'e}gions Voisines.},
  author = {Jaccard, Paul},
  year = {1901},
  month = jan,
  journal = {Bulletin de la Societe Vaudoise des Sciences Naturelles},
  volume = {37},
  pages = {241--72},
  doi = {10.5169/seals-266440},
  file = {C:\Users\nhiot\OneDrive\zotero\1901\Jaccard - 1901 - Distribution de la Flore Alpine dans le Bassin des.pdf}
}

@incollection{jardinoActesTALN20052005,
  title = {Actes de {{TALN}} 2005 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 2005 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Jardino, Mich{\`e}le},
  year = {2005},
  month = jun,
  publisher = {LIMSI / ATALA},
  address = {Dourdan},
  keywords = {nosource}
}

@inproceedings{johnAvaDataInsights2017,
  title = {Ava: {{From Data}} to {{Insights Through Conversations}}.},
  shorttitle = {Ava},
  booktitle = {{{CIDR}}},
  author = {John, Rogers Jeffrey Leo and Potti, Navneet and Patel, Jignesh M.},
  year = {2017},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2017\John et al. - 2017 - Ava From Data to Insights Through Conversations..pdf}
}

@misc{jonesJSONWebAlgorithms,
  title = {{{JSON Web Algorithms}} ({{JWA}})},
  author = {Jones {$<$}mbj@microsoft.com{$>$}, Michael},
  url = {https://tools.ietf.org/html/rfc7518},
  urldate = {2020-10-02},
  langid = {english},
  file = {C:\Users\nhiot\Zotero\storage\KEQBMPBE\rfc7518.html}
}

@misc{jonesJSONWebKey,
  title = {{{JSON Web Key}} ({{JWK}})},
  author = {Jones {$<$}mbj@microsoft.com{$>$}, Michael},
  url = {https://tools.ietf.org/html/rfc7517},
  urldate = {2020-10-02},
  langid = {english},
  file = {C:\Users\nhiot\Zotero\storage\32FZ6UPF\rfc7517.html}
}

@inproceedings{joshiTreeAdjoiningGrammars1985,
  title = {Tree Adjoining Grammars: {{How}} Much Context-Sensitivity Is Required to Provide Reasonable Structural Descriptions?},
  shorttitle = {Tree Adjoining Grammars},
  booktitle = {Natural {{Language Parsing}}: {{Psychological}}, {{Computational}}, and {{Theoretical Perspectives}}},
  author = {Joshi, Aravind K.},
  editor = {Zwicky, Arnold M. and Dowty, David R. and Karttunen, Lauri},
  year = {1985},
  series = {Studies in {{Natural Language Processing}}},
  pages = {206--250},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/CBO9780511597855.007},
  urldate = {2023-10-23},
  abstract = {Since the late 1970s there has been vigorous activity in constructing highly constrained grammatical systems by eliminating the transformational component either totally or partially. There is increasing recognition of the fact that the entire range of dependencies that transformational grammars in their various incarnations have tried to account for can be captured satisfactorily by classes of rules that are nontransformational and at the same time highly constrained in terms of the classes of grammars and languages they define.Two types of dependencies are especially important: subcategorization and filler-gap dependencies. Moreover, these dependencies can be unbounded. One of the motivations for transformations was to account for unbounded dependencies. The so-called nontransformational grammars account for the unbounded dependencies in different ways. In a tree adjoining grammar (TAG) unboundedness is achieved by factoring the dependencies and recursion in a novel and linguistically interesting manner. All dependencies are defined on a finite set of basic structures (trees), which are bounded. Unboundedness is then a corollary of a particular composition operation called adjoining. There are thus no unbounded dependencies in a sense.This factoring of recursion and dependencies is in contrast to transformational grammars (TG), where recursion is defined in the base and the transformations essentially carry out the checking of the dependencies. The phrase linking grammars (PLGs) (Peters and Ritchie, 1982) and the lexical functional grammars (LFGs) (Kaplan and Bresnan, 1983) share this aspect of TGs; that is, recursion builds up a set a structures, some of which are then filtered out by transformations in a TG, by the constraints on linking in a PLG, and by the constraints introduced via the functional structures in an LFG.},
  isbn = {978-0-511-59785-5},
  keywords = {nosource}
}

@inproceedings{jouiliEmpiricalComparisonGraph2013,
  title = {An {{Empirical Comparison}} of {{Graph Databases}}},
  booktitle = {2013 {{International Conference}} on {{Social Computing}}},
  author = {Jouili, S. and Vansteenberghe, V.},
  year = {2013},
  month = sep,
  pages = {708--715},
  doi = {10/ggwv5g},
  abstract = {In recent years, more and more companies provide services that can not be anymore achieved efficiently using relational databases. As such, these companies are forced to use alternative database models such as XML databases, object-oriented databases, document-oriented databases and, more recently graph databases. Graph databases only exist for a few years. Although there have been some comparison attempts, they are mostly focused on certain aspects only. In this paper, we present a distributed graph database comparison framework and the results we obtained by comparing four important players in the graph databases market: Neo4j, Orient DB, Titan and DEX.},
  keywords = {alternative database models,Benchmark testing,Biological system modeling,Database Benchmark,database management systems,DEX,Distributed databases,distributed graph database comparison framework,distributed processing,document oriented databases,graph,graph database,graph databases market,graph theory,graph traversal,Loading,Neo4j,nosource,object oriented databases,Orient DB,relational databases,Servers,Time measurement,Titan,XML databases}
}

@book{jurafskySpeechLanguageProcessing2009,
  title = {Speech and Language Processing : An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
  shorttitle = {Speech and {{Language Processing}}},
  author = {Jurafsky, Dan and Martin, James H.},
  year = {2009},
  month = jul,
  volume = {4},
  publisher = {Pearson education, Asia},
  address = {Upper Saddle River, N.J.},
  url = {http://www.amazon.com/Speech-Language-Processing-2nd-Edition/dp/0131873210/ref=pd\_bxgy\_b\_img\_y},
  abstract = {An explosion of Web-based language techniques, merging of distinct fields, availability of phone-based dialogue systems, and much more make this an exciting time in speech and language processing. The first of its kind to thoroughly cover language technology - at all levels and with all modern technologies - this book takes an empirical approach to the subject, based on applying statistical and other machine-learning algorithms to large corporations. Builds each chapter around one or more worked examples demonstrating the main idea of the chapter, usingthe examples to illustrate the relative strengths and weaknesses of various approaches. Adds coverage of statistical sequence labeling, information extraction, question answering and summarization, advanced topics in speech recognition, speech synthesis. Revises coverage of language modeling, formal grammars, statistical parsing, machine translation, and dialog processing. A useful reference for professionals in any of the areas of speech and language processing. -- Book Description from Website.},
  isbn = {978-0-13-187321-6 0-13-187321-0},
  keywords = {language},
  file = {C:\Users\nhiot\OneDrive\zotero\2019\Jurafsky et Martin - 2019 - Speech and language processing  an introduction t.pdf}
}

@inproceedings{kambhatlaCombiningLexicalSyntactic2004,
  title = {Combining Lexical, Syntactic, and Semantic Features with Maximum Entropy Models for Extracting Relations},
  booktitle = {Proceedings of the {{ACL}} 2004 on {{Interactive}} Poster and Demonstration Sessions},
  author = {Kambhatla, Nanda},
  year = {2004},
  month = jul,
  series = {{{ACLdemo}} '04},
  pages = {22--es},
  publisher = {Association for Computational Linguistics},
  address = {USA},
  doi = {10/cskwmd},
  urldate = {2020-11-26},
  abstract = {Extracting semantic relationships between entities is challenging because of a paucity of annotated data and the errors induced by entity detection modules. We employ Maximum Entropy models to combine diverse lexical, syntactic and semantic features derived from the text. Our system obtained competitive results in the Automatic Content Extraction (ACE) evaluation. Here we present our general approach and describe our ACE results.},
  file = {C:\Users\nhiot\OneDrive\zotero\2004\Kambhatla - 2004 - Combining lexical, syntactic, and semantic feature.pdf}
}

@article{kanakamHPLALGORITHMSEMANTIC2017,
  title = {{{HPL ALGORITHM FOR SEMANTIC INFORMATION RETRIEVAL WITH RDF AND SPARQL}}},
  author = {Kanakam, Prathyusha and Suryanarayana, S. Mahaboob Hussain {and} D.},
  year = {2017},
  month = sep,
  journal = {Journal of Industrial Pollution Control},
  volume = {33},
  number = {2},
  pages = {1534--1541},
  issn = {ISSN (0970-2083)},
  url = {http://www.icontrolpollution.com/peer-reviewed/hpl-algorithm-for-semantic-information-retrieval-with-rdfrnand-sparql-86310.html},
  urldate = {2019-02-01},
  abstract = {As the web composed with lots of unstructured data, retrieving the accurate information for the user's posed query from it is a critical issue. Most of the search engines are fails to achieve the accurate outcomes for the users. In order to overcome this and to obtain efficient results, SPARQL query language is used to convert users' posed natural language queries to machine understandable format. Semantic web technology based on SPARQL is used to acquire useful information from the RDF knowledge base, which gives beneficial information to the user. In this paper, HPL algorithm is effectively utilized to aware of querying the semantic web. It also makes use of Linked Open Data Quality Assessment (LODQA) system to perform a semantic search that converts normal user defined queries into machine understandable formal logic. By combining both these systems, the interpretation of a natural language query into SPARQL queries is made easy that grabs knowledge from ontological database that are stored in Resource Description Framework. Accordingly, technologies and data storage possibilities are analyzed and evaluated to retrieve accurate results and a test case for the career opportunities for students.},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {00000},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2017\\Kanakam et Suryanarayana - 2017 - HPL ALGORITHM FOR SEMANTIC INFORMATION RETRIEVAL W.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\MI9VHHEP\\Kanakam et Suryanarayana - 2017 - HPL ALGORITHM FOR SEMANTIC INFORMATION RETRIEVAL W.html}
}

@article{karstLinearCurveFitting1958,
  title = {Linear Curve Fitting Using Least Deviations},
  author = {Karst, O. J.},
  year = {1958},
  journal = {Journal of the American Sstatistical Association},
  pages = {118--132},
  doi = {10/gfvn2z},
  keywords = {nosource},
  annotation = {00000}
}

@inproceedings{kaufmannNLPReduceNaiveDomainindependent2007a,
  ids = {kaufmannNLPReduceNaiveDomainindependent2007},
  title = {{{NLP-Reduce}}: {{A}} ``Na{\"i}ve'' but {{Domain-independent Natural Language Interface}} for {{Querying Ontologies}}},
  shorttitle = {{{NLP-Reduce}}},
  booktitle = {4th {{European}} Semantic Web Conference {{ESWC}}},
  author = {Kaufmann, Esther and Bernstein, Abraham and Fischer, Lorenz},
  year = {2007},
  pages = {1--2},
  publisher = {Springer Berlin},
  abstract = {Casual users are typically overwhelmed by the formal logic of the Semantic Web. The question is how to help casual users to query a web based on logic that they do not seem to understand. An often proposed solution is the use of natural language interfaces. Such tools, however, suffer from the problem that entries have to be grammatical. Furthermore, the systems are hardly adaptable to new domains. We address these issues by presenting NLPReduce, a ``na{\textasciidieresis}{\i}ve,'' domain-independent natural language interface for the Semantic Web. The simple approach deliberately avoids any complex linguistic and semantic technology while still achieving good retrieval performance as shown by the preliminary evaluation.},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {00000},
  file = {C:\Users\nhiot\OneDrive\zotero\2007\Kaufmann et al. - 2007 - NLP-Reduce A “naïve” but Domain-independent Natur.pdf}
}

@article{kayedPostalAddressExtraction2022,
  title = {Postal Address Extraction from the Web: A Comprehensive Survey},
  shorttitle = {Postal Address Extraction from the Web},
  author = {Kayed, Mohammed and Dakrory, Sara and Ali, A. A.},
  year = {2022},
  month = feb,
  journal = {Artificial Intelligence Review},
  volume = {55},
  number = {2},
  pages = {1085--1120},
  publisher = {Springer},
  issn = {1573-7462},
  doi = {10.1007/s10462-021-09983-1},
  urldate = {2022-03-04},
  abstract = {The Web is a source of information for Location-Based Service (LBS) applications. These applications lack postal addresses for the user's Point of Interests (POIs) such as schools, hospitals, restaurants, etc., as these locations are annotated manually by using the yellow pages or by the location owners (users/companies). Our study in this paper confirms that Google Maps, a common LBS application, only contains about \$\$32.5{\textbackslash}\%\$\$of the public schools that are registered officially in the documents provided by the Directorate of Education in Egypt. However, the remaining missed school addresses could be fished from the Web (e.g., social media). To the best of our knowledge, no prior survey has been published to compare the previous Web postal address extraction approaches. Additionally, all proposed approaches for address extraction are local (could be working in specific countries/locations with particular languages) and could not be used or even adapted to work in other countries/locations with other languages. Furthermore, the problem of Web postal address extraction is not addressed in many countries such as Arab countries (e.g. Egypt). This paper discusses the issue of address extraction, highlights and compares the recently used techniques in extracting addresses from Web pages. In addition, it investigates the discrepancy of knowledge among existing systems. Moreover, it provides a comprehensive review of the geographical Gazetteers used in the Web postal address approaches and compares their data quality dimensions.},
  langid = {english},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2022\\Kayed et al. - 2022 - Postal address extraction from the web a comprehe.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\6I86MJRC\\s10462-021-09983-1.html}
}

@article{keInDepthComparisonSt2019,
  ids = {keIndepthComparisonSt2019,keIndepthComparisonSt2019a},
  title = {An {{In-Depth Comparison}} of s-t {{Reliability Algorithms}} over {{Uncertain Graphs}}},
  author = {Ke, Xiangyu and Khan, Arijit and Quan, Leroy Lim Hong},
  year = {2019},
  month = apr,
  journal = {Proceedings of the VLDB Endowment},
  volume = {12},
  number = {8},
  eprint = {1904.05300},
  pages = {864--876},
  publisher = {VLDB Endowment},
  doi = {10/ggpwv2},
  urldate = {2020-03-25},
  abstract = {Uncertain, or probabilistic, graphs have been increasingly used to represent noisy linked data in many emerging applications, and have recently attracted the attention of the database research community. A fundamental problem on uncertain graphs is the s-t reliability, which measures the probability that a target node t is reachable from a source node s in a probabilistic (or uncertain) graph, i.e., a graph where every edge is assigned a probability of existence. Due to the inherent complexity of the s-t reliability estimation problem (\#P-hard), various sampling and indexing based efficient algorithms were proposed in the literature. However, since they have not been thoroughly compared with each other, it is not clear whether the later algorithm outperforms the earlier ones. More importantly, the comparison framework, datasets, and metrics were often not consistent (e.g., different convergence criteria were employed to find the optimal number of samples) across these works. We address this serious concern by re-implementing six state-of-the-art s-t reliability estimation methods in a common system and code base, using several medium and large-scale, real-world graph datasets, identical evaluation metrics, and query workloads. Through our systematic and in-depth analysis of experimental results, we report surprising findings, such as many follow-up algorithms can actually be several orders of magnitude inefficient, less accurate, and more memory intensive compared to the ones that were proposed earlier. We conclude by discussing our recommendations on the road ahead.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Databases,Computer Science - Social and Information Networks},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2019\\Ke et al. - 2019 - An In-Depth Comparison of s-t Reliability Algorith.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\XQ22Y6H9\\1904.html}
}

@inproceedings{kelloggNaturalLanguageCompiler1968,
  ids = {kelloggNaturalLanguageCompiler1968b},
  title = {A Natural Language Compiler for On-Line Data Management},
  booktitle = {Proceedings of the {{December}} 9-11, 1968, Fall Joint Computer Conference, Part {{I}} on - {{AFIPS}} '68 ({{Fall}}, Part {{I}})},
  author = {Kellogg, Charles H.},
  year = {1968},
  month = dec,
  series = {{{AFIPS}} '68 ({{Fall}}, Part {{I}})},
  pages = {473--492},
  publisher = {ACM Press},
  address = {San Francisco, California},
  doi = {10/djhxnv},
  urldate = {2022-01-12},
  abstract = {During the past few years there has been a rapid advance in the technology of time-sharing systems and software to permit quick access to large files of structured data. This has led to a growing interest in communicating with computer files directly in a natural language such as English. The natural language systems described in the literature are largely small-scale research vehicles dealing with small data bases of restricted subject scope. Giuliano (1965), among others, has questioned the generalization of these systems to wider universes of discourse. Developments in this area have been reviewed by Simmons (1966), and by Bobrow, Fraser and Quillan (1967). In contrast, the work in on-line data management has been more concerned with the efficient organization of structured data to allow for quick access and maintenance of large volumes of formatted information [see the reviews by Kellogg (1967), Climenson (1966), and Minker and Sable (1967)].},
  isbn = {978-1-4503-7899-4},
  langid = {english},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1968\\Kellogg - 1968 - A natural language compiler for on-line data manag.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\7L98T6TX\\1476589.html}
}

@inproceedings{kertkeidkachornT2KGEndtoEndSystem2017,
  ids = {kertkeidkachornT2KGEndtoendSystem2017},
  title = {{{T2KG}}: {{An End-to-End System}} for {{Creating Knowledge Graph}} from {{Unstructured Text}}},
  shorttitle = {{{T2KG}}},
  booktitle = {Workshops at the {{Thirty-First AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Kertkeidkachorn, Natthawut and Ichise, Ryutaro},
  year = {2017},
  month = mar,
  url = {https://www.aaai.org/ocs/index.php/WS/AAAIW17/paper/view/15129},
  urldate = {2020-05-18},
  abstract = {Knowledge Graph (KG) plays a crucial role in many modern applications. Nevertheless, constructing KG from unstructured text is a challenging problem due to its nature. Consequently, many approaches propose to transform unstructured text to structured text in order to create a KG. Such approaches cannot yet provide reasonable results for mapping an extracted predicate to its identical predicate in another KG. Predicate mapping is an essential procedure because it can reduce the heterogeneity problem and increase searchability over a KG. In this paper, we propose T2KG system, an end-to-end system with keeping such problem into consideration. In the system, a hybrid combination of a rule-based approach and a similarity-based approach is presented for mapping a predicate to its identical predicate in a KG. Based on preliminary experimental results, the hybrid approach improves the recall by 10.02\% and the F-measure by 6.56\% without reducing the precision in the predicate mapping task. Furthermore, although the KG creation is conducted in open domains, the system still achieves approximately 50\% of F-measure for generating triples in the KG creation task.},
  copyright = {Authors who publish a paper in an  AAAI Technical Report  agree to the following terms:     Author(s) agree to grant to AAAI (1) the perpetual, nonexclusive world rights to use the submitted paper as part of an AAAI publication, in all languages and for all editions. (2) The right to use the paper, together with the author's name and pertinent biographical data, in advertising and promotion of it and the AAAI publication. (3) The right to publish or cause to be published the paper in connection with any republication of the AAAI publication in any medium including electronic. (4) The right to, and authorize others to, publish or cause to be published the paper in whole or in part, individually or in conjunction with other works, in any medium including electronic.   The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.   The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys' fees incurred therein.   Author(s) retain all proprietary rights (such as patent rights).   In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
  langid = {english},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2017\\Kertkeidkachorn et Ichise - 2017 - T2KG An End-to-End System for Creating Knowledge .pdf;C\:\\Users\\nhiot\\Zotero\\storage\\FRXQNTS3\\15129.html}
}

@article{khadirOntologyLearningGrand2021,
  title = {Ontology Learning: {{Grand}} Tour and Challenges},
  shorttitle = {Ontology Learning},
  author = {Khadir, Ahlem Ch{\'e}rifa and Aliane, Hassina and Guessoum, Ahmed},
  year = {2021},
  month = feb,
  journal = {Computer Science Review},
  volume = {39},
  pages = {100339},
  publisher = {Elsevier},
  issn = {1574-0137},
  doi = {10.1016/j.cosrev.2020.100339},
  urldate = {2024-02-29},
  abstract = {Ontologies are at the core of the semantic web. As knowledge bases, they are very useful resources for many artificial intelligence applications. Ontology learning, as a research area, proposes techniques to automate several tasks of the ontology construction process to simplify the tedious work of manually building ontologies. In this paper we present the state of the art of this field. Different classes of approaches are covered (linguistic, statistical, and machine learning), including some recent ones (deep-learning-based approaches). In addition, some relevant solutions (frameworks), which offer strategies and built-in methods for ontology learning, are presented. A descriptive summary is made to point out the capabilities of the different contributions based on criteria that have to do with the produced ontology components and the degree of automation. We also highlight the challenge of evaluating ontologies to make them reliable, since it is not a trivial task in this field; it actually represents a research area on its own. Finally, we identify some unresolved issues and open questions.},
  keywords = {Deep learning,Linguistic and statistical approaches,Machine learning,Ontologies,Ontology learning},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2021\\Khadir et al. - 2021 - Ontology learning Grand tour and challenges.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\EG5Z6MEJ\\S1574013720304391.html}
}

@article{kimFastStringMatching1994,
  title = {Fast String Matching Using an N-Gram Algorithm},
  author = {Kim, Jong Yong and Shawe-Taylor, John},
  year = {1994},
  journal = {Software: Practice and Experience},
  volume = {24},
  number = {1},
  pages = {79--88},
  publisher = {Wiley Online Library},
  issn = {1097-024X},
  doi = {10/b6277n},
  urldate = {2020-04-08},
  abstract = {Experimental results are given for the application of a new n-gram algorithm to substring searching in DNA strings. The results confirm theoretical predictions of expected running times based on the assumption that the data are drawn from a stationary ergodic source. They also confirm that the algorithms tested are the most efficient known for searches involving larger patterns.},
  copyright = {Copyright {\copyright} 1994 John Wiley \& Sons, Ltd},
  langid = {english},
  optabstract = {Experimental results are given for the application of a new n-gram algorithm to substring searching in DNA strings. The results confirm theoretical predictions of expected running times based on the assumption that the data are drawn from a stationary ergodic source. They also confirm that the algorithms tested are the most efficient known for searches involving larger patterns.},
  optcopyright = {Copyright {\copyright} 1994 John Wiley \& Sons, Ltd},
  optdoi = {10/b6277n},
  optissn = {1097-024X},
  optkeywords = {Boyer-Moore algorithm, Pattern matching, String searching},
  optlanguage = {en},
  opturl = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.4380240105},
  opturldate = {2020-04-08},
  keywords = {Boyer-Moore algorithm,Pattern matching,String searching},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1994\\Kim et Shawe‐Taylor - 1994 - Fast string matching using an n-gram algorithm.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\W8RXJJA9\\spe.html}
}

@article{kimGENIACorpusSemantically2003,
  ids = {kimGENIACorpusSemantically2003a,kimGENIACorpusSemantically2003b},
  title = {{{GENIA}} Corpus: {{A}} Semantically Annotated Corpus for Bio-Textmining},
  author = {Kim, J.-D. and Ohta, T. and Tateisi, Y. and Tsujii, J.},
  year = {2003},
  month = jul,
  journal = {Bioinformatics},
  volume = {19},
  number = {suppl\_1},
  pages = {i180--i182},
  publisher = {Oxford University Press},
  issn = {1367-4811, 1367-4803},
  doi = {10.1093/bioinformatics/btg1023},
  urldate = {2024-03-21},
  abstract = {Motivation: Natural language processing (NLP) methods are regarded as being useful to raise the potential of text mining from biological literature. The lack of an extensively annotated corpus of this literature, however, causes a major bottleneck for applying NLP techniques. GENIA corpus is being developed to provide reference materials to let NLP techniques work for bio-textmining. Results: GENIA corpus version 3.0 consisting of 2000 MEDLINE abstracts has been released with more than 400\,000 words and almost 100\,000 annotations for biological terms.Availability: GENIA corpus is freely available at http://www-tsujii.is.s.u-tokyo.ac.jp/GENIAKeywords: Text Mining, Information Extraction, Corpus, Natural Language Processing, Computational Molecular Biology*To whom correspondence should be addressed.},
  langid = {english},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2003\\Kim et al. - 2003 - GENIA corpus—a semantically annotated corpus for b.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\FZIAQ84V\\227927.html}
}

@inproceedings{klopfensteinRiseBotsSurvey2017,
  title = {The {{Rise}} of {{Bots}}: {{A Survey}} of {{Conversational Interfaces}}, {{Patterns}}, and {{Paradigms}}},
  shorttitle = {The {{Rise}} of {{Bots}}},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Designing Interactive Systems}}},
  author = {Klopfenstein, Lorenz Cuno and Delpriori, Saverio and Malatini, Silvia and Bogliolo, Alessandro},
  year = {2017},
  month = jun,
  series = {{{DIS}} '17},
  pages = {555--565},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10/gfxhtr},
  urldate = {2021-03-11},
  abstract = {This work documents the recent rise in popularity of messaging bots: chatterbot-like agents with simple, textual interfaces that allow users to access information, make use of services, or provide entertainment through online messaging platforms. Conversational interfaces have been often studied in their many facets, including natural language processing, artificial intelligence, human-computer interaction, and usability. In this work we analyze the recent trends in chatterbots and provide a survey of major messaging platforms, reviewing their support for bots and their distinguishing features. We then argue for what we call "Botplication", a bot interface paradigm that makes use of context, history, and structured conversation elements for input and output in order to provide a conversational user experience while overcoming the limitations of text-only interfaces.},
  isbn = {978-1-4503-4922-2},
  keywords = {botplication,bots,conversational UI,messaging,mobile UI},
  file = {C:\Users\nhiot\OneDrive\zotero\2017\Klopfenstein et al. - 2017 - The Rise of Bots A Survey of Conversational Inter.pdf}
}

@article{knuthSemanticsContextfreeLanguages1968,
  title = {Semantics of Context-Free Languages},
  author = {Knuth, Donald E.},
  year = {1968},
  month = jun,
  journal = {Mathematical Systems Theory},
  volume = {2},
  number = {2},
  pages = {127--145},
  issn = {0025-5661, 1433-0490},
  doi = {10.1007/BF01692511},
  urldate = {2023-10-27},
  abstract = {``Meaning'' may be assigned to a string in a context-free language by defining ``attributes'' of the symbols in a derivation tree for that string. The attributes can be defined by functions associated with each production in the grammar. This paper examines the implications of this process when some of the attributes are ``synthesized'', i.e., defined solely in terms of attributes of thedescendants of the corresponding nonterminal symbol, while other attributes are ``inherited'', i.e., defined in terms of attributes of theancestors of the nonterminal symbol. An algorithm is given which detects when such semantic rules could possibly lead to circular definition of some attributes. An example is given of a simple programming language defined with both inherited and synthesized attributes, and the method of definition is compared to other techniques for formal specification of semantics which have appeared in the literature.},
  langid = {english},
  keywords = {Computational Mathematic,Derivation Tree,Formal Specification,Programming Language,Simple Programming},
  file = {C:\Users\nhiot\OneDrive\zotero\1968\Knuth - 1968 - Semantics of context-free languages.pdf}
}

@article{kofod-petersenContextRepresentationReasoning2005,
  title = {Context: {{Representation}} and {{Reasoning}}. {{Representing}} and {{Reasoning}} about {{Context}} in a {{Mobile Environment}}},
  shorttitle = {Context},
  author = {{Kofod-Petersen}, Anders and Mikalsen, Marius},
  year = {2005},
  journal = {Revue d'Intelligence Artificielle},
  volume = {19},
  pages = {479--498},
  doi = {10/bwg762},
  abstract = {Today the computer is changing from a big, grey, and noisy thing on our desk to a small, portable, and ever-networked item most of us are carrying around. This new found mobility imposes a shift in how we view computers and the way we work with them. When interaction can occur anywhere at any time it is imperative that the system adapts to the user in whatever situation the user is in. To facilitate this adaptivity we propose a two tier architecture. A middleware layer implementing a general mechanism for aggregating and maintaining contextual information. The second part offers automatic situation assessment through Case-Based Reasoning. We demonstrate a multi-agent system for supplying context-sensitive services in a mobile environment. R{\'E}SUM{\'E}.De nos jours, l'ordinateur est en passe de changer de l'objet gros, gris et bruyant sur notre bureau {\`a} un objet petit, transportable et connect{\'e} que la plupart d'entre nous transporte. Cette nouvelle mobilit{\'e} impose sur notre vision des ordinateurs et la mani{\`e}re dont nous travaillons avec eux. Lorsque nous pouvons interagir avec les autres n'importe o{\`u}, n'importe quand, il est imp{\'e}ratif que ce soit le syst{\`e}me qui s'adapte {\`a} l'utilisateur et {\`a} la situation, et non l'inverse. Pour faciliter cette adaptativit{\'e}, nous proposons une architecture deux-tiers avec une couche logiciel personnalis{\'e} impl{\'e}mentant un m{\'e}canisme g{\'e}n{\'e}ral d'agr{\'e}gation et de maintenance d'informations contextuelles. Dans un deuxi{\`e}me temps, nous proposons une {\'e}valuation automatique d'une situation {\`a} partir d'un raisonnement {\`a} base de cas. Nous montrons finalement comment un syst{\`e}me multi-agent peut fournir des services sensibles au contexte dans un environnement mobile.},
  annotation = {00000},
  file = {C:\Users\nhiot\OneDrive\zotero\2005\Kofod-Petersen et Mikalsen - 2005 - Context Representation and Reasoning. Representin.pdf}
}

@inproceedings{konstantinidisFormalApproachRDF2008,
  ids = {konstantinidisFormalApproachRDF},
  title = {A {{Formal Approach}} for {{RDF}}/{{S Ontology Evolution}}},
  booktitle = {{{ECAI}} 2008},
  author = {Konstantinidis, George and Flouris, Giorgos and Antoniou, Grigoris and Christophides, Vassilis},
  year = {2008},
  pages = {70--74},
  publisher = {IOS Press},
  doi = {10.3233/978-1-58603-891-5-70},
  urldate = {2023-08-07},
  abstract = {Abstract. In this paper, we consider the problem of ontology evolution in the face of a change operation. We devise a general-purpose algorithm for determining the effects and side-effects of a requested elementary or complex change operation. Our work is inspired by belief revision principles (i.e., validity, success and minimal change) and allows us to handle any change operation in a provably rational and consistent manner. To the best of our knowledge, this is the first approach overcoming the limitations of existing solutions, which deal with each change operation on a per-case basis. Additionally, we rely on our general change handling algorithm to implement specialized versions of it, one per desired change operation, in order to compute the equivalent set of effects and side-effects. 2 1},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2008\\Konstantinidis et al. - 2008 - A Formal Approach for RDFS Ontology Evolution2.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\3D7HDMBB\\books.html}
}

@inproceedings{konstantinovaReviewRelationExtraction2014,
  title = {Review of {{Relation Extraction Methods}}: {{What Is New Out There}}?},
  shorttitle = {Review of {{Relation Extraction Methods}}},
  booktitle = {International {{Conference}} on {{Analysis}} of {{Images}}, {{Social Networks}} and {{Texts}}},
  author = {Konstantinova, Natalia},
  editor = {Ignatov, Dmitry I. and Khachay, Mikhail Yu. and Panchenko, Alexander and Konstantinova, Natalia and Yavorsky, Rostislav E.},
  year = {2014},
  series = {Communications in {{Computer}} and {{Information Science}}},
  pages = {15--28},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10/ghj6mf},
  abstract = {Relation extraction is a part of Information Extraction and an established task in Natural Language Processing. This paper presents an overview of the main directions of research and recent advances in the field. It reviews various techniques used for relation extraction including knowledge-based, supervised and self-supervised methods. We also mention applications of relation extraction and identify current trends in the way the field is developing.},
  isbn = {978-3-319-12580-0},
  langid = {english},
  keywords = {Information extraction,Natural language processing,Relation extraction,Review},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2014\\Konstantinova - 2014 - Review of Relation Extraction Methods What Is New.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\ZDQ75SNW\\978-3-319-12580-0_2.html}
}

@article{konysKnowledgeRepositoryOntology2019,
  title = {Knowledge {{Repository}} of {{Ontology Learning Tools}} from {{Text}}},
  author = {Konys, Agnieszka},
  year = {2019},
  month = jan,
  journal = {Procedia Computer Science},
  series = {Knowledge-{{Based}} and {{Intelligent Information}} \& {{Engineering Systems}}: {{Proceedings}} of the 23rd {{International Conference KES2019}}},
  volume = {159},
  pages = {1614--1628},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2019.09.332},
  urldate = {2024-02-29},
  abstract = {Ontologies are one of the fundamental elements of the Semantic Web, and they have gained a lot of popularity and recognition because they are viewed as the answer to the need for interoperable semantics in modern information systems. The intermingling of techniques in areas such as natural language processing, information retrieval, machine learning, data mining, and knowledge representation provide a lot of possibilities for development of ontology learning approaches. A rise in focus on the ability to cope with the scale of Web data required for ontology learning forces the potential growth of cross-language research, emphasizing the automatic or semi-automatic generation of the tools dedicated to text mining and information extraction. This paper presents the integration of ontology learning tools from text in the knowledge repository to incorporate the applied techniques and outputs of an ontology learning algorithm into the one complex multifunctional solution. The proposed knowledge repository covers various applicability of existing techniques of learning ontologies from text, and offers competency question-based reasoning mechanism for individuals to specify their profiles of ontology learning tools. The validation stage is also provided in the form of applied reasoning.},
  keywords = {Knowledge repository,Learning techniques,Ontology learning tools,Ontology learninng from text},
  file = {C:\Users\nhiot\OneDrive\zotero\2019\Konys - 2019 - Knowledge Repository of Ontology Learning Tools fr.pdf}
}

@inproceedings{korenPersonalizedInteractiveFaceted2008,
  title = {Personalized {{Interactive Faceted Search}}},
  booktitle = {Proceeding of the 17th International Conference on {{World Wide Web}}  - {{WWW}} '08},
  author = {Koren, Jonathan and Zhang, Yi and Liu, Xue},
  year = {2008},
  series = {{{WWW}} '08},
  pages = {477--486},
  publisher = {ACM Press},
  address = {Beijing, China},
  doi = {10/bwqrmv},
  urldate = {2019-02-15},
  abstract = {Faceted search is becoming a popular method to allow users to interactively search and navigate complex information spaces. A faceted search system presents users with key-value metadata that is used for query refinement. While popular in e-commerce and digital libraries, not much research has been conducted on which metadata to present to a user in order to improve the search experience. Nor are there repeatable benchmarks for evaluating a faceted search engine. This paper proposes the use of collaborative filtering and personalization to customize the search interface to each user's behavior. This paper also proposes a utility based framework to evaluate the faceted interface. In order to demonstrate these ideas and better understand personalized faceted search, several faceted search algorithms are proposed and evaluated using the novel evaluation methodology.},
  isbn = {978-1-60558-085-2},
  langid = {english},
  keywords = {collaborative recommendation,evaluation,faceted search,interactive search,personalization,user modeling},
  file = {C:\Users\nhiot\OneDrive\zotero\2008\Koren et al. - 2008 - Personalized Interactive Faceted Search.pdf}
}

@inproceedings{koutrasDataLanguageNovel2019,
  title = {Data as a {{Language}}: {{A Novel Approach}} to {{Data Integration}}},
  shorttitle = {Data as a Language},
  booktitle = {2019 {{International Conference}} on {{Very Large Database PhD Workshop}}, {{VLDB-PhD}} 2019},
  author = {Koutras, Christos},
  year = {2019},
  pages = {4},
  abstract = {In modern enterprises, both operational and organizational data is typically spread across multiple heterogeneous systems, databases and file systems. Recognizing the value of their data assets, companies and institutions construct data lakes, storing disparate datasets from di↵erent departments and systems. However, for those datasets to become useful, they need to be cleaned and integrated. Data can be well documented, structured and encoded in di↵erent schemata, but also unstructured with implicit, human-understandable semantics. Due to the sheer scale of the data itself but also the multitude of representations and schemata, data integration techniques need to scale without relying heavily on human labor. Existing integration approaches fail to address hidden semantics without human input or some form of ontology, making large scale integration a daunting task. The goal of my doctoral work is to devise scalable data integration methods, employing modern machine learning to exploit semantics and facilitate discovery of novel relationship types. In order to capture semantics with minimal human intervention, we propose a new approach which we call Data as a Language (DaaL). By leveraging embeddings from the Natural Language Processing (NLP) literature, DaaL aims at extracting semantics from structured and semi-structured data, allowing the exploration of relevance and similarity among di↵erent data sources. This paper discusses existing data integration mechanisms and elaborates on how NLP techniques can be used in data integration, alongside challenges and research directions.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2019\Koutras - 2019 - Data as a Language A Novel Approach to Data Integ.pdf}
}

@inproceedings{kozarevaCauseEffectRelationLearning2012,
  ids = {10.5555/2392954.2392961,kozarevaCauseeffectRelationLearning2012,kozarevaCauseeffectRelationLearning2012a,kozarevaCauseeffectRelationLearning2012b},
  title = {Cause-{{Effect Relation Learning}}},
  booktitle = {Workshop {{Proceedings}} of {{TextGraphs-7}}: {{Graph-based Methods}} for {{Natural Language Processing}}},
  author = {Kozareva, Zornitsa},
  year = {2012},
  month = jul,
  series = {{{TextGraphs-7}} '12},
  pages = {39--43},
  publisher = {Association for Computational Linguistics},
  address = {USA},
  urldate = {2020-11-17},
  abstract = {To be able to answer the question What causes tumors to shrink?, one would require a large cause-effect relation repository. Many efforts have been payed on is-a and part-of relation leaning, however few have focused on cause-effect learning. This paper describes an automated bootstrapping procedure which can learn and produce with minimal effort a cause-effect term repository. To filter out the erroneously extracted information, we incorporate graph-based methods. To evaluate the performance of the acquired cause-effect terms, we conduct three evaluations: (1) human-based, (2) comparison with existing knowledge bases and (3) application driven (SemEval-1 Task 4) in which the goal is to identify the relation between pairs of nominals. The results show that the extractions at rank 1500 are 89\% accurate, they comprise 61\% from the terms used in the SemEval-1 Task 4 dataset and can be used in the future to produce additional training examples for the same task.},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2012\\Kozareva - 2012 - Cause-Effect Relation Learning.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\EGIIM6W9\\2392954.html}
}

@inproceedings{kramdiApprocheGeneriquePour2009,
  title = {Approche G{\'e}n{\'e}rique Pour l'extraction de Relations {\`a} Partir de Textes},
  booktitle = {Journ{\'e}es {{Francophones}} d'{{Ing{\'e}nierie}} Des {{Connaissances}}},
  author = {Kramdi, Seif Eddine and Haemmerl{\'e}, Ollivier and Hernandez, Nathalie},
  year = {2009},
  pages = {97--108},
  address = {Tunisia},
  url = {https://hal.archives-ouvertes.fr/hal-00384415},
  urldate = {2019-06-05},
  abstract = {Cet article s'int{\'e}resse {\`a} l'extraction de relations dans le contexte du web s{\'e}mantique, en vue de proc{\'e}der {\`a} de la construction d'ontologies aussi bien qu'{\`a} de l'annotation automatique de documents. Notre approche permet l'extraction de relations entre entit{\'e}s {\`a} partir de textes. Elle ne fait pas d'hypoth{\`e}se sur les entit{\'e}s, de mani{\`e}re {\`a} la rendre aussi g{\'e}n{\'e}rique que possible, et {\`a} autoriser par exemple l'extraction de relations entre concepts aussi bien que l'extraction de relations entre instances de concepts. Pour atteindre cet objectif, nous nous fondons sur l'algorithme LP2. Afin d'adapter cet algorithme {\`a} l'extraction de relations, nous proposons une nouvelle notion de contexte reposant sur un graphe de d{\'e}pendances, g{\'e}n{\'e}r{\'e} par un analyseur syntaxique. Un tel graphe de d{\'e}pendances est bien adapt{\'e} {\`a} la repr{\'e}sentation de relations, puisqu'il permet, notamment, de rep{\'e}rer ais{\'e}ment les diff{\'e}rents arguments d'un verbe dans une phrase. Nous pr{\'e}sentons l'impl{\'e}mentation r{\'e}alis{\'e}e suivie d'une premi{\`e}re phase d'exp{\'e}rimentations.},
  keywords = {⛔ No DOI found},
  annotation = {00007},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2009\\Kramdi et al. - 2009 - Approche générique pour l'extraction de relations .pdf;C\:\\Users\\nhiot\\Zotero\\storage\\SDHGFIXK\\hal-00384415.html}
}

@inproceedings{krebsSemeval2018Task102018,
  title = {Semeval-2018 Task 10: {{Capturing}} Discriminative Attributes},
  shorttitle = {Semeval-2018 Task 10},
  booktitle = {Proceedings of {{The}} 12th {{International Workshop}} on {{Semantic Evaluation}}},
  author = {Krebs, Alicia and Lenci, Alessandro and Paperno, Denis},
  year = {2018},
  pages = {732--740},
  doi = {10/gg5hms},
  file = {C:\Users\nhiot\OneDrive\zotero\2018\Krebs et al. - 2018 - Semeval-2018 task 10 Capturing discriminative att.pdf}
}

@incollection{krotzschAttributedDescriptionLogics2017,
  title = {Attributed {{Description Logics}}: {{Ontologies}} for {{Knowledge Graphs}}},
  shorttitle = {Attributed {{Description Logics}}},
  booktitle = {The {{Semantic Web}} -- {{ISWC}} 2017},
  author = {Kr{\"o}tzsch, Markus and Marx, Maximilian and Ozaki, Ana and Thost, Veronika},
  editor = {{d'Amato}, Claudia and Fernandez, Miriam and Tamma, Valentina and Lecue, Freddy and {Cudr{\'e}-Mauroux}, Philippe and Sequeda, Juan and Lange, Christoph and Heflin, Jeff},
  year = {2017},
  volume = {10587},
  pages = {418--435},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-68288-4_25},
  urldate = {2021-06-14},
  abstract = {In modelling real-world knowledge, there often arises a need to represent and reason with meta-knowledge. To equip description logics (DLs) for dealing with such ontologies, we enrich DL concepts and roles with finite sets of attribute--value pairs, called annotations, and allow concept inclusions to express constraints on annotations. We show that this may lead to increased complexity or even undecidability, and we identify cases where this increased expressivity can be achieved without incurring increased complexity of reasoning. In particular, we describe a tractable fragment based on the lightweight description logic EL, and we cover SROIQ, the DL underlying OWL 2 DL.},
  isbn = {978-3-319-68287-7 978-3-319-68288-4},
  langid = {english},
  file = {C:\Users\nhiot\OneDrive\zotero\2017\Krötzsch et al. - 2017 - Attributed Description Logics Ontologies for Know.pdf}
}

@article{kublerAdverseEventAnalysis2005,
  title = {Adverse {{Event Analysis}} and {{MedDRA}}: {{Business}} as {{Usual}} or {{Challenge}}?},
  shorttitle = {Adverse {{Event Analysis}} and {{MedDRA}}},
  author = {K{\"u}bler, J{\"u}rgen and Vonk, Richardus and Beimel, Stefan and Gunselmann, Winfried and Homering, Martin and Nehrdich, Detlef and K{\"o}ster, J{\"u}rgen and Theobald, Karlheini and Voleske, Peter},
  year = {2005},
  month = jan,
  journal = {Drug Information Journal},
  volume = {39},
  number = {1},
  pages = {63--72},
  issn = {0092-8615},
  doi = {10/fx74sk},
  urldate = {2019-11-04},
  abstract = {The Medical Dictionary for Regulatory Activities (MedDRA) is a dictionary of medical terms which covers signs and symptoms; diseases; diagnoses; therapeutic indications; names and qualitative results of indications; surgical and medical procedures; and medical, social, and family history. Until now, the pharmaceutical industry was only obligated to use MedDRA for submissions of individual case safety reports. Neither the MedDRA Maintenance and Support Services Organization nor regulatory bodies provided any recommendations on the use of MedDRA for analyses and reporting of clinical trial data. As the pharmaceutical industry introduces MedDRA in its day-to-day processes, there is an urgent need to fill this gap. The Biometry Subgroup of the German Association of Research-Based Pharmaceutical Companies has established a working group which will provide recommendations on the use of MedDRA in reporting and labeling. This paper aims to start a discussion about the use of MedDRA in the analysis of clinical trials.},
  langid = {english},
  file = {C:\Users\nhiot\OneDrive\zotero\2005\Kübler et al. - 2005 - Adverse Event Analysis and MedDRA Business as Usu.pdf}
}

@inproceedings{laffertyConditionalRandomFields2001,
  ids = {Lafferty01conditionalrandom},
  title = {Conditional Random Fields: {{Probabilistic}} Models for Segmenting and Labeling Sequence Data},
  shorttitle = {Conditional Random Fields},
  booktitle = {Icml},
  author = {Lafferty, John and McCallum, Andrew and Pereira, Fernando},
  year = {2001},
  volume = {1},
  pages = {3},
  publisher = {Williamstown, MA},
  url = {http://isoft.postech.ac.kr/{\textasciitilde}gblee/Course/CS704/LectureNotes/ConditionalRandomFields.pdf},
  urldate = {2024-03-21},
  keywords = {⛔ No DOI found,nosource},
  file = {C:\Users\nhiot\OneDrive\zotero\2001\Lafferty et al. - 2001 - Conditional random fields Probabilistic models fo.pdf}
}

@inproceedings{laiSUNNYNLPSemEval2018Task2018,
  title = {{{SUNNYNLP}} at {{SemEval-2018 Task}} 10: {{A Support-Vector-Machine-Based Method}} for {{Detecting Semantic Difference}} Using {{Taxonomy}} and {{Word Embedding Features}}},
  shorttitle = {{{SUNNYNLP}} at {{SemEval-2018 Task}} 10},
  booktitle = {Proceedings of {{The}} 12th {{International Workshop}} on {{Semantic Evaluation}}},
  author = {Lai, Sunny and Leung, Kwong Sak and Leung, Yee},
  year = {2018},
  month = jun,
  pages = {741--746},
  publisher = {Association for Computational Linguistics},
  address = {New Orleans, Louisiana},
  doi = {10/gg5hmt},
  urldate = {2020-07-21},
  abstract = {We present SUNNYNLP, our system for solving SemEval 2018 Task 10: ``Capturing Discriminative Attributes''. Our Support-Vector-Machine(SVM)-based system combines features extracted from pre-trained embeddings and statistical information from Is-A taxonomy to detect semantic difference of concepts pairs. Our system is demonstrated to be effective in detecting semantic difference and is ranked 1st in the competition in terms of F1 measure. The open source of our code is coined SUNNYNLP.},
  file = {C:\Users\nhiot\OneDrive\zotero\2018\Lai et al. - 2018 - SUNNYNLP at SemEval-2018 Task 10 A Support-Vector.pdf}
}

@article{lamportImplementationReliableDistributed2016,
  title = {The {{Implementation}} of {{Reliable Distributed Multiprocess Systems}}},
  author = {Lamport, Leslie},
  year = {2016},
  month = dec,
  journal = {Computer Networks},
  volume = {2},
  url = {https://www.microsoft.com/en-us/research/publication/implementation-reliable-distributed-multiprocess-systems/},
  urldate = {2019-01-17},
  abstract = {In [27], I introduced the idea of implementing any distributed system by using an algorithm to implement an arbitrary state machine in a distributed system. However, the algorithm in [27] assumed that processors never fail and all messages are delivered. This paper gives a fault-tolerant algorithm. It's a real-time algorithm, assuming upper bounds on message {\dots}},
  langid = {american},
  keywords = {⛔ No DOI found},
  annotation = {00000},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2016\\Lamport - 2016 - The Implementation of Reliable Distributed Multipr.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\J7S7EGLA\\implementation-reliable-distributed-multiprocess-systems.html}
}

@inproceedings{langlaisEnrichissementLexiqueBilingue2007,
  title = {{Enrichissement d'un lexique bilingue par analogie}},
  booktitle = {{Actes de la 14{\`e}me conf{\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs}},
  author = {Langlais, Philippe and Patry, Alexandre},
  year = {2007},
  month = jun,
  pages = {101--110},
  publisher = {IRIT / ATALA},
  address = {Toulouse, France},
  url = {https://aclanthology.org/2007.jeptalnrecital-long.9},
  urldate = {2023-10-23},
  abstract = {La pr{\'e}sence de mots inconnus dans les applications langagi{\`e}res repr{\'e}sente un d{\'e}fi de taille bien connu auquel n'{\'e}chappe pas la traduction automatique. Les syst{\`e}mes professionnels de traduction offrent {\`a} cet effet {\`a} leurs utilisateurs la possibilit{\'e} d'enrichir un lexique de base avec de nouvelles entr{\'e}es. R{\'e}cemment, Stroppa et Yvon (2005) d{\'e}montraient l'int{\'e}r{\^e}t du raisonnement par analogie pour l'analyse morphologique d'une langue. Dans cette {\'e}tude, nous montrons que le raisonnement par analogie offre {\'e}galement une r{\'e}ponse adapt{\'e}e au probl{\`e}me de la traduction d'entr{\'e}es lexicales inconnues.},
  langid = {french},
  keywords = {nosource},
  file = {C:\Users\nhiot\OneDrive\zotero\2007\Langlais et Patry - 2007 - Enrichissement d'un lexique bilingue par analogie.pdf}
}

@inproceedings{largeronSoftJaccardMesureSimilarite2009,
  ids = {largeronSoftjaccardMesureSimilarite2009},
  title = {{{SoftJaccard}}: Une Mesure de Similarit{\'e} Entre Ensembles de Cha{\^i}nes de Caract{\`e}res Pour l'unification d'entit{\'e}s Nomm{\'e}es},
  shorttitle = {{{SoftJaccard}}},
  booktitle = {Extraction et {{Gestion}} Des {{Connaissances}} ({{EGC}} 2009)},
  author = {Largeron, Christine and Kaddour, Bernard and Fernandez, Maria P.},
  year = {2009},
  month = jan,
  volume = {RNTI-E-15},
  pages = {443--444},
  publisher = {C{\'e}padu{\`e}s-{\'E}ditions},
  address = {Strasbourg, France},
  url = {https://hal-ujm.archives-ouvertes.fr/ujm-00366422},
  urldate = {2021-11-30},
  abstract = {Parmi les mesures de similarit{\'e} classiques utilisables sur des ensembles figure l'indice de Jaccard. Dans le cadre de cet article, nous en proposons une extension pour comparer des ensembles de cha{\^i}nes de caract{\`e}res. Cette mesure hybride permet de combiner une distance entre cha{\^i}nes de caract{\`e}res, telle que la distance de Levenstein, et l'indice de Jaccard. Elle est particuli{\`e}rement adapt{\'e}e pour mettre en correspondance des champs compos{\'e}s de plusieurs cha{\^i}nes de caract{\`e}res, comme par exemple, lorsqu'on se propose d'unifier des noms d'entit{\'e}s nomm{\'e}es.},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2009\Largeron et al. - 2009 - SoftJaccard une mesure de similarité entre ensemb.pdf}
}

@misc{LargeScaleInformationExtraction,
  title = {Large-{{Scale Information Extraction}} from {{Textual Definitions}} through {{Deep Syntactic}} and {{Semantic Analysis}} ({{TACL}}) - {{TechTalks}}.Tv},
  url = {http://techtalks.tv/talks/large-scale-information-extraction-from-textual-definitions-through-deep-syntactic-and-semantic-analysis-tacl/63071/},
  urldate = {2020-01-21},
  keywords = {nosource}
}

@inproceedings{laseckiChorusCrowdpoweredConversational2013,
  title = {Chorus: A Crowd-Powered Conversational Assistant},
  shorttitle = {Chorus},
  booktitle = {Proceedings of the 26th Annual {{ACM}} Symposium on {{User}} Interface Software and Technology},
  author = {Lasecki, Walter S. and Wesley, Rachel and Nichols, Jeffrey and Kulkarni, Anand and Allen, James F. and Bigham, Jeffrey P.},
  year = {2013},
  month = oct,
  pages = {151--162},
  publisher = {ACM},
  address = {St. Andrews Scotland, United Kingdom},
  doi = {10/gjbss6},
  urldate = {2021-03-11},
  abstract = {Despite decades of research attempting to establish conversational interaction between humans and computers, the capabilities of automated conversational systems are still limited. In this paper, we introduce Chorus, a crowd-powered conversational assistant. When using Chorus, end users converse continuously with what appears to be a single conversational partner. Behind the scenes, Chorus leverages multiple crowd workers to propose and vote on responses. A shared memory space helps the dynamic crowd workforce maintain consistency, and a game-theoretic incentive mechanism helps to balance their efforts between proposing and voting. Studies with 12 end users and 100 crowd workers demonstrate that Chorus can provide accurate, topical responses, answering nearly 93\% of user queries appropriately, and staying on-topic in over 95\% of responses. We also observed that Chorus has advantages over pairing an end user with a single crowd worker and end users completing their own tasks in terms of speed, quality, and breadth of assistance. Chorus demonstrates a new future in which conversational assistants are made usable in the real world by combining human and machine intelligence, and may enable a useful new way of interacting with the crowds powering other systems.},
  isbn = {978-1-4503-2268-3},
  langid = {english},
  file = {C:\Users\nhiot\OneDrive\zotero\2013\Lasecki et al. - 2013 - Chorus a crowd-powered conversational assistant.pdf}
}

@inproceedings{lausenSPARQLingConstraintsRDF2008,
  title = {{{SPARQLing}} Constraints for {{RDF}}},
  booktitle = {{{EDBT}}, 11th International Conference on Extending Database Technology, France, Proceedings},
  author = {Lausen, Georg and Meier, Michael and Schmidt, Michael},
  year = {2008},
  month = mar,
  series = {{{EDBT}} '08},
  pages = {499--509},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/1353343.1353404},
  urldate = {2023-12-24},
  abstract = {The goal of the Semantic Web is to support semantic interoperability between applications exchanging data on the web. The idea heavily relies on data being made available in machine readable format, using semantic markup languages. In this regard, the W3C has standardized RDF as the basic markup language for the Semantic Web. In contrast to relational databases, where data relationships are implicitly given by schema information as well as primary and foreign key constraints, relationships in semantic markup languages are made explicit. When mapping relational data into RDF, it is desirable to maintain the information implied by the origin constraints. As an improvement over existing approaches, our scheme allows for translating conventional databases into RDF without losing general constraints and vital key information. As much as in the relational model, those information are indispensable for data consistency and, as shown by example, can serve as a basis for semantic query optimization. We underline the practicability of our approach by showing that SPARQL, the most popular query language for RDF, can be used as a constraint language, akin to SQL in the relational context. As a theoretical contribution, we also discuss satisfiability for interesting classes of constraints and combinations thereof.},
  isbn = {978-1-59593-926-5},
  langid = {english},
  keywords = {nosource},
  file = {C:\Users\nhiot\OneDrive\zotero\2008\Lausen et al. - 2008 - SPARQLing constraints for RDF.pdf}
}

@inproceedings{lavergne2010practical,
  title = {Practical Very Large Scale {{CRFs}}},
  booktitle = {Proceedings the 48th Annual Meeting of the Association for Computational Linguistics ({{ACL}})},
  author = {Lavergne, Thomas and Capp{\'e}, Olivier and Yvon, Fran{\c c}ois},
  year = {2010},
  month = jul,
  pages = {504--513},
  publisher = {Association for Computational Linguistics},
  address = {Uppsala, Sweden},
  url = {http://www.aclweb.org/anthology/P10-1052},
  keywords = {nosource}
}

@misc{LDBCGraphalyticsBenchmark,
  title = {{{LDBC Graphalytics Benchmark}} ({{LDBC Graphalytics}})},
  url = {https://ldbcouncil.org/benchmarks/graphalytics/},
  urldate = {2021-09-01},
  keywords = {nosource}
}

@misc{LearningMultilingualNamed2017,
  title = {Learning Multilingual Named Entity Recognition from {{Wikipedia}}},
  year = {2017},
  month = oct,
  publisher = {figshare},
  doi = {10.6084/m9.figshare.5462500.v1},
  urldate = {2024-03-21},
  abstract = {This is the data associated with Joel Nothman, Nicky Ringland, Will Radford, Tara Murphy and James R. Curran (2013), "Learning multilingual named entity recognition from Wikipedia", Artificial Intelligence 194 (DOI: 10.1016/j.artint.2012.03.006). A preprint is included here as wikiner-preprint.pdfThis data was originally available at http://schwa.org/resources (which linked to http://schwa.org/projects/resources/wiki/Wikiner).The .bz2 files are NER training corpora produced as reported in the Artificial Intelligence paper. wp2 and wp3 are differentiated by wp3 using a higher level of link inference. They use a pipe-delimited format that can be converted to CoNLL 2003 format with system2conll.pl.nothman08types.tsv is a manual classification of articles first used in Joel Nothman, James R. Curran and Tara Murphy (2008), "Transforming Wikipedia into Named Entity Training Data", In Proceedings of the Australasian Language Technology Association Workshop 2008. http://aclanthology.coli.uni-saarland.de/pdf/U/U08/U08-1016.pdfpopular.tsv and random.tsv are manual article classifications developed for the Artifiical Intelligence paper based on different strategies for sampling articles from Wikipedia in order to account for Wikipedia's biased distribution (see that paper). scheme.tsv maps these fine-grained labels to coarser annotations including CoNLL 2003-style.wikigold.conll.txt is a manual NER annotation of some Wikipedia text as presented in Dominic Balasuriya and Nicky  Ringland and Joel Nothman and Tara Murphy and James R. Curran (2009), in Proceedings of the 2009 Workshop on The People's Web Meets NLP: Collaboratively Constructed Semantic Resources (http://www.aclweb.org/anthology/W/W09/W09-3302).See also corpora produced similarly in an enhanced version of this work work (Pan et al., "Cross-lingual Name Tagging and Linking for 282 Languages", ACL 2017) at http://nlp.cs.rpi.edu/wikiann/.},
  langid = {english},
  file = {C:\Users\nhiot\Zotero\storage\IDT9KVAN\5462500.html}
}

@article{Lemmatisation2019,
  title = {Lemmatisation},
  year = {2019},
  month = jan,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Lemmatisation\&oldid=879619831},
  urldate = {2019-01-22},
  abstract = {Lemmatisation (or lemmatization) in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.In computational linguistics, lemmatisation is the algorithmic process of determining the lemma of a word based on its intended meaning. Unlike stemming, lemmatisation depends on correctly identifying the intended part of speech and meaning of a word in a sentence, as well as within the larger context surrounding that sentence, such as neighboring sentences or even an entire document. As a result, developing efficient lemmatisation algorithms is an open area of research.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {00000  Page Version ID: 879619831},
  file = {C:\Users\nhiot\Zotero\storage\9XDE346T\index.html}
}

@inproceedings{leroyCompCertFormallyVerified2016,
  title = {{{CompCert}} - {{A Formally Verified Optimizing Compiler}}},
  booktitle = {{{ERTS}} 2016: {{Embedded Real Time Software}} and {{Systems}}, 8th {{European Congress}}},
  author = {Leroy, Xavier and Blazy, Sandrine and K{\"a}stner, Daniel and Schommer, Bernhard and Pister, Markus and Ferdinand, Christian},
  year = {2016},
  month = jan,
  publisher = {SEE},
  address = {Toulouse, France},
  url = {https://hal.inria.fr/hal-01238879},
  urldate = {2018-12-30},
  abstract = {CompCert is the first commercially available optimizing compiler that is formally verified, using machine-assisted mathematical proofs, to be exempt from mis-compilation. The executable code it produces is proved to behave exactly as specified by the semantics of the source C program. This article gives an overview of the design of CompCert and its proof concept and then focuses on aspects relevant for industrial application. We briefly summarize practical experience and give an overview of recent CompCert development aiming at industrial usage. CompCert's intended use is the compilation of life-critical and mission-critical software meeting high levels of assurance. In this context tool qualification is of paramount importance. We summarize the confidence argument of CompCert and give an overview of relevant qualification strategies.},
  annotation = {00000},
  file = {C:\Users\nhiot\OneDrive\zotero\2016\Leroy et al. - 2016 - CompCert - A Formally Verified Optimizing Compiler.pdf}
}

@book{leskovecMiningMassiveDatasets2020,
  ids = {leskovecMiningMassiveData2020,leskovecMiningMassiveDatasets},
  title = {Mining of {{Massive Datasets}}},
  author = {Leskovec, Jure and Rajaraman, Anand and Ullman, Jeff},
  year = {2020},
  publisher = {Cambridge university press},
  langid = {english},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2020\\Leskovec et al. - 2020 - Mining of Massive Datasets.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\CIVS8ISL\\books.html}
}

@article{liangQueryingKnowledgeGraphs2021,
  ids = {shiqiQueryingKnowledgeGraphs},
  title = {Querying Knowledge Graphs in Natural Language},
  author = {Liang, Shiqi and Stockinger, Kurt and {de Farias}, Tarcisio Mendes and Anisimova, Maria and Gil, Manuel},
  year = {2021},
  month = jan,
  journal = {Journal of Big Data},
  volume = {8},
  number = {1},
  pages = {3},
  publisher = {Springer Nature BV},
  issn = {2196-1115},
  doi = {10/ghs3bh},
  urldate = {2021-01-14},
  abstract = {Knowledge graphs are a powerful concept for querying large amounts of data. These knowledge graphs are typically enormous and are often not easily accessible to end-users because they require specialized knowledge in query languages such as SPARQL. Moreover, end-users need a deep understanding of the structure of the underlying data models often based on the Resource Description Framework (RDF). This drawback has led to the development of Question-Answering (QA) systems that enable end-users to express their information needs in natural language. While existing systems simplify user access, there is still room for improvement in the accuracy of these systems. In this paper we propose a new QA system for translating natural language questions into SPARQL queries. The key idea is to break up the translation process into 5 smaller, more manageable sub-tasks and use ensemble machine learning methods as well as Tree-LSTM-based neural network models to automatically learn and translate a natural language question into a SPARQL query. The performance of our proposed QA system is empirically evaluated using the two renowned benchmarks-the 7th Question Answering over Linked Data Challenge (QALD-7) and the Large-Scale Complex Question Answering Dataset (LC-QuAD). Experimental results show that our QA system outperforms the state-of-art systems by 15\% on the QALD-7 dataset and by 48\% on the LC-QuAD dataset, respectively. In addition, we make our source code available.},
  langid = {english},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2021\\Liang et al. - 2021 - Querying knowledge graphs in natural language.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\XVRMJWXY\\1.html}
}

@inproceedings{libkinIncompleteDataWhat2014,
  title = {Incomplete Data: What Went Wrong, and How to Fix It},
  shorttitle = {Incomplete Data},
  booktitle = {Proceedings of the 33rd {{ACM SIGMOD-SIGACT-SIGART}} Symposium on {{Principles}} of Database Systems},
  author = {Libkin, Leonid},
  year = {2014},
  month = jun,
  series = {{{PODS}} '14},
  pages = {1--13},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2594538.2594561},
  urldate = {2023-08-03},
  abstract = {Incomplete data is ubiquitous: the more data we accumulate and the more widespread tools for integrating and exchanging data become, the more instances of incompleteness we have. And yet the subject is poorly handled by both practice and theory. Many queries for which students get full marks in their undergraduate courses will not work correctly in the presence of incomplete data, but these ways of evaluating queries are cast in stone -- SQL standard. We have many theoretical results on handling incomplete data but they are, by and large, about showing high complexity bounds, and thus are often dismissed by practitioners. Even worse, we have a basic theoretical notion of what it means to answer queries over incomplete data, and yet this is not at all what practical systems do. Is there a way out of this predicament? Can we have a theory of incompleteness that will appeal to theoreticians and practitioners alike, by explaining incompleteness and being at the same time implementable and useful for applications? After giving a critique of both the practice and the theory of handling incompleteness in databases, the paper outlines a possible way out of this crisis. The key idea is to combine three hitherto used approaches to incompleteness: one based on certain answers and representation systems, one based on viewing incomplete databases as logical theories, and one based on orderings expressing relative value of information.},
  isbn = {978-1-4503-2375-8},
  keywords = {incomplete information,query evaluation},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2014\\Libkin - 2014 - Incomplete data what went wrong, and how to fix i.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\VN5CSSYT\\2594538.html}
}

@article{libkinSQLThreeValuedLogic2016,
  title = {{{SQL}}'s {{Three-Valued Logic}} and {{Certain Answers}}},
  author = {Libkin, Leonid},
  year = {2016},
  journal = {ACM Transactions on Database Systems (TODS)},
  volume = {41},
  number = {1},
  pages = {1--28},
  publisher = {ACM New York, NY, USA},
  issn = {0362-5915},
  doi = {10.1145/2877206},
  abstract = {The goal of the article is to bridge the difference between theoretical and practical approaches to answering queries over databases with nulls. Theoretical research has long ago identified the notion of correctness of query answering over incomplete data: one needs to find certain answers, which are true regardless of how incomplete information is interpreted. This serves as the notion of correctness of query answering, but carries a huge complexity tag. In practice, on the other hand, query answering must be very efficient, and to achieve this, SQL uses three-valued logic for evaluating queries on databases with nulls. Due to the complexity mismatch, the two approaches cannot coincide, but perhaps they are related in some way. For instance, does SQL always produce answers we can be certain about? This is not so: SQL's and certain answers semantics could be totally unrelated. We show, however, that a slight modification of the three-valued semantics for relational calculus queries can provide the required certainty guarantees. The key point of the new scheme is to fully utilize the three-valued semantics, and classify answers not into certain or noncertain, as was done before, but rather into certainly true, certainly false, or unknown. This yields relatively small changes to the evaluation procedure, which we consider at the level of both declarative (relational calculus) and procedural (relational algebra) queries. These new evaluation procedures give us certainty guarantees even for queries returning tuples with null values.},
  keywords = {certain answers,incomplete information,Null values,query evaluation,three-valued logic},
  file = {C:\Users\nhiot\OneDrive\zotero\2016\Libkin - 2016 - SQL’s Three-Valued Logic and Certain Answers.pdf}
}

@misc{LingPipeHome,
  title = {{{LingPipe Home}}},
  url = {http://alias-i.com/lingpipe/},
  urldate = {2019-01-22},
  annotation = {00000},
  file = {C:\Users\nhiot\Zotero\storage\KM4L46XU\lingpipe.html}
}

@article{linkArithmeticTheoryConsistency2002,
  title = {Towards an {{Arithmetic Theory}} of {{Consistency Enforcement}} Based on {{Preservation}} of {$\delta$}-Constraints},
  author = {Link, Sebastian and Schewe, Klaus-Dieter},
  year = {2002},
  month = jan,
  journal = {Electronic Notes in Theoretical Computer Science},
  series = {{{CATS}}'02, {{Computing}}: The {{Australasian Theory Symposium}}},
  volume = {61},
  pages = {64--83},
  issn = {1571-0661},
  doi = {10.1016/S1571-0661(04)00306-8},
  urldate = {2023-08-16},
  abstract = {Consistency Enforcement provides an alternative theory to common verification techniques within formal specification languages. We consider specifications in the form of guarded commands. The basic idea is then to replace a program specification S by its greatest consistent specialization (GCS) SI which is provably consistent with respect to a given static constraint I, preserves the effects of S according to a specialization order and is maximal with these properties. The theory has been shown to provide several strengths. In particular, the enforcement process for a huge class of complex specifications can be reduced to its basic components. Moreover, the result can be obtained sequentially and is independent from the order of the given constraints. In addition, arithmetic logic has been used to show that GCSs can be efficiently computed for a reasonably large class of program specifications and invariants. However, all results have been achieved with respect to the underlying specialization order. The simplicity of this order reveals some obvious weaknesses. In this paper, we show how the specialization order can be replaced by the notion of {$\delta$}-constraints. Specialization of a program specification S turns out to be equivalent to the preservation of all {$\delta$}-constraints on the underlying state space of S. Obviously, this enables us to weaken the specialization order towards the preservation of certain {$\delta$}-constraints. We define maximal consistent effect preservers (MCEs), show that these are closely related to GCSs and prove that MCEs can be obtained sequentially and independently from the order of a given set of static constraints. This backs up the conjecture that the notion of MCEs leads towards a tailored theory of consistency enforcement.},
  keywords = {arithmetic logic,consistency,constraints,formal specifications,GCS,guarded commands,MCE},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2002\\Link et Schewe - 2002 - Towards an Arithmetic Theory of Consistency Enforc.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\LBEYDF3M\\S1571066104003068.html}
}

@article{linkArithmeticTheoryConsistency2002a,
  title = {An Arithmetic Theory of Consistency Enforcement},
  author = {Link, Sebastian and Schewe, Klaus-Dieter},
  year = {2002},
  journal = {Acta Cybern.},
  volume = {15},
  number = {3},
  pages = {379--416},
  keywords = {⛔ No DOI found,nosource}
}

@misc{LinkPredictionNeo4j,
  title = {Link {{Prediction}} with {{Neo4j Part}} 1: {{An Introduction}} - {{Neo4j Developer Blog}} - {{Medium}}},
  url = {https://medium.com/neo4j/link-prediction-with-neo4j-part-1-an-introduction-713aa779fd9},
  urldate = {2020-03-11},
  file = {C:\Users\nhiot\Zotero\storage\W97XUH3P\link-prediction-with-neo4j-part-1-an-introduction-713aa779fd9.html}
}

@article{linMedTimeTemporalInformation2013,
  ids = {linMedTimeTemporalInformation2013b},
  title = {{{MedTime}}: {{A}} Temporal Information Extraction System for Clinical Narratives},
  shorttitle = {{{MedTime}}},
  author = {Lin, Yu-Kai and Chen, Hsinchun and Brown, Randall A.},
  year = {2013},
  month = dec,
  journal = {Journal of Biomedical Informatics},
  series = {2012 I2b2 {{NLP Challenge}} on {{Temporal Relations}} in {{Clinical Data}}},
  volume = {46},
  pages = {S20--S28},
  publisher = {Elsevier},
  issn = {1532-0464},
  doi = {10/ghktch},
  urldate = {2020-11-20},
  abstract = {Temporal information extraction from clinical narratives is of critical importance to many clinical applications. We participated in the EVENT/TIMEX3 track of the 2012 i2b2 clinical temporal relations challenge, and presented our temporal information extraction system, MedTime. MedTime comprises a cascade of rule-based and machine-learning pattern recognition procedures. It achieved a micro-averaged f-measure of 0.88 in both the recognitions of clinical events and temporal expressions. We proposed and evaluated three time normalization strategies to normalize relative time expressions in clinical texts. The accuracy was 0.68 in normalizing temporal expressions of dates, times, durations, and frequencies. This study demonstrates and evaluates the integration of rule-based and machine-learning-based approaches for high performance temporal information extraction from clinical narratives.},
  langid = {english},
  keywords = {Event recognition,i2b2,Temporal expression recognition and normalization,Temporal information extraction},
  annotation = {QID: Q45958814},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2013\\Lin et al. - 2013 - MedTime A temporal information extraction system .pdf;C\:\\Users\\nhiot\\Zotero\\storage\\NP62C92K\\S1532046413001135.html}
}

@article{linSimilarityMeasureText2014,
  title = {A Similarity Measure for Text Classification and Clustering},
  author = {Lin, Yung-Shen and Jiang, Jung-Yi and Lee, Shie-Jue},
  year = {2014},
  month = jul,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {26},
  number = {7},
  pages = {1575--1590},
  issn = {1558-2191},
  doi = {10.1109/tkde.2013.19},
  abstract = {Measuring the similarity between documents is an important operation in the text processing field. In this paper, a new similarity measure is proposed. To compute the similarity between two documents with respect to a feature, the proposed measure takes the following three cases into account: a) The feature appears in both documents, b) the feature appears in only one document, and c) the feature appears in none of the documents. For the first case, the similarity increases as the difference between the two involved feature values decreases. Furthermore, the contribution of the difference is normally scaled. For the second case, a fixed value is contributed to the similarity. For the last case, the feature has no contribution to the similarity. The proposed measure is extended to gauge the similarity between two sets of documents. The effectiveness of our measure is evaluated on several real-world data sets for text classification and clustering problems. The results show that the performance obtained by the proposed measure is better than that achieved by other measures.},
  keywords = {accuracy,Approximation methods,classifiers,clustering algorithms,Clustering algorithms,Document classification,document clustering,Educational institutions,entropy,Euclidean distance,Text processing,Vectors},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2014\\Lin et al. - 2014 - A similarity measure for text classification and c.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\EEYQAHWW\\6420834.html}
}

@article{lipscombMedicalSubjectHeadings2000,
  title = {Medical {{Subject Headings}} ({{MeSH}})},
  author = {Lipscomb, Carolyn E.},
  year = {2000},
  month = jul,
  journal = {Bulletin of the Medical Library Association},
  volume = {88},
  number = {3},
  pages = {265--266},
  publisher = {Medical Library Association},
  issn = {0025-7338},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC35238/},
  urldate = {2023-09-25},
  langid = {english},
  pmcid = {PMC35238},
  pmid = {10928714},
  keywords = {{History, 20th Century},⛔ No DOI found,MEDLARS,MEDLINE,National Library of Medicine (U.S.),Subject Headings,Unified Medical Language System,United States},
  annotation = {QID: Q30895814},
  file = {C:\Users\nhiot\OneDrive\zotero\2000\Lipscomb - 2000 - Medical Subject Headings (MeSH).pdf}
}

@article{liuResearchFrameworkPharmacovigilance2015,
  title = {A Research Framework for Pharmacovigilance in Health Social Media: {{Identification}} and Evaluation of Patient Adverse Drug Event Reports},
  shorttitle = {A Research Framework for Pharmacovigilance in Health Social Media},
  author = {Liu, Xiao and Chen, Hsinchun},
  year = {2015},
  month = dec,
  journal = {Journal of Biomedical Informatics},
  volume = {58},
  pages = {268--279},
  issn = {1532-0464},
  doi = {10/gfvn2n},
  urldate = {2019-02-11},
  abstract = {Social media offer insights of patients' medical problems such as drug side effects and treatment failures. Patient reports of adverse drug events from social media have great potential to improve current practice of pharmacovigilance. However, extracting patient adverse drug event reports from social media continues to be an important challenge for health informatics research. In this study, we develop a research framework with advanced natural language processing techniques for integrated and high-performance patient reported adverse drug event extraction. The framework consists of medical entity extraction for recognizing patient discussions of drug and events, adverse drug event extraction with shortest dependency path kernel based statistical learning method and semantic filtering with information from medical knowledge bases, and report source classification to tease out noise. To evaluate the proposed framework, a series of experiments were conducted on a test bed encompassing about postings from major diabetes and heart disease forums in the United States. The results reveal that each component of the framework significantly contributes to its overall effectiveness. Our framework significantly outperforms prior work.},
  keywords = {Adverse drug event extraction,Health social media analytics,Information search and retrieval,Knowledge acquisition,Pharmacovigilance,Text mining},
  annotation = {00034 QID: Q38401125},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2015\\Liu et Chen - 2015 - A research framework for pharmacovigilance in heal.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\YWAP54QF\\Liu et Chen - 2015 - A research framework for pharmacovigilance in heal.html}
}

@article{lodhiTextClassificationUsing2002,
  ids = {lodhiTextClassificationUsing2002a,lodhiTextClassificationUsing2002b},
  title = {Text {{Classification}} Using {{String Kernels}}},
  author = {Lodhi, Huma and Saunders, Craig and {Shawe-Taylor}, John and Cristianini, Nello and Watkins, Chris},
  year = {2002},
  month = mar,
  journal = {The Journal of Machine Learning Research},
  volume = {2},
  number = {Feb},
  pages = {419--444},
  issn = {1532-4435},
  doi = {10/d629vp},
  abstract = {We propose a novel approach for categorizing text documents based on the use of a special kernel. The kernel is an inner product in the feature space generated by all subsequences of length {$<$}em{$>$}k{$<$}/em{$>$}. A subsequence is any ordered sequence of {$<$}em{$>$}k{$<$}/em{$>$} characters occurring in the text though not necessarily contiguously. The subsequences are weighted by an exponentially decaying factor of their full length in the text, hence emphasising those occurrences that are close to contiguous. A direct computation of this feature vector would involve a prohibitive amount of computation even for modest values of {$<$}em{$>$}k{$<$}/em{$>$}, since the dimension of the feature space grows exponentially with {$<$}em{$>$}k{$<$}/em{$>$}. The paper describes how despite this fact the inner product can be efficiently evaluated by a dynamic programming technique. Experimental comparisons of the performance of the kernel compared with a standard word feature space kernel (Joachims, 1998) show positive results on modestly sized datasets. The case of contiguous subsequences is also considered for comparison with the subsequences kernel with different decay factors. For larger documents and datasets the paper introduces an approximation technique that is shown to deliver good approximations efficiently for large datasets.},
  keywords = {approximating kernels,kernels and support vector machines,string subsequence kernel,text classification},
  annotation = {QID: Q77695122},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2002\\Lodhi et al. - 2002 - Text Classification using String Kernels.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\7IDRTWWA\\lodhi02a.html}
}

@misc{LOINCFreelyAvailable,
  title = {{{LOINC}} --- {{The}} Freely Available Standard for Identifying Health Measurements, Observations, and Documents.},
  url = {https://loinc.org/},
  urldate = {2019-01-29},
  langid = {american},
  annotation = {00000},
  file = {C:\Users\nhiot\Zotero\storage\6MYUWK6D\loinc.org.html}
}

@incollection{macqueenProceedings5thBerkeley1967,
  title = {Proceedings of 5th {{Berkeley Symposium}} on {{Mathematical Statistics}} and {{Probability}}},
  author = {MacQueen, J. B.},
  year = {1967},
  pages = {281--297},
  publisher = {University of California Press},
  keywords = {nosource},
  annotation = {00000}
}

@misc{MaDICSMassesDonnees,
  title = {{{MaDICS}} {\textbar} {{Masses}} de {{Donn{\'e}es}}, {{Informations}} et {{Connaissances}} En {{Sciences}}},
  url = {http://www.madics.fr/},
  urldate = {2020-02-18},
  langid = {american},
  file = {C:\Users\nhiot\Zotero\storage\5KH64I9N\www.madics.fr.html}
}

@incollection{maedcheBootstrappingOntologyBasedInformation2003,
  title = {Bootstrapping an {{Ontology-Based Information Extraction System}}},
  booktitle = {Intelligent {{Exploration}} of the {{Web}}},
  author = {Maedche, Alexander and Neumann, G{\"u}nter and Staab, Steffen},
  editor = {Szczepaniak, Piotr S. and Segovia, Javier and Kacprzyk, Janusz and Zadeh, Lotfi A.},
  year = {2003},
  series = {Studies in {{Fuzziness}} and {{Soft Computing}}},
  pages = {345--359},
  publisher = {Physica-Verlag HD},
  address = {Heidelberg},
  doi = {10.1007/978-3-7908-1772-0_21},
  urldate = {2019-01-30},
  abstract = {Automatic intelligent web exploration will benefit from shallow information extraction techniques if the latter can be brought to work within many different domains. The major bottleneck for this, however, lies in the so far difficult and expensive modeling of lexical knowledge, extraction rules, and an ontology that together define the information extraction system. In this paper we present a bootstrapping approach that allows for the fast creation of an ontology-based information extracting system relying on several basic components, viz. a core information extraction system, an ontology engineering environment and an inference engine. We make extensive use of machine learning techniques to support the semi-automatic, incremental bootstrapping of the domain-specific target information extraction system.},
  isbn = {978-3-7908-1772-0},
  langid = {english},
  keywords = {information extraction,machine learning,Ontologies},
  annotation = {00000},
  file = {C:\Users\nhiot\OneDrive\zotero\2003\Maedche et al. - 2003 - Bootstrapping an Ontology-Based Information Extrac.pdf}
}

@misc{maggiesmsftUsePowerBI,
  title = {Use {{Power BI Q}}\&{{A}} to Explore and Create Visuals - {{Power BI}}},
  author = {{maggiesMSFT}},
  url = {https://docs.microsoft.com/en-us/power-bi/create-reports/power-bi-tutorial-q-and-a},
  urldate = {2020-05-14},
  abstract = {How to use Power BI Q\&A to create new visualizations on dashboards and in reports.},
  langid = {american},
  file = {C:\Users\nhiot\Zotero\storage\9R6ZRPPI\power-bi-tutorial-q-and-a.html}
}

@phdthesis{mahfoudhAdaptationOntologiesAvec2015,
  title = {{Adaptation d'ontologies avec les grammaires de graphes typ{\'e}s : {\'e}volution et fusion}},
  shorttitle = {{Adaptation d'ontologies avec les grammaires de graphes typ{\'e}s}},
  author = {Mahfoudh, Mariem},
  year = {2015},
  month = may,
  url = {https://tel.archives-ouvertes.fr/tel-01528579},
  urldate = {2019-03-07},
  abstract = {{\'E}tant une repr{\'e}sentation formelle et explicite des connaissances d'un domaine, les ontologies font r{\'e}guli{\`e}rement l'objet de nombreux changements et ont ainsi besoin d'{\^e}tre constamment adapt{\'e}es pour notamment pouvoir {\^e}tre r{\'e}utilis{\'e}es et r{\'e}pondre aux nouveaux besoins. Leur r{\'e}utilisation peut prendre diff{\'e}rentes formes ({\'e}volution, alignement, fusion, etc.), et pr{\'e}sente plusieurs verrous scientifiques. L'un des plus importants est la pr{\'e}servation de la consistance de l'ontologie lors de son changement. Afin d'y r{\'e}pondre, nous nous int{\'e}ressons dans cette th{\`e}se {\`a} {\'e}tudier les changements ontologiques et proposons un cadre formel capable de faire {\'e}voluer et de fusionner des ontologies sans affecter leur consistance. Premi{\`e}rement, nous proposons TGGOnto (Typed Graph Grammars for Ontologies), un nouveau formalisme permettant la repr{\'e}sentation des ontologies et leurs changements par les grammaires de graphes typ{\'e}s. Un couplage entre ces deux formalismes est d{\'e}fini afin de profiter des concepts des grammaires de graphes, notamment les NAC (Negative Application Conditions), pour la pr{\'e}servation de la consistance de l'ontologie adapt{\'e}e.Deuxi{\`e}mement, nous proposons EvOGG (Evolving Ontologies with Graph Grammars), une approche d'{\'e}volution d'ontologies qui se base sur le formalisme GGTOnto et traite les inconsistances d'une mani{\`e}re a priori. Nous nous int{\'e}ressons aux ontologies OWL et nous traitons {\`a} la fois : (1) l'enrichissement d'ontologies en {\'e}tudiant leur niveau structurel et (2) le peuplement d'ontologies en {\'e}tudiant les changements qui affectent les individus et leurs assertions. L'approche EvOGG d{\'e}finit des changements ontologiques de diff{\'e}rents types ({\'e}l{\'e}mentaires, compos{\'e}es et complexes) et assure leur impl{\'e}mentation par l'approche alg{\'e}brique de transformation de graphes, SPO (Simple PushOut). Troisi{\`e}mement, nous proposons GROM (Graph Rewriting for Ontology Merging), une approche de fusion d'ontologies capable d'{\'e}viter les redondances de donn{\'e}es et de diminuer les conflits dans le r{\'e}sultat de fusion. L'approche propos{\'e}e se d{\'e}compose en trois {\'e}tapes : (1) la recherche de similarit{\'e} entre concepts en se basant sur des techniques syntaxiques, structurelles et s{\'e}mantiques ; (2) la fusion d'ontologies par l'approche alg{\'e}brique SPO ; (3) l'adaptation de l'ontologie globale r{\'e}sultante par le biais des r{\`e}gles de r{\'e}{\'e}criture de graphes.Afin de valider les travaux men{\'e}s dans cette th{\`e}se, nous avons d{\'e}velopp{\'e} plusieurs outils open source bas{\'e}s sur l'outil AGG (Attributed Graph Grammar). Ces outils ont {\'e}t{\'e} appliqu{\'e}s sur un ensemble d'ontologies, essentiellement sur celles d{\'e}velopp{\'e}es dans le cadre du projet europ{\'e}en CCAlps (Creatives Companies in Alpine Space) qui a financ{\'e} les travaux de cette th{\`e}se.},
  langid = {french},
  school = {Universit{\'e} de Haute Alsace-Mulhouse},
  keywords = {nosource},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2015\\Mahfoudh - 2015 - Adaptation d'ontologies avec les grammaires de gra.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\ZHAL55QA\\tel-01528579.html}
}

@article{mahfoudhAlgebraicGraphTransformations2015,
  title = {Algebraic Graph Transformations for Formalizing Ontology Changes and Evolving Ontologies},
  author = {Mahfoudh, Mariem and Forestier, Germain and Thiry, Laurent and Hassenforder, Michel},
  year = {2015},
  journal = {Knowledge-Based Systems},
  volume = {73},
  pages = {212--226},
  issn = {0950-7051},
  doi = {10.1016/j.knosys.2014.10.007},
  keywords = {AGG,Algebraic graph transformations,Consistency,nosource,Ontology evolution,Typed Graph Grammars},
  annotation = {QID: Q114826529},
  file = {C:\Users\nhiot\OneDrive\zotero\2015\Mahfoudh et al. - 2015 - Algebraic graph transformations for formalizing on.pdf}
}

@article{maierTestingImplicationsData1979,
  title = {Testing Implications of Data Dependencies},
  author = {Maier, David and Mendelzon, Alberto O. and Sagiv, Yehoshua},
  year = {1979},
  month = dec,
  journal = {ACM Transactions on Database Systems (TODS)},
  volume = {4},
  number = {4},
  pages = {455--469},
  publisher = {ACM New York, NY, USA},
  issn = {0362-5915},
  doi = {10.1145/320107.320115},
  urldate = {2023-08-08},
  abstract = {Presented is a computation method---the chase---for testing implication of data dependencies by a set of data dependencies. The chase operates on tableaux similar to those of Aho, Sagiv, and Ullman. The chase includes previous tableau computation methods as special cases. By interpreting tableaux alternately as mappings or as templates for relations, it is possible to test implication of join dependencies (including multivalued dependencies) and functional dependencies by a set of dependencies.},
  keywords = {chase,data dependencies,functional dependencies,join dependencies,multivalued dependencies,relational databases,tableaux},
  annotation = {QID: Q114614050},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1979\\Maier et al. - 1979 - Testing implications of data dependencies.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\5DK3BYZS\\320107.html}
}

@inproceedings{maillotConsistencyEvaluationRDF2014,
  title = {Consistency {{Evaluation}} of {{RDF Data}}: {{How Data}} and {{Updates}} Are {{Relevant}}},
  shorttitle = {Consistency {{Evaluation}} of {{RDF Data}}},
  booktitle = {Tenth International Conference on Signal-Image Technology and Internet-Based Systems, {{SITIS}} 2014, Marrakech, Morocco, November 23-27, 2014},
  author = {Maillot, Pierre and Raimbault, Thomas and Genest, David and Loiseau, St{\'e}phane},
  year = {2014},
  month = nov,
  pages = {187--193},
  publisher = {IEEE},
  doi = {10.1109/SITIS.2014.39},
  abstract = {Trust and quality maintenance have always been problematic in the Semantic Web RDF bases. Numerous propositions to address these problems of data integration have been made, either based on ontologies or on additional metadata. However ontologies suffer from a adaptation speed slower than the data evolution speed and metadata requires ad-hoc manipulations of data by addition of extra-data. In this article we propose an original approach, based exclusively on data from the base, to evaluate the consistency of a candidate update to a RDF base, and finally to know if this update is relevant to the base. Our approach is inspired by case-based reasoning and uses similarity evaluation and query relaxation methods to compare a candidate update to the data from the base. If the modifications of a candidate update make the target part of the base more similar to other part (s) of the base, then this candidate update is considered consistent with the base and can be applied.},
  keywords = {Case-based reasoning,Cognition,Consistency,Context,Data Integration,Databases,Ontologies,Ontology,Resource description framework,Semantic Web,Similarity,Weight measurement},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2014\\Maillot et al. - 2014 - Consistency Evaluation of RDF Data How Data and U.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\7463EG8S\\7081546.html}
}

@inproceedings{marsh-perzanowski-1998-muc,
  title = {{{MUC-7}} Evaluation of {{IE}} Technology: {{Overview}} of Results},
  booktitle = {Seventh Message Understanding Conference ({{MUC-7}}): {{Proceedings}} of a Conference Held in Fairfax, Virginia, {{April}} 29 - May 1, 1998},
  author = {Marsh, Elaine and Perzanowski, Dennis},
  year = {1998},
  url = {https://www.aclweb.org/anthology/M98-1002},
  keywords = {⛔ No DOI found,nosource}
}

@article{Martinez:2017jbp,
  title = {Replace or {{Retrieve Keywords In Documents}} at {{Scale}}},
  author = {Singh, Vikash},
  year = {2017},
  month = nov,
  journal = {arXiv:1711.00046 [cs]},
  eprint = {1711.00046},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1711.00046},
  urldate = {2020-01-27},
  abstract = {In this paper we introduce, the FlashText algorithm for replacing keywords or finding keywords in a given text. FlashText can search or replace keywords in one pass over a document. The time complexity of this algorithm is not dependent on the number of terms being searched or replaced. For a document of size N (characters) and a dictionary of M keywords, the time complexity will be O(N). This algorithm is much faster than Regex, because regex time complexity is O(MxN). It is also different from Aho Corasick Algorithm, as it doesn't match substrings. FlashText is designed to only match complete words (words with boundary characters on both sides). For an input dictionary of \{Apple\}, this algorithm won't match it to 'I like Pineapple'. This algorithm is also designed to go for the longest match first. For an input dictionary \{Machine, Learning, Machine learning\} on a string 'I like Machine learning', it will only consider the longest match, which is Machine Learning. We have made python implementation of this algorithm available as open-source on GitHub, released under the permissive MIT License.},
  archiveprefix = {arxiv},
  keywords = {⛔ No DOI found,Computer Science - Data Structures and Algorithms},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2017\\Singh - 2017 - Replace or Retrieve Keywords In Documents at Scale.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\ZHFVQH2H\\1711.html}
}

@article{martinWikidataConstraintsMARS2020,
  title = {Wikidata {{Constraints}} on {{MARS}} ({{Extended Technical Report}})},
  author = {Martin, David L. and {Patel-Schneider}, Peter F.},
  year = {2020},
  month = aug,
  journal = {arXiv:2008.03900 [cs]},
  eprint = {2008.03900},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2008.03900},
  urldate = {2021-03-19},
  abstract = {Wikidata constraints, albeit useful, are represented and processed in an incomplete, ad hoc fashion. Constraint declarations do not fully express their meaning, and thus do not provide a precise, unambiguous basis for constraint specification, or a logical foundation for constraint-checking implementations. In prior work we have proposed a logical framework for Wikidata as a whole, based on multi-attributed relational structures (MARS) and related logical languages. In this paper we explain how constraints are handled in the proposed framework, and show that nearly all of Wikidata's existing property constraints can be completely characterized in it, in a natural and economical fashion. We also give characterizations for several proposed property constraints, and show that a variety of non-property constraints can be handled in the same framework.},
  archiveprefix = {arxiv},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2020\\Martin et Patel-Schneider - 2020 - Wikidata Constraints on MARS (Extended Technical R.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\T562G5Z8\\2008.html}
}

@inproceedings{marxLogicMARSOntologies2017,
  title = {Logic on {{MARS}}: {{Ontologies}} for {{Generalised Property Graphs}}},
  shorttitle = {Logic on {{MARS}}},
  booktitle = {Proceedings of the {{Twenty-Sixth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Marx, Maximilian and Kr{\"o}tzsch, Markus and Thost, Veronika},
  year = {2017},
  month = aug,
  pages = {1188--1194},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  address = {Melbourne, Australia},
  doi = {10/gkms99},
  urldate = {2021-06-14},
  abstract = {Graph-structured data is used to represent large information collections, called knowledge graphs, in many applications. Their exact format may vary, but they often share the concept that edges can be annotated with additional information, such as validity time or provenance information. Property Graph is a popular graph database format that also provides this feature. We give a formalisation of a generalised notion of Property Graphs, called multi-attributed relational structures (MARS), and introduce a matching knowledge representation formalism, multi-attributed predicate logic (MAPL). We analyse the expressive power of MAPL and suggest a simpler, rule-based fragment of MAPL that can be used for ontological reasoning on Property Graphs. To the best of our knowledge, this is the first approach to making Property Graphs and related data structures accessible to symbolic AI.},
  isbn = {978-0-9992411-0-3},
  langid = {english},
  file = {C:\Users\nhiot\OneDrive\zotero\2017\Marx et al. - 2017 - Logic on MARS Ontologies for Generalised Property.pdf}
}

@incollection{maurelActesTALN20012001,
  title = {Actes de {{TALN}} 2001 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 2001 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Maurel, Denis},
  year = {2001},
  month = jul,
  publisher = {Universit{\'e} de Tours / ATALA},
  address = {Tours},
  keywords = {nosource}
}

@inproceedings{mausamOpenLanguageLearning2012,
  title = {Open {{Language Learning}} for {{Information Extraction}}},
  booktitle = {Proceedings of the 2012 {{Joint Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and {{Computational Natural Language Learning}}},
  author = {{Mausam} and Schmitz, Michael and Bart, Robert and Soderland, Stephen and Etzioni, Oren},
  year = {2012},
  series = {{{EMNLP-CoNLL}} '12},
  pages = {523--534},
  publisher = {Association for Computational Linguistics},
  address = {Stroudsburg, PA, USA},
  url = {http://dl.acm.org/citation.cfm?id=2390948.2391009},
  urldate = {2019-05-24},
  abstract = {Open Information Extraction (IE) systems extract relational tuples from text, without requiring a pre-specified vocabulary, by identifying relation phrases and associated arguments in arbitrary sentences. However, state-of-the-art Open IE systems such as ReVerb and woe share two important weaknesses -- (1) they extract only relations that are mediated by verbs, and (2) they ignore context, thus extracting tuples that are not asserted as factual. This paper presents ollie, a substantially improved Open IE system that addresses both these limitations. First, ollie achieves high yield by extracting relations mediated by nouns, adjectives, and more. Second, a context-analysis step increases precision by including contextual information from the sentence in the extractions. ollie obtains 2.7 times the area under precision-yield curve (AUC) compared to ReVerb and 1.9 times the AUC of woeparse.},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2012\Mausam et al. - 2012 - Open Language Learning for Information Extraction.pdf}
}

@inproceedings{mccloskySelfTrainingBiomedicalParsing2008,
  title = {Self-{{Training}} for {{Biomedical Parsing}}},
  booktitle = {Proceedings of {{ACL-08}}: {{HLT}}, {{Short Papers}}},
  author = {McClosky, David and Charniak, Eugene},
  editor = {Moore, Johanna D. and Teufel, Simone and Allan, James and Furui, Sadaoki},
  year = {2008},
  month = jun,
  pages = {101--104},
  publisher = {Association for Computational Linguistics},
  address = {Columbus, Ohio},
  url = {https://aclanthology.org/P08-2026},
  urldate = {2024-03-21},
  file = {C:\Users\nhiot\Zotero\storage\Y83PTDGH\McClosky et Charniak - 2008 - Self-Training for Biomedical Parsing.pdf}
}

@inproceedings{mcdonaldSimpleAlgorithmsComplex2005,
  ids = {mcdonaldSimpleAlgorithmsComplex2005a,mcdonaldSimpleAlgorithmsComplex2005b},
  title = {Simple Algorithms for Complex Relation Extraction with Applications to Biomedical {{IE}}},
  booktitle = {Proceedings of the 43rd {{Annual Meeting}} on {{Association}} for {{Computational Linguistics}}},
  author = {McDonald, Ryan and Pereira, Fernando and Kulick, Seth and Winters, Scott and Jin, Yang and White, Pete},
  year = {2005},
  month = jun,
  series = {{{ACL}} '05},
  pages = {491--498},
  publisher = {Association for Computational Linguistics},
  address = {USA},
  doi = {10/fk66zd},
  abstract = {A complex relation is any n-ary relation in which some of the arguments may be be unspecified. We present here a simple two-stage method for extracting complex relations between named entities in text. The first stage creates a graph from pairs of entities that are likely to be related, and the second stage scores maximal cliques in that graph as potential complex relation instances. We evaluate the new method against a standard baseline for extracting genomic variation relations from biomedical text.},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2005\\McDonald et al. - 2005 - Simple algorithms for complex relation extraction .pdf;C\:\\Users\\nhiot\\Zotero\\storage\\BTTKUGC6\\1219840.html}
}

@book{mctearConversationalInterface2016,
  title = {The Conversational Interface},
  author = {McTear, Michael Frederick and Callejas, Zoraida and Griol, David},
  year = {2016},
  volume = {6},
  publisher = {Springer},
  file = {C:\Users\nhiot\Zotero\storage\EDRGYVK3\10.html}
}

@inproceedings{mihalceaTextRankBringingOrder2004,
  ids = {mihalceaTextrankBringingOrder2004},
  title = {{{TextRank}}: {{Bringing Order}} into {{Text}}},
  shorttitle = {{{TextRank}}},
  booktitle = {Proceedings of the 2004 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Mihalcea, Rada and Tarau, Paul},
  editor = {Lin, Dekang and Wu, Dekai},
  year = {2004},
  month = jul,
  pages = {404--411},
  publisher = {Association for Computational Linguistics},
  address = {Barcelona, Spain},
  url = {https://aclanthology.org/W04-3252},
  urldate = {2024-03-22},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2004\\Mihalcea et Tarau - 2004 - Textrank Bringing order into text.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\NVUULF2E\\Mihalcea et Tarau - 2004 - TextRank Bringing Order into Text.pdf}
}

@inproceedings{mihovDirectConstructionMinimal2001,
  title = {Direct {{Construction}} of {{Minimal Acyclic Subsequential Transducers}}},
  booktitle = {Implementation and Application of Automata, 5th International Conference, {{CIAA}} 2000, London, Ontario, Canada, July 24-25, 2000, Revised Papers},
  author = {Mihov, Stoyan and Maurel, Denis},
  year = {2001},
  series = {Lecture Notes in Computer Science},
  volume = {2088},
  pages = {217--229},
  publisher = {Springer},
  doi = {10.1007/3-540-44674-5_18},
  abstract = {This paper presents an algorithm for direct building of minimal acyclic subsequential transducer, which represents a finite relation given as a sorted list of words with their outputs. The algorithm constructs the minimal transducer directly without constructing intermediate tree-like or pseudo-minimal transducers. In NLP applications our algorithm provides significantly better efficiency than the other algorithms building minimal transducer for large-scale natural language dictionaries. Some experimental comparisons are presented at the end of the paper.},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2001\\Mihov et Maurel - 2001 - Direct Construction of Minimal Acyclic Subsequenti.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\5XISYCIH\\summary.html}
}

@article{mikolovExploitingSimilaritiesLanguages2013,
  title = {Exploiting {{Similarities}} among {{Languages}} for {{Machine Translation}}},
  author = {Mikolov, Tomas and Le, Quoc V. and Sutskever, Ilya},
  year = {2013},
  month = sep,
  journal = {arXiv:1309.4168 [cs]},
  eprint = {1309.4168},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1309.4168},
  urldate = {2021-03-23},
  abstract = {Dictionaries and phrase tables are the basis of modern statistical machine translation systems. This paper develops a method that can automate the process of generating and extending dictionaries and phrase tables. Our method can translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It uses distributed representation of words and learns a linear mapping between vector spaces of languages. Despite its simplicity, our method is surprisingly effective: we can achieve almost 90\% precision@5 for translation of words between English and Spanish. This method makes little assumption about the languages, so it can be used to extend and refine dictionaries and translation tables for any language pairs.},
  archiveprefix = {arxiv},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2013\\Mikolov et al. - 2013 - Exploiting Similarities among Languages for Machin.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\GKYIC4Z9\\1309.html}
}

@inproceedings{minardDOINGDEFTCascade2020,
  ids = {minardDOINGDEFTCascade2020a},
  title = {{DOING@DEFT : cascade de CRF pour l'annotation d'entit{\'e}s cliniques imbriqu{\'e}es}},
  shorttitle = {{DOING@DEFT}},
  booktitle = {{Actes de la 6e conf{\'e}rence conjointe Journ{\'e}es d'{\'E}tudes sur la Parole (JEP, 33e {\'e}dition), Traitement Automatique des Langues Naturelles (TALN, 27e {\'e}dition), Rencontre des {\'E}tudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (R{\'E}CITAL, 22e {\'e}dition). Atelier D{\'E}fi Fouille de Textes}},
  author = {Minard, Anne-Lyse and Roques, Andr{\'e}ane and Hiot, Nicolas and {Halfeld-Ferrari}, Mirian and Savary, Agata},
  year = {2020},
  month = jun,
  pages = {66--78},
  publisher = {ATALA et AFCP},
  address = {Nancy, France},
  url = {https://hal.archives-ouvertes.fr/hal-02784743},
  urldate = {2023-08-07},
  abstract = {Cet article pr{\'e}sente le syst{\`e}me d{\'e}velopp{\'e} par l'{\'e}quipe DOING pour la campagne d'{\'e}valuation DEFT 2020 portant sur la similarit{\'e} s{\'e}mantique et l'extraction d'information fine. L'{\'e}quipe a particip{\'e} uniquement {\`a} la t{\^a}che 3 : ``extraction d'information''. Nous avons utilis{\'e} une cascade de CRF pour annoter les diff{\'e}rentes informations {\`a} rep{\'e}rer. Nous nous sommes concentr{\'e}s sur la question de l'imbrication des entit{\'e}s et de la pertinence d'un type d'entit{\'e} pour apprendre {\`a} reconna{\^i}tre un autre. Nous avons {\'e}galement test{\'e} l'utilisation d'une ressource externe, MedDRA, pour am{\'e}liorer les performances du syst{\`e}me et d'un pipeline plus complexe mais ne g{\'e}rant pas l'imbrication des entit{\'e}s. Nous avons soumis 3 runs et nous obtenons en moyenne sur toutes les classes des F-mesures de 0,64, 0,65 et 0,61.},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/taln/MinardRHAS20.bib},
  copyright = {All rights reserved},
  hal_id = {hal-02784743},
  hal_version = {v3},
  langid = {french},
  pdf = {https://hal.archives-ouvertes.fr/hal-02784743v3/file/212.pdf},
  keywords = {⛔ No DOI found,apprentissage automatique,cas cliniques,CRF.,entit{\'e}s cliniques,entit{\'e}s imbriqu{\'e}es,extraction d'information fine,me,nosource},
  timestamp = {Tue, 15 Dec 2020 17:40:18 +0100},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2020\\Minard et al. - 2020 - DOING@DEFT  cascade de CRF pour l'annotation d'en.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\DY6IJYRR\\hal-02784743.html}
}

@inproceedings{mintzDistantSupervisionRelation2009,
  title = {Distant Supervision for Relation Extraction without Labeled Data},
  booktitle = {Proceedings of the {{Joint Conference}} of the 47th {{Annual Meeting}} of the {{ACL}} and the 4th {{International Joint Conference}} on {{Natural Language Processing}} of the {{AFNLP}}: {{Volume}} 2 - {{ACL-IJCNLP}} '09},
  author = {Mintz, Mike and Bills, Steven and Snow, Rion and Jurafsky, Dan},
  year = {2009},
  volume = {2},
  pages = {1003},
  publisher = {Association for Computational Linguistics},
  address = {Suntec, Singapore},
  doi = {10.3115/1690219.1690287},
  urldate = {2021-02-16},
  abstract = {Modern models of relation extraction for tasks like ACE are based on supervised learning of relations from small hand-labeled corpora. We investigate an alternative paradigm that does not require labeled corpora, avoiding the domain dependence of ACEstyle algorithms, and allowing the use of corpora of any size. Our experiments use Freebase, a large semantic database of several thousand relations, to provide distant supervision. For each pair of entities that appears in some Freebase relation, we find all sentences containing those entities in a large unlabeled corpus and extract textual features to train a relation classifier. Our algorithm combines the advantages of supervised IE (combining 400,000 noisy pattern features in a probabilistic classifier) and unsupervised IE (extracting large numbers of relations from large corpora of any domain). Our model is able to extract 10,000 instances of 102 relations at a precision of 67.6\%. We also analyze feature performance, showing that syntactic parse features are particularly helpful for relations that are ambiguous or lexically distant in their expression.},
  isbn = {978-1-932432-46-6},
  langid = {english},
  file = {C:\Users\nhiot\OneDrive\zotero\2009\Mintz et al. - 2009 - Distant supervision for relation extraction withou.pdf}
}

@article{mishchenkoBlueprintConstraintsolvingApproach2022,
  ids = {mishchenkoBlueprintConstraintsolvingApproach2022a},
  title = {Blueprint: A Constraint-Solving Approach for Document Extraction},
  shorttitle = {Blueprint},
  author = {Mishchenko, Andrey and Danco, Dominique and Jindal, Abhilash and Blue, Adrian},
  year = {2022},
  month = sep,
  journal = {Proceedings of the VLDB Endowment},
  volume = {15},
  number = {12},
  pages = {3459--3471},
  publisher = {VLDB Endowment},
  issn = {2150-8097},
  doi = {10.14778/3554821.3554836},
  abstract = {Blueprint is a declarative domain-specific language for document extraction. Users describe document layout using spatial, textual, semantic, and numerical fuzzy constraints, and the language runtime extracts the field-value mappings that best satisfy the constraints in a given document. We used Blueprint to develop several document extraction solutions in a commercial setting. This approach to the extraction problem proved powerful. Concise Blueprint programs were able to generate good accuracy on a broad set of use cases. However, a major goal of our work was to build a system that non-experts, and in particular non-engineers, could use effectively, and we found that writing declarative fuzzy constraint-based extraction programs was not intuitive for many users: a large up-front learning investment was required to be effective, and debugging was often challenging. To address these issues, we developed a no-code IDE for Blueprint, called Studio, as well as program synthesis functionality for automatically generating Blueprint programs from training data, which could be created by labeling document samples in our IDE. Overall, the IDE significantly improved the Blueprint development experience and the results users were able to achieve. In this paper, we discuss the design, implementation, and deployment of Blueprint and Studio. We compare our system with a state-of-the-art deep-learning based extraction tool and show that our system can achieve comparable accuracy results, with comparable development time, for appropriately-chosen use cases, while providing better interpretability and debuggability.},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2022\\Mishchenko et al. - 2022 - Blueprint a constraint-solving approach for docum.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\JYYX6VTL\\3554821.html}
}

@article{mittalDescribingComplexCharts1998,
  ids = {mittalDescribingComplexCharts},
  title = {Describing {{Complex Charts}} in {{Natural Language}}: {{A Caption Generation System}}},
  shorttitle = {Describing {{Complex Charts}} in {{Natural Language}}},
  author = {Mittal, Vibhu O. and Moore, Johanna D. and Carenini, Giuseppe and Roth, Steven},
  year = {1998},
  journal = {Computational Linguistics},
  volume = {24},
  number = {3},
  pages = {431--467},
  publisher = {MIT Press},
  address = {Cambridge, MA},
  url = {https://aclanthology.org/J98-3004},
  urldate = {2023-08-07},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\1998\Mittal et al. - 1998 - Describing Complex Charts in Natural Language A C.pdf}
}

@article{mohanARIESTransactionRecovery1992,
  title = {{{ARIES}}: {{A Transaction Recovery Method Supporting Fine-granularity Locking}} and {{Partial Rollbacks Using Write-ahead Logging}}},
  shorttitle = {{{ARIES}}},
  author = {Mohan, C. and Haderle, Don and Lindsay, Bruce and Pirahesh, Hamid and Schwarz, Peter},
  year = {1992},
  month = mar,
  journal = {ACM Trans. Database Syst.},
  volume = {17},
  number = {1},
  pages = {94--162},
  issn = {0362-5915},
  doi = {10/d8bxvt},
  urldate = {2019-01-17},
  abstract = {DB2TM, IMS, and TandemTM systems. ARIES is applicable not only to database management systems but also to persistent object-oriented languages, recoverable file systems and transaction-based operating systems. ARIES has been implemented, to varying degrees, in IBM's OS/2TM Extended Edition Database Manager, DB2, Workstation Data Save Facility/VM, Starburst and QuickSilver, and in the University of Wisconsin's EXODUS and Gamma database machine.},
  keywords = {buffer management,latching,locking,space management,write-ahead logging},
  annotation = {00000},
  file = {C:\Users\nhiot\OneDrive\zotero\1992\Mohan et al. - 1992 - ARIES A Transaction Recovery Method Supporting Fi.pdf}
}

@inproceedings{molzbergerGraphBasedRuleMiningFramework2005a,
  ids = {molzbergerGraphBasedRuleMiningFramework2005},
  title = {A {{Graph-Based Rule-Mining Framework}} for {{Natural Language Learning}} and {{Understanding}}},
  booktitle = {{{LWA}}},
  author = {Molzberger, Lukas},
  year = {2005},
  pages = {202--209},
  publisher = {Citeseer},
  abstract = {Learning and understanding natural languages are usually considered as independent tasks in natural language processing. These two tasks, however, are strongly interrelated and are presumably unsolvable as separate problems. In this paper, we present an algorithm called Frequent Rule Graph Miner (FRGM) that tackles these problems by alternately improving on the language model and the example interpretations. FRGM is based on an effective graph-mining algorithm adapted for enumerating frequent rulegraphs and is applicable to different layers of natural language processing such as morphology, syntax, semantics and pragmatics.},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {00000},
  file = {C:\Users\nhiot\OneDrive\zotero\2005\Molzberger - 2005 - A Graph-Based Rule-Mining Framework for Natural La.pdf}
}

@article{montenegroSurveyConversationalAgents2019,
  ids = {montenegroSurveyConversationalAgents2019a},
  title = {Survey of Conversational Agents in Health},
  author = {Montenegro, Joao Luis Zeni and {da Costa}, Cristiano Andr{\'e} and {da Rosa Righi}, Rodrigo},
  year = {2019},
  month = sep,
  journal = {Expert Systems with Applications},
  volume = {129},
  pages = {56--67},
  issn = {0957-4174},
  doi = {10/gf2w39},
  urldate = {2021-03-11},
  abstract = {Artificial intelligence (AI) has transformed the world and the relationships among humans as the learning capabilities of machines have allowed for a new means of communication between humans and machines. In the field of health, there is much interest in new technologies that help to improve and automate services in hospitals. This article aims to explore the literature related to conversational agents applied to health care, searching for definitions, patterns, methods, architectures, and data types. Furthermore, this work identifies an agent application taxonomy, current challenges, and research gaps. In this work, we use a systematic literature review approach. We guide and refine this study and the research questions by applying Population, Intervention, Comparison, Outcome, and Context (PICOC) criteria. The present study investigated approximately 4145 articles involving conversational agents in health published over the last ten years. In this context, we finally selected 40 articles based on their approaches and objectives as related to our main subject. As a result, we developed a taxonomy, identified the main challenges in the field, and defined the main types of dialog and contexts related to conversational agents in health. These results contributed to discussions regarding conversational health agents, and highlighted some research gaps for future study.},
  langid = {english},
  keywords = {Chatbot,Conversational agents,Expert systems,Health,Systematic review},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2019\\Montenegro et al. - 2019 - Survey of conversational agents in health.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\TFQDH8SM\\S0957417419302283.html}
}

@inproceedings{mooneySubsequenceKernelsRelation2005,
  ids = {bunescuSubsequenceKernelsRelation2005},
  title = {Subsequence Kernels for Relation Extraction},
  booktitle = {Proceedings of the 18th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Mooney, Raymond and Bunescu, Razvan},
  year = {2005},
  month = dec,
  series = {{{NIPS}}'05},
  volume = {18},
  pages = {171--178},
  publisher = {MIT Press},
  address = {Cambridge, MA, USA},
  abstract = {We present a new kernel method for extracting semantic relations between entities in natural language text, based on a generalization of subsequence kernels. This kernel uses three types of subsequence patterns that are typically employed in natural language to assert relationships between two entities. Experiments on extracting protein interactions from biomedical corpora and top-level relations from newspaper corpora demonstrate the advantages of this approach.},
  file = {C:\Users\nhiot\OneDrive\zotero\2005\Mooney et Bunescu - 2005 - Subsequence kernels for relation extraction.pdf}
}

@inproceedings{moranteLearningScopeNegation2008,
  title = {Learning the Scope of Negation in Biomedical Texts},
  booktitle = {Proceedings of the {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} - {{EMNLP}} '08},
  author = {Morante, Roser and Liekens, Anthony and Daelemans, Walter},
  year = {2008},
  pages = {715},
  publisher = {Association for Computational Linguistics},
  address = {Honolulu, Hawaii},
  doi = {10.3115/1613715.1613805},
  urldate = {2023-10-06},
  abstract = {In this paper we present a machine learning system that finds the scope of negation in biomedical texts. The system consists of two memory-based engines, one that decides if the tokens in a sentence are negation signals, and another that finds the full scope of these negation signals. Our approach to negation detection differs in two main aspects from existing research on negation. First, we focus on finding the scope of negation signals, instead of determining whether a term is negated or not. Second, we apply supervised machine learning techniques, whereas most existing systems apply rule-based algorithms. As far as we know, this way of approaching the negation scope finding task is novel.},
  langid = {english},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2008\\Morante et al. - 2008 - Learning the scope of negation in biomedical texts.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\Z6Q4NUMF\\learning-the-scope-of-negation-Q3PJj6.html}
}

@techreport{moret-baillyMiseJourCoherente2019,
  type = {{Rapport de Stage}},
  title = {{Mise {\`a} jour coh{\'e}rente des bases de donn{\'e}es avec des valeurs nulles marqu{\'e}es}},
  author = {{Moret-Bailly}, Lucas},
  year = {2019-04-08/2019-06-14},
  address = {Orl{\'e}ans},
  institution = {Laboratoire d'Informatique Fondamentale d'Orl{\'e}ans (LIFO)},
  collaborator = {{Halfeld-Ferrari}, Mirian and Chabin, Jacques},
  langid = {french},
  file = {C:\Users\nhiot\OneDrive\zotero\2019\Moret-Bailly - 2019 - Mise à jour cohérente des bases de données avec de.pdf}
}

@article{moroEntityLinkingMeets2014,
  title = {Entity {{Linking}} Meets {{Word Sense Disambiguation}}: A {{Unified Approach}}},
  shorttitle = {Entity {{Linking}} Meets {{Word Sense Disambiguation}}},
  author = {Moro, Andrea and Raganato, Alessandro and Navigli, Roberto},
  year = {2014},
  month = dec,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {2},
  pages = {231--244},
  issn = {2307-387X},
  doi = {10/gf5qpg},
  urldate = {2020-01-21},
  abstract = {Entity Linking (EL) and Word Sense Disambiguation (WSD) both address the lexical ambiguity of language. But while the two tasks are pretty similar, they differ in a fundamental respect: in EL the textual mention can be linked to a named entity which may or may not contain the exact mention, while in WSD there is a perfect match between the word form (better, its lemma) and a suitable word sense.},
  langid = {english},
  annotation = {QID: Q106526339},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2014\\Moro et al. - 2014 - Entity Linking meets Word Sense Disambiguation a .pdf;C\:\\Users\\nhiot\\Zotero\\storage\\D9LBQG5B\\tacl_a_00179.html}
}

@inproceedings{moschittiMakingTreeKernels2006,
  title = {Making {{Tree Kernels Practical}} for {{Natural Language Learning}}},
  booktitle = {11th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Moschitti, Alessandro},
  year = {2006},
  month = apr,
  publisher = {Association for Computational Linguistics},
  address = {Trento, Italy},
  url = {https://www.aclweb.org/anthology/E06-1015},
  urldate = {2020-11-26},
  file = {C:\Users\nhiot\OneDrive\zotero\2006\Moschitti - 2006 - Making Tree Kernels Practical for Natural Language.pdf}
}

@article{murffAutomatedIdentificationPostoperative2011,
  ids = {murffAutomatedIdentificationPostoperative2011a},
  title = {Automated {{Identification}} of {{Postoperative Complications Within}} an {{Electronic Medical Record Using Natural Language Processing}}},
  author = {Murff, Harvey J. and FitzHenry, Fern and Matheny, Michael E. and Gentry, Nancy and Kotter, Kristen L. and Crimin, Kimberly and Dittus, Robert S. and Rosen, Amy K. and Elkin, Peter L. and Brown, Steven H. and Speroff, Theodore},
  year = {2011},
  month = aug,
  journal = {JAMA},
  volume = {306},
  number = {8},
  pages = {848--855},
  publisher = {American Medical Association},
  issn = {0098-7484},
  doi = {10/bcrnsr},
  urldate = {2020-05-10},
  abstract = {{$<$}h3{$>$}Context{$<$}/h3{$>$}Currently most automated methods to identify patient safety occurrences rely on administrative data codes; however, free-text searches of electronic medical records could represent an additional surveillance approach.{$<$}h3{$>$}Objective{$<$}/h3{$>$}To evaluate a natural language processing search--approach to identify postoperative surgical complications within a comprehensive electronic medical record.{$<$}h3{$>$}Design, Setting, and Patients{$<$}/h3{$>$}Cross-sectional study involving 2974 patients undergoing inpatient surgical procedures at 6 Veterans Health Administration (VHA) medical centers from 1999 to 2006.{$<$}h3{$>$}Main Outcome Measures{$<$}/h3{$>$}Postoperative occurrences of acute renal failure requiring dialysis, deep vein thrombosis, pulmonary embolism, sepsis, pneumonia, or myocardial infarction identified through medical record review as part of the VA Surgical Quality Improvement Program. We determined the sensitivity and specificity of the natural language processing approach to identify these complications and compared its performance with patient safety indicators that use discharge coding information.{$<$}h3{$>$}Results{$<$}/h3{$>$}The proportion of postoperative events for each sample was 2\% (39 of 1924) for acute renal failure requiring dialysis, 0.7\% (18 of 2327) for pulmonary embolism, 1\% (29 of 2327) for deep vein thrombosis, 7\% (61 of 866) for sepsis, 16\% (222 of 1405) for pneumonia, and 2\% (35 of 1822) for myocardial infarction. Natural language processing correctly identified 82\% (95\% confidence interval [CI], 67\%-91\%) of acute renal failure cases compared with 38\% (95\% CI, 25\%-54\%) for patient safety indicators. Similar results were obtained for venous thromboembolism (59\%, 95\% CI, 44\%-72\% vs 46\%, 95\% CI, 32\%-60\%), pneumonia (64\%, 95\% CI, 58\%-70\% vs 5\%, 95\% CI, 3\%-9\%), sepsis (89\%, 95\% CI, 78\%-94\% vs 34\%, 95\% CI, 24\%-47\%), and postoperative myocardial infarction (91\%, 95\% CI, 78\%-97\%) vs 89\%, 95\% CI, 74\%-96\%). Both natural language processing and patient safety indicators were highly specific for these diagnoses.{$<$}h3{$>$}Conclusion{$<$}/h3{$>$}Among patients undergoing inpatient surgical procedures at VA medical centers, natural language processing analysis of electronic medical records to identify postoperative complications had higher sensitivity and lower specificity compared with patient safety indicators based on discharge coding.},
  langid = {english},
  annotation = {QID: Q37921539},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2011\\Murff et al. - 2011 - Automated Identification of Postoperative Complica.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\JG85SDTG\\1108490.html}
}

@inproceedings{nakamura-delloyeExtractionRelationsPatrons2011,
  title = {Extraction de Relations et de Patrons de Relations Entre Entit{\'e}s Nomm{\'e}es En Vue de l'enrichissement d'une Ontologie},
  booktitle = {{{TOTh}} 2011 : {{Terminologie}} \& {{Ontologie}} : {{Th{\'e}ories}} et {{Applications}}},
  author = {{Nakamura-Delloye}, Yayoi and Stern, Rosa},
  year = {2011},
  month = may,
  pages = {50},
  address = {Annecy, France},
  url = {https://hal.archives-ouvertes.fr/hal-00601801},
  urldate = {2019-06-05},
  abstract = {Nous proposons dans cet article une m{\'e}thode non-supervis{\'e}e d'extraction des relations et des patrons de relations entre entit{\'e}s nomm{\'e}es, r{\'e}alis{\'e}e dans le cadre de la cr{\'e}ation et l'enrichissement d'une ontologie. La m{\'e}thode propos{\'e}e se caract{\'e}rise par l'exploitation des r{\'e}sultats d'analyse syntaxique, notamment les chemins syntaxiques reliant deux entit{\'e}s nomm{\'e}es dans les arbres de d{\'e}pendance. Les informations sur les relations syntaxiques pr{\'e}sentes entre les composants sont mises {\`a} profit pour le calcul de la similarit{\'e} employ{\'e}e pour la phase principale de classification. Nous pr{\'e}sentons {\'e}galement le m{\'e}canisme con{\c c}u pour l'int{\'e}gration des r{\'e}sultats obtenus dans une ontologie.},
  annotation = {00005},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2011\\Nakamura-Delloye et Stern - 2011 - Extraction de relations et de patrons de relations.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\3QBINZJH\\hal-00601801.html}
}

@article{nambiarAnalysisSeriousAdverse2018,
  title = {Analysis of Serious Adverse Event: {{Writing}} a Narrative},
  shorttitle = {Analysis of Serious Adverse Event},
  author = {Nambiar, Indu},
  year = {2018},
  journal = {Perspectives in Clinical Research},
  volume = {9},
  number = {2},
  pages = {103--106},
  issn = {2229-3485},
  doi = {10.4103/picr.PICR_52_18},
  urldate = {2023-08-07},
  abstract = {One of the reasons that a research molecule undergoes through different phases of clinical trials is to establish the safety and efficacy of the drug. To establish the safety profile of any drug, the most important aspect is to evaluate every adverse event (AE) that a clinical trial individual experiences. This detail is provided by any investigator or sponsor through the narratives that is prepared post the analysis of a serious AE. The purpose of this article is to emphasize the importance of these narratives.},
  pmcid = {PMC5950607},
  pmid = {29862205},
  file = {C:\Users\nhiot\Zotero\storage\HCPVBUWV\Nambiar - 2018 - Analysis of serious adverse event Writing a narra.pdf}
}

@article{narechaniaNL4DVToolkitGenerating2020,
  title = {{{NL4DV}}: {{A Toolkit}} for {{Generating Analytic Specifications}} for {{Data Visualization}} from {{Natural Language Queries}}},
  shorttitle = {{{NL4DV}}},
  author = {Narechania, Arpit and Srinivasan, Arjun and Stasko, John},
  year = {2020},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  publisher = {IEEE},
  keywords = {⛔ No DOI found},
  annotation = {QID: Q100529390},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2020\\Narechania et al. - 2020 - NL4DV A Toolkit for Generating Analytic Specifica.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\DPF3QKD7\\9222342.html}
}

@article{nasarNamedEntityRecognition2021,
  title = {Named {{Entity Recognition}} and {{Relation Extraction}}: {{State-of-the-Art}}},
  shorttitle = {Named {{Entity Recognition}} and {{Relation Extraction}}},
  author = {Nasar, Zara and Jaffry, Syed Waqar and Malik, Muhammad Kamran},
  year = {2021},
  month = feb,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {1},
  pages = {1--39},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3445965},
  urldate = {2024-03-20},
  abstract = {With the advent of Web 2.0, there exist many online platforms that result in massive textual-data production. With ever-increasing textual data at hand, it is of immense importance to extract information nuggets from this data. One approach towards effective harnessing of this unstructured textual data could be its transformation into structured text. Hence, this study aims to present an overview of approaches that can be applied to extract key insights from textual data in a structured way. For this, Named Entity Recognition and Relation Extraction are being majorly addressed in this review study. The former deals with identification of named entities, and the latter deals with problem of extracting relation between set of entities. This study covers early approaches as well as the developments made up till now using machine learning models. Survey findings conclude that deep-learning-based hybrid and joint models are currently governing the state-of-the-art. It is also observed that annotated benchmark datasets for various textual-data generators such as Twitter and other social forums are not available. This scarcity of dataset has resulted into relatively less progress in these domains. Additionally, the majority of the state-of-the-art techniques are offline and computationally expensive. Last, with increasing focus on deep-learning frameworks, there is need to understand and explain the under-going processes in deep architectures.},
  langid = {english},
  keywords = {deep learning,Information extraction,joint modeling,named entity recognition,relation extraction},
  file = {C:\Users\nhiot\OneDrive\zotero\2022\Nasar et al. - 2022 - Named Entity Recognition and Relation Extraction .pdf}
}

@article{NaturalLanguageProcessing2019,
  title = {Natural Language Processing},
  year = {2019},
  month = jan,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Natural\_language\_processing\&oldid=879609467},
  urldate = {2019-01-23},
  abstract = {Natural language processing (NLP) is a subfield of computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data. Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {00000  Page Version ID: 879609467},
  file = {C:\Users\nhiot\Zotero\storage\W8S235WZ\index.html}
}

@book{naudeDesignGuidelinesPublic2005,
  title = {Design Guidelines for Public Transport Facilities},
  author = {Naude, Stef and Jones, John and Louw, P.},
  year = {2005},
  month = jul,
  publisher = {SATC},
  url = {https://repository.up.ac.za/handle/2263/6332},
  urldate = {2019-01-16},
  abstract = {Paper presented at the 24th Annual Southern African Transport Conference 11 - 13 July 2005 "Transport challenges for 2010", CSIR International Convention Centre, Pretoria, South Africa.},
  copyright = {University of Pretoria},
  isbn = {978-1-920017-12-5},
  langid = {english},
  annotation = {00000},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2005\\Naude et al. - 2005 - Design guidelines for public transport facilities.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\NVC2GFVG\\6332.html}
}

@article{navigliBabelNetAutomaticConstruction2012,
  title = {{{BabelNet}}: {{The}} Automatic Construction, Evaluation and Application of a Wide-Coverage Multilingual Semantic Network},
  shorttitle = {{{BabelNet}}},
  author = {Navigli, Roberto and Ponzetto, Simone Paolo},
  year = {2012},
  month = dec,
  journal = {Artificial Intelligence},
  volume = {193},
  pages = {217--250},
  issn = {0004-3702},
  doi = {10/f4gt4q},
  urldate = {2020-01-21},
  abstract = {We present an automatic approach to the construction of BabelNet, a very large, wide-coverage multilingual semantic network. Key to our approach is the integration of lexicographic and encyclopedic knowledge from WordNet and Wikipedia. In addition, Machine Translation is applied to enrich the resource with lexical information for all languages. We first conduct in vitro experiments on new and existing gold-standard datasets to show the high quality and coverage of BabelNet. We then show that our lexical resource can be used successfully to perform both monolingual and cross-lingual Word Sense Disambiguation: thanks to its wide lexical coverage and novel semantic relations, we are able to achieve state-of the-art results on three different SemEval evaluation tasks.},
  langid = {english},
  keywords = {Graph algorithms,Knowledge acquisition,Semantic networks,Word sense disambiguation},
  annotation = {QID: Q36485657},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2012\\Navigli et Ponzetto - 2012 - BabelNet The automatic construction, evaluation a.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\NKTE32DY\\S0004370212000793.html}
}

@article{navigliWordSenseDisambiguation2009,
  title = {Word {{Sense Disambiguation}}: {{A Survey}}},
  shorttitle = {Word {{Sense Disambiguation}}},
  author = {Navigli, Roberto},
  year = {2009},
  month = feb,
  journal = {ACM computing surveys (CSUR)},
  volume = {41},
  number = {2},
  pages = {1--69},
  issn = {0360-0300},
  doi = {10/dhsjt2},
  urldate = {2019-06-20},
  abstract = {Word sense disambiguation (WSD) is the ability to identify the meaning of words in context in a computational manner. WSD is considered an AI-complete problem, that is, a task whose solution is at least as hard as the most difficult problems in artificial intelligence. We introduce the reader to the motivations for solving the ambiguity of words and provide a description of the task. We overview supervised, unsupervised, and knowledge-based approaches. The assessment of WSD systems is discussed in the context of the Senseval/Semeval campaigns, aiming at the objective evaluation of systems participating in several different disambiguation tasks. Finally, applications, open problems, and future directions are discussed.},
  langid = {english},
  keywords = {lexical ambiguity,lexical semantics,semantic annotation,sense annotation,Word sense disambiguation,word sense discrimination,WSD},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2009\\Navigli - 2009 - Word Sense Disambiguation A Survey.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\YSBAZJAG\\citation.html}
}

@incollection{nazarenkoActesTALN20092009,
  title = {Actes de {{TALN}} 2009 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 2009 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Nazarenko, Adeline and Poibeau, Thierry},
  year = {2009},
  month = jun,
  publisher = {LIPN / ATALA},
  address = {Senlis},
  keywords = {nosource}
}

@book{needhamGraphAlgorithmsPractical2019,
  title = {Graph {{Algorithms}}: {{Practical Examples}} in {{Apache Spark}} and {{Neo4j}}},
  shorttitle = {Graph {{Algorithms}}},
  author = {Needham, Mark and Hodler, Amy E.},
  year = {2019},
  month = may,
  publisher = {"O'Reilly Media, Inc."},
  abstract = {Discover how graph algorithms can help you leverage the relationships within your data to develop more intelligent solutions and enhance your machine learning models. You'll learn how graph analytics are uniquely suited to unfold complex structures and reveal difficult-to-find patterns lurking in your data. Whether you are trying to build dynamic network models or forecast real-world behavior, this book illustrates how graph algorithms deliver value---from finding vulnerabilities and bottlenecks to detecting communities and improving machine learning predictions.This practical book walks you through hands-on examples of how to use graph algorithms in Apache Spark and Neo4j---two of the most common choices for graph analytics. Also included: sample code and tips for over 20 practical graph algorithms that cover optimal pathfinding, importance through centrality, and community detection.Learn how graph analytics vary from conventional statistical analysisUnderstand how classic graph algorithms work, and how they are appliedGet guidance on which algorithms to use for different types of questionsExplore algorithm examples with working code and sample datasets from Spark and Neo4jSee how connected feature extraction can increase machine learning accuracy and precisionWalk through creating an ML workflow for link prediction combining Neo4j and Spark},
  googlebooks = {yYWZDwAAQBAJ},
  isbn = {978-1-4920-4765-0},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / General,Computers / Computer Graphics,Computers / Intelligence (AI) \& Semantics,Computers / Mathematical \& Statistical Software,Computers / Programming / Algorithms,Computers / Software Development \& Engineering / Computer Graphics,Mathematics / Graphic Methods},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2019\\Needham et Hodler - 2019 - Graph Algorithms Practical Examples in Apache Spa.epub;C\:\\Users\\nhiot\\OneDrive\\zotero\\2019\\Needham et Hodler - 2019 - Graph Algorithms Practical Examples in Apache Spa2.pdf}
}

@misc{needhamLinkPredictionNeo4j2019,
  title = {Link {{Prediction}} with {{Neo4j Part}} 2: {{Predicting}} Co-Authors Using Scikit-Learn},
  shorttitle = {Link {{Prediction}} with {{Neo4j Part}} 2},
  author = {Needham, Mark},
  year = {2019},
  month = mar,
  journal = {Medium},
  url = {https://towardsdatascience.com/link-prediction-with-neo4j-part-2-predicting-co-authors-using-scikit-learn-78b42356b44c},
  urldate = {2020-03-11},
  abstract = {This is the 2nd in series of posts on the link prediction functions that were recently added to the Neo4j Graph Algorithms Library.},
  langid = {english},
  file = {C:\Users\nhiot\Zotero\storage\7L22GWHX\Needham - 2019 - Link Prediction with Neo4j Part 2 Predicting co-a.html}
}

@misc{Neo4jGraphAlgorithms,
  title = {The {{Neo4j Graph Algorithms User Guide}} v3.5},
  url = {https://neo4j.com/docs/graph-algorithms/current/},
  urldate = {2020-03-11},
  file = {C:\Users\nhiot\Zotero\storage\AHJ6KRYF\current.html}
}

@misc{neo4jNaturalLanguageProcessing2016,
  title = {Natural {{Language Processing}} with {{Graphs}}},
  author = {{Neo4j}},
  year = {2016},
  url = {https://www.youtube.com/watch?v=BVMx24dtko0},
  urldate = {2021-02-16},
  abstract = {William Lyon, Developer Relations Enginner, Neo4j:During this webinar, we'll provide an overview of graph databases, followed by a survey of the role for graph databases in natural language processing tasks, including: modeling text as a graph, mining word associations from a text corpus using a graph data model, and mining opinions from a corpus of product reviews. We'll conclude with a demonstration of how graphs can enable content recommendation based on keyword extraction.},
  keywords = {nosource}
}

@inproceedings{neumannScispaCyFastRobust2019,
  title = {{{ScispaCy}}: {{Fast}} and {{Robust Models}} for {{Biomedical Natural Language Processing}}},
  shorttitle = {{{ScispaCy}}},
  booktitle = {Proceedings of the 18th {{BioNLP Workshop}} and {{Shared Task}}},
  author = {Neumann, Mark and King, Daniel and Beltagy, Iz and Ammar, Waleed},
  editor = {{Demner-Fushman}, Dina and Cohen, Kevin Bretonnel and Ananiadou, Sophia and Tsujii, Junichi},
  year = {2019},
  month = aug,
  eprint = {1902.07669},
  primaryclass = {cs},
  pages = {319--327},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/W19-5034},
  urldate = {2024-03-21},
  abstract = {Despite recent advances in natural language processing, many statistical models for processing text perform extremely poorly under domain shift. Processing biomedical and clinical text is a critically important application area of natural language processing, for which there are few robust, practical, publicly available models. This paper describes scispaCy, a new Python library and models for practical biomedical/scientific text processing, which heavily leverages the spaCy library. We detail the performance of two packages of models released in scispaCy and demonstrate their robustness on several tasks and datasets. Models and code are available at https://allenai.github.io/scispacy/.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  annotation = {QID: Q101248419},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2019\\Neumann et al. - 2019 - ScispaCy Fast and Robust Models for Biomedical Na4.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\D6RZYE87\\1902.html}
}

@book{newmanBuildingMicroservicesDesigning2015,
  title = {Building {{Microservices}}: {{Designing Fine-Grained Systems}}},
  shorttitle = {Building {{Microservices}}},
  author = {Newman, Sam},
  year = {2015},
  month = feb,
  publisher = {"O'Reilly Media, Inc."},
  url = {http://shop.oreilly.com/product/0636920033158.do},
  abstract = {Distributed systems have become more fine-grained in the past 10 years, shifting from code-heavy monolithic applications to smaller, self-contained microservices. But developing these systems brings its own set of headaches. With lots of examples and practical advice, this book takes a holistic view of the topics that system architects and administrators must consider when building, managing, and evolving microservice architectures.Microservice technologies are moving quickly. Author Sam Newman provides you with a firm grounding in the concepts while diving into current solutions for modeling, integrating, testing, deploying, and monitoring your own autonomous services. You'll follow a fictional company throughout the book to learn how building a microservice architecture affects a single domain.Discover how microservices allow you to align your system design with your organization's goalsLearn options for integrating a service with the rest of your systemTake an incremental approach when splitting monolithic codebasesDeploy individual microservices through continuous integrationExamine the complexities of testing and monitoring distributed servicesManage security with user-to-service and service-to-service modelsUnderstand the challenges of scaling microservice architectures},
  googlebooks = {jjl4BgAAQBAJ},
  isbn = {978-1-4919-5033-3},
  langid = {english},
  keywords = {Computers / Enterprise Applications / General,Computers / Programming / General,Computers / Software Development \& Engineering / General,Computers / Systems Architecture / Distributed Systems \& Computing,Computers / Systems Architecture / General,Computers / Web / Web Programming},
  annotation = {00000},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2015\\Newman - 2015 - Building Microservices Designing Fine-Grained Sys.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\3UFIWYB6\\0636920033158.html}
}

@article{nguyen-ducMEDTEXTransferringExplaining2020,
  title = {{{MED-TEX}}: {{Transferring}} and {{Explaining Knowledge}} with {{Less Data}} from {{Pretrained Medical Imaging Models}}},
  shorttitle = {{{MED-TEX}}},
  author = {{Nguyen-Duc}, Thanh and Zhao, He and Cai, Jianfei and Phung, Dinh},
  year = {2020},
  month = aug,
  journal = {arXiv:2008.02593 [cs, eess]},
  volume = {2008},
  eprint = {2008.02593},
  primaryclass = {cs, eess},
  url = {http://arxiv.org/abs/2008.02593},
  urldate = {2020-11-20},
  abstract = {Deep neural network based image classification methods usually require a large amount of training data and lack interpretability, which are critical in the medical imaging domain. In this paper, we develop a novel knowledge distillation and model interpretation framework for medical image classification that jointly solves the above two issues. Specifically, to address the data-hungry issue, we propose to learn a small student model with less data by distilling knowledge only from a cumbersome pretrained teacher model. To interpret the teacher model as well as assisting the learning of the student, an explainer module is introduced to highlight the regions of an input medical image that are important for the predictions of the teacher model. Furthermore, the joint framework is trained by a principled way derived from the information-theoretic perspective. Our framework performance is demonstrated by the comprehensive experiments on the knowledge distillation and model interpretation tasks compared to state-of-the-art methods on a fundus disease dataset.},
  archiveprefix = {arxiv},
  keywords = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2020\\Nguyen-Duc et al. - 2020 - MED-TEX Transferring and Explaining Knowledge wit.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\RGEDLDKD\\2008.html}
}

@phdthesis{nguyenQueryingWebData2018,
  title = {Querying the {{Web}} of {{Data}} Guaranteeing Valid Answers with Respect to given Criteria},
  author = {Nguyen, Thanh Binh},
  year = {2018},
  month = dec,
  address = {Orl{\'e}ans},
  collaborator = {{Halfeld-Ferrari}, Mirian and Markhoff, B{\'e}atrice and Chabin, Jacques},
  langid = {english},
  school = {Universit{\'e} d'Orl{\'e}ans et de Tours},
  keywords = {nosource}
}

@inproceedings{niaziLeaderElectionUsing2015,
  title = {Leader {{Election Using NewSQL Database Systems}}},
  booktitle = {Distributed {{Applications}} and {{Interoperable Systems}}},
  author = {Niazi, Salman and Ismail, Mahmoud and Berthou, Gautier and Dowling, Jim},
  editor = {Bessani, Alysson and Bouchenak, Sara},
  year = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {158--172},
  publisher = {Springer International Publishing},
  abstract = {Leader election protocols are a fundamental building block for replicated distributed services. They ease the design of leader-based coordination protocols that tolerate failures. In partially synchronous systems, designing a leader election algorithm, that does not permit multiple leaders while the system is unstable, is a complex task. As a result many production systems use third-party distributed coordination services, such as ZooKeeper and Chubby, to provide a reliable leader election service. However, adding a third-party service such as ZooKeeper to a distributed system incurs additional operational costs and complexity. ZooKeeper instances must be kept running on at least three machines to ensure its high availability. In this paper, we present a novel leader election protocol using NewSQL databases for partially synchronous systems, that ensures at most one leader at any given time. The leader election protocol uses the database as distributed shared memory. Our work enables distributed systems that already use NewSQL databases to save the operational overhead of managing an additional third-party service for leader election. Our main contribution is the design, implementation and validation of a practical leader election algorithm, based on NewSQL databases, that has performance comparable to a leader election implementation using a state-of-the-art distributed coordination service, ZooKeeper.},
  isbn = {978-3-319-19129-4},
  langid = {english},
  keywords = {Clock Drift,Leader Election,Leader Process,Shared Memory,Synchronous System},
  annotation = {00000},
  file = {C:\Users\nhiot\OneDrive\zotero\2015\Niazi et al. - 2015 - Leader Election Using NewSQL Database Systems.pdf}
}

@article{niekraszOntologyBasedDiscourseUnderstanding2005,
  title = {Ontology-{{Based Discourse Understanding}} for a {{Persistent Meeting Assistant}}},
  author = {Niekrasz, John and Purver, Matthew and Dowding, John and Peters, Stanley},
  year = {2005},
  pages = {8},
  abstract = {In this paper, we present research toward ontology-based understanding of discourse in meetings and describe an ontology of multimodal discourse designed for this purpose. We investigate its application in an integrated but modular architecture which uses semantically annotated knowledge of communicative meeting activity as well as discourse subject matter. We highlight how this approach assists in improving system performance over time and supports understanding in a changing and persistent environment. We also describe current and future plans for ontology-driven robust naturallanguage understanding in the presence of the highly ambiguous and errorful input typical of the meeting domain.},
  langid = {english},
  keywords = {⛔ No DOI found},
  annotation = {00000},
  file = {C:\Users\nhiot\OneDrive\zotero\2005\Niekrasz et al. - 2005 - Ontology-Based Discourse Understanding for a Persi.pdf}
}

@article{nielsenNetworkDesignPublic2007,
  title = {Network Design for Public Transport Success: Theory and Examples},
  shorttitle = {Network Design for Public Transport Success},
  author = {Nielsen, G. and Lange, T.},
  year = {2007},
  month = aug,
  journal = {INTERNATIONAL CONFERENCE ON COMPETITION AND OWNERSHIP IN LAND PASSENGER TRANSPORT, 10TH, 2007, HAMILTON ISLAND, QUEENSLAND, AUSTRALIA},
  url = {https://trid.trb.org/view.aspx?id=855076},
  urldate = {2019-01-16},
  keywords = {⛔ No DOI found},
  annotation = {00000},
  file = {C:\Users\nhiot\OneDrive\zotero\2007\Nielsen et Lange - 2007 - Network design for public transport success theor.pdf}
}

@article{nikolaouQueryingIncompleteInformation2016,
  title = {Querying Incomplete Information in {{RDF}} with {{SPARQL}}},
  author = {Nikolaou, Charalampos and Koubarakis, Manolis},
  year = {2016},
  month = aug,
  journal = {Artificial Intelligence},
  volume = {237},
  pages = {138--171},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2016.04.005},
  urldate = {2023-08-08},
  abstract = {Incomplete information has been studied in-depth in relational databases and knowledge representation. In the context of the Web, incomplete information issues have been studied in detail for XML, but very few papers exist that do the same for RDF. In this paper we make the first general proposal for extending RDF with the ability to represent property values that exist but are unknown or partially known using constraints. Following ideas from incomplete information literature, we develop a semantics for this extension of RDF, called RDFi, and study query evaluation for SPARQL. We transfer the concept of representation systems from incomplete information in relational databases to the case of RDFi and identify two very important fragments of SPARQL that can be used to define a representation system for RDFi. The first corresponds to the monotone fragment of graph patterns that uses only the operators AND, UNION, and FILTER. The second corresponds to the well-designed graph patterns, that is, a fragment that uses only operators AND, FILTER, and OPT, and enjoys interesting properties that make query evaluation efficient. We prove that each of the two fragments can be used to define a representation system for CONSTRUCT queries without blank nodes in their templates. We also define the fundamental concept of certain answers to SPARQL queries over RDFi databases and present an algorithm for its computation. Then, we present complexity results for computing certain answers by considering equality, temporal, and spatial constraint languages and the class of CONSTRUCT queries of our representation systems. Finally, we demonstrate the usefulness of RDFi in geospatial Semantic Web applications by giving a number of examples and comparing the modeling capabilities of RDFi with related formalisms found in the literature.},
  langid = {english},
  keywords = {Incomplete information,RDF,Semantic Web,SPARQL},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2016\\Nikolaou et Koubarakis - 2016 - Querying incomplete information in RDF with SPARQL.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\N8CIQSEH\\S0004370216300467.html}
}

@misc{nlpOntologydrivenNLPLanguage19:00:40.0,
  type = {{{CT915}}},
  title = {Ontology-Driven {{NLP}} ({{Language Processing}})},
  author = {{nlp}},
  year = {19:00:40.0},
  url = {https://www.ibm.com/developerworks/community/blogs/nlp/entry/ontology\_driven\_nlp},
  urldate = {2019-01-30},
  abstract = {Ontologies provide semantic context. Identifying entities in unstructured text is a picture only half complete.  Ontology models complete the picture by showing how these entities relate to other entities, whether in the document or in the wider world. I realize that this sentence is really marked up and there's arrows and red text going all over the place.  So let's examine this closely.  We've only recognized (e.g. annotated) two words in this entire sentence:  William Shakespeare as a Playwright and Hamlet as a Play.  But look at the depth of the understanding that we have.  There's a model depicted on this image, and we want to examine...},
  copyright = {Copyright IBM Corporation 2012},
  langid = {english},
  annotation = {00000},
  file = {C:\Users\nhiot\Zotero\storage\TBRHUU8Y\ontology_driven_nlp.html}
}

@misc{NlpWordFrequency,
  title = {Nlp - {{Word}} Frequency Algorithm for Natural Language Processing},
  journal = {Stack Overflow},
  url = {https://stackoverflow.com/questions/90580/word-frequency-algorithm-for-natural-language-processing},
  urldate = {2019-01-22},
  annotation = {00000},
  file = {C:\Users\nhiot\Zotero\storage\Y7FMU25Q\word-frequency-algorithm-for-natural-language-processing.html}
}

@article{nothmanLearningMultilingualNamed2013,
  ids = {nothmanLearningMultilingualNamed2013a},
  title = {Learning Multilingual Named Entity Recognition from {{Wikipedia}}},
  author = {Nothman, Joel and Ringland, Nicky and Radford, Will and Murphy, Tara and Curran, James R.},
  year = {2013},
  month = jan,
  journal = {Artificial Intelligence},
  series = {Artificial {{Intelligence}}, {{Wikipedia}} and {{Semi-Structured Resources}}},
  volume = {194},
  pages = {151--175},
  publisher = {Elsevier},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2012.03.006},
  urldate = {2024-03-21},
  abstract = {We automatically create enormous, free and multilingual silver-standard training annotations for named entity recognition (ner) by exploiting the text and structure of Wikipedia. Most ner systems rely on statistical models of annotated data to identify and classify names of people, locations and organisations in text. This dependence on expensive annotation is the knowledge bottleneck our work overcomes. We first classify each Wikipedia article into named entity (ne) types, training and evaluating on 7200 manually-labelled Wikipedia articles across nine languages. Our cross-lingual approach achieves up to 95\% accuracy. We transform the links between articles into ne annotations by projecting the target article's classifications onto the anchor text. This approach yields reasonable annotations, but does not immediately compete with existing gold-standard data. By inferring additional links and heuristically tweaking the Wikipedia corpora, we better align our automatic annotations to gold standards. We annotate millions of words in nine languages, evaluating English, German, Spanish, Dutch and Russian Wikipedia-trained models against conll shared task data and other gold-standard corpora. Our approach outperforms other approaches to automatic ne annotation (Richman and Schone, 2008 [61], Mika et al., 2008 [46]) competes with gold-standard training when tested on an evaluation corpus from a different source; and performs 10\% better than newswire-trained models on manually-annotated Wikipedia text.},
  keywords = {Annotated corpora,Information extraction,Named entity recognition,Semi-structured resources,Semi-supervised learning,Wikipedia},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2013\\Nothman et al. - 2013 - Learning multilingual named entity recognition fro.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\XE4CSF9N\\S0004370212000276.html}
}

@phdthesis{nouvelReconnaissanceEntitesNommees2012,
  type = {Thesis},
  title = {Reconnaissance Des Entit{\'e}s Nomm{\'e}es Par Exploration de R{\`e}gles d'annotation : Interpr{\'e}ter Les Marqueurs d'annotation Comme Instructions de Structuration Locale.},
  shorttitle = {Reconnaissance Des Entit{\'e}s Nomm{\'e}es Par Exploration de R{\`e}gles d'annotation},
  author = {Nouvel, Damien},
  year = {2012},
  month = nov,
  url = {http://www.theses.fr/2012TOUR4011},
  urldate = {2019-02-07},
  abstract = {Le d{\'e}veloppement des technologies de l'information et de la communication {\`a} modifi{\'e} en profondeur la mani{\`e}re dont nous avons acc{\`e}s aux connaissances. Face {\`a} l'afflux de donn{\'e}es et {\`a} leur diversit{\'e}, il est n{\'e}cessaire de meure su point des technologies performantes et robustes pour y rechercher des informations. Notre travail porte sur le reconnaissance des entit{\'e}s nomm{\'e}es et leur annotation su sein de transcriptions d'{\'e}missions radiodiffus{\'e}es ou t{\'e}l{\'e}visuelles. En premi{\`e}re partie, nous abordons le probl{\'e}matique de la reconnaissance automatique des entit{\'e}s nomm{\'e}es. Apr{\`e}s une caract{\'e}risation de leur nature linguistique, nous proposons une approche par instructions, fond{\'e}e sur les marqueurs (balises) d'annotation, qui consid{\`e}re ces {\'e}l{\'e}ments isol{\'e}ment (d{\'e}but ou fin d'une annotation). En seconde partie, nous faisons {\'e}tat des travaux en fouille de donn{\'e}es et pr{\'e}sentons un cadre formel pour explorer les donn{\'e}es. Nous y proposons une formulation alternative par segments, qui limite la combinatoire lors de l'exploration. Les motifs corr{\'e}l{\'e}s {\`a} un ou plusieurs marqueurs d'annotation sont extraits comme r{\`e}gles d'annotation. La derni{\`e}re partie d{\'e}crit le cadre exp{\'e}rimental, quelques sp{\'e}cificit{\'e}s de l'impl{\'e}mentation du syst{\`e}me (mXS) et les r{\'e}sultats obtenus. Nous montrons l'int{\'e}r{\^e}t d'extraire largement les r{\`e}gles d'annotation et exp{\'e}rimentons les motifs de segments. Nous fournissons des r{\'e}sultats chiffr{\'e}s relatifs aux performances du syst{\`e}me {\`a} divers point de vue et dans diverses configurations. Ils montrent que l'approche que nous proposons est comp{\'e}titive et qu'elle ouvre des perspectives dans le cadre de l'observation des langues naturelles et de l'annotation automatique.},
  school = {Tours},
  keywords = {Annotation rules,Apprentissage automatique,Connecteurs (linguistique),Data mining,Entit{\'e}s nomm{\'e}es,Exploration de donn{\'e}es,Gloses,Informatique,Marqueurs discursifs,Named entities,Natural language processing,R{\`e}gles d'annotation,Recherche de l'information,Structures de donn{\'e}es (informatique),Traitement automatique du langage naturel},
  annotation = {00000},
  file = {C:\Users\nhiot\Zotero\storage\YF2N82VV\2012TOUR4011.html}
}

@article{nozzaLearningToAdaptWordEmbeddings2021,
  ids = {nozzaLearningToAdaptWordEmbeddings2021a},
  title = {{{LearningToAdapt}} with Word Embeddings: {{Domain}} Adaptation of {{Named Entity Recognition}} Systems},
  shorttitle = {{{LearningToAdapt}} with Word Embeddings},
  author = {Nozza, Debora and Manchanda, Pikakshi and Fersini, Elisabetta and Palmonari, Matteo and Messina, Enza},
  year = {2021},
  month = may,
  journal = {Information Processing \& Management},
  volume = {58},
  number = {3},
  pages = {102537},
  issn = {0306-4573},
  doi = {10/gh48v6},
  urldate = {2021-02-24},
  abstract = {The task of Named Entity Recognition (NER) is aimed at identifying named entities in a given text and classifying them into pre-defined domain entity types such as persons, organizations, locations. Most of the existing NER systems make use of generic entity type classification schemas, however, the comparison and integration of (more or less) different entity types among different NER systems is a complex problem even for human experts. In this paper, we propose a supervised approach called L2AWE (Learning To Adapt with Word Embeddings) which aims at adapting a NER system trained on a source classification schema to a given target one. In particular, we validate the hypothesis that the embedding representation of named entities can improve the semantic meaning of the feature space used to perform the adaptation from a source to a target domain. The results obtained on benchmark datasets of informal text show that L2AWE not only outperforms several state of the art models, but it is also able to tackle errors and uncertainties given by NER systems.},
  langid = {english},
  keywords = {Domain adaptation,Named Entity Recognition,Word embeddings},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2021\\Nozza et al. - 2021 - LearningToAdapt with word embeddings Domain adapt.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\RTWZDGLA\\S0306457321000455.html}
}

@inproceedings{nuruzzamanSurveyChatbotImplementation2018,
  title = {A {{Survey}} on {{Chatbot Implementation}} in {{Customer Service Industry}} through {{Deep Neural Networks}}},
  booktitle = {2018 {{IEEE}} 15th {{International Conference}} on E-{{Business Engineering}} ({{ICEBE}})},
  author = {Nuruzzaman, M. and Hussain, O. K.},
  year = {2018},
  month = oct,
  pages = {54--61},
  doi = {10/gjbddm},
  abstract = {Nowadays it is the era of intelligent machine. With the advancement of artificial intelligent, machine learning and deep learning, machines have started to impersonate as human. Conversational software agents activated by natural language processing is known as chatbot, are an excellent example of such machine. This paper presents a survey on existing chatbots and techniques applied into it. It discusses the similarities, differences and limitations of the existing chatbots. We compared 11 most popular chatbot application systems along with functionalities and technical specifications. Research showed that nearly 75\% of customers have experienced poor customer service and generation of meaningful, long and informative responses remains a challenging task. In the past, methods for developing chatbots have relied on hand-written rules and templates. With the rise of deep learning these models were quickly replaced by end-to-end neural networks. More specifically, Deep Neural Networks is a powerful generative-based model to solve the conversational response generation problems. This paper conducted an in-depth survey of recent literature, examining over 70 publications related to chatbots published in the last 5 years. Based on literature review, this study made a comparison from selected papers according to method adopted. This paper also presented why current chatbot models fails to take into account when generating responses and how this affects the quality conversation.},
  keywords = {artificial intelligent machine learning,Chatbot,chatbot application systems,conversational response generation problems,conversational software agents,Credit cards,customer service industry,customer services,deep learning,Deep learning,Deep Learning,deep neural networks,Dialogue System,end-to-end neural networks,generative-based model,hand-written rules,hand-written templates,human computer interaction,in-depth survey,Insurance,learning (artificial intelligence),natural language processing,Natural Language Processing,neural nets,Neural Network,Neural networks,service industries,software agents,Task analysis},
  file = {C:\Users\nhiot\Zotero\storage\SRRZ6EBJ\8592630.html}
}

@inproceedings{onetChaseProcedureIts2013,
  title = {The {{Chase Procedure}} and Its {{Applications}} in {{Data Exchange}}},
  booktitle = {Dagstuhl {{Follow-Ups}}},
  author = {Onet, Adrian},
  year = {2013},
  volume = {5},
  pages = {1--37},
  publisher = {Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik},
  doi = {10.4230/DFU.VOL5.10452.1},
  urldate = {2024-01-03},
  abstract = {The initial and basic role of the chase procedure was to test logical implication between sets of dependencies in order to determine equivalence of database instances known to satisfy a given set of dependencies and to determine query equivalence under database constrains. Recently the chase procedure has experienced a revival due to its application in data exchange. In this chapter we review the chase algorithm and its properties as well as its application in data exchange.},
  copyright = {https://creativecommons.org/licenses/by/3.0/legalcode},
  langid = {english},
  keywords = {{000 Computer science, knowledge, general works},Computer Science,nosource},
  file = {C:\Users\nhiot\OneDrive\zotero\2013\Onet - 2013 - The Chase Procedure and its Applications in Data E.pdf}
}

@misc{OpenAPISpecification,
  title = {{{OpenAPI Specification}}},
  url = {http://spec.openapis.org/oas/v3.0.3},
  urldate = {2020-10-02},
  file = {C:\Users\nhiot\Zotero\storage\WRE5GJRZ\v3.0.html}
}

@misc{OpenTelemetry,
  title = {{{OpenTelemetry}}},
  journal = {OpenTelemetry},
  url = {https://opentelemetry.io/},
  urldate = {2022-09-07},
  abstract = {The OpenTelemetry Project Site},
  langid = {english},
  file = {C:\Users\nhiot\Zotero\storage\TCN5CRV6\opentelemetry.io.html}
}

@article{osmanGraphBasedTextRepresentation2020,
  title = {Graph-{{Based Text Representation}} and {{Matching}}: {{A Review}} of the {{State}} of the {{Art}} and {{Future Challenges}}},
  shorttitle = {Graph-{{Based Text Representation}} and {{Matching}}},
  author = {Osman, Ahmed Hamza and Barukub, Omar Mohammed},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {87562--87583},
  issn = {2169-3536},
  doi = {10/ghmwkc},
  abstract = {Graph-based text representation is one of the important preprocessing steps in data and text mining, Natural Language Processing (NLP), and information retrieval approaches. The graph-based methods focus on how to represent text documents in the shape of a graph to exploit the best features of their characteristics. This study reviews and lists the advantages and disadvantages of such methods employed or developed in graph-based text representations. The literature shows that some of the proposed graph-based methods suffer from a lack of representing texts in certain situations. Currently, several techniques are commonly used in graph-based text representation. However, there are still some weaknesses and shortages in these techniques and tools that significantly affect the success of graph representation and graph matching. In this review, we conduct an inclusive survey of the state of the art in graph-based text representation and learning. We provide a formal description of the problem of graph-based text representation and introduce some basic concepts. More significantly, this study proposes a new taxonomy of graph-based text representation, categorizing the existing studies based on representation characteristics and scheme techniques. In terms of the representation scheme taxonomy, we introduce four main types of conceptual graph schemes and summarize the challenges faced in each scheme. The main issues of graph representation, such as research topics and the sub-taxonomy of graph models for web documents, are introduced and categorized. This research also covers some tasks of understanding natural language processing (NLP) that depend on different types of graph structures. In addition, the graph matching taxonomy implements three main categories based on the matching approach, including structural-, semantic-, and similarity-based approaches. Moreover, a deep comparison of these approaches is discussed and reported in terms of methods and tools, the concepts of matching and locality, and the application domains that use these tools. Finally, the paper recommends seven promising future study directions in the graph-based text representation field. These recommendation points are summarized and highlighted as open problems and challenges of graph-based text representation and learning to facilitate and fill the research gaps for scientific researchers in this field.},
  keywords = {graph,graph matching,Graph representation,Labeling,Natural language processing,NLP,Numerical models,representation scheme,Semantics,Taxonomy,text mining,Text mining,Tools},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2020\\Osman et Barukub - 2020 - Graph-Based Text Representation and Matching A Re.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\L66HE9JL\\9088989.html}
}

@misc{OverviewNeo4jInternals,
  title = {An Overview of {{Neo4j Internals}}},
  url = {https://www.slideshare.net/thobe/an-overview-of-neo4j-internals},
  urldate = {2023-09-01},
  abstract = {An overview of Neo4j Internals - Download as a PDF or view online for free},
  langid = {english},
  file = {C:\Users\nhiot\Zotero\storage\SQ7UL5X5\an-overview-of-neo4j-internals.html}
}

@techreport{pagePageRankCitationRanking1999,
  type = {Technical {{Report}}},
  title = {The {{PageRank Citation Ranking}}: {{Bringing Order}} to the {{Web}}.},
  shorttitle = {The {{PageRank}} Citation Ranking},
  author = {Page, Lawrence and Brin, Sergey and Motwani, Rajeev and Winograd, Terry},
  year = {1999},
  month = nov,
  number = {1999-66},
  pages = {1--14},
  institution = {Stanford InfoLab},
  url = {http://ilpubs.stanford.edu:8090/422/},
  abstract = {The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes PageRank, a mathod for rating Web pages objectively and mechanically, effectively measuring the human interest and attention devoted to them. We compare PageRank to an idealized random Web surfer. We show how to efficiently compute PageRank for large numbers of pages. And, we show how to apply PageRank to search and to user navigation.},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\1999\Page et al. - 1999 - The PageRank Citation Ranking Bringing Order to t.pdf}
}

@article{paredes-valverdeONLIOntologybasedSystem2015,
  title = {{{ONLI}}: {{An}} Ontology-Based System for Querying {{DBpedia}} Using Natural Language Paradigm},
  shorttitle = {{{ONLI}}},
  author = {{Paredes-Valverde}, Mario Andr{\'e}s and {Rodr{\'i}guez-Garc{\'i}a}, Miguel {\'A}ngel and {Ruiz-Mart{\'i}nez}, Antonio and {Valencia-Garc{\'i}a}, Rafael and {Alor-Hern{\'a}ndez}, Giner},
  year = {2015},
  month = jul,
  journal = {Expert Systems with Applications},
  volume = {42},
  number = {12},
  pages = {5163--5176},
  issn = {0957-4174},
  doi = {10/gfvn2v},
  urldate = {2019-01-29},
  abstract = {The Semantic Web has emerged as an extension of the current Web, in which Web content has well-defined meaning through the addition of logic-based metadata. However, current mechanisms for information retrieval from semantic knowledge bases restrict their use to only experienced users. To address this gap, the natural language processing (NLP) is deemed to be very intuitive from a use point of view, due to it hides the formality of a knowledge base as well as the executable query language. This paper presents a novel ontology-based information retrieval system for DBpedia called ONLI (Ontology-based Natural Language Interface). ONLI proposes the use of an ontology model in order to represent both the syntactic question's structure and the question's context. This model allows inferring the answer type expected by the user through an established question's classification. These features allow reducing the search space thus increasing the probability of providing the correct answer. From this perspective, ONLI was evaluated in terms of their ability to find the correct answer into DBpedia's content, achieving promising results and proving to be very useful to non-experienced users.},
  keywords = {DBpedia,Natural language processing,Ontology,Semantic Web},
  annotation = {00000 QID: Q60584339},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2015\\Paredes-Valverde et al. - 2015 - ONLI An ontology-based system for querying DBpedi.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\YT7U2ZU4\\S0957417415001414.html}
}

@article{parkEvaluatingOntologyExtraction2010,
  title = {Evaluating Ontology Extraction Tools Using a Comprehensive Evaluation Framework},
  author = {Park, Jinsoo and Cho, Wonchin and Rho, Sangkyu},
  year = {2010},
  month = oct,
  journal = {Data \& Knowledge Engineering},
  volume = {69},
  number = {10},
  pages = {1043--1061},
  issn = {0169-023X},
  doi = {10.1016/j.datak.2010.07.002},
  urldate = {2024-02-29},
  abstract = {Ontologies are a key component of the Semantic Web; thus, they are widely used in various applications. However, most ontologies are still built manually, a time-consuming activity which requires many resources. Several tools such as ontology editing tools, ontology merging tools, and ontology extraction tools have therefore been proposed to speed up ontology development. To minimize building time, one promising solution is the automation of the ontology development process. Consequently, the need for an automatic ontology extraction tool has increased in the last two decades and many tools have been developed for this purpose. However, there is still no comprehensive framework for evaluating such tools. In this paper, we proposed a set of criteria for evaluating ontology extraction tools and carried out an evaluation experiment on four ontology extraction tools (i.e., OntoLT, Text2Onto, OntoBuilder, and DODDLE-OWL) using our proposed evaluation framework. Based on the results of our experiment, we concluded that ontology extraction tools still lack the ability to automate the extraction process fully and thus require functional performance improvement.},
  keywords = {Ontology,Ontology extraction tool,Tool evaluation},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2010\\Park et al. - 2010 - Evaluating ontology extraction tools using a compr.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\9QRSC33N\\S0169023X10000844.html}
}

@article{pasquierAttributedGraphMining2017,
  title = {Attributed Graph Mining in the Presence of Automorphism},
  author = {Pasquier, Claude and Flouvat, Fr{\'e}d{\'e}ric and Sanhes, J{\'e}r{\'e}my and {Selmaoui-Folcher}, Nazha},
  year = {2017},
  month = feb,
  journal = {Knowledge and Information Systems},
  volume = {50},
  number = {2},
  pages = {569--584},
  issn = {0219-1377, 0219-3116},
  doi = {10/f9rbsh},
  urldate = {2018-12-29},
  langid = {english},
  keywords = {Attributed graph,Automorphism,Frequent pattern mining,Itemset mining,Structure mining},
  annotation = {00000 QID: Q114827306},
  file = {C:\Users\nhiot\OneDrive\zotero\2017\Pasquier et al. - 2017 - Attributed graph mining in the presence of automor.pdf}
}

@inproceedings{passosLexiconInfusedPhrase2014,
  ids = {passosLexiconInfusedPhrase,passosLexiconInfusedPhrase2014a,passosLexiconInfusedPhrasea,passosLexiconInfusedPhraseb},
  title = {Lexicon {{Infused Phrase Embeddings}} for {{Named Entity Resolution}}},
  booktitle = {Proceedings of the {{Eighteenth Conference}} on {{Computational Natural Language Learning}}},
  author = {Passos, Alexandre and Kumar, Vineet and Mccallum, Andrew},
  year = {2014},
  pages = {78--86},
  publisher = {Citeseer},
  doi = {10.3115/v1/W14-1609},
  urldate = {2024-03-20},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2014\\Passos et al. - 2014 - Lexicon Infused Phrase Embeddings for Named Entity3.pdf;C\:\\Users\\nhiot\\OneDrive\\zotero\\2014\\Passos et al. - 2014 - Lexicon Infused Phrase Embeddings for Named Entity4.pdf}
}

@article{patel-schneiderUsingDescriptionLogics2015,
  title = {Using {{Description Logics}} for {{RDF Constraint Checking}} and {{Closed-World Recognition}}},
  author = {{Patel-Schneider}, Peter},
  year = {2015},
  month = feb,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {29},
  number = {1},
  pages = {247--253},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v29i1.9177},
  urldate = {2023-12-27},
  abstract = {RDF and Description Logics work in an open-world setting where absence of information is not information about absence.  Nevertheless, Description Logic axioms can be interpreted in a closed-world setting and in this setting they can be used for both constraint checking and closed-world recognition against information sources.  When the information sources are expressed in well-behaved RDF or RDFS (i.e., RDF graphs interpreted in the RDF or RDFS semantics) this constraint checking and closed-world recognition is simple to describe.  Further this constraint checking can be implemented as SPARQL querying and thus effectively performed.},
  copyright = {Copyright (c)},
  langid = {english},
  keywords = {Constraints,nosource},
  file = {C:\Users\nhiot\OneDrive\zotero\2015\Patel-Schneider - 2015 - Using Description Logics for RDF Constraint Checki.pdf}
}

@article{patel-schneiderWikidataMARS2020,
  ids = {patel-schneiderWikidataMARS2020a},
  title = {Wikidata on {{MARS}}},
  author = {{Patel-Schneider}, Peter F. and Martin, David},
  year = {2020},
  month = aug,
  journal = {arXiv:2008.06599 [cs]},
  eprint = {2008.06599},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2008.06599},
  urldate = {2021-05-27},
  abstract = {Multi-attributed relational structures (MARSs) have been proposed as a formal data model for generalized property graphs, along with multi-attributed rule-based predicate logic (MARPL) as a useful rule-based logic in which to write inference rules over property graphs. Wikidata can be modelled in an extended MARS that adds the (imprecise) datatypes of Wikidata. The rules of inference for the Wikidata ontology can be modelled as a MARPL ontology, with extensions to handle the Wikidata datatypes and functions over these datatypes. Because many Wikidata qualifiers should participate in most inference rules in Wikidata a method of implicitly handling qualifier values on a per-qualifier basis is needed to make this modelling useful. The meaning of Wikidata is then the extended MARS that is the closure of running these rules on the Wikidata data model. Wikidata constraints can be modelled as multi-attributed predicate logic (MAPL) formulae, again extended with datatypes, that are evaluated over this extended MARS. The result models Wikidata in a way that fixes several of its major problems.},
  archiveprefix = {arxiv},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2020\\Patel-Schneider et Martin - 2020 - Wikidata on MARS.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\A9MH96QY\\2008.html}
}

@book{patelAppliedNaturalLanguage2021,
  ids = {patelAppliedNaturalLanguage},
  title = {Applied {{Natural Language Processing}} in the {{Enterprise}}},
  author = {Patel, Ankur A. and Arasanipalai, Ajay Uppili},
  year = {2021},
  month = may,
  publisher = {O'Reilly Media, Inc.},
  url = {https://books.google.fr/books?hl=fr\&lr=\&id=5dktEAAAQBAJ\&oi=fnd\&pg=PR2\&dq=Explosion+AI\%27s+NLP+library:+spaCy+\&ots=19cpr4BVQR\&sig=s5m8nE0RvH8gD3hU4qhpT7z-7AU},
  abstract = {NLP has exploded in popularity over the last few years. But while Google, Facebook, OpenAI, and others continue to release larger language models, many teams still struggle with building NLP applications that live up to the hype. This hands-on guide helps you get up to speed on the latest and most promising trends in NLP.With a basic understanding of machine learning and some Python experience, you'll learn how to build, train, and deploy models for real-world applications in your organization. Authors Ankur Patel and Ajay Uppili Arasanipalai guide you through the process using code and examples that highlight the best practices in modern NLP.Use state-of-the-art NLP models such as BERT and GPT-3 to solve NLP tasks such as named entity recognition, text classification, semantic search, and reading comprehensionTrain NLP models with performance comparable or superior to that of out-of-the-box systemsLearn about Transformer architecture and modern tricks like transfer learning that have taken the NLP world by stormBecome familiar with the tools of the trade, including spaCy, Hugging Face, and fast.aiBuild core parts of the NLP pipeline--including tokenizers, embeddings, and language models--from scratch using Python and PyTorchTake your models out of Jupyter notebooks and learn how to deploy, monitor, and maintain them in production},
  isbn = {978-1-4920-6254-7},
  langid = {english},
  keywords = {⛔ No DOI found,Computers / Artificial Intelligence / General,Computers / Artificial Intelligence / Natural Language Processing,Computers / Business \& Productivity Software / Business Intelligence,Computers / Data Science / Data Analytics,Computers / Data Science / General,Computers / Data Science / Machine Learning,Computers / Machine Theory}
}

@inproceedings{pathakEzDISupervisedNLP2015,
  title = {{{ezDI}}: {{A}} Supervised {{NLP}} System for Clinical Narrative Analysis},
  booktitle = {Proceedings of the 9th International Workshop on Semantic Evaluation, {{SemEval}}@{{NAACL-HLT}} 2015, Denver, Colorado, {{USA}}, June 4-5, 2015},
  author = {Pathak, Parth and Patel, Pinal and Panchal, Vishal and Soni, Sagar and Dani, Kinjal and Patel, Amrish and Choudhary, Narayan},
  editor = {Cer, Daniel M. and Jurgens, David and Nakov, Preslav and Zesch, Torsten},
  year = {2015},
  pages = {412--416},
  publisher = {The Association for Computer Linguistics},
  doi = {10.18653/v1/s15-2071},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/semeval/PathakPPSDPC15.bib},
  keywords = {nosource},
  timestamp = {Tue, 28 Jan 2020 10:29:20 +0100},
  file = {C:\Users\nhiot\OneDrive\zotero\2015\Pathak et al. - 2015 - ezDI A supervised NLP system for clinical narrati.pdf}
}

@article{pavloWhatReallyNew2016,
  title = {What's {{Really New}} with {{NewSQL}}?},
  author = {Pavlo, Andrew and Aslett, Matthew},
  year = {2016},
  month = sep,
  journal = {SIGMOD Rec.},
  volume = {45},
  number = {2},
  pages = {45--55},
  issn = {0163-5808},
  doi = {10/f866dr},
  urldate = {2019-01-09},
  abstract = {A new class of database management systems (DBMSs) called NewSQL tout their ability to scale modern on-line transaction processing (OLTP) workloads in a way that is not possible with legacy systems. The term NewSQL was first used by one of the authors of this article in a 2011 business analysis report discussing the rise of new database systems as challengers to these established vendors (Oracle, IBM, Microsoft). The other author was working on what became one of the first examples of a NewSQL DBMS. Since then several companies and research projects have used this term (rightly and wrongly) to describe their systems. Given that relational DBMSs have been around for over four decades, it is justifiable to ask whether the claim of NewSQL's superiority is actually true or whether it is simply marketing. If they are indeed able to get better performance, then the next question is whether there is anything scientifically new about them that enables them to achieve these gains or is it just that hardware has advanced so much that now the bottlenecks from earlier years are no longer a problem. To do this, we first discuss the history of databases to understand how NewSQL systems came about. We then provide a detailed explanation of what the term NewSQL means and the different categories of systems that fall under this definition.},
  annotation = {00000},
  file = {C:\Users\nhiot\OneDrive\zotero\2016\Pavlo et Aslett - 2016 - What's Really New with NewSQL.pdf}
}

@inproceedings{pellegriniScotchSoftwarePackage1996,
  title = {Scotch: {{A}} Software Package for Static Mapping by Dual Recursive Bipartitioning of Process and Architecture Graphs},
  shorttitle = {Scotch},
  booktitle = {High-{{Performance Computing}} and {{Networking}}},
  author = {Pellegrini, Fran{\c c}ois and Roman, Jean},
  editor = {Liddell, Heather and Colbrook, Adrian and Hertzberger, Bob and Sloot, Peter},
  year = {1996},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {493--498},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10/fbztg6},
  abstract = {The combinatorial optimization problem of assigning the communicating processes of a parallel program onto a parallel machine so as to minimize its overall execution time is called static mapping.This paper presents Scotch, a software package for static mapping based on the recursive bipartitioning of both the source process graph and the target architecture graph. Scotch can map any weighted source graph onto any weighted target graph in a time linear in the number of source edges and logarithmic in the number of target vertices. We give brief descriptions of the algorithm and its bipartitioning methods, and compare its performance to other mapping and partitioning programs.},
  isbn = {978-3-540-49955-8},
  langid = {english},
  keywords = {Gain Array,Parallel Machine,Static Mapping,Target Architecture,Target Machine},
  file = {C:\Users\nhiot\OneDrive\zotero\1996\Pellegrini et Roman - 1996 - Scotch A software package for static mapping by d.pdf}
}

@article{pengCrossSentenceNaryRelation2017,
  ids = {pengCrosssentenceNaryRelation2017},
  title = {Cross-{{Sentence N-ary Relation Extraction}} with {{Graph LSTMs}}},
  author = {Peng, Nanyun and Poon, Hoifung and Quirk, Chris and Toutanova, Kristina and Yih, Wen-tau},
  year = {2017},
  month = dec,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {5},
  pages = {101--115},
  publisher = {MIT Press},
  doi = {10/gf3mwb},
  urldate = {2020-11-17},
  abstract = {Past work in relation extraction has focused on binary relations in single                     sentences. Recent NLP inroads in high-value domains have sparked interest in the                     more general setting of extracting n-ary relations                     that span multiple sentences. In this paper, we explore a general relation                     extraction framework based on graph long short-term memory networks (graph                     LSTMs) that can be easily extended to cross-sentence n-ary relation extraction. The graph formulation provides a                     unified way of exploring different LSTM approaches and incorporating various                     intra-sentential and inter-sentential dependencies, such as sequential,                     syntactic, and discourse relations. A robust contextual representation is                     learned for the entities, which serves as input to the relation classifier. This                     simplifies handling of relations with arbitrary arity, and enables multi-task                     learning with related relations. We evaluate this framework in two important                     precision medicine settings, demonstrating its effectiveness with both                     conventional supervised learning and distant supervision. Cross-sentence                     extraction produced larger knowledge bases. and multi-task learning                     significantly improved extraction accuracy. A thorough analysis of various LSTM                     approaches yielded useful insight the impact of linguistic analysis on                     extraction accuracy.},
  annotation = {QID: Q119735240},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2017\\Peng et al. - 2017 - Cross-Sentence N-ary Relation Extraction with Grap.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\VFEY5BIK\\tacl_a_00049.html}
}

@inproceedings{pereiraParsingDeduction1983,
  title = {Parsing as Deduction},
  booktitle = {21st Annual Meeting of the Association for Computational Linguistics},
  author = {Pereira, Fernando C. N. and Warren, David H. D.},
  year = {1983},
  month = jun,
  pages = {137--144},
  publisher = {Association for Computational Linguistics},
  address = {Cambridge, Massachusetts, USA},
  doi = {10.3115/981311.981338},
  keywords = {nosource}
}

@incollection{peroniUNDOUnitedNations2017,
  title = {{{UNDO}}: {{The United Nations System Document Ontology}}},
  shorttitle = {{{UNDO}}},
  booktitle = {The {{Semantic Web}} -- {{ISWC}} 2017},
  author = {Peroni, Silvio and Palmirani, Monica and Vitali, Fabio},
  editor = {{d'Amato}, Claudia and Fernandez, Miriam and Tamma, Valentina and Lecue, Freddy and {Cudr{\'e}-Mauroux}, Philippe and Sequeda, Juan and Lange, Christoph and Heflin, Jeff},
  year = {2017},
  volume = {10588},
  pages = {175--183},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-68204-4_18},
  urldate = {2019-01-29},
  abstract = {Akoma Ntoso is an OASIS Committee Specification Draft standard for the electronic representations of parliamentary, normative and judicial documents in XML. Recently, it has been officially adopted by the United Nations (UN) as the main electronic format for making UN documents machine-processable. However, Akoma Ntoso does not force nor define any formal ontology for allowing the description of real-world objects, concepts and relations mentioned in documents. In order to address this gap, in this paper we introduce the United Nations System Document Ontology (UNDO), i.e. an OWL 2 DL ontology developed and adopted by the United Nations that aims at providing a framework for the formal description of all these entities.},
  isbn = {978-3-319-68203-7 978-3-319-68204-4},
  langid = {english},
  annotation = {00000},
  file = {C:\Users\nhiot\OneDrive\zotero\2017\Peroni et al. - 2017 - UNDO The United Nations System Document Ontology.pdf}
}

@inproceedings{pichlerComplexityEvaluatingTuple2011,
  title = {The Complexity of Evaluating Tuple Generating Dependencies},
  booktitle = {Proceedings of the 14th {{International Conference}} on {{Database Theory}}},
  author = {Pichler, Reinhard and Skritek, Sebastian},
  year = {2011},
  month = mar,
  series = {{{ICDT}} '11},
  pages = {244--255},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1938551.1938583},
  urldate = {2023-08-16},
  abstract = {Dependencies have played an important role in database design for many years. More recently, they have also turned out to be central to data integration and data exchange. In this work we concentrate on tuple generating dependencies (tgds) which enforce the presence of certain tuples in a database instance if certain other tuples are already present. Previous complexity results in data integration and data exchange mainly referred to the data complexity. In this work, we study the query complexity and combined complexity of a fundamental problem related to tgds, namely checking if a given tgd is satisfied by a database instance. We also address an important variant of this problem which deals with updates (by inserts or deletes) of a database: Here we have to check if all previously satisfied tgds are still satisfied after an update. We show that the query complexity and combined complexity of these problems are much higher than the data complexity. However, we also prove sufficient conditions on the tgds to reduce this high complexity.},
  isbn = {978-1-4503-0529-7},
  file = {C:\Users\nhiot\OneDrive\zotero\2011\Pichler et Skritek - 2011 - The complexity of evaluating tuple generating depe.pdf}
}

@incollection{pierrelActesTALN20022002,
  title = {Actes de {{TALN}} 2002 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 2002 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Pierrel, Jean-Marie},
  year = {2002},
  month = jun,
  publisher = {ATILF / ATALA},
  address = {Nancy},
  keywords = {nosource}
}

@article{plantevitCombiningSequenceItemset2009,
  title = {Combining {{Sequence}} and {{Itemset Mining}} to {{Discover Named Entities}} in {{Biomedical Texts}}: {{A New Type}} of {{Pattern}}},
  shorttitle = {Combining {{Sequence}} and {{Itemset Mining}} to {{Discover Named Entities}} in {{Biomedical Texts}}},
  author = {Plantevit, Marc and Charnois, Thierry and Kl{\'e}ma, Jiri and Rigotti, Christophe and Cr{\'e}milleux, Bruno},
  year = {2009},
  journal = {Int. J. of Data Mining, Modelling and Management},
  volume = {1},
  number = {2},
  pages = {119--148},
  doi = {10/bbpfsj},
  urldate = {2019-01-23},
  abstract = {Biomedical named entity recognition (NER) is a challenging problem. In this paper, we show that mining techniques, such as sequential pattern mining and sequential rule mining, can be useful to tackle this problem but present some limitations. We demonstrate and analyse these limitations and introduce a new kind of pattern called LSR pattern that offers an excellent trade-off between the high precision of sequential rules and the high recall of sequential patterns. We formalise the LSR pattern mining problem first. Then we show how LSR patterns enable us to successfully tackle biomedical NER problems. We report experiments carried out on real datasets that underline the relevance of our proposition.},
  keywords = {biomedical named entity recognition problem,constraint-based pattern mining,LSR patterns,NER,sequential patterns},
  annotation = {00000},
  file = {C:\Users\nhiot\OneDrive\zotero\2009\Plantevit et al. - 2009 - Combining Sequence and Itemset Mining to Discover .pdf}
}

@inproceedings{pokornyGraphDatabasesTheir2015,
  title = {Graph {{Databases}}: {{Their Power}} and {{Limitations}}},
  shorttitle = {Graph {{Databases}}},
  booktitle = {Computer {{Information Systems}} and {{Industrial Management}}: 14th {{IFIP TC}} 8 {{International Conference}}, {{CISIM}} 2015, {{Warsaw}}, {{Poland}}, {{September}} 24-26, 2015, {{Proceedings}} 14},
  author = {Pokorn{\'y}, Jaroslav},
  editor = {Saeed, Khalid and Homenda, Wladyslaw},
  year = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {58--69},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-24369-6_5},
  abstract = {Real world data offers a lot of possibilities to be represented as graphs. As a result we obtain undirected or directed graphs, multigraphs and hypergraphs, labelled or weighted graphs and their variants. A development of graph modelling brings also new approaches, e.g., considering constraints. Processing graphs in a database way can be done in many different ways. Some graphs can be represented as JSON or XML structures and processed by their native database tools. More generally, a graph database is specified as any storage system that provides index-free adjacency, i.e. an explicit graph structure. Graph database technology contains some technological features inherent to traditional databases, e.g. ACID properties and availability. Use cases of graph databases like Neo4j, OrientDB, InfiniteGraph, FlockDB, AllegroGraph, and others, document that graph databases are becoming a common means for any connected data. In Big Data era, important questions are connected with scalability for large graphs as well as scaling for read/write operations. For example, scaling graph data by distributing it in a network is much more difficult than scaling simpler data models and is still a work in progress. Still a challenge is pattern matching in graphs providing, in principle, an arbitrarily complex identity function. Mining complete frequent patterns from graph databases is also challenging since supporting operations are computationally costly. In this paper, we discuss recent advances and limitations in these areas as well as future directions.},
  isbn = {978-3-319-24369-6},
  langid = {english},
  keywords = {Big graphs,Graph database,Graph querying,Graph scalability,Graph storage},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2015\\Pokorný - 2015 - Graph Databases Their Power and Limitations.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\Z5XWGHCZ\\978-3-319-24369-6_5.html}
}

@inproceedings{pokornyHowStoreProcess2014,
  title = {How to {{Store}} and {{Process Big Data}}: {{Are Today}}'s {{Databases Sufficient}}?},
  shorttitle = {How to {{Store}} and {{Process Big Data}}},
  booktitle = {Computer {{Information Systems}} and {{Industrial Management}}},
  author = {Pokorn{\'y}, Jaroslav},
  editor = {Saeed, Khalid and Sn{\'a}{\v s}el, V{\'a}clav},
  year = {2014},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {5--10},
  publisher = {Springer Berlin Heidelberg},
  abstract = {The development and extensive use of highly distributed and scalable systems to process Big Data is widely considered. New data management architectures, e.g. distributed file systems and NoSQL databases, are used in this context. On the other hand, features of Big Data like their complexity and data analytics demands indicate that these tools solve Big Data problems only partially. A development of so called NewSQL databases is highly relevant and even special category of Big Data Management Systems is considered. In this work we will shortly discuss these trends and evaluate some current approaches to Big Data management and processing, identify the current challenges, and suggest possible research directions.},
  isbn = {978-3-662-45237-0},
  langid = {english},
  keywords = {ACID,Big Data,Hadoop,NewSQL,NoSQL,SQL-on-Hadoop},
  annotation = {00000},
  file = {C:\Users\nhiot\OneDrive\zotero\2014\Pokorný - 2014 - How to Store and Process Big Data Are Today’s Dat.pdf}
}

@article{polepallirameshAutomaticallyRecognizingMedication2014,
  title = {Automatically {{Recognizing Medication}} and {{Adverse Event Information From Food}} and {{Drug Administration}}'s {{Adverse Event Reporting System Narratives}}},
  author = {Polepalli Ramesh, Balaji and Belknap, Steven M and Li, Zuofeng and Frid, Nadya and West, Dennis P and Yu, Hong},
  year = {2014},
  month = jun,
  journal = {JMIR Medical Informatics},
  volume = {2},
  number = {1},
  pages = {e10},
  issn = {2291-9694},
  doi = {10/gfwrgn},
  urldate = {2019-06-20},
  langid = {english},
  annotation = {QID: Q34881857},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2014\\Polepalli Ramesh et al. - 2014 - Automatically Recognizing Medication and Adverse E.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\M2DVHN93\\automatically-recognizing-medication-and-adverse-event-informatio.html}
}

@misc{polesystematicparis-regionBuildingHighperformanceScalable11:49:43UTC,
  type = {Technologie},
  title = {Building a High-Performance, Scalable {{ML}} \& {{NLP}} Platform with {{Python}}, {\dots}},
  author = {{P{\^o}le Systematic Paris-Region}},
  year = {11:49:43 UTC},
  url = {https://fr.slideshare.net/PoleSystematicParisRegion/building-a-highperformance-scalable-ml-nlp-platform-with-python-sheer-el-showk},
  urldate = {2019-12-12},
  abstract = {PyParis 2017 http://pyparis.org},
  keywords = {nosource}
}

@phdthesis{pollardGeneralizedPhraseStructure1984,
  title = {Generalized Phrase Structure Grammars, Head Grammars, and Natural Language},
  author = {Pollard, Carl},
  year = {1984},
  school = {Stanford University, CA},
  keywords = {nosource}
}

@article{poratGentleIntroductionTensors2014,
  ids = {poratGentleIntroductionTensors},
  title = {A {{Gentle Introduction}} to {{Tensors}}},
  author = {Porat, Boaz},
  year = {2014},
  pages = {87},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2014\Porat - 2014 - A Gentle Introduction to Tensors.pdf}
}

@misc{porterSnowballLanguageStemming2001,
  title = {Snowball: {{A}} Language for Stemming Algorithms},
  shorttitle = {Snowball},
  author = {Porter, Martin F.},
  year = {2001},
  month = oct,
  url = {https://snowball.tartarus.org/texts/introduction},
  keywords = {⛔ No DOI found,NaturalLanguage(Processing) dipl\textsubscript{l}iteratur information\textsubscript{r}etrieval stemming},
  file = {C:\Users\nhiot\Zotero\storage\JHUK5JZ3\introduction.html}
}

@misc{PowerBIBusiness,
  title = {Q\&{{A}} for {{Power BI}} Business Users - {{Power BI}}},
  journal = {Microsoft},
  url = {https://docs.microsoft.com/en-us/power-bi/consumer/end-user-q-and-a},
  abstract = {Documentation overview topic for Power BI Q\&A natural language queries.},
  langid = {american},
  file = {C:\Users\nhiot\Zotero\storage\K2YDPQWE\end-user-q-and-a.html}
}

@inproceedings{pradhanTaskShAReCLEF2013,
  title = {Task 1: {{ShARe}}/{{CLEF eHealth}} Evaluation Lab 2013},
  booktitle = {Working Notes for {{CLEF}} 2013 Conference , Valencia, Spain, September 23-26, 2013},
  author = {Pradhan, Sameer and Elhadad, No{\'e}mie and South, Brett R. and Mart{\'i}nez, David and Christensen, Lee M. and Vogel, Amy and Suominen, Hanna and Chapman, Wendy W. and Savova, Guergana K.},
  editor = {Forner, Pamela and Navigli, Roberto and Tufis, Dan and Ferro, Nicola},
  year = {2013},
  series = {{{CEUR}} Workshop Proceedings},
  volume = {1179},
  publisher = {CEUR-WS.org},
  url = {http://ceur-ws.org/Vol-1179/CLEF2013wn-CLEFeHealth-PradhanEt2013.pdf},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/clef/PradhanESMCVSCS13.bib},
  keywords = {⛔ No DOI found,nosource},
  timestamp = {Wed, 12 Feb 2020 16:44:31 +0100},
  file = {C:\Users\nhiot\OneDrive\zotero\2013\Pradhan et al. - 2013 - Task 1 ShAReCLEF eHealth evaluation lab 2013.pdf}
}

@misc{preston-wernerSemanticVersioning,
  title = {Semantic {{Versioning}} 2.0.0},
  author = {{Preston-Werner}, Tom},
  journal = {Semantic Versioning},
  url = {https://semver.org/},
  urldate = {2019-01-16},
  abstract = {Semantic Versioning spec and website},
  langid = {english},
  annotation = {00000},
  file = {C:\Users\nhiot\Zotero\storage\66FNCYBX\semver.org.html}
}

@book{prissConceptualStructuresIntegration2002,
  title = {Conceptual {{Structures}}: {{Integration}} and {{Interfaces}}: 10th {{International Conference}} on {{Conceptual Structures}}, {{ICCS}} 2002 {{Borovets}}, {{Bulgaria}}, {{July}} 15-19, 2002 {{Proceedings}}},
  shorttitle = {Conceptual {{Structures}}},
  author = {Priss, Uta and Corbett, Dan and Angelova, Galia},
  year = {2002},
  volume = {2393},
  publisher = {Springer Science \& Business Media},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2002\\Priss et al. - 2002 - Conceptual Structures Integration and Interfaces.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\FIIMBM6L\\books.html}
}

@misc{PropertyGraphSchema,
  title = {Property {{Graph Schema Optimization}} for {{Domain-Specific Knowledge Graphs}} {\textbar} {{IEEE Conference Publication}} {\textbar} {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9458757},
  urldate = {2024-01-03},
  file = {C:\Users\nhiot\Zotero\storage\VRL9C68D\9458757.html}
}

@misc{puigMpuigSpacylookup2020,
  title = {Mpuig/Spacy-Lookup},
  author = {Puig, Marc},
  year = {2020},
  month = jan,
  url = {https://github.com/mpuig/spacy-lookup},
  urldate = {2020-01-27},
  abstract = {Named Entity Recognition based on dictionaries. Contribute to mpuig/spacy-lookup development by creating an account on GitHub.},
  copyright = {MIT},
  keywords = {named-entity-recognition,natural-language-processing,ner,nlp,nosource,spacy,spacy-extension,spacy-pipeline}
}

@misc{PygtriePygtrieDocumentation,
  title = {Pygtrie --- Pygtrie Documentation},
  url = {https://pygtrie.readthedocs.io/en/latest/},
  urldate = {2019-01-24},
  annotation = {00000},
  file = {C:\Users\nhiot\Zotero\storage\FVTIS5PY\latest.html}
}

@misc{Quill,
  title = {Quill},
  journal = {Narrative Science},
  url = {https://narrativescience.com/quill/},
  urldate = {2020-05-14},
  abstract = {Quill turns data into stories and embeds them directly into your favorite dashboards. Learn more about how data storytelling will transform your business.},
  langid = {american},
  file = {C:\Users\nhiot\Zotero\storage\2WMK63ZL\quill.html}
}

@inproceedings{raadDetectionLiensIdentite2017,
  title = {D{\'e}tection de Liens d'identit{\'e} Contextuels Dans Une Base de Connaissances.},
  booktitle = {{{IC}} 2017 - 28es {{Journ{\'e}es}} Francophones d'{{Ing{\'e}nierie}} Des {{Connaissances}}},
  author = {Raad, Joe and Pernelle, Nathalie and Sa{\"i}s, Fatiha},
  editor = {Roussey, Catherine},
  year = {2017},
  month = jul,
  series = {Actes {{IC}} 2017 28es {{Journ{\'e}es}} Francophones d'{{Ing{\'e}nierie}} Des {{Connaissances}}},
  pages = {56--67},
  address = {Caen, France},
  url = {https://hal.archives-ouvertes.fr/hal-01570053},
  urldate = {2019-03-04},
  abstract = {De nombreuses applications du Web de donn{\'e}es exploitent des liens d'identit{\'e}s d{\'e}clar{\'e}s {\`a} l'aide du constructeur owl :sameAs. Cependant, diff{\'e}rentes {\'e}tudes ont montr{\'e} qu'une utilisation abusive de ces liens peut conduire {\`a} des inf{\'e}rences erron{\'e}es ou contradictoires. Dans ce papier nous proposons de calculer des liens d'identit{\'e}s contextuels qui permettent d'expliciter les contextes dans lesquels ces liens sont valides. La notion de contexte que nous proposons est repr{\'e}sent{\'e}e en se basant sur l'ontologie de domaine dans laquelle les instances sont repr{\'e}sent{\'e}es. Nous avons exp{\'e}riment{\'e} cette approche dans le domaine des donn{\'e}es scientifiques o{\`u} les {\'e}l{\'e}ments d{\'e}crivant les exp{\'e}riences partagent rarement un lien d'identit{\'e} tel que d{\'e}fini par owl :sameAs.},
  keywords = {Bases de connaissances,Contextes,Enrichissement,Liage de donn{\'e}es,Ontologies},
  file = {C:\Users\nhiot\OneDrive\zotero\2017\Raad et al. - 2017 - Détection de liens d'identité contextuels dans une.pdf}
}

@article{raadDetectionLiensIdentite2018,
  title = {D{\'e}tection de Liens d'identit{\'e} Erron{\'e}s En Utilisant La D{\'e}tection de Communaut{\'e}s Dans Les Graphes d'identit{\'e}},
  author = {Raad, Joe and BECK, Wouter and Pernelle, Nathalie and Sais, Fatiha and Harmelen, Frank},
  year = {2018},
  month = aug,
  journal = {Ing{\'e}nierie des syst{\`e}mes d'information},
  volume = {23},
  number = {3-4},
  pages = {61--88},
  doi = {10.3166/isi.23.3-4.61-88},
  file = {C:\Users\nhiot\OneDrive\zotero\2018\Raad et al. - 2018 - Détection de liens d’identité erronés en utilisant2.pdf}
}

@article{radfordImprovingLanguageUnderstanding2018,
  ids = {radfordImprovingLanguageUnderstanding},
  title = {Improving Language Understanding by Generative Pre-Training},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year = {2018},
  publisher = {OpenAI},
  url = {https://www.mikecaptain.com/resources/pdf/GPT-1.pdf},
  urldate = {2024-03-20},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2018\Radford et al. - 2018 - Improving language understanding by generative pre.pdf}
}

@book{raflaIncrementalizingRASAOpenSource2019,
  title = {Incrementalizing {{RASA}}'s {{Open-Source Natural Language Understanding Pipeline}}},
  author = {Rafla, Andrew and Kennington, Casey},
  year = {2019},
  month = jul,
  abstract = {As spoken dialogue systems and chatbots are gaining more widespread adoption, commercial and open-sourced services for natural language understanding are emerging. In this paper, we explain how we altered the open-source RASA natural language understanding pipeline to process incrementally (i.e., word-by-word), following the incremental unit framework proposed by Schlangen and Skantze. To do so, we altered existing RASA components to process incrementally, and added an update-incremental intent recognition model as a component to RASA. Our evaluations on the Snips dataset show that our changes allow RASA to function as an effective incremental natural language understanding service.},
  file = {C:\Users\nhiot\OneDrive\zotero\2019\Rafla et Kennington - 2019 - Incrementalizing RASA's Open-Source Natural Langua.pdf}
}

@misc{ramanSpacyRedisMagic2019,
  title = {Spacy + {{Redis}} = {{Magic}}},
  author = {Raman, Venkat},
  year = {2019},
  month = sep,
  journal = {Medium},
  url = {https://towardsdatascience.com/spacy-redis-magic-60f25c21303d},
  urldate = {2019-12-12},
  abstract = {Word similarity task optimized through Spacy and Redis},
  langid = {english},
  file = {C:\Users\nhiot\Zotero\storage\PUAVA7FL\spacy-redis-magic-60f25c21303d.html}
}

@inproceedings{rameshSurveyDesignTechniques2017,
  ids = {rameshSurveyDesignTechniques2017a},
  title = {A {{Survey}} of {{Design Techniques}} for {{Conversational Agents}}},
  booktitle = {Information, {{Communication}} and {{Computing Technology}}},
  author = {Ramesh, Kiran and Ravishankaran, Surya and Joshi, Abhishek and Chandrasekaran, K.},
  editor = {Kaushik, Saroj and Gupta, Daya and Kharb, Latika and Chahal, Deepak},
  year = {2017},
  series = {Communications in {{Computer}} and {{Information Science}}},
  pages = {336--350},
  publisher = {Springer},
  address = {Singapore},
  doi = {10/gjbdjj},
  abstract = {A conversational agent also referred to as chatbot is a computer program which tries to generate human like responses during a conversation. Earlier chatbots employed much simpler retrieval based pattern matching design techniques. However, with time a number of new chatbots evolved with an aim to make it more human like and hence to pass the Turing test. Now, most of the chatbots employ generative knowledge based techniques. This paper will discuss about various chatbot design techniques, classification of chatbot and discussion on how the modern chatbots have evolved from simple pattern matching, retrieval based model to modern complex knowledge based models. A table of major conversational agents in chronological order along with their design techniques is also provided at the end of the paper.},
  isbn = {978-981-10-6544-6},
  langid = {english},
  keywords = {AIML,Chatscript,Long short term memory network (LSTM),Markov chains,Ontologies,Parsing,Pattern matching,Recurrent neural network (RNN),Sequence to sequence model (seq2seq)},
  file = {C:\Users\nhiot\Zotero\storage\PMH33WCM\978-981-10-6544-6_31.html}
}

@inproceedings{ramshawTextChunkingUsing1995,
  ids = {ramshaw-marcus-1995-text,ramshawTextChunkingUsing1995a},
  title = {Text {{Chunking Using Transformation-Based Learning}}},
  booktitle = {Third {{Workshop}} on {{Very Large Corpora}}},
  author = {Ramshaw, Lance A. and Marcus, Mitchell P.},
  editor = {Ide, Nancy and V{\'e}ronis, Jean and Armstrong, Susan and Church, Kenneth and Isabelle, Pierre and Manzi, Sandra and Tzoukermann, Evelyne and Yarowsky, David},
  year = {1995},
  month = may,
  series = {Text, {{Speech}} and {{Language Technology}}},
  volume = {11},
  eprint = {cmp-lg/9505040},
  pages = {157--176},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-017-2390-9_10},
  urldate = {2024-03-22},
  abstract = {Eric Brill introduced transformation-based learning and showed that it can do part-of-speech tagging with fairly high accuracy. The same method can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive ``baseNP'' chunks. For this purpose, it is convenient to view chunking as a tagging problem by encoding the chunk structure in new tags attached to each word. In automatic tests using Treebank-derived data, this technique achieved recall and precision rates of roughly 92\% for baseNP chunks and 88\% for somewhat more complex chunks that partition the sentence. Some interesting adaptations to the transformation-based learning approach are also suggested by this application.},
  archiveprefix = {arxiv},
  isbn = {978-90-481-5349-7 978-94-017-2390-9},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1995\\Ramshaw et Marcus - 1995 - Text Chunking Using Transformation-Based Learning.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\ZWF7ZGYA\\9505040.html}
}

@inproceedings{rashkinEvent2MindCommonsenseInference2018,
  title = {{{Event2Mind}}: {{Commonsense Inference}} on {{Events}}, {{Intents}}, and {{Reactions}}},
  shorttitle = {{{Event2Mind}}},
  booktitle = {{{ACL}}},
  author = {Rashkin, Hannah and Sap, Maarten and Allaway, Emily and Smith, Noah A. and Choi, Yejin},
  year = {2018},
  eprint = {1805.06939},
  doi = {10/gf8fnb},
  abstract = {We investigate a new commonsense inference task: given an event described in a short free-form text ("X drinks coffee in the morning"), a system reasons about the likely intents ("X wants to stay awake") and reactions ("X feels alert") of the event's participants. To support this study, we construct a new crowdsourced corpus of 25,000 event phrases covering a diverse range of everyday events and situations. We report baseline performance on this task, demonstrating that neural encoder-decoder models can successfully compose embedding representations of previously unseen events and reason about the likely intents and reactions of the event participants. In addition, we demonstrate how commonsense inference on people's intents and reactions can help unveil the implicit gender inequality prevalent in modern movie scripts.},
  archiveprefix = {arxiv},
  keywords = {Baseline (configuration management),Commonsense reasoning,Crowdsourcing,Encoder,Social inequality},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2018\\Rashkin et al. - 2018 - Event2Mind Commonsense Inference on Events, Inten.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\9Z69Z3ZT\\1805.html}
}

@inproceedings{rauExtractingCompanyNames1991,
  title = {Extracting Company Names from Text},
  booktitle = {The {{Seventh IEEE Conference}} on {{Artificial Intelligence Application}}},
  author = {Rau, L. F.},
  year = {1991},
  volume = {1},
  pages = {29--32},
  publisher = {IEEE},
  doi = {10.1109/CAIA.1991.120841},
  urldate = {2024-03-20},
  keywords = {⛔ No DOI found}
}

@article{RealtimeDatabase2019,
  title = {Real-Time Database},
  year = {2019},
  month = jan,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Real-time\_database\&oldid=877746190},
  urldate = {2019-01-23},
  abstract = {A real-time database is a database system which uses real-time processing to handle workloads whose state is constantly changing. This differs from traditional databases containing persistent data, mostly unaffected by time. For example, a stock market changes very rapidly and is dynamic. The graphs of the different markets appear to be very unstable and yet a database has to keep track of current values for all of the markets of the New York Stock Exchange. Real-time processing means that a transaction is processed fast enough for the result to come back and be acted on right away. Real-time databases are useful for accounting, banking, law, medical records, multi-media, process control, reservation systems, and scientific data analysis.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {00000  Page Version ID: 877746190},
  file = {C:\Users\nhiot\Zotero\storage\NN3J6TUG\index.html}
}

@misc{REDACCorpusTexte2008,
  title = {{{REDAC}} : {{Corpus}} Texte {{Wikip{\'e}diaFR2008}}},
  year = {2008},
  month = jun,
  url = {http://redac.univ-tlse2.fr/corpus/wikipedia.html},
  urldate = {2023-10-06},
  abstract = {Le corpus Wikip{\'e}dia-FR a {\'e}t{\'e} constitu{\'e} {\`a} partir du dump de la version fran{\c c}aise de l'encyclop{\'e}die Wikip{\'e}dia du 18/06/2008. Ce dump correspond {\`a} la version << HTML statique >> disponible {\`a} l'adresse : http://dumps.wikimedia.org/. Les traitements appliqu{\'e}s {\`a} ce dump sont minimaux et ont consist{\'e} {\`a} extraire les parties textuelles des articles. Les sommaires num{\'e}rot{\'e}s de d{\'e}but d'articles ont {\'e}t{\'e} supprim{\'e}s, ainsi que les sections << voir aussi >> (liens vers des r{\'e}f{\'e}rences externes). Les parties << notes >> ont {\'e}t{\'e} conserv{\'e}es. Le corpus a {\'e}t{\'e} {\'e}tiquet{\'e} morphosyntaxiquement avec TreeTagger, de l'Universit{\'e} de Stuttgart.},
  file = {C:\Users\nhiot\Zotero\storage\XEYQ9D4E\wikipedia.html}
}

@misc{Redis,
  title = {Redis},
  url = {https://redis.io/},
  urldate = {2019-12-12},
  file = {C:\Users\nhiot\Zotero\storage\X4HWFRKZ\redis.io.html}
}

@misc{RedisSystemProperties,
  title = {Redis {{System Properties}}},
  url = {https://db-engines.com/en/system/Redis},
  urldate = {2019-12-12},
  file = {C:\Users\nhiot\Zotero\storage\GHXZH6A5\Redis.html}
}

@inproceedings{reiterLogicalReconstructionRelational1989,
  title = {Towards a {{Logical Reconstruction}} of {{Relational Database Theory}}},
  booktitle = {Readings in {{Artificial Intelligence}} and {{Databases}}},
  author = {Reiter, Raymond},
  editor = {Mylopolous, John and Brodie, Michael},
  year = {1989},
  month = jan,
  pages = {301--327},
  publisher = {Elsevier},
  address = {San Francisco (CA)},
  doi = {10.1016/B978-0-934613-53-8.50025-X},
  urldate = {2023-08-07},
  abstract = {Insofar as database theory can be said to owe a debt to logic, the currency on loan is model theoretic in the sense that a database can be viewed as a particular kind of first order interpretation, and query evaluation is a process of truth Junctional evaluation of first order formulae with respect to this interpretation. It is this model theoretic paradigm which leads, for example, to many valued propositional logies for databases with null values. In this chapter I argue that a proof theoretic view of databases is possible, and indeed much more fruitful. Specifically, I show how relational databases can be seen as special theories of first order logic, namely theories incorporating the following assumptions:1.The domain closure assumption. The individuals occurring in the database are all and only the existing individuals.2.The unique name assumption. Individuals with distinct names are distinct.3.The closed world assumption. The only possible instances of a relation are those implied by the database. It will follow that a proof theoretic paradigm for relational databases provides a correct treatment of:1.Query evaluation for databases that have incomplete information, including null values.2.Integrity constraints and their enforcement.3.Conceptual modelling and the extension of the relational model to incorporate more real world semantics.},
  isbn = {978-0-934613-53-8},
  langid = {english},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1989\\Reiter - 1989 - Towards a Logical Reconstruction of Relational Dat.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\4VPCJ9R4\\B978093461353850025X.html}
}

@article{reiterSoundSometimesComplete1986,
  title = {A Sound and Sometimes Complete Query Evaluation Algorithm for Relational Databases with Null Values},
  author = {Reiter, Raymond},
  year = {1986},
  month = apr,
  journal = {Journal of the ACM (JACM)},
  volume = {33},
  number = {2},
  pages = {349--370},
  issn = {0004-5411},
  doi = {10.1145/5383.5388},
  abstract = {A sound and, in certain cases, complete method is described for evaluating queries in relational databases with null values where these nulls represent existing but unknown individuals. The soundness and completeness results are proved relative to a formalization of such databases as suitable theories of first-order logic. Because the algorithm conforms to the relational algebra, it may easily be incorporated into existing relational systems.},
  keywords = {nosource},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1986\\Reiter - 1986 - A sound and sometimes complete query evaluation al.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\J9PHZZNY\\5383.html}
}

@inproceedings{rensinkRepresentingFirstOrderLogic2004,
  title = {Representing {{First-Order Logic Using Graphs}}},
  booktitle = {Graph {{Transformations}}},
  author = {Rensink, Arend},
  editor = {Ehrig, Hartmut and Engels, Gregor and {Parisi-Presicce}, Francesco and Rozenberg, Grzegorz},
  year = {2004},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {319--335},
  publisher = {Springer Berlin Heidelberg},
  abstract = {We show how edge-labelled graphs can be used to represent first-order logic formulae. This gives rise to recursively nested structures, in which each level of nesting corresponds to the negation of a set of existentials. The model is a direct generalisation of the negative application conditions used in graph rewriting, which count a single level of nesting and are thereby shown to correspond to the fragment {$\exists$} {\textlnot} {$\exists$} of first-order logic. Vice versa, this generalisation may be used to strengthen the notion of application conditions. We then proceed to show how these nested models may be flattened to (sets of) plain graphs, by allowing some structure on the labels. The resulting formulae-as-graphs may form the basis of a unification of the theories of graph transformation and predicate transformation.},
  isbn = {978-3-540-30203-2},
  langid = {english},
  keywords = {Application Condition,Depth Indicator,Graph Condition,Graph Grammar,Graph Transformation},
  annotation = {00000},
  file = {C:\Users\nhiot\OneDrive\zotero\2004\Rensink - 2004 - Representing First-Order Logic Using Graphs.pdf}
}

@misc{ReSEauxNumeriquesDonnees,
  title = {{r{\'e}SEaux Num{\'e}riques de Donn{\'e}es s{\'e}mantiques: Utilit{\'e} et vie Priv{\'e}e}},
  shorttitle = {{r{\'e}SEaux Num{\'e}riques de Donn{\'e}es s{\'e}mantiques}},
  journal = {Agence nationale de la recherche},
  url = {https://anr.fr/Projet-ANR-18-CE23-0010},
  urldate = {2024-02-06},
  abstract = {La quantit{\'e} de donn{\'e}es produites par les particuliers et les entreprises a explos{\'e} durant les derni{\`e}res d{\'e}cennies. Leur exploitation offre des opportunit{\'e}s mais questionne le respect de la vie priv{\'e}e. Tandis que les concepts de donn{\'e}es li{\'e}es et ouvertes gagnent en importance, le grand public exprime une m{\'e}fiance croissante vis-{\`a}-vis de l'exploitation des donn{\'e}es personnelles. Cela conduit {\`a} un nouveau d{\'e}fi : comment pr{\'e}server la vie priv{\'e}e tout en fournissant des donn{\'e}es utilisables?},
  langid = {french},
  file = {C:\Users\nhiot\Zotero\storage\QTUVRMLG\Projet-ANR-18-CE23-0010.html}
}

@misc{ResourceDescriptionFramework,
  title = {Resource {{Description Framework}} ({{RDF}}) {{Schema Specification}}},
  url = {https://www.w3.org/TR/1999/PR-rdf-schema-19990303/},
  urldate = {2019-06-12},
  file = {C:\Users\nhiot\Zotero\storage\TG43QI8S\PR-rdf-schema-19990303.html}
}

@article{rigauxBasesDonneesDocumentaires2018,
  title = {Bases de Donn{\'e}es Documentaires et Distribu{\'e}es},
  author = {Rigaux, Philippe},
  year = {2018},
  month = sep,
  pages = {447},
  keywords = {⛔ No DOI found},
  annotation = {00000},
  file = {C:\Users\nhiot\OneDrive\zotero\2018\Rigaux - 2018 - Bases de données documentaires et distribuées.pdf}
}

@book{robinsonGraphDatabasesNew2015,
  title = {Graph {{Databases}}: {{New Opportunities}} for {{Connected Data}}},
  shorttitle = {Graph {{Databases}}},
  author = {Robinson, Ian and Webber, Jim and Eifrem, Emil},
  year = {2015},
  month = jun,
  publisher = {"O'Reilly Media, Inc."},
  abstract = {Discover how graph databases can help you manage and query highly connected data. With this practical book, you'll learn how to design and implement a graph database that brings the power of graphs to bear on a broad range of problem domains. Whether you want to speed up your response to user queries or build a database that can adapt as your business evolves, this book shows you how to apply the schema-free graph model to real-world problems.This second edition includes new code samples and diagrams, using the latest Neo4j syntax, as well as information on new functionality. Learn how different organizations are using graph databases to outperform their competitors. With this book's data modeling, query, and code examples, you'll quickly be able to implement your own solution.Model data with the Cypher query language and property graph modelLearn best practices and common pitfalls when modeling with graphsPlan and implement a graph database solution in test-driven fashionExplore real-world examples to learn how and why organizations use a graph databaseUnderstand common patterns and components of graph database architectureUse analytical techniques and algorithms to mine graph database information},
  googlebooks = {RTvcCQAAQBAJ},
  isbn = {978-1-4919-3086-1},
  langid = {english},
  keywords = {Computers / Data Science / Data Analytics,Computers / Data Science / Data Modeling \& Design,Computers / Data Science / Data Visualization,Computers / Data Science / Data Warehousing,Computers / Data Science / General},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2015\\Robinson et al. - 2015 - Graph Databases New Opportunities for Connected D.epub;C\:\\Users\\nhiot\\OneDrive\\zotero\\2015\\Robinson et al. - 2015 - Graph Databases New Opportunities for Connected D.pdf}
}

@misc{rogersLdtLinguisticDiagnostics,
  title = {Ldt: {{Linguistic}} Diagnostics for Word Embeddings},
  shorttitle = {Ldt},
  author = {Rogers, Anna},
  url = {http://ldtoolkit.space/},
  urldate = {2020-01-21},
  copyright = {Apache Software License},
  keywords = {Text Processing - Linguistic},
  file = {C:\Users\nhiot\Zotero\storage\79Q6JPUU\ldt.html}
}

@article{rothnieIntroductionSystemDistributed1980,
  title = {Introduction to a {{System}} for {{Distributed Databases}} ({{SDD-1}})},
  author = {Rothnie, Jr., J. B. and Bernstein, P. A. and Fox, S. and Goodman, N. and Hammer, M. and Landers, T. A. and Reeve, C. and Shipman, D. W. and Wong, E.},
  year = {1980},
  month = mar,
  journal = {ACM Trans. Database Syst.},
  volume = {5},
  number = {1},
  pages = {1--17},
  issn = {0362-5915},
  doi = {10/cqv8kz},
  urldate = {2019-01-17},
  abstract = {The declining cost of computer hardware and the increasing data processing needs of geographically dispersed organizations have led to substantial interest in distributed data management. SDD-1 is a distributed database management system currently being developed by Computer Corporation of America. Users interact with SDD-1 precisely as if it were a nondistributed database system because SDD-1 handles all issues arising from the distribution of data. These issues include distributed concurrency control, distributed query processing, resiliency to component failure, and distributed directory management. This paper presents an overview of the SDD-1 design and its solutions to the above problems. This paper is the first of a series of companion papers on SDD-1 (Bernstein and Shipman [2], Bernstein et al. [4], and Hammer and Shipman [14]).},
  keywords = {concurrency control,database reliability,distributed database system,query processing,relational data model},
  annotation = {00000},
  file = {C:\Users\nhiot\OneDrive\zotero\1980\Rothnie et al. - 1980 - Introduction to a System for Distributed Databases.pdf}
}

@article{rovettoCausalityOntologyDisease2015,
  title = {Causality and the Ontology of Disease},
  author = {Rovetto, Robert J. and Mizoguchi, Riichiro},
  year = {2015},
  month = jan,
  journal = {Applied Ontology},
  volume = {10},
  number = {2},
  pages = {79--105},
  issn = {1570-5838},
  doi = {10/gfwssm},
  urldate = {2019-03-12},
  abstract = {The goal of this paper is two-fold: first, to emphasize causality in disease ontology and knowledge representation, presenting a general and cursory discussion of causality and causal chains; and second, to clarify and develop the River Flow Model of},
  langid = {english},
  annotation = {00000},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2015\\Rovetto et Mizoguchi - 2015 - Causality and the ontology of disease.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\WWMRUSLB\\ao147.html}
}

@article{royFastBesteffortSearch2014,
  title = {Fast Best-Effort Search on Graphs with Multiple Attributes},
  author = {Roy, Senjuti Basu and {Eliassi-Rad}, Tina and Papadimitriou, Spiros},
  year = {2014},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {27},
  number = {3},
  pages = {755--768},
  publisher = {IEEE},
  doi = {10/f62xg7},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2014\\Roy et al. - 2014 - Fast best-effort search on graphs with multiple at.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\4EGPAVQ6\\6877688.html}
}

@book{Rozenberg:1997:HGG:278918,
  title = {Handbook of Graph Grammars and Computing by Graph Transformation: {{Volume I}}. {{Foundations}}},
  editor = {Rozenberg, Grzegorz},
  year = {1997},
  publisher = {World Scientific Publishing Co., Inc.},
  address = {River Edge, NJ, USA},
  isbn = {981-02-2884-8},
  keywords = {nosource}
}

@article{ruanQAnalysisQuestionanswerDriven2019,
  title = {{{QAnalysis}}: A Question-Answer Driven Analytic Tool on Knowledge Graphs for Leveraging Electronic Medical Records for Clinical Research},
  shorttitle = {{{QAnalysis}}},
  author = {Ruan, Tong and Huang, Yueqi and Liu, Xuli and Xia, Yuhang and Gao, Ju},
  year = {2019},
  month = apr,
  journal = {BMC Medical Informatics and Decision Making},
  volume = {19},
  number = {1},
  pages = {1--13},
  publisher = {BioMed Central},
  issn = {1472-6947},
  doi = {10/gj7chh},
  urldate = {2021-07-05},
  abstract = {While doctors should analyze a large amount of electronic medical record (EMR) data to conduct clinical research, the analyzing process requires information technology (IT) skills, which is difficult for most doctors in China.},
  keywords = {Context-free grammar,Electronic medical record,Graph database,Statistical question answering},
  annotation = {QID: Q64089395},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2019\\Ruan et al. - 2019 - QAnalysis a question-answer driven analytic tool .pdf;C\:\\Users\\nhiot\\Zotero\\storage\\FQ8TV6JA\\s12911-019-0798-8.html}
}

@inproceedings{saggionOntologyBasedInformationExtraction2007,
  title = {Ontology-{{Based Information Extraction}} for {{Business Intelligence}}},
  booktitle = {The {{Semantic Web}}},
  author = {Saggion, Horacio and Funk, Adam and Maynard, Diana and Bontcheva, Kalina},
  editor = {Aberer, Karl and Choi, Key-Sun and Noy, Natasha and Allemang, Dean and Lee, Kyung-Il and Nixon, Lyndon and Golbeck, Jennifer and Mika, Peter and Maynard, Diana and Mizoguchi, Riichiro and Schreiber, Guus and {Cudr{\'e}-Mauroux}, Philippe},
  year = {2007},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {843--856},
  publisher = {Springer Berlin Heidelberg},
  abstract = {Business Intelligence (BI) requires the acquisition and aggregation of key pieces of knowledge from multiple sources in order to provide valuable information to customers or feed statistical BI models and tools. The massive amount of information available to business analysts makes information extraction and other natural language processing tools key enablers for the acquisition and use of that semantic information. We describe the application of ontology-based extraction and merging in the context of a practical e-business application for the EU MUSING Project where the goal is to gather international company intelligence and country/region information. The results of our experiments so far are very promising and we are now in the process of building a complete end-to-end solution.},
  isbn = {978-3-540-76298-0},
  langid = {english},
  keywords = {Business Intelligence,Cross-source Entity Coreference,Ontology-based Information Extraction},
  file = {C:\Users\nhiot\OneDrive\zotero\2007\Saggion et al. - 2007 - Ontology-Based Information Extraction for Business.pdf}
}

@article{saitoTreeSimplificationPlateaux2014,
  title = {Tree Simplification and the 'plateaux' Phenomenon of Graph {{Laplacian}} Eigenvalues},
  author = {Saito, Naoki and Woei, Ernest},
  year = {2014},
  month = oct,
  journal = {arXiv:1410.7842 [cs]},
  eprint = {1410.7842},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1410.7842},
  urldate = {2021-11-05},
  abstract = {We developed a procedure of reducing the number of vertices and edges of a given tree, which we call the "tree simplification procedure," without changing its topological information. Our motivation for developing this procedure was to reduce computational costs of graph Laplacian eigenvalues of such trees. When we applied this procedure to a set of trees representing dendritic structures of retinal ganglion cells of a mouse and computed their graph Laplacian eigenvalues, we observed two "plateaux" (i.e., two sets of multiple eigenvalues) in the eigenvalue distribution of each such simplified tree. In this article, after describing our tree simplification procedure, we analyze why such eigenvalue plateaux occur in a simplified tree, and explain such plateaux can occur in a more general graph if it satisfies a certain condition, identify these two eigenvalues specifically as well as the lower bound to their multiplicity.},
  archiveprefix = {arxiv},
  keywords = {{05C07, 05C50, 15A42, 65F15},⛔ No DOI found,Computer Science - Discrete Mathematics},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2014\\Saito et Woei - 2014 - Tree simplification and the 'plateaux' phenomenon .pdf;C\:\\Users\\nhiot\\Zotero\\storage\\7IDGYRNT\\1410.html}
}

@book{sakuraiTheoryApplicationsAdvanced2012,
  title = {Theory and {{Applications}} for {{Advanced Text Mining}}},
  author = {Sakurai, Shigeaki},
  year = {2012},
  month = nov,
  publisher = {BoD -- Books on Demand},
  abstract = {Due to the growth of computer technologies and web technologies, we can easily collect and store large amounts of text data. We can believe that the data include useful knowledge. Text mining techniques have been studied aggressively in order to extract the knowledge from the data since late 1990s. Even if many important techniques have been developed, the text mining research field continues to expand for the needs arising from various application fields. This book is composed of 9 chapters introducing advanced text mining techniques. They are various techniques from relation extraction to under or less resourced language. I believe that this book will give new knowledge in the text mining field and help many readers open their new research fields.},
  googlebooks = {EfqdDwAAQBAJ},
  isbn = {978-953-51-0852-8},
  langid = {english},
  keywords = {Computers / General,Computers / Human-Computer Interaction (HCI)}
}

@inproceedings{savaryRelationExtractionClinical2022,
  title = {Relation {{Extraction}} from~{{Clinical Cases}} for~a~{{Knowledge Graph}}},
  booktitle = {European {{Conference}} on {{Advances}} in {{Databases}} and {{Information Systems}}},
  author = {Savary, Agata and Silvanovich, Alena and Minard, Anne-Lyse and Hiot, Nicolas and {Halfeld-Ferrari}, Mirian},
  editor = {Chiusano, Silvia and Cerquitelli, Tania and Wrembel, Robert and N{\o}rv{\aa}g, Kjetil and Catania, Barbara and {Vargas-Solar}, Genoveva and Zumpano, Ester},
  year = {2022},
  series = {Communications in {{Computer}} and {{Information Science}}},
  pages = {353--365},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-15743-1_33},
  abstract = {We describe a system for automatic extraction of semantic relations between entities in a medical corpus of clinical cases. It builds upon a previously developed module for entity extraction and upon a morphosyntactic parser. It uses experimentally designed rules based on syntactic dependencies and trigger words, as well as on sequencing and nesting of entities of particular types. The results obtained on a small corpus are promising. Our larger perspective is transforming information extracted from medical texts into knowledge graphs.},
  isbn = {978-3-031-15743-1},
  langid = {english},
  keywords = {me,nosource},
  file = {C:\Users\nhiot\OneDrive\zotero\2022\Savary et al. - 2022 - Relation Extraction from Clinical Cases for a Know.pdf}
}

@article{savovaMayoClinicalText2010,
  ids = {savovaMayoClinicalText2010a},
  title = {Mayo Clinical {{Text Analysis}} and {{Knowledge Extraction System}} ({{cTAKES}}): Architecture, Component Evaluation and Applications},
  shorttitle = {Mayo Clinical {{Text Analysis}} and {{Knowledge Extraction System}} ({{cTAKES}})},
  author = {Savova, Guergana K. and Masanz, James J. and Ogren, Philip V. and Zheng, Jiaping and Sohn, Sunghwan and {Kipper-Schuler}, Karin C. and Chute, Christopher G.},
  year = {2010},
  month = sep,
  journal = {Journal of the American Medical Informatics Association},
  volume = {17},
  number = {5},
  pages = {507--513},
  publisher = {Oxford Academic},
  issn = {1067-5027},
  doi = {10/bfxww4},
  urldate = {2020-11-20},
  abstract = {Abstract. We aim to build and evaluate an open-source natural language processing system for information extraction from electronic medical record clinical free},
  langid = {english},
  annotation = {QID: Q34371773},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2010\\Savova et al. - 2010 - Mayo clinical Text Analysis and Knowledge Extracti.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\33ZR3N7J\\830823.html}
}

@article{scheweLimitationsRuleTriggering1998,
  ids = {ScT98},
  title = {Limitations of Rule Triggering Systems for Integrity Maintenance in the Context of Transition Specifications},
  author = {Schewe, Klaus-Dieter and Thalheim, Bernhard},
  year = {1998},
  month = jan,
  journal = {Acta Cybernetica},
  volume = {13},
  number = {3},
  pages = {277--304},
  publisher = {University of Szeged},
  issn = {2676-993X},
  doi = {10.1007/3-540-63699-4_12},
  urldate = {2023-08-08},
  abstract = {Integrity Maintenance is considered one of the major application fields of rule triggering systems (RTSs). In the case of a given integrity constraint being violated by a database transition these systems trigger repairing actions. Then it is necessary to guarantee the termination of the RTS, its determinacy and the consistency of final states. Transition specifications provide some kind of dynamic semantics requiring certasin effects on database states to occur. In the context of transition specifications integrity maintenance has to cope with the additional problem of effect preservation. Limitations of RTSs with respect to this extended problems are investigated. It will be shown that for any set of constraints there exist non-repairable transitions, which depend on the closure of the constraint set. This implies that integrity maintenance by RTSs is only possible, if the constraint implication problem is decidable. Even if unrepairable transitions are excluded, this does not prevent the RTS to produce undesired behaviour. Analyzing the behaviour of RTSs leads to the definition of critical paths in associated rule hypergraphs and the requirement of such paths being absent. It will be shown that this requirement can be satisfied if the underlying set of constraints is stratified, but this notion turns out to be too strong to be also necessary. A sufficient and necessary condition for the absence of critical paths is obtained, if sets of constraints are required to be locally stratified.},
  copyright = {Copyright (c)},
  langid = {english},
  optbibsource = {dblp computer science bibliography, https://dblp.org},
  opttimestamp = {Tue, 09 Jul 2013 18:10:34 +0200},
  keywords = {nosource},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1998\\Schewe et Thalheim - 1998 - Limitations of rule triggering systems for integri2.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\QNHWPH6A\\294150.html}
}

@article{scheweTheoryConsistencyEnforcement1999,
  title = {Towards a Theory of Consistency Enforcement},
  author = {Schewe, Klaus-Dieter and Thalheim, Bernhard},
  year = {1999},
  month = feb,
  journal = {Acta Informatica},
  volume = {36},
  number = {2},
  pages = {97--141},
  issn = {1432-0525},
  doi = {10.1007/s002360050155},
  urldate = {2023-08-16},
  abstract = {State oriented specifications with invariants occur in almost all formal specification languages. Hence the problem is to prove the consistency of the specified operations with respect to the invariants. Whilst the problem seems to be easily solvable in predicative specifications, it usually requires sophisticated verification efforts, when specifications in the style of Dijkstra's guarded commands as e.g. in the specification language B are used. As an alternative consistency enforcement will be discussed in this paper. The basic idea is to replace inconsistent operations by new consistent ones preserving at the same time the intention of the old one. More precisely, this can be formalized by consistent spezializations, where specialization is a specific partial order on operations defined via predicate transformers. It will be shown that greatest consistent specializations (GCSs) always exist and are compatible with conjunctions of invariants. Then under certain mild restrictions the general construction of such GCSs is possible. Precisely, given the GCSs of simple basic assignments the GCS of a complex operation results from replacing involved assignments by their GCSs and the investigation of a guard. In general, GCS construction can be embedded in refinement calculi and therefore strengthens the systematic development of correct programs.},
  langid = {english},
  keywords = {Formal Specification,General Construction,Partial Order,Specification Language,Systematic Development},
  annotation = {QID: Q57376473},
  file = {C:\Users\nhiot\OneDrive\zotero\1999\Schewe et Thalheim - 1999 - Towards a theory of consistency enforcement.pdf}
}

@article{schmidtSP2benchSPARQLPerformance2008,
  title = {{{SP2bench}}: {{A SPARQL}} Performance Benchmark},
  shorttitle = {{{SP2bench}}},
  author = {Schmidt, Michael and Schallhorn, Thomas and Lausen, Georg and Pinkel, Christoph},
  year = {2008},
  month = jul,
  journal = {Semantic Web Information Management: A Model-Based Perspective},
  issn = {978-3-642-04328-4},
  doi = {10/fn23xq},
  abstract = {Recently, the SPARQL query language for RDF has reached the W3C recommendation status. In response to this emerging standard, the database community is currently exploring efficient storage techniques for RDF data and evaluation strategies for SPARQL queries. A meaningful analysis and comparison of these approaches necessitates a comprehensive and universal benchmark platform. To this end, we have developed SP\$\^{}2\$Bench, a publicly available, language-specific SPARQL performance benchmark. SP\$\^{}2\$Bench is settled in the DBLP scenario and comprises both a data generator for creating arbitrarily large DBLP-like documents and a set of carefully designed benchmark queries. The generated documents mirror key characteristics and social-world distributions encountered in the original DBLP data set, while the queries implement meaningful requests on top of this data, covering a variety of SPARQL operator constellations and RDF access patterns. As a proof of concept, we apply SP\$\^{}2\$Bench to existing engines and discuss their strengths and weaknesses that follow immediately from the benchmark results.},
  annotation = {QID: Q57955220},
  file = {C:\Users\nhiot\OneDrive\zotero\2008\Schmidt et al. - 2008 - SP2bench A SPARQL performance benchmark.pdf}
}

@misc{SCOWLFriends,
  title = {{{SCOWL}} ({{And Friends}})},
  url = {http://wordlist.aspell.net/},
  urldate = {2019-12-12},
  file = {C:\Users\nhiot\Zotero\storage\3YYBPXZ5\wordlist.aspell.net.html}
}

@incollection{Segura2008,
  title = {Automated Merging of Feature Models Using Graph Transformations},
  booktitle = {Generative and Transformational Techniques in Software Engineering {{II}}: {{International}} Summer School, {{GTTSE}} 2007, Braga, Portugal, July 2-7, 2007. {{Revised}} Papers},
  author = {Segura, Sergio and Benavides, David and {Ruiz-Cort{\'e}s}, Antonio and Trinidad, Pablo},
  year = {2008},
  pages = {489--505},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  keywords = {nosource}
}

@misc{SENDUPSEmanticNetwork,
  title = {{SENDUP -- SEmantic Network of Data: Utility and Privacy}},
  shorttitle = {{SENDUP -- SEmantic Network of Data}},
  url = {https://www.univ-orleans.fr/lifo/evenements/sendup-project/},
  urldate = {2024-02-06},
  langid = {french},
  file = {C:\Users\nhiot\Zotero\storage\STM6E5CY\sendup-project.html}
}

@misc{Sense2vecSpaCyGensim,
  title = {Sense2vec with {{spaCy}} and {{Gensim}} {$\cdot$} {{Blog}} {$\cdot$} {{Explosion AI}}},
  journal = {Explosion AI},
  url = {https://explosion.ai/blog/sense2vec-with-spacy},
  urldate = {2019-01-23},
  abstract = {If you were doing text analytics in 2015, you were probably using word2vec. Sense2vec (Trask et. al, 2015) is a new twist on word2vec that lets you learn more interesting, detailed and context-sensitive word vectors. This post motivates the idea, explains our implementation, and comes with an interactive demo that we've found surprisingly addictive.},
  langid = {english},
  annotation = {00000},
  file = {C:\Users\nhiot\Zotero\storage\RXG8VNRG\sense2vec-with-spacy.html}
}

@article{SentimentAnalysis2019,
  title = {Sentiment Analysis},
  year = {2019},
  month = jan,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Sentiment\_analysis\&oldid=877438534},
  urldate = {2019-01-22},
  abstract = {Opinion mining (sometimes known as sentiment analysis or emotion AI) refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {00000  Page Version ID: 877438534},
  file = {C:\Users\nhiot\Zotero\storage\6QZH3K6G\index.html}
}

@inproceedings{seretanCollocationTranslationBased2007,
  title = {{Collocation translation based on sentence alignment and parsing}},
  booktitle = {{Actes de la 14{\`e}me conf{\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs}},
  author = {Seretan, Violeta and Wehrli, {\'E}ric},
  year = {2007},
  month = jun,
  pages = {375--384},
  publisher = {IRIT / ATALA},
  address = {Toulouse, France},
  url = {https://aclanthology.org/2007.jeptalnrecital-long.37},
  urldate = {2023-10-23},
  abstract = {Bien que de nombreux efforts aient {\'e}t{\'e} d{\'e}ploy{\'e}s pour extraire des collocations {\`a} partir de corpus de textes, seule une minorit{\'e} de travaux se pr{\'e}occupent aussi de rendre le r{\'e}sultat de l'extraction pr{\^e}t {\`a} {\^e}tre utilis{\'e} dans les applications TAL qui pourraient en b{\'e}n{\'e}ficier, telles que la traduction automatique. Cet article d{\'e}crit une m{\'e}thode pr{\'e}cise d'identification de la traduction des collocations dans un corpus parall{\`e}le, qui pr{\'e}sente les avantages suivants : elle peut traiter des collocation flexibles (et pas seulement fig{\'e}es) ; elle a besoin de ressources limit{\'e}es et d'un pouvoir de calcul raisonnable (pas d'alignement complet, pas d'entra{\^i}nement) ; elle peut {\^e}tre appliqu{\'e}e {\`a} plusieurs paires des langues et fonctionne m{\^e}me en l'absence de dictionnaires bilingues. La m{\'e}thode est bas{\'e}e sur l'information syntaxique provenant du parseur multilingue Fips. L'{\'e}valuation effectu{\'e}e sur 4000 collocations de type verbe-objet correspondant {\`a} plusieurs paires de langues a montr{\'e} une pr{\'e}cision moyenne de 89.8\% et une couverture satisfaisante (70.9\%). Ces r{\'e}sultats sont sup{\'e}rieurs {\`a} ceux enregistr{\'e}s dans l'{\'e}valuation d'autres m{\'e}thodes de traduction de collocations.},
  langid = {french},
  keywords = {nosource},
  file = {C:\Users\nhiot\OneDrive\zotero\2007\Seretan et Wehrli - 2007 - Collocation translation based on sentence alignmen.pdf}
}

@inproceedings{setlurEvizaNaturalLanguage2016,
  title = {Eviza: {{A Natural Language Interface}} for {{Visual Analysis}}},
  shorttitle = {Eviza},
  booktitle = {Proceedings of the 29th {{Annual Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {Setlur, Vidya and Battersby, Sarah E. and Tory, Melanie and Gossweiler, Rich and Chang, Angel X.},
  year = {2016},
  month = oct,
  series = {{{UIST}} '16},
  pages = {365--377},
  publisher = {Association for Computing Machinery},
  address = {Tokyo, Japan},
  doi = {10/ggvjct},
  urldate = {2020-05-10},
  abstract = {Natural language interfaces for visualizations have emerged as a promising new way of interacting with data and performing analytics. Many of these systems have fundamental limitations. Most return minimally interactive visualizations in response to queries and often require experts to perform modeling for a set of predicted user queries before the systems are effective. Eviza provides a natural language interface for an interactive query dialog with an existing visualization rather than starting from a blank sheet and asking closed-ended questions that return a single text answer or static visualization. The system employs a probabilistic grammar based approach with predefined rules that are dynamically updated based on the data from the visualization, as opposed to computationally intensive deep learning or knowledge based approaches. The result of an interaction is a change to the view (e.g., filtering, navigation, selection) providing graphical answers and ambiguity widgets to handle ambiguous queries and system defaults. There is also rich domain awareness of time, space, and quantitative reasoning built in, and linking into existing knowledge bases for additional semantics. Eviza also supports pragmatics and exploring multi-modal interactions to help enhance the expressiveness of how users can ask questions about their data during the flow of visual analysis.},
  isbn = {978-1-4503-4189-9},
  keywords = {ambiguity,natural language,parser,pragmatics,probabilistic grammar,visual data analysis,visualization},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2016\\Setlur et al. - 2016 - Eviza A Natural Language Interface for Visual Ana.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\GE9A57G8\\2984511.html}
}

@inproceedings{setlurInferencingUnderspecifiedNatural2019,
  title = {Inferencing Underspecified Natural Language Utterances in Visual Analysis},
  booktitle = {Proceedings of the 24th {{International Conference}} on {{Intelligent User Interfaces}}},
  author = {Setlur, Vidya and Tory, Melanie and Djalali, Alex},
  year = {2019},
  month = mar,
  pages = {40--51},
  publisher = {ACM},
  address = {Marina del Ray California},
  doi = {10/ggdd77},
  urldate = {2021-03-02},
  abstract = {Handling ambiguity and underspecification of users' utterances is challenging, particularly for natural language interfaces that help with visual analytical tasks. Constraints in the underlying analytical platform and the users' expectations of high precision and recall require thoughtful inferencing to help generate useful responses. In this paper, we introduce a system to resolve partial utterances based on syntactic and semantic constraints of the underlying analytical expressions. We extend inferencing based on best practices in information visualization to generate useful visualization responses. We employ heuristics to help constrain the solution space of possible inferences, and apply ranking logic to the interpretations based on relevancy. We evaluate the quality of inferred interpretations based on relevancy and analytical usefulness.},
  isbn = {978-1-4503-6272-6},
  langid = {english},
  file = {C:\Users\nhiot\OneDrive\zotero\2019\Setlur et al. - 2019 - Inferencing underspecified natural language uttera.pdf}
}

@misc{setup,
  title = {{{SetUp}}: A Tool for Consistent Updates of {{RDF}} Knowledge Graphs},
  author = {Chabin, Jacques and Eichler, C{\'e}dric and {Halfeld-Ferrari}, Mirian and Hiot, Nicolas},
  url = {https://tinyurl.com/qwfdgwg},
  keywords = {nosource}
}

@inproceedings{shahContextAwareOntology2012,
  title = {Context {{Aware Ontology Based Information Extraction}}},
  booktitle = {Proceedings of the 18th {{International Conference}} on {{Management}} of {{Data}}},
  author = {Shah, Sapan and Reddy, Sreedhar},
  year = {2012},
  series = {{{COMAD}} '12},
  pages = {32--43},
  publisher = {Computer Society of India},
  address = {Mumbai, India, India},
  url = {http://dl.acm.org/citation.cfm?id=2694443.2694455},
  urldate = {2019-01-31},
  abstract = {We have developed an ontology based information extraction system where property and relation name occurrences are used to identify domain entities using patterns written in terms of dependency relations. Our key intuition is that, with respect to a given ontology, properties and relations are much easier to identify than entities, as the former generally occur in a limited number of terminological variations. Once identified, properties and relations provide cues to identify related entities. To achieve this, we have developed a pattern language which uses the grammatical relations of dependency parsing as well as linguistic features over text fragments. Ontology constructs such as classes, properties and relations are integral to pattern specification and provide a means for extracting entities and property values. The pattern matcher uses the patterns to construct an object graph from a text document. The object graph comprises entity, property and relation nodes. We have developed a global context aware algorithm to determine the ontological types of these nodes. Type of one node can help determine the types of other related nodes. We use the concept of entropy to measure the uncertainty associated with the type of a node. The type information is then propagated through the graph from low entropy nodes to high entropy nodes in an iterative fashion. We show how the global propagation algorithm does better than a local algorithm in determining the types of nodes. The main contributions of this paper are: an ontology aware pattern language; a global context aware type identification algorithm.},
  keywords = {⛔ No DOI found},
  annotation = {00000},
  file = {C:\Users\nhiot\OneDrive\zotero\2012\Shah et Reddy - 2012 - Context Aware Ontology Based Information Extractio.pdf}
}

@article{ShallowParsing2018,
  title = {Shallow Parsing},
  year = {2018},
  month = may,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Shallow\_parsing\&oldid=840995422},
  urldate = {2019-01-22},
  abstract = {Shallow parsing (also chunking, "light parsing") is an analysis of a sentence which first identifies constituent parts of sentences (nouns, verbs, adjectives, etc.) and then links them to higher order units that have discrete grammatical meanings (noun groups or phrases, verb groups, etc.). While the most elementary chunking algorithms simply link constituent parts on the basis of elementary search patterns (e.g. as specified by Regular Expressions), approaches that use machine learning techniques (classifiers, topic modeling, etc.) can take contextual information into account and thus compose chunks in such a way that they better reflect the semantic relations between the basic constituents. That is, these more advanced methods get around the problem that combinations of elementary constituents can have different higher level meanings depending on the context of the sentence.   It is a technique widely used in natural language processing. It is similar to the concept of lexical analysis for computer languages. Under the name of the Shallow Structure Hypothesis, it is also used as an explanation for why second language learners often fail to parse complex sentences correctly.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {00000  Page Version ID: 840995422},
  file = {C:\Users\nhiot\Zotero\storage\WYP8B7TJ\index.html}
}

@article{shamsfardLearningOntologiesNatural2004,
  title = {Learning Ontologies from Natural Language Texts},
  author = {Shamsfard, Mehrnoush and Barforoush, Ahmad Abdollahzadeh},
  year = {2004},
  journal = {International journal of human-computer studies},
  volume = {60},
  number = {1},
  pages = {17--63},
  publisher = {Elsevier},
  doi = {10.1016/j.ijhcs.2003.08.001},
  urldate = {2024-02-29},
  file = {C:\Users\nhiot\OneDrive\zotero\2004\Shamsfard et Barforoush - 2004 - Learning ontologies from natural language texts.pdf}
}

@misc{shipilevOpenJDKCodeTools2007,
  ids = {OpenJDKJmh,shipilevOpenJDKJMHProject2016},
  title = {{{OpenJDK Code Tools}}: {{JMH}}},
  shorttitle = {{{OpenJDK Code Tools}}},
  author = {Shipilev, Aleksey and Kuksenko, Sergey and Astrand, Anders and Friberg, Staffan and Loef, Henrik},
  year = {2007},
  url = {https://openjdk.java.net/projects/code-tools/jmh/},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\Zotero\storage\CZYF294Y\jmh.html}
}

@article{shiSimpleBERTModels2019,
  title = {Simple {{BERT Models}} for {{Relation Extraction}} and {{Semantic Role Labeling}}},
  author = {Shi, Peng and Lin, Jimmy},
  year = {2019},
  month = apr,
  journal = {arXiv:1904.05255 [cs]},
  eprint = {1904.05255},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1904.05255},
  urldate = {2019-11-05},
  abstract = {We present simple BERT-based models for relation extraction and semantic role labeling. In recent years, state-of-the-art performance has been achieved using neural models by incorporating lexical and syntactic features such as part-of-speech tags and dependency trees. In this paper, extensive experiments on datasets for these two tasks show that without using any external features, a simple BERT-based model can achieve state-of-the-art performance. To our knowledge, we are the first to successfully apply BERT in this manner. Our models provide strong baselines for future research.},
  archiveprefix = {arxiv},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2019\\Shi et Lin - 2019 - Simple BERT Models for Relation Extraction and Sem.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\RP7I9YN5\\1904.html}
}

@article{shumElizaXiaoIceChallenges2018,
  ids = {shumElizaXiaoIceChallenge},
  title = {From {{Eliza}} to {{XiaoIce}}: {{Challenges}} and {{Opportunities}} with {{Social Chatbots}}},
  shorttitle = {From {{Eliza}} to {{XiaoIce}}},
  author = {Shum, Heung-yeung and He, Xiao-dong and Li, Di},
  year = {2018},
  month = feb,
  journal = {Frontiers of Information Technology \& Electronic Engineering},
  volume = {19},
  number = {1},
  eprint = {1801.01957},
  primaryclass = {cs},
  pages = {10--26},
  issn = {2095-9230},
  doi = {10.1631/FITEE.1700826},
  urldate = {2023-08-07},
  abstract = {Conversational systems have come a long way since their inception in the 1960s. After decades of research and development, we've seen progress from Eliza and Parry in the 60's and 70's, to task-completion systems as in the DARPA Communicator program in the 2000s, to intelligent personal assistants such as Siri in the 2010s, to today's social chatbots like XiaoIce. Social chatbots' appeal lies not only in their ability to respond to users' diverse requests, but also in being able to establish an emotional connection with users. The latter is done by satisfying users' need for communication, affection, as well as social belonging. To further the advancement and adoption of social chatbots, their design must focus on user engagement and take both intellectual quotient (IQ) and emotional quotient (EQ) into account. Users should want to engage with a social chatbot; as such, we define the success metric for social chatbots as conversation-turns per session (CPS). Using XiaoIce as an illustrative example, we discuss key technologies in building social chatbots from core chat to visual awareness to skills. We also show how XiaoIce can dynamically recognize emotion and engage the user throughout long conversations with appropriate interpersonal responses. As we become the first generation of humans ever living with AI, we have a responsibility to design social chatbots to be both useful and empathetic, so they will become ubiquitous and help society as a whole.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Artificial intelligence,Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Conversational system,Intelligent personal assistant,Social Chatbot,TP391,XiaoIce},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2018\\Shum et al. - 2018 - From Eliza to XiaoIce Challenges and Opportunitie.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\DDHIDHSA\\1801.html}
}

@article{silvaBuildingKnowledgeGraph2018,
  title = {Building a {{Knowledge Graph}} from {{Natural Language Definitions}} for {{Interpretable Text Entailment Recognition}}},
  author = {Silva, Vivian S. and Freitas, Andr{\'e} and Handschuh, Siegfried},
  year = {2018},
  month = jun,
  journal = {arXiv:1806.07731 [cs]},
  eprint = {1806.07731},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1806.07731},
  urldate = {2020-05-18},
  abstract = {Natural language definitions of terms can serve as a rich source of knowledge, but structuring them into a comprehensible semantic model is essential to enable them to be used in semantic interpretation tasks. We propose a method and provide a set of tools for automatically building a graph world knowledge base from natural language definitions. Adopting a conceptual model composed of a set of semantic roles for dictionary definitions, we trained a classifier for automatically labeling definitions, preparing the data to be later converted to a graph representation. WordNetGraph, a knowledge graph built out of noun and verb WordNet definitions according to this methodology, was successfully used in an interpretable text entailment recognition approach which uses paths in this graph to provide clear justifications for entailment decisions.},
  archiveprefix = {arxiv},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2018\\Silva et al. - 2018 - Building a Knowledge Graph from Natural Language D.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\IKLTLWQT\\1806.html}
}

@inproceedings{silvestreTejoSupervisedAnomaly2015,
  title = {Tejo: {{A Supervised Anomaly Detection Scheme}} for {{NewSQL Databases}}},
  shorttitle = {Tejo},
  booktitle = {Software {{Engineering}} for {{Resilient Systems}}},
  author = {Silvestre, Guthemberg and Sauvanaud, Carla and Ka{\^a}niche, Mohamed and Kanoun, Karama},
  editor = {Fantechi, Alessandro and Pelliccione, Patrizio},
  year = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {114--127},
  publisher = {Springer International Publishing},
  abstract = {The increasing availability of streams of data and the need of auto-tuning applications have made big data mainstream. NewSQL databases have become increasingly important to ensure fast data processing for the emerging stream processing platforms. While many architectural improvements have been made on NewSQL databases to handle fast data processing, anomalous events on the underlying, complex cloud environments may undermine their performance. In this paper, we present Tejo, a supervised anomaly detection scheme for NewSQL databases. Unlike general-purpose anomaly detection for the cloud, Tejo characterizes anomalies in NewSQL database clusters based on Service Level Objective (SLO) metrics. Our experiments with VoltDB, a prominent NewSQL database, shed some light on the impact of anomalies on these databases and highlight the key design choices to enhance anomaly detection.},
  isbn = {978-3-319-23129-7},
  langid = {english},
  annotation = {00000},
  file = {C:\Users\nhiot\OneDrive\zotero\2015\Silvestre et al. - 2015 - Tejo A Supervised Anomaly Detection Scheme for Ne.pdf}
}

@phdthesis{sirangeloRepresentingQueryingIncomplete2014,
  type = {{{HDR}}},
  title = {Representing and {{Querying Incomplete Information}}: A {{Data Interoperability Perspective}}},
  shorttitle = {Representing and {{Querying Incomplete Information}}},
  author = {Sirangelo, Cristina},
  year = {2014},
  month = dec,
  address = {Cachan},
  url = {https://tel.archives-ouvertes.fr/tel-01092547},
  urldate = {2023-08-07},
  abstract = {This habilitation thesis presents some of my most recent work, which has been done in collaboration with several other people. In particular this thesis concentrates on our contributions to the study of incomplete information in the context of data interoperability. In this scenario data is heterogenous and decentralized, needs to be integrated from several sources and exchanged between different applications. Incompleteness, i.e. the presence of ``missing'' or ``unknown'' portions of data, is naturally generated in data exchange and integration, due to data heterogeneity. The management of incomplete information poses new challenges in this context.The focus of our study is the development of models of incomplete information suitable to data interoperability tasks, and the study of techniques for efficiently querying several forms of incompleteness.},
  langid = {english},
  school = {Ecole Normale Sup{\'e}rieure de Cachan},
  keywords = {nosource},
  file = {C:\Users\nhiot\OneDrive\zotero\2014\Sirangelo - 2014 - Representing and Querying Incomplete Information .pdf}
}

@article{skavantzosNormalizingPropertyGraphs2023,
  ids = {skavantzosphilippNormalizingPropertyGraphs2023},
  title = {Normalizing {{Property Graphs}}},
  author = {Skavantzos, Philipp and Link, Sebastian},
  year = {2023},
  month = aug,
  journal = {Proceedings of the VLDB Endowment},
  volume = {16},
  number = {11},
  pages = {3031--3043},
  publisher = {VLDB Endowment},
  issn = {2150-8097},
  doi = {10.14778/3611479.3611506},
  urldate = {2023-10-18},
  abstract = {Normalization aims at minimizing sources of potential data inconsistency and costs of update maintenance incurred by data redundancy. For relational databases, different classes of dependencies cause data redundancy and have resulted in proposals such as Third, Boyce-Codd, Fourth and Fifth Normal Form. Features of more advanced data models make it challenging to extend achievements from the relational model to missing, non-atomic, or uncertain data. We initiate research on the normalization of graph data, starting with a class of functional dependencies tailored to property graphs. We show that this class captures important semantics of applications, constitutes a rich source of data redundancy, its implication problem can be decided in linear time, and facilitates the normalization of property graphs flexibly tailored to their labels and properties that are targeted by applications. We normalize property graphs into Boyce-Codd Normal Form without loss of data and dependencies whenever possible for the target labels and properties, but guarantee Third Normal Form in general. Experiments on real-world property graphs quantify and qualify various benefits of graph normalization: 1) removing redundant property values as sources of inconsistent data, 2) detecting inconsistency as violation of functional dependencies, 3) reducing update overheads by orders of magnitude, and 4) significant speed ups of aggregate queries.},
  langid = {english},
  file = {C:\Users\nhiot\OneDrive\zotero\2023\Skavantzos et Link - 2023 - Normalizing Property Graphs.pdf}
}

@article{soderlandLearningInformationExtraction1999,
  title = {Learning {{Information Extraction Rules}} for {{Semi-Structured}} and {{Free Text}}},
  author = {Soderland, Stephen},
  year = {1999},
  month = feb,
  journal = {Machine Learning},
  volume = {34},
  number = {1},
  pages = {233--272},
  issn = {1573-0565},
  doi = {10/bhcc93},
  urldate = {2019-05-24},
  abstract = {A wealth of on-line text information can be made available to automatic processing by information extraction (IE) systems. Each IE application needs a separate set of rules tuned to the domain and writing style. WHISK helps to overcome this knowledge-engineering bottleneck by learning text extraction rules automatically.WHISK is designed to handle text styles ranging from highly structured to free text, including text that is neither rigidly formatted nor composed of grammatical sentences. Such semi-structured text has largely been beyond the scope of previous systems. When used in conjunction with a syntactic analyzer and semantic tagging, WHISK can also handle extraction from free text such as news stories.},
  langid = {english},
  keywords = {information extraction,natural language processing,rule learning},
  file = {C:\Users\nhiot\OneDrive\zotero\1999\Soderland - 1999 - Learning Information Extraction Rules for Semi-Str.pdf}
}

@article{sohnComprehensiveTemporalInformation2013,
  title = {Comprehensive Temporal Information Detection from Clinical Text: Medical Events, Time, and {{TLINK}} Identification},
  shorttitle = {Comprehensive Temporal Information Detection from Clinical Text},
  author = {Sohn, Sunghwan and Wagholikar, Kavishwar B and Li, Dingcheng and Jonnalagadda, Siddhartha R and Tao, Cui and Komandur Elayavilli, Ravikumar and Liu, Hongfang},
  year = {2013},
  month = sep,
  journal = {Journal of the American Medical Informatics Association},
  volume = {20},
  number = {5},
  pages = {836--842},
  issn = {1067-5027},
  doi = {10/f469f2},
  urldate = {2021-05-03},
  abstract = {Background Temporal information detection systems have been developed by the Mayo Clinic for the 2012 i2b2 Natural Language Processing Challenge.Objective To construct automated systems for EVENT/TIMEX3 extraction and temporal link (TLINK) identification from clinical text.Materials and methods The i2b2 organizers provided 190 annotated discharge summaries as the training set and 120 discharge summaries as the test set. Our Event system used a conditional random field classifier with a variety of features including lexical information, natural language elements, and medical ontology. The TIMEX3 system employed a rule-based method using regular expression pattern match and systematic reasoning to determine normalized values. The TLINK system employed both rule-based reasoning and machine learning. All three systems were built in an Apache Unstructured Information Management Architecture framework.Results Our TIMEX3 system performed the best (F-measure of 0.900, value accuracy 0.731) among the challenge teams. The Event system produced an F-measure of 0.870, and the TLINK system an F-measure of 0.537.Conclusions Our TIMEX3 system demonstrated good capability of regular expression rules to extract and normalize time information. Event and TLINK machine learning systems required well-defined feature sets to perform well. We could also leverage expert knowledge as part of the machine learning features to further improve TLINK identification performance.},
  annotation = {QID: Q37129517},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2013\\Sohn et al. - 2013 - Comprehensive temporal information detection from .pdf;C\:\\Users\\nhiot\\Zotero\\storage\\ZAXGAXVB\\727595.html}
}

@article{solimandoValidatingXMLDocument2014,
  title = {Validating {{XML}} Document Adaptations via {{Hedge Automata}} Transformations},
  author = {Solimando, Alessandro and Delzanno, Giorgio and Guerrini, Giovanna},
  year = {2014},
  month = dec,
  journal = {Theoretical Computer Science},
  series = {Games, {{Automata}}, {{Logic}} and {{Formal Verification}}},
  volume = {560},
  pages = {251--268},
  issn = {0304-3975},
  doi = {10/f6w7rw},
  urldate = {2021-11-30},
  abstract = {We present an automata-based method for the static analysis of user-defined XML document adaptations, expressed as sequences of update primitives of XQuery Update. The key feature of the method is the use of an automatic inference algorithm for extracting the type, expressed as a Hedge Automaton, of a sequence of document updates. The type is computed starting from the original schema S and from rewriting rules that formally define the operational semantics of a sequence of document updates. Type inclusion can then be used as a conformance test w.r.t. the type extracted from the target schema S{$\prime$}.},
  langid = {english},
  keywords = {Hedge Automata,Hedge rewriting systems,XML,XML Schema},
  file = {C:\Users\nhiot\OneDrive\zotero\2014\Solimando et al. - 2014 - Validating XML document adaptations via Hedge Auto.pdf}
}

@phdthesis{souletUserCentricPatternDiscovery2019,
  type = {Habilitation {\`a} Diriger Des Recherches},
  title = {User-{{Centric Pattern Discovery}}},
  author = {Soulet, Arnaud},
  year = {2019},
  month = nov,
  url = {https://hal.archives-ouvertes.fr/tel-02386176},
  urldate = {2020-02-17},
  abstract = {Pattern mining is an enumeration technique used to discover knowledge from databases. This Habilitation thesis summarizes our main contributions regarding user-centric pattern mining. First, we introduce the pattern-oriented relational algebra (PORA), which is the formalism used throughout the thesis. We add a domain operator to the relational algebra to generate hypotheses about the data and a cover operator to compare the hypotheses to the data. Beyond the declaration of mining processes, this algebra makes it possible to reason on the queries to deduce properties or to optimize queries with rewriting rules. A second part presents our work where the user's preferences guide the mining of patterns. In other words, pattern mining is seen as an optimization problem where only the best patterns in the sense of a preference relation are preserved. For this purpose, the cover operator is implemented to play the role of preference relation by comparing patterns two-by-two in order to retain the best ones. Finally, we are interested in model construction to improve the complementarity between mined patterns. The last part details our contributions where the method of analysis of the patterns guides their discovery. With this vision, the patterns are analyzed with a sharpness proportional to their interest. Rather than mining all the patterns, it is then sufficient to sample them with a probability proportional to their interest for presenting them to the user. We are extending PORA to reformulate the principle of pattern sampling algebraically. We show the interest of pattern sampling for the construction of anytime and interactive systems. Finally, a conclusion summarizes and discusses several research perspectives.},
  school = {Universit{\'e} de Tours},
  keywords = {Data Mining,Extraction de Connaissances dans des bases de Donn{\'e}es,Extraction de motifs,Fouille de donn{\'e}es,Knowledge Discovery in Databases,Pattern Mining},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2019\\Soulet - 2019 - User-Centric Pattern Discovery.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\5977R5BY\\tel-02386176.html}
}

@misc{SpaCy101Everything,
  title = {{{spaCy}} 101: {{Everything}} You Need to Know {$\cdot$} {{spaCy Usage Documentation}}},
  shorttitle = {{{spaCy}} 101},
  journal = {spaCy 101: Everything you need to know},
  url = {https://spacy.io/usage/spacy-101},
  urldate = {2024-03-21},
  abstract = {The most important concepts, explained in simple terms},
  langid = {english},
  file = {C:\Users\nhiot\Zotero\storage\WYT35G4P\spacy-101.html}
}

@article{sparckjonesStatisticalInterpretationTerm1972,
  ids = {sparckjonesStatisticalInterpretationTerm1972a,sparckjonesStatisticalInterpretationTerm2004},
  title = {A Statistical Interpretation of Term Specificity and Its Application in Retrieval},
  author = {Sparck Jones, Karen},
  year = {1972},
  month = jan,
  journal = {Journal of Documentation},
  volume = {28},
  number = {1},
  pages = {11--21},
  publisher = {MCB UP Ltd},
  issn = {0022-0418},
  doi = {10.1108/eb026526},
  urldate = {2023-09-22},
  abstract = {The exhaustivity of document descriptions and the specificity of index terms are usually regarded as independent. It is suggested that specificity should be interpreted statistically, as a function of term use rather than of term meaning. The effects on retrieval of variations in term specificity are examined, experiments with three test collections showing in particular that frequently-occurring terms are required for good overall performance. It is argued that terms should be weighted according to collection frequency, so that matches on less frequent, more specific, terms are of greater value than matches on frequent terms. Results for the test collections show that considerable improvements in performance are obtained with this very simple procedure.},
  keywords = {nosource},
  annotation = {QID: Q54296753},
  file = {C:\Users\nhiot\OneDrive\zotero\1972\Sparck Jones - 1972 - A statistical interpretation of term specificity a.pdf}
}

@article{spyratosHIFUNHighLevel2018,
  title = {{{HIFUN}} - a High Level Functional Query Language for Big Data Analytics},
  author = {Spyratos, Nicolas and Sugibuchi, Tsuyoshi},
  year = {2018},
  month = dec,
  journal = {Journal of Intelligent Information Systems},
  volume = {51},
  number = {3},
  pages = {529--555},
  issn = {0925-9902, 1573-7675},
  doi = {10/gfj6ks},
  urldate = {2019-05-22},
  abstract = {We present a high level query language, called HIFUN, for defining analytic queries over big datasets, independently of how these queries are evaluated. An analytic query in HIFUN is defined to be a well-formed expression of a functional algebra that we define in the paper. The operations of this algebra combine functions to create HIFUN queries in much the same way as the operations of the relational algebra combine relations to create algebraic queries. The contributions of this paper are: (a) the definition of a formal framework in which to study analytic queries in the abstract; (b) the encoding of a HIFUN query either as a MapReduce job or as an SQL group-by query; and (c) the definition of a formal method for rewriting HIFUN queries and, as a case study, its application to the rewriting of MapReduce jobs and of SQL group-by queries. We emphasize that, although theoretical in nature, our work uses only basic and well known mathematical concepts, namely functions and their basic operations.},
  langid = {english},
  keywords = {Big data analytics,Data modeling,MapReduce,Query language},
  file = {C:\Users\nhiot\OneDrive\zotero\2018\Spyratos et Sugibuchi - 2018 - HIFUN - a high level functional query language for.pdf}
}

@article{srinivasanHowAskWhat2020,
  title = {How to Ask What to Say?: {{Strategies}} for Evaluating Natural Language Interfaces for Data Visualization},
  shorttitle = {How to Ask What to Say?},
  author = {Srinivasan, Arjun and Stasko, John},
  year = {2020},
  journal = {IEEE Computer Graphics and Applications},
  volume = {40},
  number = {4},
  pages = {96--103},
  publisher = {IEEE},
  doi = {10/gh59fr},
  annotation = {QID: Q96437111},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2020\\Srinivasan et Stasko - 2020 - How to ask what to say Strategies for evaluating.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\BJA7ADJU\\9118800.html}
}

@inproceedings{stavrakasImplementingQueryLanguage2004,
  title = {Implementing a {{Query Language}} for {{Context-Dependent Semistructured Data}}},
  booktitle = {Advances in {{Databases}} and {{Information Systems}}},
  author = {Stavrakas, Yannis and Pristouris, Kostis and Efandis, Antonis and Sellis, Timos},
  editor = {Bencz{\'u}r, Andr{\'a}s and Demetrovics, J{\'a}nos and Gottlob, Georg},
  year = {2004},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {173--188},
  publisher = {Springer Berlin Heidelberg},
  abstract = {In today's global environment, the structure and presentation of information may depend on the underlying context of the user. To address this issue, in previous work we have proposed multidimensional semistructured data (MSSD), where an information entity can have alternative variants, or facets, each holding under some world, and MOEM, a data model suitable for representing MSSD. In this paper we briefly present MQL, a query language for MSSD that supports context-driven queries, and we attempt to motivate the direct use of context in data models and query languages by comparing MOEM and MQL with equivalent, context-unaware forms of representing and querying information. Specifically, we implemented an evaluation process for MQL during which MQL queries are translated to equivalent Lorel queries, and MOEM databases are transformed to corresponding OEM databases. The comparison between the two query languages and data models demonstrates the benefits of treating context as first-class citizen. We illustrate this query translation process using a cross-world MQL query, which has no direct counterpart in context-unaware query languages and data models.},
  isbn = {978-3-540-30204-9},
  langid = {english},
  keywords = {Complex Node,Context Node,Path Expression,Query Language,Semistructured Data},
  file = {C:\Users\nhiot\OneDrive\zotero\2004\Stavrakas et al. - 2004 - Implementing a Query Language for Context-Dependen.pdf}
}

@inproceedings{steinmetzNaturalLanguageQuestions2019,
  title = {From {{Natural Language Questions}} to {{SPARQL Queries}}: {{A Pattern-based Approach}}},
  shorttitle = {From {{Natural Language Questions}} to {{SPARQL Queries}}},
  booktitle = {{{BTW}} 2019},
  author = {Steinmetz, Nadine and Arning, Ann-Katrin and Sattler, Kai-Uwe},
  year = {2019},
  pages = {289--308},
  publisher = {Gesellschaft f{\"u}r Informatik, Bonn},
  doi = {10/ggdnr4},
  urldate = {2019-11-29},
  abstract = {Linked Data knowledge bases are valuable sources of knowledge which give insights, reveal facts about various relationships and provide a large amount of metadata in well-structured form. Although the format of semantic information -- namely as RDF(S) -- is kept simple by representing each fact as a triple of subject, property and object, the access to the knowledge is only available using SPARQL queries on the data. Therefore, Question Answering (QA) systems provide a user-friendly way to access any type of knowledge base and especially for Linked Data sources to get insight into the semantic information. As RDF(S) knowledge bases are usually structured in the same way and provide per se semantic metadata about the contained information, we provide a novel approach that is independent from the underlying knowledge base. Thus, the main contribution of our proposed approach constitutes the simple replaceability of the underlying knowledge base. The algorithm is based on general question and query patterns and only accesses the knowledge base for the actual query generation and execution. This paper presents the proposed approach and an evaluation in comparison to state-of-the-art Linked Data approaches for challenges of QA systems.},
  isbn = {978-3-88579-683-1},
  langid = {english},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2019\\Steinmetz et al. - 2019 - From Natural Language Questions to SPARQL Queries.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\PZUKHUTP\\21702.html}
}

@article{Stemming2019,
  title = {Stemming},
  year = {2019},
  month = jan,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Stemming\&oldid=878398715},
  urldate = {2019-01-22},
  abstract = {In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form---generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root. Algorithms for stemming have been studied in computer science since the 1960s. Many search engines treat words with the same stem as synonyms as a kind of query expansion, a process called conflation. A computer program or subroutine that stems word may be called a stemming program, stemming algorithm, or stemmer.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {00000  Page Version ID: 878398715},
  file = {C:\Users\nhiot\Zotero\storage\GS7743LK\index.html}
}

@misc{StemmingLemmatization,
  title = {Stemming and Lemmatization},
  url = {https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html},
  urldate = {2019-01-22},
  annotation = {00000},
  file = {C:\Users\nhiot\Zotero\storage\S6RYDZFJ\stemming-and-lemmatization-1.html}
}

@article{StopWords2019,
  title = {Stop Words},
  year = {2019},
  month = jan,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Stop\_words\&oldid=877552413},
  urldate = {2019-01-22},
  abstract = {In computing, stop words are words which are filtered out before or after processing of natural language data (text). Though "stop words" usually refers to the most common words in a language, there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list. Some tools specifically avoid removing these stop words to support phrase search. Any group of words can be chosen as the stop words for a given purpose. For some search engines, these are some of the most common, short function words, such as the, is, at, which, and on. In this case, stop words can cause problems when searching for phrases that include them, particularly in names such as "The Who", "The The", or "Take That". Other search engines remove some of the most common words---including lexical words, such as "want"---from a query in order to improve performance.Hans Peter Luhn, one of the pioneers in information retrieval, is credited with coining the phrase and using the concept. The phrase "stop word", which is not in Luhn's 1959 presentation, and the associated terms "stop list" and "stoplist" appear in the literature shortly afterwards.A predecessor concept was used in creating some concordances. For example, the first Hebrew concordance, Me'ir nativ, contained a one-page list of unindexed words, with nonsubstantive prepositions and conjunctions which are similar to modern stop words.In SEO terminology, stop words are the most common words that most search engines avoid, saving space and time in processing large data during crawling or indexing. This helps search engines to save space in their databases.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {00000  Page Version ID: 877552413},
  file = {C:\Users\nhiot\Zotero\storage\RRHLA8WV\index.html}
}

@inproceedings{strubellEnergyPolicyConsiderations2019,
  title = {Energy and {{Policy Considerations}} for {{Deep Learning}} in {{NLP}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  year = {2019},
  month = jun,
  pages = {3645--3650},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/P19-1355},
  urldate = {2023-10-23},
  abstract = {Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.},
  keywords = {Computer Science - Computation and Language},
  annotation = {QID: Q64512333},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2019\\Strubell et al. - 2019 - Energy and Policy Considerations for Deep Learning2.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\IYNS5HUA\\1906.html}
}

@inproceedings{szubertNodeEmbeddingsGraph2019,
  title = {Node {{Embeddings}} for {{Graph Merging}}: {{Case}} of {{Knowledge Graph Construction}}},
  shorttitle = {Node {{Embeddings}} for {{Graph Merging}}},
  booktitle = {Proceedings of the {{Thirteenth Workshop}} on {{Graph-Based Methods}} for {{Natural Language Processing}} ({{TextGraphs-13}})},
  author = {Szubert, Ida and Steedman, Mark},
  year = {2019},
  month = nov,
  pages = {172--176},
  publisher = {Association for Computational Linguistics},
  address = {Hong Kong},
  doi = {10/ghgnhp},
  urldate = {2021-05-18},
  abstract = {Combining two graphs requires merging the nodes which are counterparts of each other. In this process errors occur, resulting in incorrect merging or incorrect failure to merge. We find a high prevalence of such errors when using AskNET, an algorithm for building Knowledge Graphs from text corpora. AskNET node matching method uses string similarity, which we propose to replace with vector embedding similarity. We explore graph-based and word-based embedding models and show an overall error reduction of from 56\% to 23.6\%, with a reduction of over a half in both types of incorrect node matching.},
  file = {C:\Users\nhiot\OneDrive\zotero\2019\Szubert et Steedman - 2019 - Node Embeddings for Graph Merging Case of Knowled.pdf}
}

@inproceedings{Taentzer2003AGGAG,
  title = {{{AGG}}: {{A}} Graph Transformation Environment for Modeling and Validation of Software},
  booktitle = {{{AGTIVE}}},
  author = {Taentzer, Gabriele},
  year = {2003},
  keywords = {⛔ No DOI found,nosource}
}

@inproceedings{talukdarContextPatternInduction2006,
  title = {A {{Context Pattern Induction Method}} for {{Named Entity Extraction}}},
  booktitle = {Proceedings of the {{Tenth Conference}} on {{Computational Natural Language Learning}}},
  author = {Talukdar, Partha Pratim and Brants, Thorsten and Liberman, Mark and Pereira, Fernando},
  year = {2006},
  series = {{{CoNLL-X}} '06},
  pages = {141--148},
  publisher = {Association for Computational Linguistics},
  address = {Stroudsburg, PA, USA},
  doi = {10/ft94gt},
  urldate = {2019-01-23},
  abstract = {We present a novel context pattern induction method for information extraction, specifically named entity extraction. Using this method, we extended several classes of seed entity lists into much larger high-precision lists. Using token membership in these extended lists as additional features, we improved the accuracy of a conditional random field-based named entity tagger. In contrast, features derived from the seed lists decreased extractor accuracy.},
  annotation = {http://web.archive.org/web/20200221093919/https://dl.acm.org/doi/10.5555/1596276.1596303},
  file = {C:\Users\nhiot\OneDrive\zotero\2006\Talukdar et al. - 2006 - A Context Pattern Induction Method for Named Entit.pdf}
}

@article{taokokMultiThreadingPrologImplement2012,
  title = {A {{Multi-Threading In Prolog To Implement K-Mean Clustering}}},
  author = {Taokok, Surasith and Pongpanich, Prach and Kerdprasop, Nittaya and Kerdprasop, Kittisak},
  year = {2012},
  journal = {published in Latest Advances in systems Science and Computational intelligence},
  pages = {120--126},
  abstract = {Prolog presented in this paper is a logic programming language with multi-threading support. Several programmers use multi-threading to improve execution time. This is due to the fact that the multithreads can split tasks to work in concurrency. In this paper, we propose an algorithm and its implementation of k-means clustering with multi-threading using Prolog. The main objective is to speedup execution time. The experimentation compares k-means and multi-thread k-means, as well as percentage of speedup time execution. The result has proved our claim.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2012\Taokok et al. - 2012 - A Multi-Threading In Prolog To Implement K-Mean Cl.pdf}
}

@article{teitzHeuristicMethodsEstimating1968,
  title = {Heuristic {{Methods}} for {{Estimating}} the {{Generalized Vertex Median}} of a {{Weighted Graph}}},
  author = {Teitz, Michael B. and Bart, Polly},
  year = {1968},
  month = oct,
  journal = {Operations Research},
  volume = {16},
  number = {5},
  pages = {955--961},
  issn = {0030-364X},
  doi = {10/fhxsvs},
  urldate = {2019-02-15},
  abstract = {The generalized vertex median of a weighted graph may be found by complete enumeration or by some heuristic method. This paper investigates alternatives and proposes a method that seems to perform well in comparison with others found in the literature.},
  annotation = {00000},
  file = {C:\Users\nhiot\Zotero\storage\PM3A7IFF\opre.16.5.html}
}

@inproceedings{tellierHowSplitRecursive2008,
  title = {How to Split Recursive Automata},
  booktitle = {Grammatical Inference: {{Algorithms}} and Applications. 9th International Colloquium, {{ICGI}} 2008 Saint-Malo, France, September 22-24, 2008 Proceedings},
  author = {Tellier, Isabelle},
  editor = {Clark, Alexander and Coste, Fran{\c c}ois and Miclet, Laurent},
  year = {2008},
  series = {{{LNAI}}},
  volume = {5278},
  pages = {200--212},
  publisher = {Springer},
  doi = {10.1007/978-3-540-88009-7_16},
  hal_id = {inria-00341770},
  hal_version = {v1},
  keywords = {categorial grammars,grammatical inference,recursive automata},
  file = {C:\Users\nhiot\Zotero\storage\5W74HWHS\Tellier - 2008 - How to split recursive automata.pdf}
}

@misc{TenminuteIntroductionSequencetosequence,
  title = {A Ten-Minute Introduction to Sequence-to-Sequence Learning in {{Keras}}},
  url = {https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html},
  urldate = {2021-02-16},
  file = {C:\Users\nhiot\Zotero\storage\IEDGXYHG\a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html}
}

@misc{theapachefoundationSecretEnginesApache2020,
  title = {Secret {{Engines}} of {{Apache cTAKES}}},
  author = {{TheApacheFoundation}},
  year = {2020},
  month = oct,
  url = {https://www.youtube.com/watch?v=QlGj5xXKC9E},
  urldate = {2020-11-20},
  abstract = {Secret Engines of Apache cTAKES Sean Finan A presentation from ApacheCon @Home 2020 https://apachecon.com/acah2020/ The Apache clinical Text Analysis and Knowledge Extraction System (cTAKES) default pipeline is a standard in the natural language processing clinical research community. What is past that standard? While the default clinical pipeline uses almost 20 analysis engines, there are dozens more in various cTAKES modules. We present and discuss the top 5 annotation engines you never knew you had. Sean Finan is a software developer in the Natural Language Processing lab at Boston Children's Hospital. He has worked with Apache cTAKES for the past 8 years, contributing code and supporting the community.},
  keywords = {nosource}
}

@inproceedings{tjong-kim-sang-de-meulder-2003-introduction,
  title = {Introduction to the {{CoNLL-2003}} Shared Task: {{Language-independent}} Named Entity Recognition},
  booktitle = {Proceedings of the Seventh Conference on Natural Language Learning at {{HLT-NAACL}} 2003},
  author = {Tjong Kim Sang, Erik F. and De Meulder, Fien},
  year = {2003},
  pages = {142--147},
  doi = {10/d8qpkd},
  keywords = {nosource}
}

@inproceedings{TjongKimSang:2002:ICS:1118853.1118877,
  title = {Introduction to the {{CoNLL-2002}} Shared Task: {{Language-independent}} Named Entity Recognition},
  booktitle = {Proceedings of the 6th Conference on Natural Language Learning - Volume 20},
  author = {Tjong Kim Sang, Erik F.},
  year = {2002},
  series = {{{COLING-02}}},
  pages = {1--4},
  publisher = {Association for Computational Linguistics},
  address = {Stroudsburg, PA, USA},
  doi = {10/ft2bf6},
  acmid = {1118877},
  numpages = {4},
  keywords = {nosource}
}

@inproceedings{tongFastBesteffortPattern2007,
  title = {Fast {{Best-effort Pattern Matching}} in {{Large Attributed Graphs}}},
  booktitle = {Proceedings of the 13th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Tong, Hanghang and Faloutsos, Christos and Faloutsos, Christos and Gallagher, Brian and {Eliassi-Rad}, Tina},
  year = {2007},
  series = {{{KDD}} '07},
  pages = {737--746},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10/fw37zq},
  urldate = {2018-12-29},
  abstract = {We focus on large graphs where nodes have attributes, such as a social network where the nodes are labelled with each person's job title. In such a setting, we want to find subgraphs that match a user query pattern. For example, a "star" query would be, "find a CEO who has strong interactions with a Manager, a Lawyer,and an Accountant, or another structure as close to that as possible". Similarly, a "loop" query could help spot a money laundering ring. Traditional SQL-based methods, as well as more recent graph indexing methods, will return no answer when an exact match does not exist. This is the first main feature of our method. It can find exact-, as well as near-matches, and it will present them to the user in our proposed "goodness" order. For example, our method tolerates indirect paths between, say, the "CEO" and the "Accountant" of the above sample query, when direct paths don't exist. Its second feature is scalability. In general, if the query has nq nodes and the data graph has n nodes, the problem needs polynomial time complexity O(n n q), which is prohibitive. Our G-Ray ("Graph X-Ray") method finds high-quality subgraphs in time linear on the size of the data graph. Experimental results on the DLBP author-publication graph (with 356K nodes and 1.9M edges) illustrate both the effectiveness and scalability of our approach. The results agree with our intuition, and the speed is excellent. It takes 4 seconds on average fora 4-node query on the DBLP graph.},
  isbn = {978-1-59593-609-7},
  keywords = {attributed graph,pattern match,random walk},
  annotation = {00000},
  file = {C:\Users\nhiot\OneDrive\zotero\2007\Tong et al. - 2007 - Fast Best-effort Pattern Matching in Large Attribu.pdf}
}

@misc{torresPybabelnetPythonPackage,
  title = {Py-Babelnet: {{Python}} Package for Babelnet {{API}}},
  shorttitle = {Py-Babelnet},
  author = {Torres, Pablo},
  copyright = {MIT},
  file = {C:\Users\nhiot\Zotero\storage\37YTXU2C\py-babelnet.html}
}

@misc{TrainingDataScience,
  title = {Training {{Data Science Neo4j}}},
  journal = {Neo4j Graph Database Platform},
  url = {https://neo4j.com/graphacademy/online-training/data-science/},
  urldate = {2020-03-11},
  abstract = {This course introduces you to using Neo4j as part of your Data Science and Machine Learning workflows. This course is structured as self-paced training and is intended for data scientists and data analysts.},
  langid = {english},
  file = {C:\Users\nhiot\Zotero\storage\DULQ54PI\data-science.html}
}

@article{traskSense2vecFastAccurate2015,
  title = {Sense2vec - {{A Fast}} and {{Accurate Method}} for {{Word Sense Disambiguation In Neural Word Embeddings}}},
  author = {Trask, Andrew and Michalak, Phil and Liu, John},
  year = {2015},
  month = nov,
  journal = {arXiv:1511.06388 [cs]},
  eprint = {1511.06388},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1511.06388},
  urldate = {2019-01-30},
  abstract = {Neural word representations have proven useful in Natural Language Processing (NLP) tasks due to their ability to efficiently model complex semantic and syntactic word relationships. However, most techniques model only one representation per word, despite the fact that a single word can have multiple meanings or "senses". Some techniques model words by using multiple vectors that are clustered based on context. However, recent neural approaches rarely focus on the application to a consuming NLP algorithm. Furthermore, the training process of recent word-sense models is expensive relative to single-sense embedding processes. This paper presents a novel approach which addresses these concerns by modeling multiple embeddings for each word based on supervised disambiguation, which provides a fast and accurate way for a consuming NLP model to select a sense-disambiguated embedding. We demonstrate that these embeddings can disambiguate both contrastive senses such as nominal and verbal senses as well as nuanced senses such as sarcasm. We further evaluate Part-of-Speech disambiguated embeddings on neural dependency parsing, yielding a greater than 8\% average error reduction in unlabeled attachment scores across 6 languages.},
  archiveprefix = {arxiv},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {00000  QID: Q101417672},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2015\\Trask et al. - 2015 - sense2vec - A Fast and Accurate Method for Word Se.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\T45U9YPM\\Trask et al. - 2015 - sense2vec - A Fast and Accurate Method for Word Se.html}
}

@misc{UnderstandingLSTMNetworks,
  title = {Understanding {{LSTM Networks}} -- Colah's Blog},
  url = {https://colah.github.io/posts/2015-08-Understanding-LSTMs/},
  urldate = {2020-04-04},
  file = {C:\Users\nhiot\Zotero\storage\FIXIDQEX\2015-08-Understanding-LSTMs.html}
}

@article{utamaEndtoendNeuralNatural2018,
  title = {An {{End-to-end Neural Natural Language Interface}} for {{Databases}}},
  author = {Utama, Prasetya and Weir, Nathaniel and Basik, Fuat and Binnig, Carsten and Cetintemel, Ugur and H{\"a}ttasch, Benjamin and Ilkhechi, Amir and Ramaswamy, Shekar and Usta, Arif},
  year = {2018},
  month = apr,
  journal = {arXiv preprint arXiv:1804.00401},
  eprint = {1804.00401},
  url = {http://arxiv.org/abs/1804.00401},
  urldate = {2019-12-03},
  abstract = {The ability to extract insights from new data sets is critical for decision making. Visual interactive tools play an important role in data exploration since they provide non-technical users with an effective way to visually compose queries and comprehend the results. Natural language has recently gained traction as an alternative query interface to databases with the potential to enable non-expert users to formulate complex questions and information needs efficiently and effectively. However, understanding natural language questions and translating them accurately to SQL is a challenging task, and thus Natural Language Interfaces for Databases (NLIDBs) have not yet made their way into practical tools and commercial products. In this paper, we present DBPal, a novel data exploration tool with a natural language interface. DBPal leverages recent advances in deep models to make query understanding more robust in the following ways: First, DBPal uses a deep model to translate natural language statements to SQL, making the translation process more robust to paraphrasing and other linguistic variations. Second, to support the users in phrasing questions without knowing the database schema and the query features, DBPal provides a learned auto-completion model that suggests partial query extensions to users during query formulation and thus helps to write complex queries.},
  archiveprefix = {arxiv},
  optabstract = {The ability to extract insights from new data sets is critical for decision making. Visual interactive tools play an important role in data exploration since they provide non-technical users with an effective way to visually compose queries and comprehend the results. Natural language has recently gained traction as an alternative query interface to databases with the potential to enable non-expert users to formulate complex questions and information needs efficiently and effectively. However, understanding natural language questions and translating them accurately to SQL is a challenging task, and thus Natural Language Interfaces for Databases (NLIDBs) have not yet made their way into practical tools and commercial products. In this paper, we present DBPal, a novel data exploration tool with a natural language interface. DBPal leverages recent advances in deep models to make query understanding more robust in the following ways: First, DBPal uses a deep model to translate natural language statements to SQL, making the translation process more robust to paraphrasing and other linguistic variations. Second, to support the users in phrasing questions without knowing the database schema and the query features, DBPal provides a learned auto-completion model that suggests partial query extensions to users during query formulation and thus helps to write complex queries.},
  optkeywords = {Computer Science - Computation and Language, Computer Science - Databases, Computer Science - Human-Computer Interaction},
  optmonth = {04},
  opturl = {http://arxiv.org/abs/1804.00401},
  opturldate = {2019-12-03},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Databases,Computer Science - Human-Computer Interaction},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2018\\Utama et al. - 2018 - An End-to-end Neural Natural Language Interface fo.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\3WER3DB7\\1804.html}
}

@article{uzunerAdvancingStateArt2020,
  title = {Advancing the State of the Art in Automatic Extraction of Adverse Drug Events from Narratives},
  author = {Uzuner, {\"O}zlem and Stubbs, Amber and Lenert, Leslie},
  year = {2020},
  month = jan,
  journal = {Journal of the American Medical Informatics Association},
  volume = {27},
  number = {1},
  pages = {1--2},
  issn = {1527-974X},
  doi = {10/gghv4k},
  urldate = {2020-01-24},
  abstract = {Adverse drug events (ADEs), defined as ``any injuries resulting from medication use, including physical harm, mental harm, or loss of function,''1 are reported to},
  langid = {english},
  annotation = {QID: Q92003775},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2020\\Uzuner et al. - 2020 - Advancing the state of the art in automatic extrac.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\7TG5WS5J\\5678873.html}
}

@phdthesis{vaighContentDataLinking2021,
  ids = {elvaighContentDataLinking2021,vaighContentDataLinking},
  title = {Content and Data Linking Leveraging Ontological Knowledge in Data Journalism},
  author = {Vaigh, Cheikh Brahim El},
  year = {2021},
  month = jan,
  url = {https://hal.inria.fr/tel-03131484},
  urldate = {2021-07-05},
  abstract = {This thesis is about the creation of links between textual content and ontological knowledge bases (KBs). It pertains several areas of research: natural language processing, information retrieval and semantic web, and in particular RDF-based KBs. We propose to study collective entity linking, which consists in linking at once mentions of entities present in a textual document to entities in a KB. To that end, we leverage semantic measures, i.e., entity relatedness measures which exploit the relationships between the entities in a KB. We contribute by the definition of well-founded entity relatedness measures that benefit to the extent possible from the properties of RDF KBs through (basic) reasoning, and thus allow to improve the state-of-the-art. Furthermore, we are also interested in the alignment of different KBs, based on KBs embedding techniques. This alignment not only allows to enrich the KBs at hand, but also to indirectly improve the collective entity linking. We contribute by an alignment criterion, based on the alignment of the dimensions of the KBs embedding spaces, which, notably does not need any prior knowledge to perform said KBs alignment.},
  langid = {english},
  school = {Universit{\'e} Rennes 1},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2021\\Vaigh - 2021 - Content and data linking leveraging ontological kn.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\C6M3VGSA\\tel-03131484.html}
}

@mastersthesis{valleGraphBasedRepresentationsTextual2011,
  title = {Graph-{{Based Representations}} for {{Textual Case-Based Reasoning}}},
  author = {Valle, Kjetil},
  year = {2011},
  month = jun,
  journal = {143},
  url = {https://ntnuopen.ntnu.no/ntnu-xmlui/handle/11250/252468},
  urldate = {2023-08-07},
  abstract = {This thesis presents a graph-based approach to the problem of text representation. The work is motivated by the need for better representations for use in textual Case-Based Reasoning (CBR). In CBR new problems are solved by reasoning based on similar past problem cases. When the cases are represented in free text format, measuring the similarity between a new problem and previously solved problems become a challenging task. The case documents need to be re-represented before they can be compared/matched.Textual CBR (TCBR) addresses this issue. We investigate automatic re-representation of textual cases, in particular measuring the salience of features (entities in the text) towards this end. We use the classical vector space model in Information Retrieval (IR) but investigate whether graph-representation and salience inference using graphs can improve on the Term Frequency (TF) and Term Frequency-Inverse Document Frequency (TF-IDF) measures, emph\{bag of words\} approaches predominant in IR.Our special focus is whether, and possibly how, the co-occurrence and the syntactic dependency relations between terms have an impact on feature weighting. We measure salience through the notion of graph centrality. We experiment with two types of application tasks, classification and case retrieval. Although classification is not a typical TCBR task, it is easier to find datasets for this application, and the centrality measures we have studied are not specific to TCBR. The experiments on this task are therefore relevant to the second application task which is our ultimate target. We test various centrality metrics described in the literature, make a distinction between local and global weighting measures and compare them for both application tasks. In general, our graph-based salience inference methods perform better than TF and TF-IDF.},
  langid = {english},
  school = {Institutt for datateknikk og informasjonsvitenskap},
  annotation = {Accepted: 2014-12-19T13:37:17Z},
  file = {C:\Users\nhiot\OneDrive\zotero\2011\Valle - 2011 - Graph-Based Representations for Textual Case-Based.pdf}
}

@inproceedings{vanekovaFuzzyRDFSemantic2005,
  ids = {vanekovaFuzzyRDFSemantic},
  title = {Fuzzy {{RDF}} in the {{Semantic Web}}: {{Deduction}} and {{Induction}}},
  shorttitle = {Fuzzy {{RDF}} in the {{Semantic Web}}},
  booktitle = {Proceedings of {{Workshop}} on {{Data Analysis}} ({{WDA}} 2005)},
  author = {Vanekov{\'a}, Veronika and Bella, J{\'a}n and Gursk{\`y}, Peter and Horv{\'a}th, Tom{\'a}{\v s}},
  year = {2005},
  pages = {16--29},
  abstract = {Abstract. This work integrates two diploma theses: Logic Programming on Ranked RDF Data and Fuzzy ILP on RDF Data. Both work with fuzzy logic and RDF data, the first one from inductive and the second from deductive point of view. We analyze the possibilities of using RDF for the purpose of logic programming. This includes defining rules for user ranking, transforming them to database select queries, taking the results as positive examples for ILP and finally learning the rules from data. 1},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2005\\Vaneková et al. - 2005 - Fuzzy RDF in the Semantic Web Deduction and Induc.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\VTFHY2IM\\summary.html}
}

@misc{vargas-solarConversationalDataExploration2023,
  ids = {vargas-solarConversationalDataExploration2023b,vargas-solarConversationalDataExploration2023c},
  title = {Conversational {{Data Exploration}}: {{A Game-Changer}} for {{Designing Data Science Pipelines}}},
  shorttitle = {Conversational {{Data Exploration}}},
  author = {{Vargas-Solar}, Genoveva and Cerquitelli, Tania and {Espinosa-Oviedo}, Javier A. and Cheval, Fran{\c c}ois and Buchaille, Anthelme and Polgar, Luca},
  year = {2023},
  month = nov,
  journal = {arXiv e-prints},
  number = {arXiv:2311.06695},
  eprint = {2311.06695},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.06695},
  urldate = {2024-03-25},
  abstract = {This paper proposes a conversational approach implemented by the system Chatin for driving an intuitive data exploration experience. Our work aims to unlock the full potential of data analytics and artificial intelligence with a new generation of data science solutions. Chatin is a cutting-edge tool that democratises access to AI-driven solutions, empowering non-technical users from various disciplines to explore data and extract knowledge from it.},
  archiveprefix = {arxiv},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Databases,Computer Science - Human-Computer Interaction,Conversational Analysis,Data Science Pipelines},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2023\\Vargas-Solar et al. - 2023 - Conversational Data Exploration A Game-Changer fo.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\6QZFS7A7\\2311.html;C\:\\Users\\nhiot\\Zotero\\storage\\QFXINSZ2\\hal-04299052.html}
}

@misc{vargas-solarTextKnowledgeGraphs2023,
  ids = {vargas-solarTextKnowledgeGraphs2023a},
  title = {From {{Text}} to {{Knowledge}} with {{Graphs}}: Modelling, Querying and Exploiting Textual Content},
  shorttitle = {From {{Text}} to {{Knowledge}} with {{Graphs}}},
  author = {{Vargas-Solar}, Genoveva and Alves, Mirian Halfeld Ferrari and Forst, Anne-Lyse Minard},
  year = {2023},
  month = oct,
  number = {arXiv:2310.06122},
  eprint = {2310.06122},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.06122},
  urldate = {2024-03-25},
  abstract = {This paper highlights the challenges, current trends, and open issues related to the representation, querying and analytics of content extracted from texts. The internet contains vast text-based information on various subjects, including commercial documents, medical records, scientific experiments, engineering tests, and events that impact urban and natural environments. Extracting knowledge from this text involves understanding the nuances of natural language and accurately representing the content without losing information. This allows knowledge to be accessed, inferred, or discovered. To achieve this, combining results from various fields, such as linguistics, natural language processing, knowledge representation, data storage, querying, and analytics, is necessary. The vision in this paper is that graphs can be a well-suited text content representation once annotated and the right querying and analytics techniques are applied. This paper discusses this hypothesis from the perspective of linguistics, natural language processing, graph models and databases and artificial intelligence provided by the panellists of the DOING session in the MADICS Symposium 2022.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Databases},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2023\\Vargas-Solar et al. - 2023 - From Text to Knowledge with Graphs modelling, que.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\MW99I9FL\\2310.html}
}

@inproceedings{vargas-solarTranslatingDataScience2023,
  title = {Translating Data Science Queries from Natural Language into Graph Analytics Queries Using {{NLDS-QL}}},
  booktitle = {Workshops of the {{EDBT}}/{{ICDT}} 2023 {{Joint Conference}}},
  author = {{Vargas-Solar}, Genoveva and Dao, Karim and {Espinosa-Oviedo}, Javier},
  year = {2023},
  month = mar,
  number = {3379},
  address = {Ioannina, Greece},
  url = {https://hal.science/hal-04272051},
  urldate = {2024-03-25},
  abstract = {This paper introduces NLDS-QL 1 , a translator of data science questions expressed in natural language (NL) into data science queries on graph databases. Our translator is based on a simplified NL described by a grammar that specifies sentences combining keywords to refer to operations on graphs with the vocabulary of the graph schema. This paper shows NLDS-QL in action within a scenario to explore and analyse a graph base with patient diagnoses generated with the open-source Synthea.},
  keywords = {⛔ No DOI found,data science queries,graph analytics,graph stores,natural language processing},
  file = {C:\Users\nhiot\OneDrive\zotero\2023\Vargas-Solar et al. - 2023 - Translating data science queries from natural lang2.pdf}
}

@inproceedings{velascoMatrixApproachGraph2006,
  title = {Matrix {{Approach}} to {{Graph Transformation}}: {{Matching}} and {{Sequences}}},
  shorttitle = {Matrix {{Approach}} to {{Graph Transformation}}},
  author = {Velasco, Pedro and Lara, Juan},
  year = {2006},
  month = sep,
  pages = {122--137},
  doi = {10/cx5cb5},
  abstract = {In this work we present our approach to (simple di-)graph transformation based on an algebra of boolean matrices. Rules are represented as boolean matrices for nodes and edges and derivations can be efficiently characterized with boolean operations only. Our objective is to analyze properties inherent to rules themselves (without considering an initial graph), so this information can be calculated at specification time. We present basic results concerning well-formedness of rules and derivations (compatibility), as well as concatenation of rules, the conditions under which they are applicable (coherence) and permutations. We introduce the match, which permits the identification of a grammar rule left hand side inside a graph. We follow a similar approach to the single pushout approach (SPO), where dangling edges are deleted, but we first adapt the rule in order to take into account any deleted edge. To this end, a notation borrowed from functional analysis is used. We study the conditions under which the calculated data at specification time can be used when the match is considered.},
  isbn = {978-3-540-38870-8},
  file = {C:\Users\nhiot\OneDrive\zotero\2006\Velasco et Lara - 2006 - Matrix Approach to Graph Transformation Matching .pdf}
}

@article{vepstasGraphQuotientsTopological2017,
  title = {Graph Quotients: A Topological Approach to Graphs},
  shorttitle = {Graph Quotients},
  author = {Vep{\v s}tas, Linas},
  year = {2017},
  journal = {Bulletin of the Novosibirsk Computing Center. Series: Computer Science},
  number = {41},
  pages = {55--90},
  publisher = {{\cyrchar\CYRF}{\cyrchar\cyre}{\cyrchar\cyrd}{\cyrchar\cyre}{\cyrchar\cyrr}{\cyrchar\cyra}{\cyrchar\cyrl}{\cyrchar\cyrsftsn}{\cyrchar\cyrn}{\cyrchar\cyro}{\cyrchar\cyre} {\cyrchar\cyrg}{\cyrchar\cyro}{\cyrchar\cyrs}{\cyrchar\cyru}{\cyrchar\cyrd}{\cyrchar\cyra}{\cyrchar\cyrr}{\cyrchar\cyrs}{\cyrchar\cyrt}{\cyrchar\cyrv}{\cyrchar\cyre}{\cyrchar\cyrn}{\cyrchar\cyrn}{\cyrchar\cyro}{\cyrchar\cyre} {\cyrchar\cyrb}{\cyrchar\cyryu}{\cyrchar\cyrd}{\cyrchar\cyrzh}{\cyrchar\cyre}{\cyrchar\cyrt}{\cyrchar\cyrn}{\cyrchar\cyro}{\cyrchar\cyre} {\cyrchar\cyru}{\cyrchar\cyrch}{\cyrchar\cyrr}{\cyrchar\cyre}{\cyrchar\cyrzh}{\cyrchar\cyrd}{\cyrchar\cyre}{\cyrchar\cyrn}{\cyrchar\cyri}{\cyrchar\cyre} {\cyrchar\cyrn}{\cyrchar\cyra}{\cyrchar\cyru}{\cyrchar\cyrk}{\cyrchar\cyri} {\cyrchar\CYRI}{\cyrchar\cyrn}{\cyrchar\cyrs}{\cyrchar\cyrt}{\cyrchar\cyri}{\cyrchar\cyrt}{\cyrchar\cyru}{\cyrchar\cyrt} {\dots}},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2017\Vepštas - 2017 - Graph quotients a topological approach to graphs.pdf}
}

@article{vrandecicWikidataFreeCollaborative2014,
  title = {Wikidata: A Free Collaborative Knowledgebase},
  shorttitle = {Wikidata},
  author = {Vrande{\v c}i{\'c}, Denny and Kr{\"o}tzsch, Markus},
  year = {2014},
  month = sep,
  journal = {Communications of the ACM},
  volume = {57},
  number = {10},
  pages = {78--85},
  publisher = {ACM New York, NY, USA},
  issn = {0001-0782, 1557-7317},
  doi = {10/gftnsk},
  urldate = {2021-03-19},
  abstract = {This collaboratively edited knowledgebase provides a common source of data for Wikipedia, and everyone else.},
  langid = {english},
  annotation = {QID: Q18507561},
  file = {C:\Users\nhiot\Zotero\storage\UY54F444\2629489.html}
}

@book{vukoticNeo4jAction2015,
  title = {Neo4j in Action},
  author = {Vukotic, Aleksa and Watt, Nicki and Abedrabbo, Tareq and Fox, Dominic and Partner, Jonas},
  year = {2015},
  volume = {22},
  publisher = {Manning Shelter Island},
  file = {C:\Users\nhiot\OneDrive\zotero\2015\Vukotic et al. - 2015 - Neo4j in action.pdf}
}

@inproceedings{wachterDOG4DAGSemiautomatedOntology2011,
  title = {{{DOG4DAG}}: Semi-Automated Ontology Generation in {{OBO-Edit}} and {{Prot{\'e}g{\'e}}}},
  shorttitle = {{{DOG4DAG}}},
  booktitle = {Proceedings of the 4th {{International Workshop}} on {{Semantic Web Applications}} and {{Tools}} for the {{Life Sciences}}},
  author = {W{\"a}chter, Thomas and Fabian, G{\"o}tz and Schroeder, Michael},
  year = {2011},
  month = dec,
  series = {{{SWAT4LS}} '11},
  pages = {119--120},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2166896.2166926},
  urldate = {2024-02-29},
  abstract = {In the biomedical domain, Prot{\'e}g{\'e} and OBO-Edit are the main ontology editors supporting the manual construction of ontologies. Since manual creation is a laborious and hence costly process, there have been efforts to automate parts of this process. Here, we give a demo of the capabilities of DOG4DAG, the Dresden Ontology Generator for Directed Acyclic Graphs, which is available as plugin to both OBO-Edit and Prot{\'e}g{\'e}. In the demo, we describe how to generate terms and in particular siblings, definitions, and is-a relationships using an example in the domain of nervous system diseases. We summarise the strengths and limits of the different the steps of the generation process.},
  isbn = {978-1-4503-1076-5},
  keywords = {biocuration,natural language processing,ontology editor,ontology generation,textmining},
  file = {C:\Users\nhiot\OneDrive\zotero\2011\Wächter et al. - 2011 - DOG4DAG semi-automated ontology generation in OBO.pdf}
}

@incollection{wallaceAnatomyALICE2009,
  title = {The Anatomy of {{ALICE}}},
  booktitle = {Parsing the {{Turing Test}}},
  author = {Wallace, Richard S.},
  year = {2009},
  pages = {181--210},
  publisher = {Springer},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2009\\Wallace - 2009 - The anatomy of ALICE.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\VMTYCEXL\\978-1-4020-6710-5_13.html}
}

@book{wallaceAnatomyALICEArtificial2000,
  title = {The {{Anatomy}} of {{ALICE Artificial Intelligence Foundation}}},
  author = {Wallace, Richard S.},
  year = {2000},
  keywords = {nosource}
}

@article{wallaceElementsAIMLStyle2003,
  title = {The Elements of {{AIML}} Style},
  author = {Wallace, Richard},
  year = {2003},
  journal = {Alice AI Foundation},
  volume = {139},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2003\Wallace - 2003 - The elements of AIML style.pdf}
}

@misc{wangAnnotatedReadingList2020,
  title = {An {{Annotated Reading List}} of {{Conversational AI}}},
  author = {Wang, Haixun},
  year = {2020},
  month = jan,
  journal = {Medium},
  url = {https://medium.com/x8-the-ai-community/a-reading-list-and-mini-survey-of-conversational-ai-32fceea97180},
  urldate = {2021-03-11},
  abstract = {A lot has been written about conversational AI, and a majority of it focuses on vertical chatbots, messenger platforms, business trends{\dots}},
  langid = {english},
  file = {C:\Users\nhiot\Zotero\storage\HUAYZ48D\a-reading-list-and-mini-survey-of-conversational-ai-32fceea97180.html}
}

@article{wangClinicalInformationExtraction2018,
  title = {Clinical Information Extraction Applications: {{A}} Literature Review},
  shorttitle = {Clinical Information Extraction Applications},
  author = {Wang, Yanshan and Wang, Liwei and {Rastegar-Mojarad}, Majid and Moon, Sungrim and Shen, Feichen and Afzal, Naveed and Liu, Sijia and Zeng, Yuqun and Mehrabi, Saeed and Sohn, Sunghwan and Liu, Hongfang},
  year = {2018},
  month = jan,
  journal = {Journal of Biomedical Informatics},
  volume = {77},
  pages = {34--49},
  publisher = {Elsevier},
  issn = {1532-0464},
  doi = {10/gc62zp},
  urldate = {2020-11-17},
  abstract = {Background With the rapid adoption of electronic health records (EHRs), it is desirable to harvest information and knowledge from EHRs to support automated systems at the point of care and to enable secondary use of EHRs for clinical and translational research. One critical component used to facilitate the secondary use of EHR data is the information extraction (IE) task, which automatically extracts and encodes clinical information from text. Objectives In this literature review, we present a review of recent published research on clinical information extraction (IE) applications. Methods A literature search was conducted for articles published from January 2009 to September 2016 based on Ovid MEDLINE In-Process \& Other Non-Indexed Citations, Ovid MEDLINE, Ovid EMBASE, Scopus, Web of Science, and ACM Digital Library. Results A total of 1917 publications were identified for title and abstract screening. Of these publications, 263 articles were selected and discussed in this review in terms of publication venues and data sources, clinical IE tools, methods, and applications in the areas of disease- and drug-related studies, and clinical workflow optimizations. Conclusions Clinical IE has been used for a wide range of applications, however, there is a considerable gap between clinical studies using EHR data and studies using clinical IE. This study enabled us to gain a more concrete understanding of the gap and to provide potential solutions to bridge this gap.},
  langid = {english},
  keywords = {Application,Clinical notes,Electronic health records,Information extraction,Natural language processing},
  annotation = {QID: Q47433545},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2018\\Wang et al. - 2018 - Clinical information extraction applications A li.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\4SSTW3HJ\\S1532046417302563.html}
}

@article{wangComparisonWordEmbeddings2018,
  title = {A Comparison of Word Embeddings for the Biomedical Natural Language Processing},
  author = {Wang, Yanshan and Liu, Sijia and Afzal, Naveed and {Rastegar-Mojarad}, Majid and Wang, Liwei and Shen, Feichen and Kingsbury, Paul and Liu, Hongfang},
  year = {2018},
  month = nov,
  journal = {Journal of Biomedical Informatics},
  volume = {87},
  pages = {12--20},
  issn = {1532-0464},
  doi = {10.1016/j.jbi.2018.09.008},
  urldate = {2023-12-07},
  abstract = {Background Word embeddings have been prevalently used in biomedical Natural Language Processing (NLP) applications due to the ability of the vector representations being able to capture useful semantic properties and linguistic relationships between words. Different textual resources (e.g., Wikipedia and biomedical literature corpus) have been utilized in biomedical NLP to train word embeddings and these word embeddings have been commonly leveraged as feature input to downstream machine learning models. However, there has been little work on evaluating the word embeddings trained from different textual resources. Methods In this study, we empirically evaluated word embeddings trained from four different corpora, namely clinical notes, biomedical publications, Wikipedia, and news. For the former two resources, we trained word embeddings using unstructured electronic health record (EHR) data available at Mayo Clinic and articles (MedLit) from PubMed Central, respectively. For the latter two resources, we used publicly available pre-trained word embeddings, GloVe and Google News. The evaluation was done qualitatively and quantitatively. For the qualitative evaluation, we randomly selected medical terms from three categories (i.e., disorder, symptom, and drug), and manually inspected the five most similar words computed by embeddings for each term. We also analyzed the word embeddings through a 2-dimensional visualization plot of 377 medical terms. For the quantitative evaluation, we conducted both intrinsic and extrinsic evaluation. For the intrinsic evaluation, we evaluated the word embeddings' ability to capture medical semantics by measruing the semantic similarity between medical terms using four published datasets: Pedersen's dataset, Hliaoutakis's dataset, MayoSRS, and UMNSRS. For the extrinsic evaluation, we applied word embeddings to multiple downstream biomedical NLP applications, including clinical information extraction (IE), biomedical information retrieval (IR), and relation extraction (RE), with data from shared tasks. Results The qualitative evaluation shows that the word embeddings trained from EHR and MedLit can find more similar medical terms than those trained from GloVe and Google News. The intrinsic quantitative evaluation verifies that the semantic similarity captured by the word embeddings trained from EHR is closer to human experts' judgments on all four tested datasets. The extrinsic quantitative evaluation shows that the word embeddings trained on EHR achieved the best F1 score of 0.900 for the clinical IE task; no word embeddings improved the performance for the biomedical IR task; and the word embeddings trained on Google News had the best overall F1 score of 0.790 for the RE task. Conclusion Based on the evaluation results, we can draw the following conclusions. First, the word embeddings trained from EHR and MedLit can capture the semantics of medical terms better, and find semantically relevant medical terms closer to human experts' judgments than those trained from GloVe and Google News. Second, there does not exist a consistent global ranking of word embeddings for all downstream biomedical NLP applications. However, adding word embeddings as extra features will improve results on most downstream tasks. Finally, the word embeddings trained from the biomedical domain corpora do not necessarily have better performance than those trained from the general domain corpora for any downstream biomedical NLP task.},
  keywords = {Information extraction,Information retrieval,Machine learning,Natural language processing,Word embeddings},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2018\\Wang et al. - 2018 - A comparison of word embeddings for the biomedical.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\2KRG9QP2\\S1532046418301825.html}
}

@inproceedings{wangCrossdomainNaturalLanguage2019,
  title = {A Cross-Domain Natural Language Interface to Databases Using Adversarial Text Method},
  booktitle = {Database},
  author = {Wang, Wenlu},
  year = {2019},
  volume = {1},
  pages = {4},
  abstract = {A natural language interface (NLI) to databases is an interface that supports natural language queries to be executed by database management systems (DBMS). However, most NLIs are domain specific due to the complexity of the natural language questions, and an NLI trained on one domain is hard to be transferred another due to the discrepancies between di↵erent ontology. Inspired by the idea of stripping domain-specific information out of natural language questions, we propose a cross-domain NLI with a general purpose question tagging strategy and a multi-language neural translation model. Our question tagging strategy is able to extract the ``skeleton'' of the question that represents its semantic structure for any domain. With question tagging, every domain will be handled equally with a single multi-language neural translation model. Our preliminary experiments show that our multi-domain model has excellent cross-domain transferability.},
  langid = {english},
  optabstract = {A natural language interface (NLI) to databases is an interface that supports natural language queries to be executed by database management systems (DBMS). However, most NLIs are domain specific due to the complexity of the natural language questions, and an NLI trained on one domain is hard to be transferred another due to the discrepancies between di?erent ontology. Inspired by the idea of stripping domain-specific information out of natural language questions, we propose a cross-domain NLI with a general purpose question tagging strategy and a multi-language neural translation model. Our question tagging strategy is able to extract the skeleton of the question that represents its semantic structure for any domain. With question tagging, every domain will be handled equally with a single multi-language neural translation model. Our preliminary experiments show that our multi-domain model has excellent cross-domain transferability.},
  optlanguage = {en},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2019\Wang - 2019 - A cross-domain natural language interface to datab.pdf}
}

@article{wangInformationExtractionKnowledge2018,
  title = {Information Extraction and Knowledge Graph Construction from Geoscience Literature},
  author = {Wang, Chengbin and Ma, Xiaogang and Chen, Jianguo and Chen, Jingwen},
  year = {2018},
  month = mar,
  journal = {Computers \& Geosciences},
  volume = {112},
  pages = {112--120},
  issn = {0098-3004},
  doi = {10/gc2263},
  urldate = {2020-08-26},
  abstract = {Geoscience literature published online is an important part of open data, and brings both challenges and opportunities for data analysis. Compared with studies of numerical geoscience data, there are limited works on information extraction and knowledge discovery from textual geoscience data. This paper presents a workflow and a few empirical case studies for that topic, with a focus on documents written in Chinese. First, we set up a hybrid corpus combining the generic and geology terms from geology dictionaries to train Chinese word segmentation rules of the Conditional Random Fields model. Second, we used the word segmentation rules to parse documents into individual words, and removed the stop-words from the segmentation results to get a corpus constituted of content-words. Third, we used a statistical method to analyze the semantic links between content-words, and we selected the chord and bigram graphs to visualize the content-words and their links as nodes and edges in a knowledge graph, respectively. The resulting graph presents a clear overview of key information in an unstructured document. This study proves the usefulness of the designed workflow, and shows the potential of leveraging natural language processing and knowledge graph technologies for geoscience.},
  langid = {english},
  keywords = {Chinese word segmentation,Chord and bigram graphs,Geological corpus,Geoscience literature,Knowledge graph},
  annotation = {QID: Q57357072},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2018\\Wang et al. - 2018 - Information extraction and knowledge graph constru.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\TPLU4HP5\\S0098300417309020.html}
}

@incollection{wehrliActesTALN20002000,
  title = {Actes de {{TALN}} 2000 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 2000 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Wehrli, Eric and Rajman, Martin},
  year = {2000},
  month = oct,
  publisher = {EPFL / ATALA},
  address = {Lausanne},
  keywords = {nosource}
}

@article{weischedelOntoNotesLargeTraining2011,
  ids = {marcusOntoNotesLargeTraining,ralphOntoNotesLargeTraining2011,weischedelOntoNotesLargeTraining,weischedelOntoNotesLargeTraining2011a},
  title = {{{OntoNotes}}: {{A}} Large Training Corpus for Enhanced Processing},
  shorttitle = {{{OntoNotes}}},
  author = {Weischedel, Ralph and Hovy, Eduard and Marcus, Mitchell and Palmer, Martha and Belvin, Robert and Pradhan, Sameer and Ramshaw, Lance and Xue, Nianwen},
  year = {2011},
  journal = {Handbook of Natural Language Processing and Machine Translation},
  pages = {59},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2011\Weischedel et al. - 2011 - OntoNotes A large training corpus for enhanced pr.pdf}
}

@misc{weischedelralphOntoNotesRelease2013,
  title = {{{OntoNotes Release}} 5.0},
  author = {{Weischedel, Ralph} and {Palmer, Martha} and {Marcus, Mitchell} and {Hovy, Eduard} and {Pradhan, Sameer} and {Ramshaw, Lance} and {Xue, Nianwen} and {Taylor, Ann} and {Kaufman, Jeff} and {Franchini, Michelle} and {El-Bachouti, Mohammed} and {Belvin, Robert} and {Houston, Ann}},
  year = {2013},
  month = oct,
  pages = {2806280 KB},
  doi = {10.35111/XMHB-2B84},
  urldate = {2024-03-21},
  abstract = {{$<$}h3{$>$}Introduction{$<$}/h3{$><$}br{$>$}  {$<$}p{$>$}OntoNotes Release 5.0 is the final release of the OntoNotes project, a collaborative effort between {$<$}a href="http://www.bbn.com/" rel="nofollow"{$>$}BBN Technologies{$<$}/a{$>$}, the {$<$}a href="http://www.colorado.edu/" rel="nofollow"{$>$}University of Colorado{$<$}/a{$>$}, the {$<$}a href="http://www.upenn.edu/" rel="nofollow"{$>$}University of Pennsylvania{$<$}/a{$>$} and the {$<$}a href="http://www.isi.edu/home" rel="nofollow"{$>$}University of Southern Californias Information Sciences Institute{$<$}/a{$>$}. The goal of the project was to annotate a large corpus comprising various genres of text (news, conversational telephone speech, weblogs, usenet newsgroups, broadcast, talk shows) in three languages (English, Chinese, and Arabic) with structural information (syntax and predicate argument structure) and shallow semantics (word sense linked to an ontology and coreference).{$<$}/p{$><$}br{$>$}  {$<$}p{$>$}OntoNotes Release 5.0 contains the content of earlier releases -- OntoNotes Release 1.0 {$<$}a href="http://catalog.ldc.upenn.edu/LDC2007T21" rel="nofollow"{$>$}LDC2007T21{$<$}/a{$>$}, OntoNotes Release 2.0 {$<$}a href="http://catalog.ldc.upenn.edu/LDC2008T04" rel="nofollow"{$>$}LDC2008T04{$<$}/a{$>$}, OntoNotes Release 3.0 {$<$}a href="http://catalog.ldc.upenn.edu/LDC2009T24" rel="nofollow"{$>$}LDC2009T24{$<$}/a{$>$} and OntoNotes Release 4.0 {$<$}a href="http://catalog.ldc.upenn.edu/LDC2011T03" rel="nofollow"{$>$}LDC2011T03{$<$}/a{$>$} -- and adds source data from and/or additional annotations for, newswire (News), broadcast news (BN), broadcast conversation (BC), telephone conversation (Tele) and web data (Web) in English and Chinese and newswire data in Arabic. Also contained is English pivot text (Old Testament and New Testament text). This cumulative publication consists of 2.9 million words with counts shown in the table below.{$<$}/p{$><$}br{$>$}  {$<$}table{$><$}br{$>$}  {$<$}tbody{$><$}br{$>$}  {$<$}tr{$><$}br{$>$}  {$<$}td{$>\&$}nbsp;{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}Arabic{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}English{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}Chinese{$<$}/td{$><$}br{$>$}  {$<$}/tr{$><$}br{$>$}  {$<$}tr{$><$}br{$>$}  {$<$}td{$>$}News{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}300k{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}625k{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}250k{$<$}/td{$><$}br{$>$}  {$<$}/tr{$><$}br{$>$}  {$<$}tr{$><$}br{$>$}  {$<$}td{$>$}BN{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}n/a{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}200k{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}250k{$<$}/td{$><$}br{$>$}  {$<$}/tr{$><$}br{$>$}  {$<$}tr{$><$}br{$>$}  {$<$}td{$>$}BC{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}n/a{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}200k{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}150k{$<$}/td{$><$}br{$>$}  {$<$}/tr{$><$}br{$>$}  {$<$}tr{$><$}br{$>$}  {$<$}td{$>$}Web{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}n/a{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}300k{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}150k{$<$}/td{$><$}br{$>$}  {$<$}/tr{$><$}br{$>$}  {$<$}tr{$><$}br{$>$}  {$<$}td{$>$}Tele{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}n/a{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}120k{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}100k{$<$}/td{$><$}br{$>$}  {$<$}/tr{$><$}br{$>$}  {$<$}tr{$><$}br{$>$}  {$<$}td{$>$}Pivot{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}n/a{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}n/a{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}300{$<$}/td{$><$}br{$>$}  {$<$}/tr{$><$}br{$>$}  {$<$}/tbody{$><$}br{$>$}  {$<$}/table{$><$}br{$>$}  {$<$}p{$>\&$}nbsp;{$<$}/p{$><$}br{$>$}  {$<$}p{$>$}The OntoNotes project built on two time-tested resources, following the {$<$}a href="http://catalog.ldc.upenn.edu/LDC99T42" rel="nofollow"{$>$}Penn Treebank{$<$}/a{$>$} for syntax and the {$<$}a href="http://catalog.ldc.upenn.edu/LDC2004T14" rel="nofollow"{$>$}Penn PropBank{$<$}/a{$>$} for predicate-argument structure. Its semantic representation includes word sense disambiguation for nouns and verbs, with some word senses connected to an ontology, and coreference.{$<$}/p{$><$}br{$>$}  {$<$}h3{$>$}Data{$<$}/h3{$><$}br{$>$}  {$<$}p{$>$}Documents describing the annotation guidelines and the routines for deriving various views of the data from the database are included in the documentation directory of this release. The annotation is provided both in separate text files for each annotation layer (Treebank, PropBank, word sense, etc.) and in the form of an integrated relational database (ontonotes-v5.0.sql.gz) with a Python API to provide convenient cross-layer access.{$<$}/p{$><$}br{$>$}  {$<$}p{$>$}It is a known issue that this release contains some non-validating XML files. The included tools, however, use a non-validating XML parser to parse the .xml files and load the appropriate values.{$<$}/p{$><$}br{$>$}  {$<$}h3{$>$}Tools{$<$}/h3{$><$}br{$>$}  {$<$}p{$>$}This release includes OntoNotes DB Tool v0.999 beta, the tool used to assemble the database from the original annotation files. It can be found in the directory tools/ontonotes-db-tool-v0.999b. This tool can be used to derive various views of the data from the database, and it provides an API that can implement new queries or views. Licensing information for the OntoNotes DB Tool package is included in its source directory.{$<$}/p{$><$}br{$>$}  {$<$}h3{$>$}Samples{$<$}/h3{$><$}br{$>$}  {$<$}p{$>$}Please view these samples:{$<$}/p{$><$}br{$>$}  {$<$}ul{$><$}br{$>$}  {$<$}li{$><$}a href="desc/addenda/LDC2013T19.cmn.jpg" rel="nofollow"{$>$}Chinese{$<$}/a{$><$}/li{$><$}br{$>$}  {$<$}li{$><$}a href="desc/addenda/LDC2013T19.ara.jpg" rel="nofollow"{$>$}Arabic{$<$}/a{$><$}/li{$><$}br{$>$}  {$<$}li{$><$}a href="desc/addenda/LDC2013T19.eng.jpg" rel="nofollow"{$>$}English{$<$}/a{$><$}/li{$><$}br{$>$}  {$<$}/ul{$><$}br{$>$}  {$<$}h3{$>$}Updates{$<$}/h3{$><$}br{$>$}  {$<$}p{$>$}Additional documentation was added on December 11, 2014\&nbsp; and is included in downloads after that date.\&nbsp;{$<$}/p{$><$}br{$>$}  {$<$}h3{$>$}Acknowledgment{$<$}/h3{$><$}br{$>$}  {$<$}p{$>$}This work is supported in part by the Defense Advanced Research Projects Agency, GALE Program Grant No. HR0011-06-1-003. The content of this publication does not necessarily reflect the position or policy of the Government, and no official endorsement should be inferred.{$<$}/p{$><$}/br{$>$}  Portions {\copyright} 2006 Abu Dhabi TV, {\copyright} 2006 Agence France Presse, {\copyright} 2006 Al-Ahram, {\copyright} 2006 Al Alam News Channel, {\copyright} 2006 Al Arabiya, {\copyright} 2006 Al Hayat, {\copyright} 2006 Al Iraqiyah, {\copyright} 2006 Al Quds-Al Arabi, {\copyright} 2006 Anhui TV, {\copyright} 2002, 2006 An Nahar, {\copyright} 2006 Asharq-al-Awsat, {\copyright} 2010 Bible League International, {\copyright} 2005 Cable News Network, LP, LLLP, {\copyright} 2000-2001 China Broadcasting System, {\copyright} 2000-2001, 2005-2006 China Central TV, {\copyright} 2006 China Military Online, {\copyright} 2000-2001 China National Radio, {\copyright} 2006 Chinanews.com, {\copyright} 2000-2001 China Television System, {\copyright} 1989 Dow Jones \& Company, Inc., {\copyright} 2006 Dubai TV, {\copyright} 2006 Guangming Daily, {\copyright} 2006 Kuwait TV, {\copyright} 2005-2006 National Broadcasting Company, Inc., {\copyright} 2006 New Tang Dynasty TV, {\copyright} 2006 Nile TV, {\copyright} 2006 Oman TV, {\copyright} 2006 PAC Ltd, {\copyright} 2006 Peoples Daily Online, {\copyright} 2005-2006 Phoenix TV, {\copyright} 2000-2001 Sinorama Magazine, {\copyright} 2006 Syria TV, {\copyright} 1996-1998, 2006 Xinhua News Agency, {\copyright} 1996, 1997, 2005, 2007, 2008, 2009, 2011, 2013 Trustees of the University of Pennsylvania}
}

@article{weizenbaumELIZAComputerProgram1966,
  ids = {weizenbaumElizaComputerProgram1983},
  title = {{{ELIZA}}---a Computer Program for the Study of Natural Language Communication between Man and Machine},
  author = {Weizenbaum, Joseph},
  year = {1966},
  month = jan,
  journal = {Communications of the ACM},
  volume = {9},
  number = {1},
  pages = {36--45},
  publisher = {ACM New York, NY, USA},
  issn = {0001-0782, 1557-7317},
  doi = {10/bgsj5s},
  urldate = {2021-03-29},
  langid = {english},
  annotation = {QID: Q55869048},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1966\\Weizenbaum - 1966 - ELIZA—a computer program for the study of natural .pdf;C\:\\Users\\nhiot\\Zotero\\storage\\YAK4SBMY\\357980.html}
}

@article{wengMedicalSubdomainClassification2017,
  ids = {weng2017medical},
  title = {Medical Subdomain Classification of Clinical Notes Using a Machine Learning-Based Natural Language Processing Approach},
  author = {Weng, Wei-Hung and Wagholikar, Kavishwar B. and McCray, Alexa T. and Szolovits, Peter and Chueh, Henry C.},
  year = {2017},
  month = dec,
  journal = {BMC Medical Informatics and Decision Making},
  volume = {17},
  number = {1},
  pages = {1--13},
  publisher = {BioMed Central},
  issn = {1472-6947},
  doi = {10.1186/s12911-017-0556-8},
  urldate = {2021-05-28},
  abstract = {The medical subdomain of a clinical note, such as cardiology or neurology, is useful content-derived metadata for developing machine learning downstream applications. To classify the medical subdomain of a note accurately, we have constructed a machine learning-based natural language processing (NLP) pipeline and developed medical subdomain classifiers based on the content of the note.},
  keywords = {{Medical Decision Making, Computer-assisted},Deep Learning,Distributed Representation,Machine Learning,Natural Language Processing,Unified Medical Language System},
  annotation = {QID: Q45943393},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2017\\Weng et al. - 2017 - Medical subdomain classification of clinical notes.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\SHS3H2R3\\s12911-017-0556-8.html}
}

@article{whangNonexhaustiveOverlappingKmeans2015,
  title = {Non-Exhaustive, {{Overlapping}} k-Means},
  author = {Whang, J. J. and Dhillon, I. S. and Gleich, D. F.},
  year = {2015},
  doi = {10/gfvn22},
  keywords = {nosource},
  annotation = {00000}
}

@misc{WhatSemanticAnnotation,
  title = {What Is {{Semantic Annotation}} - {{Tag Metadata}} in {{Text}}},
  journal = {Ontotext},
  url = {https://www.ontotext.com/knowledgehub/fundamentals/semantic-annotation/},
  urldate = {2019-02-18},
  abstract = {Semantic Annotation helps to bridge the ambiguity of the natural language when expressing notions and their computational representation.},
  langid = {american},
  annotation = {00000},
  file = {C:\Users\nhiot\Zotero\storage\M4WW2AKQ\semantic-annotation.html}
}

@article{whiteGraphSemigroupHomomorphisms1983,
  title = {Graph and Semigroup Homomorphisms on Networks of Relations},
  author = {White, Douglas R. and Reitz, Karl P.},
  year = {1983},
  month = jun,
  journal = {Social Networks},
  volume = {5},
  number = {2},
  pages = {193--234},
  issn = {0378-8733},
  doi = {10.1016/0378-8733(83)90025-4},
  urldate = {2023-10-31},
  abstract = {The algebraic definitions presented here are motivated by our search for an adequate formalization of the concepts of social roles as regularities in social network patterns. The theorems represent significant homomorphic reductions of social networks which are possible using these definitions to capture the role structure of a network. The concepts build directly on the pioneering work of S.F. Nadel (1957) and the pathbreaking approach to blockmodeling introduced by Lorrain and White (1971) and refined in subsequent years (White, Boorman and Breiger 1976;Boorman and White 1976; Arabie, Boorman and Levitt, 1978; Sailer, 1978). Blockmodeling is one of the predominant techniques for deriving structural models of social networks. When a network is represented by a directed multigraph, a blockmodel of the multigraph can be characterized as mapping points and edges onto their images in a reduced multigraph. The relations in a network or multigraph can also be composed to form a semigroup. In the first part of the paper we examine ``graph'' homomorphisms, or homomorphic mappings of the points or actors in a network. A family of basic concepts of role equivalence are introduced, and theorems presented to show the structure preserving properties of their various induced homomorphisms. This extends the ``classic'' approach to blockmodeling via the equivalence of positions. Lorrain and White (1971), Pattison (1980), Boyd, 1980, Boyd, 1982, and most recently Bonacich (1982) have explored the topic taken up in the second part of this paper, namely the homomorphic reduction of the semigroup of relations on a network, and the relation between semigroup and graph homomorphisms. Our approach allows us a significant beginning in reducing the complexity of a multigraph by collapsing relations which play a similar ``role'' in the network.},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1983\\White et Reitz - 1983 - Graph and semigroup homomorphisms on networks of r.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\9HE8RHYA\\0378873383900254.html}
}

@article{whiteRethinkingRoleConcept1989,
  title = {Rethinking the Role Concept: {{Homomorphisms}} on Social Networks},
  shorttitle = {Rethinking the Role Concept},
  author = {White, Douglas R. and Reitz, Karl P.},
  year = {1989},
  journal = {Research methods in social network analysis},
  pages = {429--488},
  publisher = {George Mason University Press Fairfax, VA},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\Zotero\storage\4DN2FNQW\books.html}
}

@misc{WikilinksNel2019,
  title = {Wikilinks/Nel},
  year = {2019},
  month = dec,
  url = {https://github.com/wikilinks/nel},
  urldate = {2019-12-18},
  abstract = {Entity linking framework. Contribute to wikilinks/nel development by creating an account on GitHub.},
  copyright = {MIT},
  howpublished = {Wikilinks},
  keywords = {entity-linking,information-extraction,machine-learning,natural-language-processing,nosource}
}

@book{wikipediaDistanceManhattanWikipedia2017,
  title = {Distance de {{Manhattan}} --- {{Wikipedia}}, {{The Free Encyclopedia}}},
  author = {{Wikipedia}},
  year = {2017},
  url = {https://fr.wikipedia.org/wiki/Distance\_de\_Manhattan},
  urldate = {2017-07-20},
  keywords = {nosource},
  annotation = {00000}
}

@article{wikipediaMethodeFormelleInformatique2018,
  title = {{M{\'e}thode formelle (informatique) --- Wikip{\'e}dia, l'encyclop{\'e}die libre}},
  author = {{Wikip{\'e}dia}},
  year = {2018},
  month = dec,
  journal = {Wikip{\'e}dia},
  url = {http://fr.wikipedia.org/w/index.php?title=M\%C3\%A9thode\_formelle\_(informatique)\&oldid=155114812},
  urldate = {2019-01-06},
  abstract = {En informatique, les m{\'e}thodes formelles sont des techniques permettant de raisonner rigoureusement, {\`a} l'aide de logique math{\'e}matique, sur des programmes informatiques ou du mat{\'e}riel {\'e}lectronique, afin de d{\'e}montrer leur validit{\'e} par rapport {\`a} une certaine sp{\'e}cification. Elles sont bas{\'e}es sur les s{\'e}mantiques des programmes, c'est-{\`a}-dire sur des descriptions math{\'e}matiques formelles du sens d'un programme donn{\'e} par son code source (ou parfois, son code objet). Ces m{\'e}thodes permettent d'obtenir une tr{\`e}s forte assurance de l'absence de bug dans les logiciels (Evaluation Assurance Level, Safety Integrity Level). Elles sont utilis{\'e}es dans le d{\'e}veloppement des logiciels les plus critiques. Leur am{\'e}lioration et l'{\'e}largissement de leurs champs d'application pratique sont la motivation de nombreuses recherches scientifiques en informatique.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {french},
  annotation = {00000  Page Version ID: 155114812},
  file = {C:\Users\nhiot\Zotero\storage\MXHXQVUP\index.html}
}

@article{wimalasuriyaOntologybasedInformationExtraction2010,
  title = {Ontology-Based Information Extraction: {{An}} Introduction and a Survey of Current Approaches},
  shorttitle = {Ontology-Based Information Extraction},
  author = {Wimalasuriya, Daya C. and Dou, Dejing},
  year = {2010},
  month = jun,
  journal = {Journal of Information Science},
  volume = {36},
  number = {3},
  pages = {306--323},
  issn = {0165-5515},
  doi = {10/b6qt7k},
  urldate = {2019-01-29},
  abstract = {Information extraction (IE) aims to retrieve certain types of information from natural language text by processing them automatically. For example, an IE system might retrieve information about geopolitical indicators of countries from a set of web pages while ignoring other types of information. Ontology-based information extraction (OBIE) has recently emerged as a subfield of information extraction. Here, ontologies - which provide formal and explicit specifications of conceptualizations - play a crucial role in the IE process. Because of the use of ontologies, this field is related to knowledge representation and has the potential to assist the development of the Semantic Web. In this paper, we provide an introduction to ontology-based information extraction and review the details of different OBIE systems developed so far. We attempt to identify a common architecture among these systems and classify them based on different factors, which leads to a better understanding on their operation. We also discuss the implementation details of these systems including the tools used by them and the metrics used to measure their performance. In addition, we attempt to identify the possible future directions for this field.},
  langid = {english},
  file = {C:\Users\nhiot\OneDrive\zotero\2010\Wimalasuriya et Dou - 2010 - Ontology-based information extraction An introduc.pdf}
}

@article{winslettModelbasedApproachUpdating1988,
  title = {A Model-Based Approach to Updating Databases with Incomplete Information},
  author = {Winslett, Marianne},
  year = {1988},
  month = jun,
  journal = {ACM Transactions on Database Systems (TODS)},
  volume = {13},
  number = {2},
  pages = {167--196},
  publisher = {ACM New York, NY, USA},
  issn = {0362-5915},
  doi = {10.1145/42338.42386},
  urldate = {2023-08-03},
  abstract = {Suppose one wishes to construct, use, and maintain a database of facts about the real world, even though the state of that world is only partially known. In the artificial intelligence domain, this problem arises when an agent has a base set of beliefs that reflect partial knowledge about the world, and then tries to incorporate new, possibly contradictory knowledge into this set of beliefs. In the database domain, one facet of this situation is the well-known null values problem. We choose to represent such a database as a logical theory, and view the models of the theory as representing possible states of the world that are consistent with all known information. How can new information be incorporated into the database? For example, given the new information that ``b or c is true,'' how can one get rid of all outdated information about b and c, add the new information, and yet in the process not disturb any other information in the database? In current-day database management systems, the difficult and tedious burden of determining exactly what to add and remove from the database is placed on the user. The goal of our research was to relieve users of that burden, by equipping the database management system with update algorithms that can automatically determine what to add and remove from the database. Under our approach, new information about the state of the world is input to the database management system as a well-formed formula that the state of the world is now known to satisfy. We have constructed database update algorithms to interpret this update formula and incorporate the new information represented by the formula into the database without further assistance from the user. In this paper we show how to embed the incomplete database and the incoming information in the language of mathematical logic, explain the semantics of our update operators, and discuss the algorithms that implement these operators.},
  langid = {english},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1988\\Winslett - 1988 - A model-based approach to updating databases with .pdf;C\:\\Users\\nhiot\\Zotero\\storage\\IP27ELLK\\42338.html}
}

@book{winslettUpdatingLogicalDatabases1990,
  title = {Updating Logical Databases},
  author = {Winslett, Marianne Southall},
  year = {1990},
  series = {Cambridge Tracts in Theoretical Computer Science},
  number = {9},
  publisher = {Cambridge University Press},
  address = {Cambridge ; New York},
  isbn = {978-0-521-37371-5},
  lccn = {QA76.9.D3 W567 1990},
  keywords = {Database management}
}

@article{wishartDrugBankKnowledgebaseDrugs2008,
  title = {{{DrugBank}}: A Knowledgebase for Drugs, Drug Actions and Drug Targets},
  shorttitle = {{{DrugBank}}},
  author = {Wishart, David S. and Knox, Craig and Guo, An Chi and Cheng, Dean and Shrivastava, Savita and Tzur, Dan and Gautam, Bijaya and Hassanali, Murtaza},
  year = {2008},
  journal = {Nucleic acids research},
  volume = {36},
  number = {suppl\_1},
  pages = {D901--D906},
  publisher = {Oxford University Press},
  doi = {10.1093/nar/gkm958},
  annotation = {QID: Q24650300},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2008\\Wishart et al. - 2008 - DrugBank a knowledgebase for drugs, drug actions .pdf;C\:\\Users\\nhiot\\Zotero\\storage\\ESEGG5Y5\\2508024.html;C\:\\Users\\nhiot\\Zotero\\storage\\PZDDAR83\\2508024.html}
}

@article{wishartDrugBankMajorUpdate2018,
  title = {{{DrugBank}} 5.0: A Major Update to the {{DrugBank}} Database for 2018},
  shorttitle = {{{DrugBank}} 5.0},
  author = {Wishart, David S. and Feunang, Yannick D. and Guo, An C. and Lo, Elvis J. and Marcu, Ana and Grant, Jason R. and Sajed, Tanvir and Johnson, Daniel and Li, Carin and Sayeeda, Zinat},
  year = {2018},
  journal = {Nucleic acids research},
  volume = {46},
  number = {D1},
  pages = {D1074--D1082},
  publisher = {Oxford University Press},
  doi = {10.1093/nar/gkx1037},
  annotation = {QID: Q47128239},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2018\\Wishart et al. - 2018 - DrugBank 5.0 a major update to the DrugBank datab.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\6JDTJNUQ\\4602867.html}
}

@misc{WolframAlphaMaking,
  title = {Wolfram{\textbar}{{Alpha}}: {{Making}} the World's Knowledge Computable},
  shorttitle = {Wolfram{\textbar}{{Alpha}}},
  url = {https://www.wolframalpha.com},
  urldate = {2020-05-14},
  abstract = {Wolfram{\textbar}Alpha brings expert-level knowledge and capabilities to the broadest possible range of people---spanning all professions and education levels.},
  file = {C:\Users\nhiot\Zotero\storage\JLWTGVYZ\www.wolframalpha.com.html}
}

@article{wongOntologyLearningText2011,
  title = {Ontology {{Learning}} from {{Text}}: {{A Look Back}} and into the {{Future}}},
  shorttitle = {Ontology {{Learning}} from {{Text}}},
  author = {Wong, Wilson and Liu, Wei and Bennamoun, Mohammed},
  year = {2011},
  month = jan,
  journal = {ACM Computing Surveys - CSUR},
  volume = {44},
  pages = {1--36},
  doi = {10.1145/2333112.2333115},
  abstract = {Ontologies are often viewed as the answer to the need for interoperable semantics in modern information systems. The explosion of textual information on the Read/Write Web coupled with the increasing demand for ontologies to power the Semantic Web have made (semi-)automatic ontology learning from text a very promising research area. This together with the advanced state in related areas, such as natural language processing, have fueled research into ontology learning over the past decade. This survey looks at how far we have come since the turn of the millennium and discusses the remaining challenges that will define the research directions in this area in the near future.},
  file = {C:\Users\nhiot\OneDrive\zotero\2011\Wong et al. - 2011 - Ontology Learning from Text A Look Back and into .pdf}
}

@article{wuEnrichingPretrainedLanguage2019,
  title = {Enriching {{Pre-trained Language Model}} with {{Entity Information}} for {{Relation Classification}}},
  author = {Wu, Shanchan and He, Yifan},
  year = {2019},
  month = may,
  journal = {arXiv:1905.08284 [cs]},
  eprint = {1905.08284},
  primaryclass = {cs},
  doi = {10/ghkfr8},
  urldate = {2020-07-21},
  abstract = {Relation classification is an important NLP task to extract relations between entities. The state-of-the-art methods for relation classification are primarily based on Convolutional or Recurrent Neural Networks. Recently, the pre-trained BERT model achieves very successful results in many NLP classification / sequence labeling tasks. Relation classification differs from those tasks in that it relies on information of both the sentence and the two target entities. In this paper, we propose a model that both leverages the pre-trained BERT language model and incorporates information from the target entities to tackle the relation classification task. We locate the target entities and transfer the information through the pre-trained architecture and incorporate the corresponding encoding of the two entities. We achieve significant improvement over the state-of-the-art method on the SemEval-2010 task 8 relational dataset.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2019\\Wu et He - 2019 - Enriching Pre-trained Language Model with Entity I.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\8YTCWB96\\1905.html}
}

@article{xuMedExMedicationInformation2010,
  ids = {xuMedExMedicationInformation2010a},
  title = {{{MedEx}}: A Medication Information Extraction System for Clinical Narratives},
  shorttitle = {{{MedEx}}},
  author = {Xu, Hua and Stenner, Shane P. and Doan, Son and Johnson, Kevin B. and Waitman, Lemuel R. and Denny, Joshua C.},
  year = {2010},
  month = jan,
  journal = {Journal of the American Medical Informatics Association},
  volume = {17},
  number = {1},
  pages = {19--24},
  publisher = {Oxford Academic},
  issn = {1067-5027},
  doi = {10/fnpntb},
  urldate = {2020-11-20},
  abstract = {Abstract. Medication information is one of the most important types of clinical data in electronic medical records. It is critical for healthcare safety and qua},
  langid = {english},
  annotation = {QID: Q34371568},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2010\\Xu et al. - 2010 - MedEx a medication information extraction system .pdf;C\:\\Users\\nhiot\\Zotero\\storage\\EW4IZWU2\\governor.html}
}

@inproceedings{xuMirrorNaturalLanguage2023,
  title = {Mirror: {{A Natural Language Interface}} for {{Data Querying}}, {{Summarization}}, and {{Visualization}}},
  shorttitle = {Mirror},
  booktitle = {Companion {{Proceedings}} of the {{ACM Web Conference}} 2023},
  author = {Xu, Canwen and McAuley, Julian and Wang, Penghan},
  year = {2023},
  month = apr,
  series = {{{WWW}} '23 {{Companion}}},
  eprint = {2303.08697},
  primaryclass = {cs},
  pages = {49--52},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3543873.3587309},
  urldate = {2024-03-25},
  abstract = {We present Mirror, an open-source platform for data exploration and analysis powered by large language models. Mirror offers an intuitive natural language interface for querying databases, and automatically generates executable SQL commands to retrieve relevant data and summarize it in natural language. In addition, users can preview and manually edit the generated SQL commands to ensure the accuracy of their queries. Mirror also generates visualizations to facilitate understanding of the data. Designed with flexibility and human input in mind, Mirror is suitable for both experienced data analysts and non-technical professionals looking to gain insights from their data.},
  archiveprefix = {arxiv},
  isbn = {978-1-4503-9419-2},
  langid = {english},
  keywords = {automatic data analysis,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Databases,Computer Science - Software Engineering,natural language interface,pretrained language model,semantic parsing},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2023\\Xu et al. - 2023 - Mirror A Natural Language Interface for Data Quer2.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\H8LEB38D\\2303.html}
}

@inproceedings{yadav-bethard-2018-survey,
  title = {A Survey on Recent Advances in Named Entity Recognition from Deep Learning Models},
  booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
  author = {Yadav, Vikas and Bethard, Steven},
  year = {2018},
  month = aug,
  pages = {2145--2158},
  publisher = {Association for Computational Linguistics},
  address = {Santa Fe, New Mexico, USA},
  url = {https://www.aclweb.org/anthology/C18-1182},
  abstract = {Named Entity Recognition (NER) is a key component in NLP systems for question answering, information retrieval, relation extraction, etc. NER systems have been studied and developed widely for decades, but accurate systems using deep neural networks (NN) have only been introduced in the last few years. We present a comprehensive survey of deep neural network architectures for NER, and contrast them with previous approaches to NER based on feature engineering and other supervised or semi-supervised learning algorithms. Our results highlight the improvements achieved by neural networks, and show how incorporating some of the lessons learned from past work on feature-based NER systems can yield further improvements.},
  keywords = {nosource}
}

@article{yagerFuzzyLogicMethods2003,
  title = {Fuzzy Logic Methods in Recommender Systems},
  author = {Yager, Ronald R.},
  year = {2003},
  month = jun,
  journal = {Fuzzy Sets and Systems},
  volume = {136},
  number = {2},
  pages = {133--149},
  issn = {0165-0114},
  doi = {10/cm3vn3},
  urldate = {2019-02-15},
  abstract = {Here we consider methodologies for constructing recommender systems. The approaches studied here differ from collaborative filtering, they are based solely on the preferences of the single individual for whom we are providing the recommendation and make no use of the preferences of other collaborators. We have called these reclusive methods. Another important feature distinguishing these reclusive methods from collaborative methods is that they require a{\textasciitilde}representation of the objects. Considerable use is made of fuzzy set methods for the representation and subsequent construction of justifications and recommendation rules. It is pointed out these reclusive methods rather than being competitive with collaborative methods are complementary.},
  keywords = {Collaborative filtering,Customization,Fuzzy methods,Recommender systems},
  annotation = {00000},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2003\\Yager - 2003 - Fuzzy logic methods in recommender systems.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\UWILWXUI\\S0165011402002233.html}
}

@inproceedings{yanGSpanGraphbasedSubstructure2002,
  title = {{{gSpan}}: Graph-Based Substructure Pattern Mining},
  shorttitle = {{{gSpan}}},
  booktitle = {2002 {{IEEE International Conference}} on {{Data Mining}}, 2002. {{Proceedings}}.},
  author = {Yan, Xifeng and Han, Jiawei},
  year = {2002},
  month = dec,
  pages = {721--724},
  doi = {10/c484q8},
  abstract = {We investigate new approaches for frequent graph-based pattern mining in graph datasets and propose a novel algorithm called gSpan (graph-based substructure pattern mining), which discovers frequent substructures without candidate generation. gSpan builds a new lexicographic order among graphs, and maps each graph to a unique minimum DFS code as its canonical label. Based on this lexicographic order gSpan adopts the depth-first search strategy to mine frequent connected subgraphs efficiently. Our performance study shows that gSpan substantially outperforms previous algorithms, sometimes by an order of magnitude.},
  keywords = {algorithm,canonical label,Chemical compounds,Computer science,Costs,data mining,Data mining,Data structures,depth-first search strategy,frequent connected subgraph mining,frequent graph-based pattern mining,frequent substructure discovery,graph datasets,graph-based substructure pattern mining,Graphics,gSpan,Itemsets,Kernel,lexicographic order,performance study,Testing,Tree graphs,tree searching,unique minimum DFS code},
  annotation = {00000},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2002\\Yan et Han - 2002 - gSpan graph-based substructure pattern mining.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\A67IDYWW\\1184038.html}
}

@inproceedings{yarowskyUnsupervisedWordSense1995,
  title = {Unsupervised Word Sense Disambiguation Rivaling Supervised Methods},
  booktitle = {Proceedings of the 33rd Annual Meeting on {{Association}} for {{Computational Linguistics}}},
  author = {Yarowsky, David},
  year = {1995},
  month = jun,
  series = {{{ACL}} '95},
  pages = {189--196},
  publisher = {Association for Computational Linguistics},
  address = {USA},
  doi = {10/d725b4},
  urldate = {2020-11-26},
  abstract = {This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints---that words tend to have one sense per discourse and one sense per collocation---exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96\%.},
  annotation = {QID: Q118444775},
  file = {C:\Users\nhiot\OneDrive\zotero\1995\Yarowsky - 1995 - Unsupervised word sense disambiguation rivaling su.pdf}
}

@article{yihSemanticParsingStaged2015,
  title = {Semantic {{Parsing}} via {{Staged Query Graph Generation}}: {{Question Answering}} with {{Knowledge Base}}},
  shorttitle = {Semantic {{Parsing}} via {{Staged Query Graph Generation}}},
  author = {Yih, Scott Wen-tau and Chang, Ming-Wei and He, Xiaodong and Gao, Jianfeng},
  year = {2015},
  month = jul,
  doi = {10/gfv8b3},
  urldate = {2019-11-27},
  abstract = {We propose a novel semantic parsing framework for question answering using a knowledge base. We define a query graph that resembles subgraphs of the knowledge base and can be directly mapped to a logical form. Semantic parsing is reduced to query graph generation, formulated as a staged search problem. Unlike traditional approaches, our method leverages {\dots}},
  langid = {american},
  annotation = {QID: Q86325193},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2015\\Yih et al. - 2015 - Semantic Parsing via Staged Query Graph Generation.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\FZYKHEUK\\semantic-parsing-via-staged-query-graph-generation-question-answering-with-knowledge-base.html}
}

@inproceedings{yinNeuralEnquirerLearning2016,
  title = {Neural {{Enquirer}}: {{Learning}} to {{Query Tables}} in {{Natural Language}}},
  shorttitle = {Neural {{Enquirer}}},
  booktitle = {Proceedings of the {{Workshop}} on {{Human-Computer Question Answering}}},
  author = {Yin, Pengcheng and Lu, Zhengdong and Li, Hang and Ben, kao},
  year = {2016},
  pages = {29--35},
  publisher = {Association for Computational Linguistics},
  address = {San Diego, California},
  doi = {10/gfvn2t},
  urldate = {2019-01-22},
  abstract = {We propose NEURAL ENQUIRER --- a neural network architecture for answering natural language (NL) questions based on a knowledge base (KB) table. Unlike existing work on end-to-end training of semantic parsers [Pasupat and Liang, 2015; Neelakantan et al., 2015], NEURAL ENQUIRER is fully ``neuralized'': it finds distributed representations of queries and KB tables, and executes queries through a series of neural network components called ``executors''. Executors model query operations and compute intermediate execution results in the form of table annotations at different levels. NEURAL ENQUIRER can be trained with gradient descent, with which the representations of queries and the KB table are jointly optimized with the query execution logic. The training can be done in an end-to-end fashion, and it can also be carried out with stronger guidance, e.g., step-by-step supervision for complex queries. NEURAL ENQUIRER is one step towards building neural network systems that can understand natural language in real-world tasks. As a proof-of-concept, we conduct experiments on a synthetic QA task, and demonstrate that the model can learn to execute reasonably complex NL queries on small-scale KB tables.},
  langid = {english},
  annotation = {00000},
  file = {C:\Users\nhiot\OneDrive\zotero\2016\Yin et al. - 2016 - Neural Enquirer Learning to Query Tables in Natur.pdf}
}

@article{yuanConstrainedShortestPath2019,
  ids = {yuanConstrainedShortestPath2019a},
  title = {Constrained Shortest Path Query in a Large Time-Dependent Graph},
  author = {Yuan, Ye and Lian, Xiang and Wang, Guoren and Ma, Yuliang and Wang, Yishu},
  year = {2019},
  month = jun,
  journal = {Proceedings of the VLDB Endowment},
  volume = {12},
  number = {10},
  pages = {1058--1070},
  publisher = {VLDB Endowment},
  issn = {21508097},
  doi = {10/ggpwsz},
  urldate = {2020-03-25},
  abstract = {The constrained shortest path (CSP) query over static graphs has been extensively studied due to its wide applications in route planning for transportation networks. However, real transportation networks often evolve over time and are thus modeled as time-dependent graphs. Therefore, in this paper, we study the CSP query over a large time-dependent graph by incorporating continuous time and weight functions in it. Specifically, we study the point CSP (PCSP) query and interval CSP (ICSP) query. We formally prove that it is NP-complete to process a PCSP query and at least EXPSPACE to answer an ICSP query. We propose approximate sequential algorithms to answer the PCSP and ICSP queries efficiently. We also develop parallel algorithms for the queries that guarantee to scale with big time-dependent graphs. Using real-life graphs, we experimentally verify the efficiency and scalability of our algorithms.},
  langid = {english},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2019\\Yuan et al. - 2019 - Constrained shortest path query in a large time-de.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\3AVNA49Q\\3339490.html}
}

@article{yuFlowsenseNaturalLanguage2019,
  title = {Flowsense: {{A}} Natural Language Interface for Visual Data Exploration within a Dataflow System},
  shorttitle = {Flowsense},
  author = {Yu, Bowen and Silva, Cl{\'a}udio T.},
  year = {2019},
  journal = {IEEE transactions on visualization and computer graphics},
  volume = {26},
  number = {1},
  pages = {1--11},
  publisher = {IEEE},
  doi = {10/ghsv5n},
  annotation = {QID: Q92821848},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2019\\Yu et Silva - 2019 - Flowsense A natural language interface for visual.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\763SWGEZ\\8807265.html}
}

@inproceedings{zafarFormalQueryGeneration2018,
  title = {Formal {{Query Generation}} for {{Question Answering}} over {{Knowledge Bases}}},
  booktitle = {European {{Semantic Web Conference}}},
  author = {Zafar, Hamid and Napolitano, Giulio and Lehmann, Jens},
  editor = {Gangemi, Aldo and Navigli, Roberto and Vidal, Maria-Esther and Hitzler, Pascal and Troncy, Rapha{\"e}l and Hollink, Laura and Tordai, Anna and Alam, Mehwish},
  year = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {714--728},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10/ggdnr8},
  abstract = {Question answering (QA) systems often consist of several components such as Named Entity Disambiguation (NED), Relation Extraction (RE), and Query Generation (QG). In this paper, we focus on the QG process of a QA pipeline on a large-scale Knowledge Base (KB), with noisy annotations and complex sentence structures. We therefore propose SQG, a SPARQL Query Generator with modular architecture, enabling easy integration with other components for the construction of a fully functional QA pipeline. SQG can be used on large open-domain KBs and handle noisy inputs by discovering a minimal subgraph based on uncertain inputs, that it receives from the NED and RE components. This ability allows SQG to consider a set of candidate entities/relations, as opposed to the most probable ones, which leads to a significant boost in the performance of the QG component. The captured subgraph covers multiple candidate walks, which correspond to SPARQL queries. To enhance the accuracy, we present a ranking model based on Tree-LSTM that takes into account the syntactical structure of the question and the tree representation of the candidate queries to find the one representing the correct intention behind the question. SQG outperforms the baseline systems and achieves a macro F1-measure of 75\% on the LC-QuAD dataset.},
  isbn = {978-3-319-93417-4},
  langid = {english},
  optabstract = {Question answering (QA) systems often consist of several components such as Named Entity Disambiguation (NED), Relation Extraction (RE), and Query Generation (QG). In this paper, we focus on the QG process of a QA pipeline on a large-scale Knowledge Base (KB), with noisy annotations and complex sentence structures. We therefore propose SQG, a SPARQL Query Generator with modular architecture, enabling easy integration with other components for the construction of a fully functional QA pipeline. SQG can be used on large open-domain KBs and handle noisy inputs by discovering a minimal subgraph based on uncertain inputs, that it receives from the NED and RE components. This ability allows SQG to consider a set of candidate entities/relations, as opposed to the most probable ones, which leads to a significant boost in the performance of the QG component. The captured subgraph covers multiple candidate walks, which correspond to SPARQL queries. To enhance the accuracy, we present a ranking model based on Tree-LSTM that takes into account the syntactical structure of the question and the tree representation of the candidate queries to find the one representing the correct intention behind the question. SQG outperforms the baseline systems and achieves a macro F1-measure of 75\% on the LC-QuAD dataset.},
  optaddress = {Cham},
  optdoi = {10/ggdnr8},
  opteditor = {Gangemi, Aldo and Navigli, Roberto and Vidal, Maria-Esther and Hitzler, Pascal and Troncy, Rapha{\"e}l and Hollink, Laura and Tordai, Anna and Alam, Mehwish},
  optlanguage = {en},
  optpublisher = {Springer International Publishing},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2018\\Zafar et al. - 2018 - Formal Query Generation for Question Answering ove.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\IBX2KWLD\\978-3-319-93417-4_46.html}
}

@article{zanioloDatabaseRelationsNull1984,
  ids = {zanioloDatabaseRelationsNull1984a},
  title = {Database Relations with Null Values},
  author = {Zaniolo, Carlo},
  year = {1984},
  month = feb,
  journal = {Journal of Computer and System Sciences},
  volume = {28},
  number = {1},
  pages = {142--166},
  issn = {0022-0000},
  doi = {10.1016/0022-0000(84)90080-1},
  urldate = {2023-08-08},
  abstract = {A new formal approach is proposed for modeling incomplete database information by means of null values. The basis of our approach is an interpretation of nulls which obviates the need for more than one type of null. The conceptual soundness of this approach is demonstrated by generalizing the formal framework of the relational data model to include null values. In particular, the set-theoretical properties of relations with nulls are studied and the definitions of set inclusion, set union, and set difference are generalized. A simple and efficient strategy for evaluating queries in the presence of nulls is provided. The operators of relational algebra are then generalized accordingly. Finally, the deep-rooted logical and computational problems of previous approaches are reviewed to emphasize the superior practicability of the solution.},
  langid = {english},
  keywords = {nosource},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1984\\Zaniolo - 1984 - Database relations with null values.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\N6ACRI36\\0022000084900801.html}
}

@article{zaslavskiyPathFollowingAlgorithm2009,
  ids = {zaslavskiyPathFollowingAlgorithm2008},
  title = {A {{Path Following Algorithm}} for the {{Graph Matching Problem}}},
  author = {Zaslavskiy, M. and Bach, F. and Vert, J.},
  year = {2009},
  month = dec,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {31},
  number = {12},
  pages = {2227--2242},
  publisher = {IEEE},
  issn = {1939-3539},
  doi = {10/cvpk77},
  abstract = {We propose a convex-concave programming approach for the labeled weighted graph matching problem. The convex-concave programming formulation is obtained by rewriting the weighted graph matching problem as a least-square problem on the set of permutation matrices and relaxing it to two different optimization problems: a quadratic convex and a quadratic concave optimization problem on the set of doubly stochastic matrices. The concave relaxation has the same global minimum as the initial graph matching problem, but the search for its global minimum is also a hard combinatorial problem. We, therefore, construct an approximation of the concave problem solution by following a solution path of a convex-concave problem obtained by linear interpolation of the convex and concave formulations, starting from the convex relaxation. This method allows to easily integrate the information on graph label similarities into the optimization problem, and therefore, perform labeled weighted graph matching. The algorithm is compared with some of the best performing graph matching methods on four data sets: simulated graphs, QAPLib, retina vessel images, and handwritten Chinese characters. In all cases, the results are competitive with the state of the art.},
  keywords = {{Image Processing, Computer-Assisted},{Pattern Recognition, Automated},Algorithms,Artificial Intelligence,Bioinformatics,classification,Computing Methodologies,concave programming,Constrained optimization,convex programming,Convex programming,convex-concave programming approach,Discrete Mathematics,gradient methods,Gradient methods,Graph algorithms,graph matching,graph theory,Graph Theory,handwritten Chinese characters,hard combinatorial problem,Humans,Image Processing and Computer Vision,image processing.,interpolation,Interpolation,labeled weighted graph matching problem,Learning,least squares approximations,least-square problem,machine learning,Machine learning,Machine learning algorithms,Mathematics of Computing,Numerical Analysis,Object recognition,Optimization,Optimization methods,path following algorithm,pattern matching,Pattern Recognition,permutation matrices,Proteins,QAPLib,quadratic concave optimization problem,Quadratic programming,Quadratic programming methods,Retina,retina vessel images,Retinal Vessels,Scene Analysis,simulated graphs,stochastic matrices,Stochastic processes},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2009\\Zaslavskiy et al. - 2009 - A Path Following Algorithm for the Graph Matching .pdf;C\:\\Users\\nhiot\\Zotero\\storage\\QWIGRSIW\\4641936.html}
}

@article{zelenkoKernelMethodsRelation2003,
  ids = {zelenkoKernelMethodsRelation2003a},
  title = {Kernel {{Methods}} for {{Relation Extraction}}},
  author = {Zelenko, Dmitry and Aone, Chinatsu and Richardella, Anthony},
  year = {2003},
  month = mar,
  journal = {The Journal of Machine Learning Research},
  volume = {3},
  number = {Feb},
  pages = {1083--1106},
  issn = {1532-4435},
  url = {https://www.jmlr.org/papers/v3/zelenko03a.html},
  abstract = {We present an application of kernel methods to extracting relations from unstructured natural language sources. We introduce kernels defined over shallow parse representations of text, and design efficient algorithms for computing the kernels. We use the devised kernels in conjunction with Support Vector Machine and Voted Perceptron learning algorithms for the task of extracting {$<$}tt{$>$}person-affiliation{$<$}/tt{$>$} and {$<$}tt{$>$}organization-location{$<$}/tt{$>$} relations from text. We experimentally evaluate the proposed methods and compare them with feature-based learning algorithms, with promising results.},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2003\Zelenko et al. - 2003 - Kernel Methods for Relation Extraction.pdf}
}

@inproceedings{zhang-etal-2020-joint,
  title = {Joint Intent Detection and Entity Linking on Spatial Domain Queries},
  booktitle = {Findings of the Association for Computational Linguistics: {{EMNLP}} 2020},
  author = {Zhang, Lei and Wang, Runze and Zhou, Jingbo and Yu, Jingsong and Ling, Zhenhua and Xiong, Hui},
  year = {2020},
  month = nov,
  pages = {4937--4947},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10/gh7zb2},
  abstract = {Continuous efforts have been devoted to language understanding (LU) for conversational queries with the fast and wide-spread popularity of voice assistants. In this paper, we first study the LU problem in the spatial domain, which is a critical problem for providing location-based services by voice assistants but is without in-depth investigation in existing studies. Spatial domain queries have several unique properties making them be more challenging for language understanding than common conversational queries, including lexical-similar but diverse intents and highly ambiguous words. Thus, a special tailored LU framework for spatial domain queries is necessary. To the end, a dataset was extracted and annotated based on the real-life queries from a voice assistant service. We then proposed a new multi-task framework that jointly learns the intent detection and entity linking tasks on the with invented hierarchical intent detection method and triple-scoring mechanism for entity linking. A specially designed spatial GCN is also utilized to model spatial context information among entities. We have conducted extensive experimental evaluations with state-of-the-art entity linking and intent detection methods, which demonstrated that can outperform all baselines with a significant margin.},
  keywords = {nosource}
}

@article{zhangDynamicUncertainCausality2014,
  title = {Dynamic {{Uncertain Causality Graph}} for {{Knowledge Representation}} and {{Probabilistic Reasoning}}: {{Statistics Base}}, {{Matrix}}, and {{Application}}},
  shorttitle = {Dynamic {{Uncertain Causality Graph}} for {{Knowledge Representation}} and {{Probabilistic Reasoning}}},
  author = {Zhang, Q. and Dong, C. and Cui, Y. and Yang, Z.},
  year = {2014},
  month = apr,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {25},
  number = {4},
  pages = {645--663},
  issn = {2162-237X},
  doi = {10/gfvn2q},
  abstract = {Graphical models for probabilistic reasoning are now in widespread use. Many approaches have been developed such as Bayesian network. A newly developed approach named as dynamic uncertain causality graph (DUCG) is initially presented in a previous paper, in which only the inference algorithm in terms of individual events and probabilities is addressed. In this paper, we first explain the statistic basis of DUCG. Then, we extend the algorithm to the form of matrices of events and probabilities. It is revealed that the representation of DUCG can be incomplete and the exact probabilistic inference may still be made. A real application of DUCG for fault diagnoses of a generator system of a nuclear power plant is demonstrated, which involves variables. Most inferences take with a laptop computer. The causal logic between inference result and observations is graphically displayed to users so that they know not only the result, but also why the result obtained.},
  keywords = {Bayesian network,belief networks,causal logic,causality,Causality,Cognition,complex system,DUCG,dynamic uncertain causality graph,exact probabilistic inference,fault diagnosis,generator system,graphical models,Heuristic algorithms,Hidden Markov models,inference algorithm,Inference algorithms,inference mechanisms,knowledge representation,laptop computer,laptop computers,Logic gates,matrix,matrix algebra,nuclear power plant,nuclear power stations,power engineering computing,Probabilistic logic,probabilistic reasoning,Probability,statistics,statistics base,uncertainty},
  annotation = {00000 QID: Q51089296},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2014\\Zhang et al. - 2014 - Dynamic Uncertain Causality Graph for Knowledge Re.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\HM5C9PBV\\6600881.html}
}

@article{zhangSimpleFastAlgorithms1989,
  title = {Simple {{Fast Algorithms}} for the {{Editing Distance}} between {{Trees}} and {{Related Problems}}},
  author = {Zhang, Kaizhong and Shasha, Dennis},
  year = {1989},
  month = dec,
  journal = {SIAM Journal on Computing},
  volume = {18},
  number = {6},
  pages = {1245--1262},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0097-5397},
  doi = {10/br6wtq},
  urldate = {2021-12-01},
  abstract = {Ordered labeled trees are trees in which the left-to-right order among siblings is significant. The distance between two ordered trees is considered to be the weighted number of edit operations (insert, delete, and modify) to transform one tree to another. The problem of approximate tree matching is also considered. Specifically, algorithms are designed to answer the following kinds of questions:1. What is the distance between two trees? 2. What is the minimum distance between \$T\_1 \$ and \$T\_2 \$ when zero or more subtrees can be removed from \$T\_2 \$? 3. Let the pruning of a tree at node n mean removing all the descendants of node n. The analogous question for prunings as for subtrees is answered. A dynamic programming algorithm is presented to solve the three questions in sequential time \$O({\textbar}T\_1 {\textbar} {\textbackslash}times {\textbar}T\_2 {\textbar} {\textbackslash}times {\textbackslash}min (\{{\textbackslash}textit\{depth\}\}(T\_1 ),\{{\textbackslash}textit\{leaves\}\}(T\_1 )) {\textbackslash}times {\textbackslash}min (\{{\textbackslash}textit\{depth\}\}(T\_2 ),\{{\textbackslash}textit\{leaves\}\}(T\_2 )))\$ and space \$O({\textbar}T\_1 {\textbar} {\textbackslash}times {\textbar}T\_2 {\textbar})\$ compared with \$O({\textbar}T\_1 {\textbar} {\textbackslash}times {\textbar}T\_2 {\textbar} {\textbackslash}times (\{{\textbackslash}textit\{depth\}\}(T\_1 ))\^{}2 {\textbackslash}times (\{{\textbackslash}textit\{depth\}\}(T\_2 ))\^{}2 )\$ for the best previous published algorithm due to Tai [J. Assoc. Comput. Mach., 26 (1979), pp, 422-433]. Further, the algorithm presented here can be parallelized to give time \$O({\textbar}T\_1 {\textbar} {\textbackslash}times {\textbar}T\_2 {\textbar})\$.},
  keywords = {68P05,68Q20,68Q25,68R10,dynamic programming,editing distance,parallel algorithm,pattern recognition,trees},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1989\\Zhang et Shasha - 1989 - Simple Fast Algorithms for the Editing Distance be.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\D6JLBYXQ\\0218082.html}
}

@inproceedings{zhaoExtractingRelationsIntegrated2005,
  title = {Extracting Relations with Integrated Information Using Kernel Methods},
  booktitle = {Proceedings of the 43rd {{Annual Meeting}} on {{Association}} for {{Computational Linguistics}}},
  author = {Zhao, Shubin and Grishman, Ralph},
  year = {2005},
  month = jun,
  series = {{{ACL}} '05},
  pages = {419--426},
  publisher = {Association for Computational Linguistics},
  address = {USA},
  doi = {10/bg2926},
  urldate = {2020-11-26},
  abstract = {Entity relation detection is a form of information extraction that finds predefined relations between pairs of entities in text. This paper describes a relation detection approach that combines clues from different levels of syntactic processing using kernel methods. Information from three different levels of processing is considered: tokenization, sentence parsing and deep dependency analysis. Each source of information is represented by kernel functions. Then composite kernels are developed to integrate and extend individual kernels so that processing errors occurring at one level can be overcome by information from other levels. We present an evaluation of these methods on the 2004 ACE relation detection task, using Support Vector Machines, and show that each level of syntactic processing contributes useful information for this task. When evaluated on the official test data, our approach produced very competitive ACE value scores. We also compare the SVM with KNN on different kernels.},
  file = {C:\Users\nhiot\OneDrive\zotero\2005\Zhao et Grishman - 2005 - Extracting relations with integrated information u.pdf}
}

@article{zhengPharmKGDedicatedKnowledge2020,
  ids = {zhengPharmKGDedicatedKnowledge2020a},
  title = {{{PharmKG}}: A Dedicated Knowledge Graph Benchmark for Bomedical Data Mining},
  shorttitle = {{{PharmKG}}},
  author = {Zheng, Shuangjia and Rao, Jiahua and Song, Ying and Zhang, Jixian and Xiao, Xianglu and Fang, Evandro Fei and Yang, Yuedong and Niu, Zhangming},
  year = {2020},
  month = dec,
  journal = {Briefings in Bioinformatics},
  number = {bbaa344},
  issn = {1477-4054},
  doi = {10/ghqsxs},
  urldate = {2021-03-19},
  abstract = {Biomedical knowledge graphs (KGs), which can help with the understanding of complex biological systems and pathologies, have begun to play a critical role in medical practice and research. However, challenges remain in their embedding and use due to their complex nature and the specific demands of their construction. Existing studies often suffer from problems such as sparse and noisy datasets, insufficient modeling methods and non-uniform evaluation metrics. In this work, we established a comprehensive KG system for the biomedical field in an attempt to bridge the gap. Here, we introduced PharmKG, a multi-relational, attributed biomedical KG, composed of more than 500~000 individual interconnections between genes, drugs and diseases, with 29 relation types over a vocabulary of {\textasciitilde}8000 disambiguated entities. Each entity in PharmKG is attached with heterogeneous, domain-specific information obtained from multi-omics data, i.e. gene expression, chemical structure and disease word embedding, while preserving the semantic and biomedical features. For baselines, we offered nine state-of-the-art KG embedding (KGE) approaches and a new biological, intuitive, graph neural network-based KGE method that uses a combination of both global network structure and heterogeneous domain features. Based on the proposed benchmark, we conducted extensive experiments to assess these KGE models using multiple evaluation metrics. Finally, we discussed our observations across various downstream biological tasks and provide insights and guidelines for how to use a KG in biomedicine. We hope that the unprecedented quality and diversity of PharmKG will lead to advances in biomedical KG construction, embedding and application.},
  annotation = {QID: Q104505738},
  file = {C\:\\Users\\nhiot\\Zotero\\storage\\G7W9NNAY\\6042240.html;C\:\\Users\\nhiot\\Zotero\\storage\\H99SQVW6\\6042240.html}
}

@article{zhengQuestionAnsweringKnowledge2018,
  title = {Question Answering over Knowledge Graphs: Question Understanding via Template Decomposition},
  shorttitle = {Question Answering over Knowledge Graphs},
  author = {Zheng, Weiguo and Yu, Jeffrey Xu and Zou, Lei and Cheng, Hong},
  year = {2018},
  month = jul,
  journal = {Proceedings of the VLDB Endowment},
  volume = {11},
  number = {11},
  pages = {1373--1386},
  issn = {2150-8097},
  doi = {10/gf3t2s},
  urldate = {2020-02-13},
  abstract = {The gap between unstructured natural language and structured data makes it challenging to build a system that supports using natural language to query large knowledge graphs. Many existing methods construct a structured query for the input question based on a syntactic parser. Once the input question is parsed incorrectly, a false structured query will be generated, which may result in false or incomplete answers. The problem gets worse especially for complex questions. In this paper, we propose a novel systematic method to understand natural language questions by using a large number of binary templates rather than semantic parsers. As sufficient templates are critical in the procedure, we present a low-cost approach that can build a huge number of templates automatically. To reduce the search space, we carefully devise an index to facilitate the online template decomposition. Moreover, we design effective strategies to perform the two-level disambiguations (i.e., entity-level ambiguity and structure-level ambiguity) by considering the query semantics. Extensive experiments over several benchmarks demonstrate that our proposed approach is effective as it significantly outperforms state-of-the-art methods in terms of both precision and recall.},
  optabstract = {The gap between unstructured natural language and structured data makes it challenging to build a system that supports using natural language to query large knowledge graphs. Many existing methods construct a structured query for the input question based on a syntactic parser. Once the input question is parsed incorrectly, a false structured query will be generated, which may result in false or incomplete answers. The problem gets worse especially for complex questions. In this paper, we propose a novel systematic method to understand natural language questions by using a large number of binary templates rather than semantic parsers. As sufficient templates are critical in the procedure, we present a low-cost approach that can build a huge number of templates automatically. To reduce the search space, we carefully devise an index to facilitate the online template decomposition. Moreover, we design effective strategies to perform the two-level disambiguations (i.e., entity-level ambiguity and structure-level ambiguity) by considering the query semantics. Extensive experiments over several benchmarks demonstrate that our proposed approach is effective as it significantly outperforms state-of-the-art methods in terms of both precision and recall.},
  optdoi = {10/gf3t2s},
  optmonth = {07},
  optnumber = {11},
  optshorttitle = {Question answering over knowledge graphs},
  opturl = {https://doi.org/10.14778/3236187.3236192},
  opturldate = {2020-02-13},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2018\\Zheng et al. - 2018 - Question answering over knowledge graphs question.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\97IHVNSX\\3236187.html}
}

@article{zhongSeq2SQLGeneratingStructured2017,
  title = {{{Seq2SQL}}: {{Generating Structured Queries}} from {{Natural Language}} Using {{Reinforcement Learning}}},
  shorttitle = {{{Seq2SQL}}},
  author = {Zhong, Victor and Xiong, Caiming and Socher, Richard},
  year = {2017},
  month = aug,
  pages = {13},
  url = {http://arxiv.org/abs/1709.00103},
  abstract = {A significant amount of the world's knowledge is stored in relational databases. However, the ability for users to retrieve facts from a database is limited due to a lack of understanding of query languages such as SQL. We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries. Our model leverages the structure of SQL queries to significantly reduce the output space of generated queries. Moreover, we use rewards from in-the-loop query execution over the database to learn a policy to generate unordered parts of the query, which we show are less suitable for optimization via cross entropy loss. In addition, we will publish WikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables from Wikipedia. This dataset is required to train our model and is an order of magnitude larger than comparable datasets. By applying policy-based reinforcement learning with a query execution environment to WikiSQL, our model Seq2SQL outperforms attentional sequence to sequence models, improving execution accuracy from 35.9\% to 59.4\% and logical form accuracy from 23.4\% to 48.3\%.},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {QID: Q87196833},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2017\\Zhong et al. - 2017 - Seq2SQL Generating Structured Queries from Natura.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\T9W9G7BG\\1709.html}
}

@article{zhouBiomedicalRelationExtraction2014,
  title = {Biomedical {{Relation Extraction}}: {{From Binary}} to {{Complex}}},
  shorttitle = {Biomedical {{Relation Extraction}}},
  author = {Zhou, Deyu and Zhong, Dayou and He, Yulan},
  year = {2014},
  month = aug,
  journal = {Computational and Mathematical Methods in Medicine},
  volume = {2014},
  pages = {e298473},
  publisher = {Hindawi},
  issn = {1748-670X},
  doi = {10/gb962b},
  urldate = {2020-11-17},
  abstract = {Biomedical relation extraction aims to uncover high-quality relations from life science literature with high accuracy and efficiency. Early biomedical relation extraction tasks focused on capturing binary relations, such as protein-protein interactions, which are crucial for virtually every process in a living cell. Information about these interactions provides the foundations for new therapeutic approaches. In recent years, more interests have been shifted to the extraction of complex relations such as biomolecular events. While complex relations go beyond binary relations and involve more than two arguments, they might also take another relation as an argument. In the paper, we conduct a thorough survey on the research in biomedical relation extraction. We first present a general framework for biomedical relation extraction and then discuss the approaches proposed for binary and complex relation extraction with focus on the latter since it is a much more difficult task compared to binary relation extraction. Finally, we discuss challenges that we are facing with complex relation extraction and outline possible solutions and future directions.},
  langid = {english},
  annotation = {QID: Q27013607},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2014\\Zhou et al. - 2014 - Biomedical Relation Extraction From Binary to Com.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\56PEL8X8\\298473.html}
}

@article{zhouLearningLocalGlobal2003,
  title = {Learning with Local and Global Consistency},
  author = {Zhou, Dengyong and Bousquet, Olivier and Lal, Thomas and Weston, Jason and Sch{\"o}lkopf, Bernhard},
  year = {2003},
  journal = {Advances in neural information processing systems},
  volume = {16},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2003\\Zhou et al. - 2003 - Learning with local and global consistency.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\VW4SPH7Q\\87682805257e619d49b8e0dfdc14affa-Abstract.html}
}

@article{zhuKnowledgedrivenDrugRepurposing2020,
  title = {Knowledge-Driven Drug Repurposing Using a Comprehensive Drug Knowledge Graph},
  author = {Zhu, Yongjun and Che, Chao and Jin, Bo and Zhang, Ningrui and Su, Chang and Wang, Fei},
  year = {2020},
  journal = {Health Informatics Journal},
  volume = {26},
  number = {4},
  pages = {2737--2750},
  publisher = {SAGE Publications Sage UK: London, England},
  doi = {10/gjhfr7},
  annotation = {QID: Q97568824},
  file = {C\:\\Users\\nhiot\\Zotero\\storage\\FJVT8S7D\\1460458220937101.html;C\:\\Users\\nhiot\\Zotero\\storage\\W3A3MFH2\\1460458220937101.html}
}

@inproceedings{zhuSemisupervisedLearningUsing2003,
  title = {Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions},
  booktitle = {Proceedings of the 20th {{International}} Conference on {{Machine}} Learning ({{ICML-03}})},
  author = {Zhu, Xiaojin and Ghahramani, Zoubin and Lafferty, John D.},
  year = {2003},
  pages = {912--919},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2003\Zhu et al. - 2003 - Semi-supervised learning using gaussian fields and.pdf}
}

@book{zipfPsychobiologyLanguage1935,
  title = {The Psycho-Biology of Language},
  author = {Zipf, G. K.},
  year = {1935},
  series = {The Psycho-Biology of Language},
  pages = {ix, 336},
  publisher = {Houghton, Mifflin},
  address = {Oxford, England},
  abstract = {Frequency counts of phonemes, morphemes, and words in samples of written discourse in diverse languages are presented in support of the generalization that the more complex any speech element, the less frequently does it occur. Thus, the greater the frequency of occurrence of words, the less tends to be their average length, and the smaller also is the number of different words. The relation between frequency and number of different words is said to be expressed by the formula ab2 = k, in which a represents the number of different words of a given frequency and b the frequency. The relationship between the magnitude of speech elements and their frequency is attributed to the operation of a "law" of linguistic change: that as the frequency of phonemes or of linguistic forms increases, their magnitude decreases. There is thus a tendency to "maintain an equilibrium" between length and frequency, and this tendency rests upon an "underlying law of economy." Human beings strive to maintain an "emotional equilibrium" between variety and repetitiveness of environmental factors and behavior. A speaker's discourse must represent a compromise between variety and repetitiveness adapted to the hearer's "tolerable limits of change in maintaining emotional equilibrium." This accounts for the maintenance of the relationship ab2 = k; the exponent of b expresses this "rate of variegation." (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C:\Users\nhiot\Zotero\storage\S2VNTYX9\1935-04756-000.html}
}

@inproceedings{zorziAutomagicallyEncodingAdverse2015,
  title = {Automagically Encoding Adverse Drug Reactions in {{MedDRA}}},
  booktitle = {2015 {{International Conference}} on {{Healthcare Informatics}}},
  author = {Zorzi, Margherita and Combi, Carlo and Lora, Riccardo and Pagliarini, Marco and Moretti, Ugo},
  year = {2015},
  pages = {90--99},
  publisher = {IEEE},
  doi = {10/ggcdhv},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2015\\Zorzi et al. - 2015 - Automagically encoding adverse drug reactions in M.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\BXXU8493\\7349679.html}
}

@inproceedings{zouNaturalLanguageQuestion2014,
  title = {Natural Language Question Answering over {{RDF}}: A Graph Data Driven Approach},
  shorttitle = {Natural Language Question Answering over {{RDF}}},
  booktitle = {Proceedings of the 2014 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Zou, Lei and Huang, Ruizhe and Wang, Haixun and Yu, Jeffrey Xu and He, Wenqiang and Zhao, Dongyan},
  year = {2014},
  month = jun,
  pages = {313--324},
  publisher = {ACM},
  address = {Snowbird Utah USA},
  doi = {10.1145/2588555.2610525},
  urldate = {2024-03-29},
  isbn = {978-1-4503-2376-5},
  langid = {english},
  optbibsource = {dblp computer science bibliography, https://dblp.org},
  optbiburl = {https://dblp.org/rec/conf/sigmod/ZouHWYHZ14.bib},
  opteditor = {Curtis E. Dyreson and Feifei Li and M. Tamer {\"O}zsu},
  opttimestamp = {Wed, 06 Mar 2019 07:05:24 +0100},
  opturl = {https://doi.org/10.1145/2588555.2610525},
  keywords = {⛔ No DOI found,nosource}
}

@incollection{zweigenbaumActesTALN19981998,
  title = {Actes de {{TALN}} 1998 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 1998 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Zweigenbaum, Pierre},
  year = {1998},
  month = jun,
  publisher = {ATALA},
  address = {Paris},
  keywords = {nosource}
}
