@inproceedings{AbG85,
  title = {Mise-{\`a}-Jour Des {{Bases}} de {{Donn{\'e}es}} Contenant de l'information Incompl{\`e}te},
  booktitle = {Journ{\'e}es Bases de Donn{\'e}es Avanc{\'e}s, 6-8 Mars 1985, St. {{Pierre}} de Chartreuse (Informal Proceedings).},
  author = {Abiteboul, Serge and Grahne, G{\"o}sta},
  year = {1985},
  optbibsource = {dblp computer science bibliography, http://dblp.org},
  optcrossref = {DBLP:conf/bda/1985},
  opttimestamp = {Tue, 31 Oct 2006 14:01:45 +0100},
  keywords = {⛔ No DOI found}
}

@book{abiteboulFoundationsDatabases1995,
  title = {Foundations of {{Databases}}},
  shorttitle = {Foundations of {{Databases}}},
  author = {Abiteboul, Serge and Hull, Richard and Vianu, Victor},
  year = {1995},
  edition = {1st},
  volume = {8},
  publisher = {Addison-Wesley Longman Publishing Co., Inc.},
  address = {USA},
  abstract = {This work presents comprehensive coverage of the foundations and theory of database systems. It is a reference to both classical material and advanced topics, bringing together many subjects including up-to-date coverage of object-oriented and logic databases. Numerous exercises are provided at three levels of difficulty. The book is intended for use by database professionals at all levels of experience, and graduate and senior level students in Advanced Theory of Databases.},
  isbn = {978-0-201-53771-0},
  langid = {english},
  keywords = {nosource},
  file = {C:\Users\nhiot\Zotero\storage\KKV3MZ2R\abiteboul95.html}
}

@inproceedings{abiteboulUpdateSemanticsIncomplete1985,
  title = {Update Semantics for Incomplete Databases},
  booktitle = {Proceedings of the 11th International Conference on {{Very Large Data Bases}} - {{Volume}} 11},
  author = {Abiteboul, Serge and Grahne, G{\"o}sta},
  editor = {Pirotte, Alain and Vassiliou, Yannis},
  year = {1985},
  month = aug,
  series = {{{VLDB}} '85},
  pages = {1--12},
  publisher = {VLDB Endowment},
  address = {Stockholm, Sweden},
  url = {https://dblp.org/rec/conf/vldb/AbiteboulG85},
  abstract = {A database containing some incomplete information is viewed as a set of possible states of the real world. The semantics of updates is given based on simple set operations on the set of states. Some basic results concerning the capabilities of known models of incomplete databases to handle updates are exhibited.},
  isbn = {0-934613-17-6},
  keywords = {⛔ No DOI found,nosource},
  file = {C:\Users\nhiot\Zotero\storage\3SJCDNV4\1286760.html}
}

@inproceedings{abraoIncrementalConstraintChecking2004,
  title = {Incremental {{Constraint Checking}} for {{XML Documents}}},
  booktitle = {Database and {{XML Technologies}}},
  author = {Abr{\~a}o, Maria Adriana and Bouchou, B{\'e}atrice and {Halfeld-Ferrari}, Mirian and Laurent, Dominique and Musicante, Martin A.},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Bellahs{\`e}ne, Zohra and Milo, Tova and Rys, Michael and Suciu, Dan and Unland, Rainer},
  year = {2004},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {3186},
  pages = {112--127},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-30081-6_9},
  urldate = {2024-01-03},
  abstract = {We introduce a method for building an XML constraint validator from a given set of schema, key and foreign key constraints. The XML constraint validator obtained by our method is a bottom-up tree transducer that is used not only for checking, in only one pass, the correctness of an XML document but also for incrementally validating updates over this document. In this way, both the verification from scratch and the update verification are based on regular (finite and tree) automata, making the whole process efficient.},
  isbn = {978-3-540-22969-8 978-3-540-30081-6},
  langid = {english},
  keywords = {Data Node,Incremental Validation,Integrity Constraint,nosource,Output Function,Target Node},
  file = {C:\Users\nhiot\OneDrive\zotero\2004\Abrão et al. - 2004 - Incremental Constraint Checking for XML Documents.pdf}
}

@article{ahoEfficientOptimizationClass1979,
  ids = {ASU79},
  title = {Efficient Optimization of a Class of Relational Expressions},
  author = {Aho, Alfred V. and Sagiv, Yehoshua and Ullman, Jeffrey D.},
  year = {1979},
  journal = {ACM Transactions on Database Systems (TODS)},
  volume = {4},
  number = {4},
  pages = {435--454},
  publisher = {ACM New York, NY, USA},
  issn = {0362-5915},
  doi = {10.1145/320107.320112},
  abstract = {The design of several database query languages has been influenced by Codd's relational algebra. This paper discusses the difficulty of optimizing queries based on the relational algebra operations select, project, and join. A matrix, called a tableau, is proposed as a useful device for representing the value of a query, and optimization of queries is couched in terms of finding a minimal tableau equivalent to a given one. Functional dependencies can be used to imply additional equivalences among tableaux. Although the optimization problem is NP-complete, a polynomial time algorithm exists to optimize tableaux that correspond to an important subclass of queries.},
  optbibsource = {dblp computer science bibliography, https://dblp.org},
  optdoi = {10.1145/320107.320112},
  opttimestamp = {Tue, 06 Nov 2018 12:51:47 +0100},
  keywords = {equivalence of queries,NP-completeness,query optimization,relational algebra,relational database,tableaux},
  file = {C:\Users\nhiot\OneDrive\zotero\1979\Aho et al. - 1979 - Efficient optimization of a class of relational ex.pdf}
}

@article{ahoTheoryJoinsRelational1979,
  title = {The Theory of Joins in Relational Databases},
  author = {Aho, Alfred V. and Beeri, Catriel and Ullman, Jeffrey D.},
  year = {1979},
  journal = {ACM Transactions on Database Systems (TODS)},
  volume = {4},
  number = {3},
  pages = {297--314},
  publisher = {ACM New York, NY, USA},
  doi = {10.1145/320083.320091},
  abstract = {Answering queries in a relational database often requires that the natural join of two or more relations be computed. However, not all joins are semantically meaningful. This paper gives an efficient algorithm to determine whether the join of several relations is semantically meaningful (lossless) and an efficient algorithm to determine whether a set of relations has a subset with a lossy join. These algorithms assume that all data dependencies are functional. Similar techniques also apply to the case where data dependencies are multivalued.},
  file = {C:\Users\nhiot\OneDrive\zotero\1979\Aho et al. - 1979 - The theory of joins in relational databases.pdf}
}

@inproceedings{akhtarConstraintsRDF2011,
  title = {Constraints in {{RDF}}},
  booktitle = {Semantics in {{Data}} and {{Knowledge Bases}}},
  author = {Akhtar, Waseem and {Cort{\'e}s-Calabuig}, {\'A}lvaro and Paredaens, Jan},
  editor = {Schewe, Klaus-Dieter and Thalheim, Bernhard},
  year = {2011},
  volume = {6834},
  pages = {23--39},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-23441-5_2},
  urldate = {2023-12-27},
  isbn = {978-3-642-23440-8 978-3-642-23441-5}
}

@article{al-aswadiAutomaticOntologyConstruction2020,
  title = {Automatic Ontology Construction from Text: A Review from Shallow to Deep Learning Trend},
  shorttitle = {Automatic Ontology Construction from Text},
  author = {{Al-Aswadi}, Fatima N. and Chan, Huah Yong and Gan, Keng Hoon},
  year = {2020},
  month = aug,
  journal = {Artificial Intelligence Review},
  volume = {53},
  number = {6},
  pages = {3901--3928},
  issn = {0269-2821, 1573-7462},
  doi = {10.1007/s10462-019-09782-9},
  urldate = {2024-04-04},
  abstract = {The explosive growth of textual data on the web coupled with the increase on demand for ontologies to promote the semantic web, have made the automatic ontology construction from the text a very promising research area. Ontology learning (OL) from text is a process that aims to (semi-) automatically extract and represent the knowledge from text in machine-readable form. Ontology is considered one of the main cornerstones of representing the knowledge in a more meaningful way on the semantic web. Usage of ontologies has proven to be beneficial and efficient in different applications (e.g. information retrieval, information extraction, and question answering). Nevertheless, manually construction of ontologies is time-consuming as well extremely laborious and costly process. In recent years, many approaches and systems that try to automate the construction of ontologies have been developed. This paper reviews various approaches, systems, and challenges of automatic ontology construction from the text. In addition, it also discusses ways the ontology construction process could be enhanced in the future by presenting techniques from shallow learning to deep learning (DL).},
  langid = {english},
  keywords = {Concept classification,Deep learning,Ontology construction,Ontology learning,Semantic relation},
  file = {C:\Users\nhiot\OneDrive\zotero\2020\Al-Aswadi et al. - 2020 - Automatic ontology construction from text a revie2.pdf}
}

@article{al-moslmiNamedEntityExtraction2020,
  ids = {al-moslmiNamedEntityExtraction2020a,al-moslmiNamedEntityExtraction2020c},
  title = {Named {{Entity Extraction}} for {{Knowledge Graphs}}: {{A Literature Overview}}},
  shorttitle = {Named {{Entity Extraction}} for {{Knowledge Graphs}}},
  author = {{Al-Moslmi}, Tareq and Gallofr{\'e} Oca{\~n}a, Marc and L. Opdahl, Andreas and Veres, Csaba},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {32862--32881},
  publisher = {IEEE},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.2973928},
  urldate = {2024-04-05},
  abstract = {An enormous amount of digital information is expressed as natural-language (NL) text that is not easily processable by computers. Knowledge Graphs (KG) offer a widely used format for representing information in computer-processable form. Natural Language Processing (NLP) is therefore needed for mining (or lifting) knowledge graphs from NL texts. A central part of the problem is to extract the named entities in the text. The paper presents an overview of recent advances in this area, covering: Named Entity Recognition (NER), Named Entity Disambiguation (NED), and Named Entity Linking (NEL). We comment that many approaches to NED and NEL are based on older approaches to NER and need to leverage the outputs of state-of-the-art NER systems. There is also a need for standard methods to evaluate and compare named-entity extraction approaches. We observe that NEL has recently moved from being stepwise and isolated into an integrated process along two dimensions: the first is that previously sequential steps are now being integrated into end-to-end processes, and the second is that entities that were previously analysed in isolation are now being lifted in each other's context. The current culmination of these trends are the deep-learning approaches that have recently reported promising results.},
  keywords = {Data mining,Hidden Markov models,Iris recognition,Knowledge graphs,named-entity disambiguation,named-entity extraction,named-entity linking,named-entity recognition,Natural language processing,natural-language processing,Semantics,Standards,Task analysis},
  file = {C:\Users\nhiot\OneDrive\zotero\2020\Al-Moslmi et al. - 2020 - Named Entity Extraction for Knowledge Graphs A Li2.pdf}
}

@inproceedings{alexRecognisingNestedNamed2007,
  title = {Recognising {{Nested Named Entities}} in {{Biomedical Text}}},
  booktitle = {Proceedings of the {{Workshop}} on {{BioNLP}} 2007: {{Biological}}, {{Translational}}, and {{Clinical Language Processing}}},
  author = {Alex, Beatrice and Haddow, Barry and Grover, Claire},
  year = {2007},
  month = jun,
  series = {{{BioNLP}} '07},
  pages = {65--72},
  publisher = {Association for Computational Linguistics},
  address = {USA},
  url = {https://aclanthology.org/W07-1009.pdf},
  urldate = {2024-03-21},
  abstract = {Although recent named entity (NE) annotation efforts involve the markup of nested entities, there has been limited focus on recognising such nested structures. This paper introduces and compares three techniques for modelling and recognising nested entities by means of a conventional sequence tagger. The methods are tested and evaluated on two biomedical data sets that contain entity nesting. All methods yield an improvement over the baseline tagger that is only trained on flat annotation.},
  langid = {english},
  keywords = {⛔ No DOI found,nosource},
  file = {C:\Users\nhiot\OneDrive\zotero\2007\Alex et al. - 2007 - Recognising Nested Named Entities in Biomedical Te2.pdf}
}

@book{alfredvahoCompilersPrinciplesTechniques2007,
  title = {Compilers {{Principles}}, {{Techniques}}, {{And Tools}}},
  author = {Alfred V Aho, Monica S. Lam},
  year = {2007},
  url = {http://archive.org/details/aho-compilers-principles-techniques-and-tools-2e\_202203},
  urldate = {2023-11-06},
  abstract = {Compilers - Principles, Techniques and ToolsSecond Edition Alfred V Aho, Monica S Lam, Ravi Sethi, Jeffrey D Ullman Pearson Education, Inc.2007 1 Introduction~ 1.1 Language Processors1.1.1 Exercises for Section 1.1 1.2 The Structure of a Compiler1.2.1 Lexical Analysis~~1.2.2 Syntax Analysis1.2.3 Semantic Analysis1.2.4 Intermediate Code Generation1.2.5 Code Optimization~1.2.6 Code Generation1.2.7 Symbol-Table Management1.2.8 The Grouping of Phases into Passes1.2.9 Compiler-Construction Tools 1.3 The Evolution of Programming Languages1.3.1 The Move to Higher-level Languages1.3.2 Impacts on Compilers1.3.3 Exercises for Section 1.3 1.4 The Science of Building a Compiler1.4.1 Modeling in Compiler Design and Implementation1.4.2 The Science of Code Optimization 1.5 Applications of Compiler Technology~1.5.1 Implement at ion of High-Level Programming Languages1.5.2 Optimizations for Computer Architectures~1.5.3 Design of New Computer Architectures1.5.4 Program Translations~1.5.5 Software Productivity Tools~ 1.6 Programming Language Basics~1.6.1 The Static/Dynamic Distinction~1.6.2 Environments and States~1.6.3 Static Scope and Block Structure1.6.4 Explicit Access Control1.6.5 Dynamic Scope~1.6.6 Parameter Passing Mechanisms~1.6.7 Aliasing~1.6.8 Exercises for Section 1.6~ 1.7 Summary of Chapter 1 1.8 References for Chapter 1~ 2 A Simple Syntax-Directed Translator 2.1 Introduction~ 2.2 Syntax Definition2.2.1 Definition of Grammars~2.2.2 Derivations~2.2.3 Parse Trees2.2.4 Ambiguity~2.2.5 Associativity of Operators2.2.6 Precedence of Operators~2.2.7 Exercises for Section 2.2~ 2.3 Syntax-Directed Translation~2.3.1 Postfix Notation~2.3.2 Synthesized Attributes~2.3.3 Simple Syntax-Directed Definitions2.3.4 Tree Traversals~2.3.5 Translation Schemes~2.3.6 Exercises for Section 2.3~ 2.4 Parsing2.4.1 Top-Down Parsing2.4.2 Predictive Parsing2.4.3 When to Use 6-Productions~2.4.4 Designing a Predictive Parser~2.4.5 Left Recursion~2.4.6 Exercises for Section 2.4~ 2.5 A Translator for Simple Expressions2.5.1 Abstract and Concrete Syntax~2.5.2 Adapting the Translation Scheme~2.5.3 Procedures for the Nonterminals~2.5.4 Simplifying the Translator~2.5.5 The Complete Program~ 2.6 Lexical Analysis~2.6.1 Removal of White Space and Comments~2.6.2 Reading Ahead~2.6.3 Constants~2.6.4 Recognizing Keywords and Identifiers2.6.5 A Lexical Analyzer~2.6.6 Exercises for Section 2.6~ 2.7 Symbol Tables2.7.1 Symbol Table Per Scope2.7.2 The Use of Symbol Tables~ 2.8 Intermediate Code Generation2.8.1 Two Kinds of Intermediate Representations2.8.2 Construction of Syntax Trees2.8.3 Static Checking~2.8.4 Three-Address Code~2.8.5 Exercises for Section 2.8~ 2.9 Summary of Chapter 2 3 Lexical Analysis 3.1 The Role of the Lexical Analyzer3.1.1 Lexical Analysis Versus Parsing~3.1.2 Tokens, Patterns, and Lexemes~3.1.3 Attributes for Tokens3.1.4 Lexical Errors~3.1.5 Exercises for Section 3.1~ 3.2 Input Buffering 153.2.1 Buffer Pairs3.2.2 Sentinels 3.3 Specification of Tokens3.3.1 Strings and Languages3.3.2 Operations on Languages3.3.3 Regular Expressions3.3.4 Regular Definitions~3.3.5 Extensions of Regular Expressions3.3.6 Exercises for Section 3.3 3.4 Recognition of Tokens~3.4.1 Transition Diagrams3.4.2 Recognition of Reserved Words and Identifiers~3.4.3 Completion of the Running Example3.4.4 Architecture of a Transition-Diagram-Based Lexical Analyzer3.4.5 Exercises for Section 3.4~ 3.5 The Lexical-Analyzer Generator Lex3.5.1 Use of Lex~3.5.2 Structure of Lex Programs~3.5.3 Conflict Resolution in Lex3.5.4 The Lookahead Operator3.5.5 Exercises for Section 3.5~ 3.6 Finite Automata~3.6.1 Nondeterministic Finite Automata~3.6.2 Transition Tables3.6.3 Acceptance of Input Strings by Automata3.6.4 Deterministic Finite Automata3.6.5 Exercises for Section 3.6~ 3.7 From Regular Expressions to Automata~3.7.1 Conversion of an NFA to a DFA3.7.2 Simulation of an NFA~3.7.3 Efficiency of NFA Simulation3.7.4 Construction of an NFA from a Regular Expression3.7.5 Efficiency of String-Processing Algorithms~3.7.6 Exercises for Section 3.7 3.8 Design of a Lexical-Analyzer Generator~3.8.1 The Structure of the Generated Analyzer~3.8.2 Pattern Matching Based on NFA's~3.8.3 DFA's for Lexical Analyzers~3.8.4 Implementing the Lookahead Operator~3.8.5 Exercises for Section 3.8~ 3.9 Optimization of DFA-Based Pattern Matchers~3.9.1 Important States of an NFA~3.9.2 Functions Computed From the Syntax Tree~3.9.3 Computing nullable, firstpos, and lastpos~3.9.4 Computing followpos~3.9.5 Converting a Regular Expression Directly to a DFA~3.9.6 Minimizing the Number of States of a DFA~3.9.7 State Minimization in Lexical Analyzers~3.9.8 Trading Time for Space in DFA Simulation~3.9.9 Exercises for Section 3.9~ 3.10 Summary of Chapter 3~ 3.11 References for Chapter 3~ 4 Syntax Analysis 4.1 Introduction~4.1.1 The Role of the Parser4.1.2 Representative Grammars~4.1.3 Syntax Error Handling~4.1.4 Error-Recovery Strategies~ 4.2 Context-Free Grammars~4.2.1 The Formal Definition of a Context-Free Grammar~4.2.2 Notational Conventions~4.2.3 Derivations~4.2.4 Parse Trees and Derivations~4.2.5 Ambiguity4.2.6 Verifying the Language Generated by a Grammar~4.2.7 Context-Free Grammars Versus Regular Expressions4.2.8 Exercises for Section 4.2 4.3 Writing a Grammar4.3.1 Lexical Versus Syntactic Analysis4.3.2 Eliminating Ambiguity4.3.3 Elimination of Left Recursion~4.3.4 Left Factoring4.3.5 Non-Context-Free Language Constructs4.3.6 Exercises for Section 4.3 4.4 Top-Down Parsing4.4.1 Recursive-Descent Parsing4.4.2 FIRST and FOLLOW~4.4.3 LL(1) Grammars4.4.4 Nonrecursive Predictive Parsing4.4.5 Error Recovery in Predictive Parsing4.4.6 Exercises for Section 4.4~ 4.5 Bottom-Up Parsing~4.5.1 Reductions~4.5.2 Handle Pruning~4.5.3 Shift-Reduce Parsing~4.5.4 Conflicts During Shift-Reduce Parsing4.5.5 Exercises for Section 4.5~ 4.6 Introduction to LR Parsing: Simple LR~4.6.1 Why LR Parsers?~4.6.2 Items and the LR(0) Automaton~4.6.3 The LR-Parsing Algorithm~4.6.4 Constructing SLR-Parsing Tables4.6.5 Viable Prefixes~4.6.6 Exercisesfor Section 4.6~ 4.7 More Powerful LR Parsers~4.7.1 Canonical LR(1) Items~4.7.2 Constructing LR(1) Sets of Items~4.7.3 Canonical LR(1) Parsing Tables~4.7.4 Constructing LALR Parsing Tables~4.7.5 Efficient Construction of LALR Parsing Tables~4.7.6 Compaction of LR Parsing Tables~4.7.7 Exercises for Section 4.7~ 4.8 Using Ambiguous Grammars~4.8.1 Precedence and Associativity to Resolve Conflicts~4.8.2 The "Dangling-Else" Ambiguity~4.8.3 Error Recovery in LR Parsing~4.8.4 Exercises for Section 4.8~ 4.9 Parser Generators~4.9.1 The Parser Generator Yacc~4.9.2 Using Yacc with Ambiguous Grammars~4.9.3 Creating Yacc Lexical Analyzers with Lex~4.9.4 Error Recovery in Yacc~4.9.5 Exercises for Section 4.9~ 4.10 Summary of Chapter 4~4.11 References for Chapter 4~ 5 Syntax-Directed Translation~ 5.1 Syntax-Directed Definitions~5.1.1 Inherited and Synthesized Attributes~5.1.2 Evaluating an SDD at the Nodes of a Parse Tree~5.1.3 Exercises for Section 5.1~ 5.2 Evaluation Orders for SDD's5.2.1 Dependency Graphs~5.2.2 Ordering the Evaluation of Attributes5.2.3 S-Attributed Definitions~5.2.4 L-Attributed Definitions~5.2.5 Semantic Rules with Controlled Side Effects5.2.6 Exercises for Section 5.2~ 5.3 Applications of Synt ax-Directed Translation~5.3.1 Construction of Syntax Trees~5.3.2 The Structure of a Type~5.3.3 Exercises for Section 5.3~ 5.4 Syntax-Directed Translation Schemes5.4.1 Postfix Translation Schemes~5.4.2 Parser-Stack Implementation of Postfix SDT's5.4.3 SDT's With Actions Inside Productions~5.4.4 Eliminating Left Recursion From SDT's5.4.5 SDT's for L-Attributed Definitions~5.4.6 Exercises for Section 5.4~ 5.5 Implementing L- Attributed SDD's~5.5.1 Translation During Recursive-Descent Parsing~5.5.2 On-The-Fly Code Generation~5.5.3 L-Attributed SDD's and LL Parsing~5.5.4 Bottom-Up Parsing of L-Attributed SDD's~5.5.5 Exercises for Section 5.5~ 5.6 Summary of Chapter 5~5.7 References for Chapter 5~ 6 Intermediate-Code Generation~ 6.1 Variants of Syntax Trees~6.1.1 Directed Acyclic Graphs for Expressions6.1.2 The Value-Number Method for Constructing DAG's~6.1.3 Exercises for Section 6.1~ 6.2 Three-Address Code~6.2.1 Addresses and Instructions~6.2.2 Quadruples~6.2.3 Triples~6.2.4 Static Single-A ssignment Form~6.2.5 Exercises for Section 6.2~ 6.3 Types and Declarations~6.3.1 Type Expressions~6.3.2 Type Equivalence~6.3.3 Declarations~6.3.4 Storage Layout for Local Names~6.3.5 Sequences of Declarations~6.3.6 Fields in Records and Classes~6.3.7 Exercises for Section 6.3~ 6.4 Translation of Expressions~6.4.1 Operations Within Expressions~6.4.2 Incremental Translation~6.4.3 Addressing Array Elements~6.4.4 Translation of Array References6.4.5 Exercises for Section 6.4~ 6.5 Type Checking~6.5.1 Rules for Type Checking6.5.2 Type Conversions6.5.3 Overloading of Functions and Operators6.5.4 Type Inference and Polymorphic Functions~6.5.5 An Algorithm for Unification~6.5.6 Exercises for Section 6.5~ 6.6 Control Flow~6.6.1 Boolean Expressions6.6.2 Short-circuit Code~6.6.3 Flow-of- Control Statements6.6.4 Control-Flow Translation of Boolean Expressions6.6.5 Avoiding Redundant Gotos~6.6.6 Boolean Values and Jumping Code~6.6.7 Exercises for Section 6.6~ 6.7 Backpatching~6.7.1 One-Pass Code Generation Using Backpatching6.7.2 Backpatching for Boolean Expressions~6.7.3 Flow-of-Control Statements~6.7.4 Break-, Continue-, and Goto-Statements~6.7.5 Exercises for Section 6.7~ 6.8 Switch-Statements~6.8.1 Translationof Switch-Statements~6.8.2 Syntax-Directed Translation of Switch-Statements6.8.3 Exercises for Section 6.8~ 6.9 Intermediate Code for Procedures6.10 Summary of Chapter 6~6.11 References for Chapter 6~ 7 Run-Time Environments~7.1 Storage Organization~7.1.1 Static Versus Dynamic Storage Allocation~ 7.2 Stack Allocation of Space~7.2.1 Activation Trees~7.2.2 Activation Records~7.2.3 Calling Sequences~7.2.4 Variable-Length Data on the Stack~7.2.5 Exercises for Section 7.2 7.3 Access to Nonlocal Data on the Stack~7.3.1 Data Access Without Nested Procedures~7.3.2 Issues With Nested Procedures~7.3.3 A Language With Nested Procedure Declarations~7.3.4 Nesting Depth7.3.5 Access Links7.3.6 Manipulating Access Links7.3.7 Access Links for Procedure Parameters~7.3.8 Displays7.3.9 Exercises for Section 7.3 7.4 Heap Management~7.4.1 The Memory Manager~7.4.2 The Memory Hierarchy of a Computer7.4.3 Locality in Programs7.4.4 Reducing Fragmentation~7.4.5 Manual Deallocation Requests7.4.6 Exercises for Section 7.4~ 7.5 Introduction to Garbage Collection7.5.1 Design Goals for Garbage Collectors7.5.2 Reachability~7.5.3 Reference Counting Garbage Collectors~7.5.4 Exercises for Section 7.5 7.6 Introduction to Trace-Based Collection7.6.1 A Basic Mark-and-Sweep Collector~7.6.2 Basic Abstraction7.6.3 Optimizing Mark-and-Sweep7.6.4 Mark-and-Compact Garbage Collectors~7.6.5 Copying collectors~7.6.6 Comparing Costs7.6.7 Exercises for Section 7.6~ 7.7 Short-Pause Garbage Collection~7.7.1 Incremental Garbage Collection7.7.2 Incremental Reachability Analysis~7.7.3 Partial-Collection Basics~7.7.4 Generational Garbage Collection7.7.5 The Train Algorithm~7.7.6 Exercises for Section 7.7~ 7.8 Advanced Topics in Garbage Collection~7.8.1 Parallel and Concurrent Garbage Collection~7.8.2 Partial Object Relocation~7.8.3 Conservative Collection for Unsafe Languages~7.8.4 Weak References7.8.5 Exercises for Section 7.8~ 7.9 Summary of Chapter 7 7.10 References for Chapter 7 5 02 8 Code Generation 8.1 Issues in the Design of a Code Generator8.1.1 Input to the Code Generator~8.1.2 The Target Program~8.1.3 Instruction Selection~8.1.4 Register Allocation~8.1.5 Evaluation Order 8.2 The Target Language8.2.1 A Simple Target Machine Model~8.2.2 Program and Instruction Costs~8.2.3 Exercises for Section 8.2~ 8.3 Addresses in the Target Code~8.3.1 Static Allocation~8.3.2 Stack Allocation8.3.3 Run-Time Addresses for Names8.3.4 Exercises for Section 8.3~ 8.4 Basic Blocks and Flow Graphs~8.4.1 Basic Blocks~8.4.2 Next-Use Information8.4.3 Flow Graphs~8.4.4 Representation of Flow Graphs8.4.5 Loops~8.4.6 Exercises for Section 8.4~ 8.5 Optimization of Basic Blocks~8.5.1 The DAG Representation of Basic Blocks8.5.2 Finding Local Common Subexpressions8.5.3 Dead Code Elimination~8.5.4 The Use of Algebraic Identities~8.5.5 Representation of Array References8.5.6 Pointer Assignments and Procedure Calls~8.5.7 Reassembling Basic Blocks From DAG's8.5.8 Exercises for Section 8.5~ 8.6 A Simple Code Generator8.6.1 Register and Address Descriptors8.6.2 The Code-Generation Algorithm~8.6.3 Design of the Function getReg8.6.4 Exercises for Section 8.6 8.7 Peephole Optimization8.7.1 Eliminating Redundant Loads and Stores~8.7.2 Eliminating Unreachable Code~8.7.3 Flow-of-Control Optimizations8.7.4 Algebraic Simplification and Reduction in Strength~8.7.5 Use of Machine Idioms~8.7.6 Exercises for Section 8.7 8.8 Register Allocation and Assignment~8.8.1 Global Register Allocation8.8.2 Usage Counts8.8.3 Register Assignment for Outer Loops8.8.4 Register Allocation by Graph Coloring8.8.5 Exercises for Section 8.8~ 8.9 Instruction Selection by Tree Rewriting8.9.1 Tree-Translation Schemes~8.9.2 Code Generation by Tiling an Input Tree8.9.3 Pattern Matching by Parsing8.9.4 Routines for Semantic Checking~8.9.5 General Tree Matching~8.9.6 Exercises for Section 8.9~ 8.10 Optimal Code Generation for Expressions~8.10.1 Ershov Numbers~8.10.2 Generating Code From Labeled Expression Trees8.10.3 Evaluating Expressions with an Insufficient Supply of Registers~8.10.4 Exercises for Section 8.10 8.11 Dynamic Programming Code-Generation~8.11.1 Contiguous Evaluation~8.11.2 The Dynamic Programming Algorithm~8.11.3 Exercises for Section 8.11~ 8.12 Summary of Chapter 8~8.13 References for Chapter 8~ 9 Machine-Independent Optimizations~ 9.1 The Principal Sources of Optimization~9.1.1 Causes of Redundancy~9.1.2 A Running Example: Quicksort9.1.3 Semantics-Preserving Transformations9.1.4 Global Common Subexpressions~9.1.5 Copy Propagation~9.1.6 Dead-Code Elimination~9.1.7 Code Motion~9.1.8 Induction Variables and Reduction in Strength~9.1.9 Exercises for Section 9.1 5 9.2 Introduction to Data-Flow Analysis9.2.1 The Data-Flow Abstraction~9.2.2 The Data-Flow Analysis Schema~9.2.3 Data-Flow Schemas on Basic Blocks9.2.4 Reaching Definitions9.2.5 Live-Variable Arlalysis~9.2.6 Available Expressions~9.2.7 Summary~9.2.8 Exercises for Section 9.2~ 9.3 Foundations of Data-Flow Analysis~9.3.1 Semilattices9.3.2 Transfer Functions9.3.3 The Iterative Algorithm for General Frameworks~9.3.4 Meaning of a Data-Flow Solution~9.3.5 Exercises for Section 9.3~ 9.4 Constant Propagation~9.4.1 Data-Flow Values for the Constant-Propagation Framework9.4.2 The Meet for the Constant-Propagation Framework9.4.3 Transfer Functions for the Constant-Propagation Framework9.4.4 Monotonicity of the Constant-Propagation Framework9.4.5 Nondistributivity of the Constant-Propagation Framework~9.4.6 Interpretation of the Results9.4.7 Exercises for Section 9.4 9.5 Partial-Redundancy Elimination9.5.1 The Sources of Redundancy9.5.2 Can All Redundancy Be Eliminated?9.5.3 The Lazy-Code-Motion Problem~9.5.4 Anticipation of Expressions~9.5.5 The Lazy-Code-Motion Algorithm~9.5.6 Exercises for Section 9.5~ 9.6 Loops in Flow Graphs9.6.1 Dominators~9.6.2 Depth-First Ordering~9.6.3 Edges in a Depth-First Spanning Tree~9.6.4 Back Edges and Reducibility~9.6.5 Depth of a Flow Graph~9.6.6 Natural Loops~9.6.7 Speed of Convergence of Iterative Data-Flow Algorithms~9.6.8 Exercises for Section 9.6~ 9.7 Region-Based Analysis~9.7.1 Regions~9.7.2 Region Hierarchies for Reducible Flow Graphs~9.7.3 Overview of a Region-Based Analysis9.7.4 Necessary Assumptions About Transfer Functions~9.7.5 An Algorithm for Region-Based Analysis9.7.6 Handling Nonreducible Flow Graphs9.7.7 Exercises for Section 9.7~ 9.8 Symbolic Analysis9.8.1 Affine Expressions of Reference Variables~9.8.2 Data-Flow Problem Formulation~9.8.3 Region-Based Symbolic Analysis9.8.4 Exercises for Section 9.8~ 9.9 Summary of Chapter 9~ 9.10 References for Chapter 9~ 10 Instruct ion-Level Parallelism~ 10.1 Processor Architectures~10.1.1 Instruction Pipelines and Branch Delays~10.1.2 Pipelined Execution~10.1.3 Multiple Instruction Issue~ 10.2 Code-Scheduling Constraints~10.2.1 Data Dependence10.2.2 Finding Dependences Among Memory Accesses10.2.3 Tradeoff Between Register Usage and Parallelism~10.2.4 Phase Ordering Between Register Allocation and Code Scheduling10.2.5 Control Dependence~10.2.6 Speculative Execution Support~10.2.7 A Basic Machine Model10.2.8 Exercises for Section 10.2~ 10.3 Basic-Block Scheduling~10.3.1 Data-Dependence Graphs~10.3.2 List Scheduling of Basic Blocks~10.3.3 Prioritized Topological Orders10.3.4 Exercises for Section 10.3~ 10.4 Global Code Scheduling~10.4.1 Primitive Code Motion10.4.2 Upward Code Motion10.4.3 Downward Code Motion~10.4.4 Updating Data Dependences~10.4.5 Global Scheduling Algorithms~10.4.6 Advanced Code Motion Techniques~10.4.7 Interaction with Dynamic Schedulers~10.4.8 Exercises for Section 10.4~ 10.5 Software Pipelining~10.5.1 Introduction~10.5.2 Software Pipelining of Loops~10.5.3 Register Allocation and Code Generation~10.5.4 Do-Across Loops~10.5.5 Goals and Constraints of Software Pipelining10.5.6 A Software-Pipelining Algorithm~10.5.7 Scheduling Acyclic Data-Dependence Graphs~10.5.8 Scheduling Cyclic Dependence Graphs~10.5.9 Improvements to the Pipelining Algorithms~10.5.10 Modular Variable Expansion~10.5.11 Conditional Statements10.5.12 Hardware Support for Software Pipelining~10.5.13 Exercises for Section 10.5~ 10.6 Summary of Chapter 10~ 10.7 References for Chapter 10~ 11 Optimizing for Parallelism and Locality~ 11.1 Basic Concepts~11.1.1 Multiprocessors~11.1.2 Parallelism in Applications~11.1.3 Loop-Level Parallelism~11.1.4 Data Locality~11.1.5 Introduction to Affine Transform Theory 11.2 Matrix Multiply: An In-Depth Example11.2.1 The Matrix-Multiplication Algorithm~11.2.2 Optimizations~11.2.3 Cache Interference~11.2.4 Exercises for Section 11.2~ 11.3 Iteration Spaces~11.3.1 Constructing Iteration Spaces from Loop Nests11.3.2 Execution Order for Loop Nests~11.3.3 Matrix Formulation of Inequalities~11.3.4 Incorporating Symbolic Constants~11.3.5 Controlling the Order of Execution~11.3.6 Changing Axes~11.3.7 Exercises for Section 11.3~ 11.4 Affine Array Indexes~11.4.1 Affine Accesses~11.4.2 Affine and Nonaffine Accesses in Practice~11.4.3 Exercises for Section 11.4~ 11.5 Data Reuse~11.5.1 Types of Reuse~11.5.2 Self Reuse~11.5.3 Self-spatial Reuse~11.5.4 Group Reuse~11.5.5 Exercises for Section 11.5~ 11.6 Array Data-Dependence Analysis~11.6.1 Definition of Data Dependence of Array Accesses~11.6.2 Integer Linear Programming~11.6.3 The GCD Test11.6.4 Heuristics for Solving Integer Linear Programs11.6.5 Solving General Integer Linear Programs11.6.6 Summary~11.6.7 Exercises for Section 11.6~ 11.7 Finding Synchronization-Free Parallelism~11.7.1 An Introductory Example~11.7.2 Affine Space Partitions~11.7.3 Space-Partition Constraints~11.7.4 Solving Space-Partition Constraints~11.7.5 A Simple Code-Generation Algorithm~11.7.6 Eliminating Empty Iterations~11.7.7 Eliminating Tests from Innermost Loops11.7.8 Source-Code Transforms11.7.9 Exercises for Section 11.7 11.8 Synchronization Between Parallel Loops~11.8.1 A Constant Number of Synchronizations 311.8.2 Program-Dependence Graphs~11.8.3 Hierarchical Time~11.8.4 The Parallelization Algorithm~11.8.5 Exercises for Section 11.8~ 11.9 Pipelining~11.9.1 What is Pipelining?~11.9.2 Successive Over-Relaxation (SOR): An Example~11.9.3 Fully Permutable Loops11.9.4 Pipelining Fully Permutable Loops11.9.5 General Theory~11.9.6 Time-Partition Constraints~11.9.7 Solving Time-Partition Constraints by Farkas' Lemma~11.9.8 Code Transformations~11.9.9 Parallelism With Minimum Synchronization~11.9.10 Exercises for Section 11.9~ 11.10 Locality Optimizations~11.10.1 Temporal Locality of Computed Data11.10.2A rray Contraction11.10.3 Partition Interleaving~11.10.4P utting it All Together11.10.5 Exercises for Section 11.10 11.11 Other Uses of Affine Transforms11.11.1 Distributed memory machines~11.11.2 Multi-Instruction-Issue Processors~11.11.3 Vector and SIMD Instructions11.11.4 Prefetching 11.12 Summary of Chapter 11~11.13 References for Chapter 11~ 12 Interprocedural Analysis~12.1 Basic Concepts12.1.1 Call Graphs~12.1.2 Context Sensitivity~12.1.3 Call Strings~12.1.4 Cloning-Based Context-Sensitive Analysis~12.1.5 Summary-Based Context-Sensitive Analysis~12.1.6 Exercises for Section 12.1~ 12.2 Why Interprocedural Analysis?~12.2.1 Virtual Method Invocation12.2.2 Pointer Alias Analysis~12.2.3 Parallelization~12.2.4 Detection of Software Errors and Vulnerabilities~12.2.5 SQL Injection~12.2.6 Buffer Overflow~ 12.3 A Logical Representation of Data Flow12.3.1 Introduction to Datalog~12.3.2 Datalog Rules12.3.3 Intensional and Extensional Predicates12.3.4 Execution of Datalog Programs~12.3.5 Incremental Evaluation of Datalog Programs12.3.6 Problematic Datalog Rules12.3.7 Exercises for Section 12.3 12.4 A Simple Pointer-Analysis Algorithm12.4.1 Why is Pointer Analysis Difficult12.4.2 A Model for Pointers and References~12.4.3 Flow Insensitivity~12.4.4 The Formulation in Datalog~12.4.5 Using Type Information12.4.6 Exercises for Section 12.412.5 Context-Insensitive Interprocedural Analysis12.5.1 Effects of a Method Invocation~12.5.2 Call Graph Discovery in Datalog12.5.3 Dynamic Loading and Reflection~12.5.4 Exercises for Section 12.5~ 12.6 Context-Sensitive Pointer Analysis12.6.1 Contexts and Call Strings12.6.2 Adding Context to Datalog Rules12.6.3 Additional Observations About Sensitivity12.6.4 Exercises for Section 12.6 12.7 Datalog Implementation by BDD's12.7.1 Binary Decision Diagrams12.7.2 Transformations on BDD7s~12.7.3 Representing Relations by BDD7s~12.7.4 Relational Operations as BDD Operations12.7.5 Using BDD7sf or Points-to Analysis~12.7.6 Exercises for Section 12.7~ 12.8 Summary of Chapter 12~ 12.9 References for Chapter 12~ A A Complete Front End~A.1 The Source LanguageA.2 Main~A.3 Lexical Analyzer~A.4 Symbol Tables and TypesA.5 Intermediate Code for Expressions~A.6 Jumping Code for Boolean Expressions~A.7 Intermediate Code for Statements~A.8 Parser~A.9 Creating the Front End~ B Finding Linearly Independent Solutions Index},
  langid = {english},
  keywords = {programming},
  file = {C:\Users\nhiot\OneDrive\zotero\2007\Alfred V Aho - 2007 - Compilers Principles, Techniques, And Tools.pdf}
}

@misc{AllenaiScispacy2020,
  title = {Allenai/Scispacy},
  year = {2020},
  month = jan,
  url = {https://github.com/allenai/scispacy},
  urldate = {2020-01-27},
  abstract = {A full spaCy pipeline and models for scientific/biomedical documents.},
  copyright = {Apache-2.0},
  howpublished = {AI2},
  keywords = {bioinformatics,biomedical,custom-pipes,nlp,nosource,scientific-documents,spacy}
}

@inproceedings{alotaibiPropertyGraphSchema2021,
  title = {Property {{Graph Schema Optimization}} for {{Domain-Specific Knowledge Graphs}}},
  booktitle = {2021 {{IEEE}} 37th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {Alotaibi, Rana and Lei, Chuan and Quamar, Abdul and Efthymiou, Vasilis and {\"O}zcan, Fatma},
  year = {2021},
  month = apr,
  pages = {924--935},
  publisher = {IEEE Computer Society},
  address = {Chania, Greece},
  issn = {2375-026X},
  doi = {10.1109/ICDE51399.2021.00085},
  urldate = {2024-01-03},
  abstract = {Enterprises are creating domain-specific knowledge graphs by curating and integrating their business data from multiple sources. Ontologies provide a semantic abstraction for such knowledge graphs to describe their data in terms of the entities involved and their relationships. There has been a lot of effort to build systems that enable efficient querying over knowledge graphs, represented as property graphs. However the problem of schema optimization in the property graph setting has been largely ignored. In this work, we show that graph schema design has significant impact on query performance, and propose two algorithms to generate an optimized property graph schema from the domain ontology. To the best of our knowledge, we are the first to present an ontology-driven approach for property graph schema optimization. The rich semantic relationships in an ontology contain a variety of opportunities to reduce edge traversals and consequently improve the graph query performance. Our experimental study with two real-world knowledge graphs shows that our algorithms produce high-quality schemas, achieving up to 2 orders of magnitude speed-up compared to alternative schema designs.},
  isbn = {978-1-72819-184-3},
  keywords = {nosource},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2021\\Alotaibi et al. - 2021 - Property Graph Schema Optimization for Domain-Spec.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\WDW8PMNM\\9458757.html}
}

@article{alshaikhdeebBiomedicalNamedEntity2016,
  title = {Biomedical {{Named Entity Recognition}}: {{A Review}}},
  shorttitle = {Biomedical {{Named Entity Recognition}}},
  author = {Alshaikhdeeb, Basel and Ahmad, Kamsuriah},
  year = {2016},
  month = dec,
  journal = {International Journal on Advanced Science, Engineering and Information Technology},
  volume = {6},
  number = {6},
  pages = {889--895},
  doi = {10.18517/ijaseit.6.6.1367},
  urldate = {2024-02-13},
  abstract = {Biomedical Named Entity Recognition (BNER) is the task of identifying biomedical instances such as chemical compounds, genes, proteins, viruses, disorders, DNAs and RNAs. The key challenge behind BNER lies on the methods that would be used for extracting such entities. Most of the methods used for BNER were relying on Supervised Machine Learning (SML) techniques. In SML techniques, the features play an essential role in terms of improving the effectiveness of the recognition process. Features can be identified as a set of discriminating and distinguishing characteristics that have the ability to indicate the occurrence of an entity. In this manner, the features should be able to generalize which means to discriminate the entities correctly even on new and unseen samples. Several studies have tackled the role of features in terms of identifying named entities. However, with the surge of biomedical researches, there is a vital demand to explore biomedical features. This paper aims to accommodate a review study on the features that could be used for BNER in which various types of features will be examined including morphological features, dictionary-based features, lexical features and distance-based features.},
  file = {C:\Users\nhiot\OneDrive\zotero\2016\Alshaikhdeeb et Ahmad - 2016 - Biomedical Named Entity Recognition A Review.pdf}
}

@inproceedings{alvesAutomaticReadingLearning2002,
  title = {Automatic Reading and Learning from Text},
  booktitle = {Proceedings of the International Symposium on Artificial Intelligence},
  author = {Alves, A. and Pereira, F. and Cardoso, A.},
  year = {2002},
  url = {https://old.cisuc.uc.pt/publication/show/624},
  urldate = {2024-04-07},
  keywords = {⛔ No DOI found}
}

@inproceedings{amaviNaturalLanguageQuerying2020,
  title = {Natural {{Language Querying System Through Entity Enrichment}}},
  booktitle = {{{ADBIS}}, {{TPDL}} and {{EDA}} 2020 {{Common Workshops}} and {{Doctoral Consortium}}: {{International Workshops}}: {{DOING}}, {{MADEISD}}, {{SKG}}, {{BBIGAP}}, {{SIMPDA}}, {{AIMinScience}} 2020 and {{Doctoral Consortium}}, {{Lyon}}, {{France}}, {{August}} 25--27, 2020, {{Proceedings}} 24},
  author = {Amavi, Joshua and {Halfeld-Ferrari}, Mirian and Hiot, Nicolas},
  editor = {Bellatreche, Ladjel and Bielikov{\'a}, M{\'a}ria and Boussa{\"i}d, Omar and Catania, Barbara and Darmont, J{\'e}r{\^o}me and Demidova, Elena and Duchateau, Fabien and Hall, Mark and Mer{\v c}un, Tanja and Novikov, Boris and Papatheodorou, Christos and Risse, Thomas and Romero, Oscar and Sautot, Lucile and Talens, Guilaine and Wrembel, Robert and {\v Z}umer, Maja},
  year = {2020},
  month = aug,
  series = {Communications in {{Computer}} and {{Information Science}}},
  volume = {1260},
  pages = {36--48},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-55814-7_3},
  abstract = {This paper focuses on a domain expert querying system over databases. It presents a solution designed for a French enterprise interested in offering a natural language interface for its clients. The approach, based on entity enrichment, aims at translating natural language queries into database queries. In this paper, the database is treated through a logical paradigm, suggesting the adaptability of our approach to different database models. The good precision of our method is shown through some preliminary experiments.},
  copyright = {All rights reserved},
  hal_id = {hal-02959502},
  hal_version = {v1},
  isbn = {978-3-030-55813-0 978-3-030-55814-7},
  langid = {english},
  keywords = {Database query,me,NLI,NLP,nosource,Question answering},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2020\\Amavi et al. - 2020 - Natural Language Querying System Through Entity En.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\XGTK6R7N\\DOING13.mp4;C\:\\Users\\nhiot\\Zotero\\storage\\MT5B2A5X\\978-3-030-55814-7_3.html}
}

@incollection{amsiliActesTALN19991999,
  title = {Actes de {{TALN}} 1999 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 1999 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Amsili, Pascal},
  year = {1999},
  month = jul,
  publisher = {ATALA},
  address = {Carg{\`e}se},
  keywords = {nosource}
}

@article{amsterdamerNaturalLanguageInterface2015,
  title = {A Natural Language Interface for Querying General and Individual Knowledge},
  author = {Amsterdamer, Yael and Kukliansky, Anna and Milo, Tova},
  year = {2015},
  month = aug,
  journal = {Proceedings of the VLDB Endowment},
  volume = {8},
  number = {12},
  pages = {1430--1441},
  issn = {2150-8097},
  doi = {10.14778/2824032.2824042},
  urldate = {2024-03-29},
  abstract = {Many real-life scenarios require the joint analysis of               general knowledge               , which includes facts about the world, with               individual knowledge               , which relates to the opinions or habits of individuals. Recently developed crowd mining platforms, which were designed for such tasks, are a major step towards the solution. However, these platforms require users to specify their information needs in a formal, declarative language, which may be too complicated for na{\"i}ve users. To make the joint analysis of general and individual knowledge accessible to the public, it is desirable to provide an interface that translates the user questions, posed in natural language (NL), into the formal query languages that crowd mining platforms support.                          While the translation of NL questions to queries over conventional databases has been studied in previous work, a setting with mixed individual and general knowledge raises unique challenges. In particular, to support the distinct query constructs associated with these two types of knowledge, the NL question must be partitioned and translated using different means; yet eventually all the translated parts should be seamlessly combined to a well-formed query. To account for these challenges, we design and implement a modular translation framework that employs new solutions along with state-of-the art NL parsing tools. The results of our experimental study, involving real user questions on various topics, demonstrate that our framework provides a high-quality translation for many questions that are not handled by previous translation tools.},
  langid = {english},
  optbiburl = {https://dblp.org/rec/journals/pvldb/AmsterdamerKM15.bib},
  optdoi = {10.14778/2824032.2824042},
  opttimestamp = {Sat, 25 Apr 2020 13:59:36 +0200},
  opturl = {http://www.vldb.org/pvldb/vol8/p1430-amsterdamer.pdf},
  keywords = {nosource}
}

@inproceedings{anglesPGKeysKeysProperty2021,
  ids = {BDFHHLLLM21},
  title = {{{PG-Keys}}: {{Keys}} for Property Graphs},
  shorttitle = {Pg-Keys},
  booktitle = {Proceedings of the 2021 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Angles, Renzo and Bonifati, Angela and Dumbrava, Stefania and Fletcher, George and Hare, Keith W. and Hidders, Jan and Lee, Victor E. and Li, Bei and Libkin, Leonid and Martens, Wim and Murlak, Filip and Perryman, Josh and Savkovic, Ognjen and Schmidt, Michael and Sequeda, Juan F. and Staworko, Slawek and Tomaszuk, Dominik},
  year = {2021},
  pages = {2423--2436},
  publisher = {ACM},
  doi = {10.1145/3448016.3457561},
  keywords = {nosource},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2021\\Angles et al. - 2021 - PG-Keys Keys for property graphs.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\7YULBJER\\3448016.html}
}

@book{baderGraphPartitioningGraph2013,
  title = {Graph {{Partitioning}} and {{Graph Clustering}}},
  author = {Bader, David and Meyerhenke, Henning and Sanders, Peter and Wagner, Dorothea},
  year = {2013},
  month = jan,
  series = {Contemporary {{Mathematics}}},
  volume = {588},
  publisher = {American Mathematical Society},
  issn = {0271-4132, 1098-3627},
  doi = {10.1090/conm/588},
  urldate = {2023-10-31},
  abstract = {Advancing research. Creating connections.},
  isbn = {978-0-8218-9038-7 978-0-8218-9868-0 978-0-8218-9869-7},
  langid = {english}
}

@article{balminIncrementalValidationXML2004,
  ids = {balminIncrementalValidationXML2004a},
  title = {Incremental Validation of {{XML}} Documents},
  author = {Balmin, Andrey and Papakonstantinou, Yannis and Vianu, Victor},
  year = {2004},
  month = dec,
  journal = {ACM Transactions on Database Systems},
  volume = {29},
  number = {4},
  pages = {710--751},
  publisher = {ACM Press},
  issn = {0362-5915, 1557-4644},
  doi = {10.1145/1042046.1042050},
  urldate = {2024-01-04},
  abstract = {We investigate the incremental validation of XML documents with respect to DTDs, specialized DTDs, and XML Schemas, under updates consisting of element tag renamings, insertions, and deletions. DTDs are modeled as extended context-free grammars. "Specialized DTDs" allow the decoupling of element types from element tags. XML Schemas are abstracted as specialized DTDs with limitations on the type assignment. For DTDs and XML Schemas, we exhibit an O(m log n) incremental validation algorithm using an auxiliary structure of size O(n), where n is the size of the document and m the number of updates. The algorithm does not handle the incremental validation of XML Schema wrt renaming of internal nodes, which is handled by the specialized DTDs incremental validation algorithm. For specialized DTDs, we provide an O(m log2 n) incremental algorithm, again using an auxiliary structure of size O(n). This is a significant improvement over brute-force re-validation from scratch.We exhibit a restricted class of DTDs called local that arise commonly in practice and for which incremental validation can be done in practically constant time by maintaining only a list of counters. We present implementations of both general incremental validation and local validation on an XML database built on top of a relational database.Our experimentation includes a study of the applicability of local validation in practice, results on the calibration of parameters of the auxiliary data structure, and results on the performance comparison between the general incremental validation technique, the local validation technique, and brute-force validation from scratch.},
  langid = {english},
  keywords = {nosource,Update,validation,XML},
  file = {C:\Users\nhiot\OneDrive\zotero\2004\Balmin et al. - 2004 - Incremental validation of XML documents2.pdf}
}

@misc{barqueDeepSequoiaCorpus2020,
  type = {{corpus, text}},
  title = {{Deep Sequoia corpus - PARSEME-FR corpus - FrSemCor}},
  author = {Barque, Lucie and Candito, Marie and Constant, Matthieu and Cordeiro, Silvio Ricardo and Crabb{\'e}, Beno{\^i}t and Fort, Kar{\"e}n and Guillaume, Bruno and Haas, Pauline and Huyghe, Richard and Perrier, Guy and Ramisch, Carlos and Ribeyre, Corentin and Savary, Agata and Seddah, Djam{\'e} and Segonne, Vincent and Tribout, Delphine and {Villemonte de la Clergerie}, Eric and Parmentier, Yannick and Pasquer, Caroline and Antoine, Jean-Yves},
  year = {2020},
  month = mar,
  publisher = {ANR},
  address = {https://github.com/UniversalDependencies/UD\_French-Sequoia},
  url = {https://deep-sequoia.inria.fr/},
  urldate = {2024-03-21},
  abstract = {The Sequoia corpus is a set of 3,099 linguistically-annotated French sentences, originating from four sources (Europarl, European Agency Reports, French regional journal L'Est R{\'e}publicain, and French wikipedia).    Several types of annotations were added over the years.  The current release comprises:    - parts-of-speech (SEQUOIA ANR-08-EMER-013 project)    - syntactic dependency trees    - deep syntactic dependency graphs (Deep sequoia project)    - multi-word expressions and named entities (PARSEME COST project and PARSEME-FR ANR-14-CERA-0001 project)    - coarse semantic tags for nouns (FrSemCor project)    See the deep sequoia page for a detailed description: https://deep-sequoia.inria.fr/},
  copyright = {Deep Sequoia Licence},
  langid = {fra},
  keywords = {⛔ No DOI found},
  annotation = {Accepted: 2021-01-21T10:36:00Z}
}

@book{barrasaBuildingKnowledgeGraphs2023,
  title = {Building {{Knowledge Graphs}}: {{A Practitioner}}'s {{Guide}}},
  author = {Barrasa, Jesus and Webber, Jim},
  year = {2023},
  month = jun,
  publisher = {"O'Reilly Media, Inc."},
  abstract = {Incredibly useful, knowledge graphs help organizations keep track of medical research, cybersecurity threat intelligence, GDPR compliance, web user engagement, and much more. They do so by storing interlinked descriptions of entities---objects, events, situations, or abstract concepts---and encoding the underlying information. How do you create a knowledge graph? And how do you move it from theory into production?Using hands-on examples, this practical book shows data scientists and data engineers how to build their own knowledge graphs. Authors Jes{\'u}s Barrasa and Jim Webber from Neo4j illustrate common patterns for building knowledge graphs that solve many of today's pressing knowledge management problems. You'll quickly discover how these graphs become increasingly useful as you add data and augment them with algorithms and machine learning.Learn the organizing principles necessary to build a knowledge graphExplore how graph databases serve as a foundation for knowledge graphsUnderstand how to import structured and unstructured data into your graphFollow examples to build integration-and-search knowledge graphsLearn what pattern detection knowledge graphs help you accomplishExplore dependency knowledge graphs through examplesUse examples of natural language knowledge graphs and chatbotsUse graph algorithms and ML to gain insight into connected data},
  googlebooks = {6MTGEAAAQBAJ},
  isbn = {978-1-09-812706-0},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / Natural Language Processing,Computers / Data Science / Data Modeling \& Design,Computers / Data Science / Data Visualization,Computers / Data Science / General,Computers / Data Science / Machine Learning},
  file = {C:\Users\nhiot\OneDrive\zotero\2023\Barrasa et Webber - 2023 - Building Knowledge Graphs A Practitioner's Guide.pdf}
}

@inproceedings{barretAbstraGenericAbstractions2022,
  title = {Abstra: {{Toward Generic Abstractions}} for {{Data}} of {{Any Model}}},
  shorttitle = {Abstra},
  booktitle = {Proceedings of the 31st {{ACM International Conference}} on {{Information}} \& {{Knowledge Management}}},
  author = {Barret, Nelly and Manolescu, Ioana and Upadhyay, Prajna},
  year = {2022},
  month = oct,
  series = {{{CIKM}} '22},
  pages = {4803--4807},
  publisher = {Association for Computing Machinery},
  address = {Atlanta GA USA},
  doi = {10.1145/3511808.3557179},
  urldate = {2024-04-04},
  abstract = {Digital data sharing leads to unprecedented opportunities to develop data-driven systems for supporting economic activities, the social and political life, and science. Many open-access datasets are RDF (Linked Data) graphs, but others are JSON or XML documents, CSV files, Neo4J property graphs, etc. Potential users need to understand a dataset in order to decide if it is useful for their goal. While some published datasets come with a schema and/or documentation, this is not always the case. We demonstrate Abstra, a dataset abstraction system, which applies on a large variety of data models. Abstra computes a description meant for humans, and integrates Information Extraction to classify dataset content among a set of categories of interest to the user. Our abstractions are conceptually close to Entity-Relationship diagrams, but our entities can have deeply nested structure.},
  isbn = {978-1-4503-9236-5},
  langid = {english},
  keywords = {data integration,dataset discovery,E-R schema},
  file = {C:\Users\nhiot\OneDrive\zotero\2022\Barret et al. - 2022 - Abstra Toward Generic Abstractions for Data of An.pdf}
}

@inproceedings{barretGenericAbstractionsData2021,
  title = {Toward {{Generic Abstractions}} for {{Data}} of {{Any Model}}},
  booktitle = {{{BDA}} 2021-{{Informal}} Publication Only},
  author = {Barret, Nelly and Manolescu, Ioana and Upadhyay, Prajna},
  year = {2021},
  month = oct,
  url = {https://hal.inria.fr/hal-03344041},
  urldate = {2021-10-26},
  abstract = {Digital data sharing leads to unprecedented opportunities to develop data-driven systems for supporting economic activities, the social and political life, and science. Many open-access datasets are RDF graphs, but others are CSV files, Neo4J property graphs, JSON or XML documents, etc. Potential users need to understand a dataset in order to decide if it is useful for their goal. While some datasets come with a schema and/or documentation, this is not always the case. Data summarization or schema inference tools have been proposed, specializing in XML, or JSON, or the RDF data models. In this work, we present a dataset abstraction approach, which () applies on relational, CSV, XML, JSON, RDF or Property Graph data; () computes an abstraction meant for humans (as opposed to a schema meant for a parser); () integrates Information Extraction data profiling, to also classify dataset content among a set of categories of interest to the user. Our abstractions are conceptually close to an Entity-Relationship diagram, if one allows nested and possibly heterogeneous structure within entities.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2021\\Barret et al. - 2021 - Toward Generic Abstractions for Data of Any Model.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\NWGV8NRL\\hal-03344041.html}
}

@book{bellatrecheNewTrendsDatabase2021,
  title = {New {{Trends}} in {{Database}} and {{Information Systems}}: {{ADBIS}} 2021 {{Short Papers}}, {{Doctoral Consortium}} and {{Workshops}}: {{DOING}}, {{SIMPDA}}, {{MADEISD}}, {{MegaData}}, {{CAoNS}}, {{Tartu}}, {{Estonia}}, {{August}} 24-26, 2021, {{Proceedings}}},
  shorttitle = {New {{Trends}} in {{Database}} and {{Information Systems}}},
  author = {Bellatreche, Ladjel and Dumas, Marlon and Karras, Panagiotis and Matulevi{\v c}ius, Raimundas and Awad, Ahmed and Weidlich, Matthias and Ivanovi{\'c}, Mirjana and Hartig, Olaf},
  year = {2021},
  series = {Communications in {{Computer}} and {{Information Science}}},
  volume = {1450},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-85082-1},
  isbn = {978-3-030-85081-4 978-3-030-85082-1},
  langid = {english},
  keywords = {artificial intelligence,computer hardware,computer networks,computer security,computer systems,computer vision,cryptography,data integration,data mining,databases,image analysis,image processing,information retrieval,machine learning,network protocols,query languages,search engines,semantics,software engineering,World Wide Web},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2021\\Bellatreche et al. - 2021 - New Trends in Database and Information Systems AD.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\7DHUEQKT\\978-3-030-85082-1.html}
}

@incollection{benamaraActesTALN20072007,
  title = {Actes de {{TALN}} 2007 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 2007 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Benamara, Farah and Hatout, Nabil and Muller, Philippe and Ozdowska, Sylwia},
  year = {2007},
  month = jun,
  publisher = {IRIT / ATALA},
  address = {Toulouse},
  keywords = {nosource}
}

@inproceedings{benediktBenchmarkingChase2017,
  title = {Benchmarking the {{Chase}}},
  booktitle = {Proceedings of the 36th {{ACM SIGMOD-SIGACT-SIGAI Symposium}} on {{Principles}} of {{Database Systems}}},
  author = {Benedikt, Michael and Konstantinidis, Georgios and Mecca, Giansalvatore and Motik, Boris and Papotti, Paolo and Santoro, Donatello and Tsamoura, Efthymia},
  year = {2017},
  month = may,
  series = {{{PODS}} '17},
  pages = {37--52},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3034786.3034796},
  urldate = {2024-01-03},
  abstract = {The chase is a family of algorithms used in a number of data management tasks, such as data exchange, answering queries under dependencies, query reformulation with constraints, and data cleaning. It is well established as a theoretical tool for understanding these tasks, and in addition a number of prototype systems have been developed. While individual chase-based systems and particular optimizations of the chase have been experimentally evaluated in the past, we provide the first comprehensive and publicly available benchmark - test infrastructure and a set of test scenarios - for evaluating chase implementations across a wide range of assumptions about the dependencies and the data. We used our benchmark to compare chase-based systems on data exchange and query answering tasks with one another, as well as with systems that can solve similar tasks developed in closely related communities. Our evaluation provided us with a number of new insights concerning the factors that impact the performance of chase implementations.},
  collaborator = {Benedikt, Michael and Konstantinidis, Georgios and Mecca, Giansalvatore and Motik, Boris and Papotti, Paolo and Santoro, Donatello and Tsamoura, Efthymia},
  copyright = {accepted\_manuscript},
  isbn = {978-1-4503-4198-1},
  langid = {english},
  keywords = {chase,nosource},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2017\\Benedikt et al. - 2017 - Benchmarking the Chase.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\T3GVYJVT\\423202.html}
}

@inproceedings{bernhardApprentissageNonSupervise2007,
  title = {{Apprentissage non supervis{\'e} de familles morphologiques par classification ascendante hi{\'e}rarchique}},
  booktitle = {{Actes de la 14{\`e}me conf{\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs}},
  author = {Bernhard, Delphine},
  year = {2007},
  month = jun,
  pages = {345--354},
  publisher = {IRIT / ATALA},
  address = {Toulouse, France},
  url = {https://aclanthology.org/2007.jeptalnrecital-long.34},
  urldate = {2023-10-23},
  abstract = {Cet article pr{\'e}sente un syst{\`e}me d'acquisition de familles morphologiques qui proc{\`e}de par apprentissage non supervis{\'e} {\`a} partir de listes de mots extraites de corpus de textes. L'approche consiste {\`a} former des familles par groupements successifs, similairement aux m{\'e}thodes de classification ascendante hi{\'e}rarchique. Les crit{\`e}res de regroupement reposent sur la similarit{\'e} graphique des mots ainsi que sur des listes de pr{\'e}fixes et de paires de suffixes acquises automatiquement {\`a} partir des corpus trait{\'e}s. Les r{\'e}sultats obtenus pour des corpus de textes de sp{\'e}cialit{\'e} en fran{\c c}ais et en anglais sont {\'e}valu{\'e}s {\`a} l'aide de la base CELEX et de listes de r{\'e}f{\'e}rence construites manuellement. L'{\'e}valuation d{\'e}montre les bonnes performances du syst{\`e}me, ind{\'e}pendamment de la langue, et ce malgr{\'e} la technicit{\'e} et la complexit{\'e} morphologique du vocabulaire trait{\'e}.},
  hal_id = {hal-00800342},
  langid = {french},
  keywords = {nosource},
  file = {C:\Users\nhiot\OneDrive\zotero\2007\Bernhard - 2007 - Apprentissage non supervisé de familles morphologi.pdf}
}

@article{beveridgeNetworkThrones2016,
  title = {Network of {{Thrones}}},
  author = {Beveridge, Andrew and Shan, Jie},
  year = {2016},
  month = apr,
  journal = {Math Horizons},
  volume = {23},
  number = {4},
  pages = {18--22},
  issn = {1072-4117, 1947-6213},
  doi = {10.4169/mathhorizons.23.4.18},
  urldate = {2024-03-18},
  langid = {english},
  file = {C:\Users\nhiot\Zotero\storage\YL67X5H9\Beveridge et Shan - 2016 - Network of Thrones.pdf}
}

@incollection{bigiActesTALN20142014,
  title = {Actes de {{TALN}} 2014 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 2014 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Bigi, Brigitte},
  year = {2014},
  month = jul,
  publisher = {LPL / ATALA},
  address = {Marseille},
  keywords = {nosource}
}

@incollection{blacheActesTALN20042004,
  title = {Actes de {{TALN}} 2004 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 2004 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Blache, Philippe},
  year = {2004},
  month = apr,
  publisher = {LPL / ATALA},
  address = {F{\`e}s, Maroc},
  keywords = {nosource}
}

@inproceedings{blancoIssuesDetectingNegation2011,
  title = {Some {{Issues}} on {{Detecting Negation}} from {{Text}}},
  shorttitle = {Some Issues on Detecting Negation from Text},
  booktitle = {Proceedings of the 24th {{International Florida Artificial Intelligence Research Society}}, {{FLAIRS}} - 24},
  author = {Blanco, Eduardo and Moldovan, Dan},
  year = {2011},
  series = {Proceedings of the 24th {{International Florida Artificial Intelligence Research Society}}, {{FLAIRS}} - 24},
  pages = {228--233},
  issn = {9781577355014},
  urldate = {2024-03-22},
  abstract = {Negation is present in all human languages and it is used to reverse the polarity of parts of a statement. It is a complex phenomenon that interacts with many other aspects of language. Besides the direct meaning, negated statements often carry a latent positive meaning. Negation can be interpreted in terms of its scope and focus. This paper explores the importance of both scope and focus to capture the meaning of negated statements. Some issues on detecting negation from text are outlined, the forms in which negation occurs are depicted and heuristics to detect its scope and focus are proposed.},
  isbn = {978-1-57735-501-4},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2011\Blanco et Moldovan - 2011 - Some Issues on Detecting Negation from Text.pdf}
}

@inproceedings{boiretPrivacyOperatorsSemantic2022,
  title = {Privacy {{Operators}} for~{{Semantic Graph Databases}} as~{{Graph Rewriting}}},
  booktitle = {New {{Trends}} in {{Database}} and {{Information Systems}}},
  author = {Boiret, Adrien and Eichler, C{\'e}dric and Nguyen, Benjamin},
  editor = {Chiusano, Silvia and Cerquitelli, Tania and Wrembel, Robert and N{\o}rv{\aa}g, Kjetil and Catania, Barbara and {Vargas-Solar}, Genoveva and Zumpano, Ester},
  year = {2022},
  series = {Communications in {{Computer}} and {{Information Science}}},
  pages = {366--377},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-15743-1_34},
  abstract = {Database sanitization allows to share and publish open (linked) data without jeopardizing privacy. During their sanitization, graph databases are transformed following graph transformations that are usually described informally or through ad-hoc processes.},
  isbn = {978-3-031-15743-1},
  langid = {english}
}

@book{bonifatiQueryingGraphs2018,
  title = {Querying {{Graphs}}},
  author = {Bonifati, Angela and Fletcher, George and Voigt, Hannes and Yakovets, Nikolay and Jagadish, H. V.},
  year = {2018},
  series = {Synthesis {{Lectures}} on {{Data Management}}},
  volume = {10},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-01864-0},
  abstract = {Graph data modeling and querying arises in many practical application domains such as social and biological networks where the primary focus is on concepts and their relationships and the rich patterns in these complex webs of interconnectivity. In this book, we present a concise unified view on the basic challenges which arise over the complete life cycle of formulating and processing queries on graph databases. To that purpose, we present all major concepts relevant to this life cycle, formulated in terms of a common and unifying ground: the property graph data model---the pre-dominant data model adopted by modern graph database systems. We aim especially to give a coherent and in-depth perspective on current graph querying and an outlook for future developments. Our presentation is self-contained, covering the relevant topics from: graph data models, graph query languages and graph query specification, graph constraints, and graph query processing. We conclude by indicating major open research challenges towards the next generation of graph data management systems.},
  isbn = {978-3-031-00736-1 978-3-031-01864-0},
  langid = {english},
  keywords = {Computers / Information Technology,Computers / Information Theory,Computers / Internet / General,Computers / Networking / General,Computers / Online Services,Computers / Programming / Algorithms,nosource},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2018\\Bonifati et al. - 2018 - Querying Graphs.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\9CCKMBG8\\978-3-031-01864-0.html}
}

@inproceedings{bouchouUpdatesIncrementalValidation2003,
  ids = {bouchouUpdatesIncrementalValidation2004},
  title = {Updates and {{Incremental Validation}} of {{XML Documents}}},
  booktitle = {The 9th International Workshop on Data Base Programming Languages ({{DBPL}})},
  author = {Bouchou, B{\'e}atrice and {Halfeld-Ferrari}, Mirian},
  editor = {Lausen, Georg and Suciu, Dan},
  year = {2003},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {216--232},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-24607-7_14},
  abstract = {We consider the incremental validation of updates on XML documents. When a valid XML document (i.e., one satisfying some constraints) is updated, it has to be verified that the new document still conforms to the imposed constraints. Incremental validation of updates leads to significant savings on computing time when compared to brute-force validation of an updated document from scratch.},
  isbn = {978-3-540-20896-9 978-3-540-24607-7},
  langid = {english},
  keywords = {Incremental Validation,Label Tree,nosource,Transition Rule,Tree Automaton,Type Check},
  file = {C:\Users\nhiot\OneDrive\zotero\2003\Bouchou et Alves - 2003 - Updates and Incremental Validation of XML Document2.pdf}
}

@article{bousquetPERTOMedProjectExploiting2010,
  ids = {bousquetPERTOMedProjectExploiting2010a},
  title = {The {{PERTOMed Project}}: {{Exploiting}} and Validating Terminological Resources of Comparable {{Russian-French-English}} Corpora within Pharmacovigilance.},
  shorttitle = {The {{PERTOMed Project}}},
  author = {Bousquet, Cedric and {Zimina-Poirot}, Maria},
  year = {2010},
  month = feb,
  journal = {Terminology in Everyday Life},
  volume = {13},
  pages = {213--232},
  publisher = {John Benjamins Publishing},
  doi = {10.1075/tlrp.13.15bou},
  urldate = {2023-09-25},
  abstract = {The PERTOMed project is a pluri-disciplinary research initiative undertaken by several institutions in France. Applications considered within the part of the project described in this article concern pharmacovigilance and adverse drug reactions. We had multiple objectives: to create a specialized Russian Internet corpus; to test new tools and methods for term extraction from comparable multilingual texts and to build terminological resources including Russian. A trilingual Russian-French-English lexicon resulting form this work is freely available from the PERTOMed server.},
  langid = {english},
  file = {C:\Users\nhiot\OneDrive\zotero\2010\Bousquet et Zimina-Poirot - 2010 - The PERTOMed Project Exploiting and validating te.pdf}
}

@inproceedings{bravoSemanticallyCorrectQuery2006,
  title = {Semantically {{Correct Query Answers}} in the {{Presence}} of {{Null Values}}},
  booktitle = {Current {{Trends}} in {{Database Technology}} -- {{EDBT}} 2006},
  author = {Bravo, Loreto and Bertossi, Leopoldo},
  editor = {Grust, Torsten and H{\"o}pfner, Hagen and Illarramendi, Arantza and Jablonski, Stefan and Mesiti, Marco and M{\"u}ller, Sascha and Patranjan, Paula-Lavinia and Sattler, Kai-Uwe and Spiliopoulou, Myra and Wijsen, Jef},
  year = {2006},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {336--357},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/11896548_27},
  abstract = {For several reasons a database may not satisfy a given set of integrity constraints (ICs), but most likely most of the information in it is still consistent with those ICs; and could be retrieved when queries are answered. Consistent answers to queries wrt a set of ICs have been characterized as answers that can be obtained from every possible minimally repaired consistent version of the original database. In this paper we consider databases that contain null values and are also repaired, if necessary, using null values. For this purpose, we propose first a precise semantics for IC satisfaction in a database with null values that is compatible with the way null values are treated in commercial database management systems. Next, a precise notion of repair is introduced that privileges the introduction of null values when repairing foreign key constraints, in such a way that these new values do not create an infinite cycle of new inconsistencies. Finally, we analyze how to specify this kind of repairs of a database that contains null values using disjunctive logic programs with stable model semantics.},
  isbn = {978-3-540-46790-8},
  langid = {english},
  keywords = {Database Instance,Disjunctive Logic Program,Integrity Constraint,Query Answer,Stable Model Semantic},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2006\\Bravo et Bertossi - 2006 - Semantically Correct Query Answers in the Presence2.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\CMVWCZF5\\semantically-correct-query-answers-in-z9VL6b.html}
}

@article{brinAnatomyLargescaleHypertextual1998,
  title = {The Anatomy of a Large-Scale Hypertextual {{Web}} Search Engine},
  author = {Brin, Sergey and Page, Lawrence},
  year = {1998},
  month = apr,
  journal = {Computer Networks and ISDN Systems},
  series = {Proceedings of the {{Seventh International World Wide Web Conference}}},
  volume = {30},
  number = {1-7},
  pages = {107--117},
  publisher = {Elsevier},
  issn = {0169-7552},
  doi = {10.1016/s0169-7552(98)00110-x},
  urldate = {2023-09-22},
  abstract = {In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/ To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of Web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the Web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and Web proliferation, creating a Web search engine today is very different from three years ago. This paper provides an in-depth description of our large-scale Web search engine --- the first such detailed public description we know of to date. Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want.},
  keywords = {Google,Information retrieval,PageRank,Search engines,World Wide Web},
  annotation = {QID: Q55970547},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1998\\Brin et Page - 1998 - The anatomy of a large-scale hypertextual Web sear.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\5FZIELHS\\S016975529800110X.html}
}

@inproceedings{briscoeRobustAccurateStatistical2002,
  title = {Robust Accurate Statistical Annotation of General Text},
  booktitle = {Proceedings of the Third International Conference on Language Resources and Evaluation ({{LREC}}'02)},
  author = {Briscoe, Ted and Carroll, John},
  editor = {Gonz{\'a}lez Rodr{\'i}guez, Manuel and Suarez Araujo, Carmen Paz},
  year = {2002},
  month = may,
  publisher = {European Language Resources Association (ELRA)},
  address = {Las Palmas, Canary Islands - Spain},
  url = {http://www.lrec-conf.org/proceedings/lrec2002/pdf/250.pdf},
  keywords = {⛔ No DOI found,nosource}
}

@article{browarnikOntologyLearningText2015,
  title = {Ontology {{Learning}} from {{Text}}: {{Why}} the {{Ontology Learning Layer Cake}} Is Not {{Viable}}},
  shorttitle = {Ontology {{Learning}} from {{Text}}},
  author = {Browarnik, Abel and Maimon, Oded},
  year = {2015},
  journal = {International Journal of Signs and Semiotic Systems (IJSSS)},
  volume = {4},
  number = {2},
  pages = {1--14},
  publisher = {IGI Global},
  issn = {2155-5028},
  doi = {10.4018/IJSSS.2015070101},
  urldate = {2024-04-06},
  abstract = {The goal of Ontology Learning from Text is to learn ontologies that represent domains or applications that change often. Manually learning and updating such ontologies is too expensive. This is the reason for the Ontology Learning discipline's emergence. The leading approach to Ontology Learning fro...},
  copyright = {Access limited to members},
  langid = {english}
}

@inproceedings{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M.F. and Lin, H.},
  year = {2020},
  month = jul,
  volume = {33},
  eprint = {2005.14165},
  primaryclass = {cs},
  pages = {1877--1901},
  publisher = {Curran Associates, Inc.},
  url = {http://arxiv.org/abs/2005.14165},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2020\\Brown et al. - 2020 - Language Models are Few-Shot Learners2.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\Y9KQQ8SU\\2005.html}
}

@article{brownMedicalDictionaryRegulatory1999,
  title = {The {{Medical Dictionary}} for {{Regulatory Activities}} ({{MedDRA}})},
  author = {Brown, Elliot G. and Wood, Louise and Wood, Sue},
  year = {1999},
  month = feb,
  journal = {Drug Safety},
  volume = {20},
  number = {2},
  pages = {109--117},
  publisher = {Springer},
  issn = {1179-1942},
  doi = {10.2165/00002018-199920020-00002},
  abstract = {The International Conference on Harmonisation has agreed upon the structure and content of the Medical Dictionary for Regulatory Activities (MedDRA) version 2.0 which should become available in the early part of 1999.},
  langid = {english},
  file = {C:\Users\nhiot\Zotero\storage\FJ536H56\00002018-199920020-00002.html}
}

@inproceedings{budanitskyLexicalSemanticRelatedness1999,
  title = {Lexical {{Semantic Relatedness}} and {{Its Application}} in {{Natural Language Processing}}},
  author = {Budanitsky, Alexander},
  year = {1999},
  url = {https://www.semanticscholar.org/paper/Lexical-Semantic-Relatedness-and-Its-Application-in-Budanitsky/c2d95e890ee904f70701fa27326d31980424d5dd},
  urldate = {2024-04-07},
  abstract = {Lexical Semantic Relatedness and Its Application in Natural Language Processing Alexander Budanitsky Department of Computer Science University of Toronto August 1999 A great variety of Natural Language Processing tasks, from word sense disambiguation to text summarization to speech recognition, rely heavily on the ability to measure semantic relatedness or distance between words of a natural language. This report is a comprehensive study of recent computational methods of measuring lexical semantic relatedness. A survey of methods, as well as their applications, is presented, and the question of evaluation is addressed both theoretically and experimentally. Application to the speci c task of intelligent spelling checking is discussed in detail: the design of a prototype system for the detection and correction of malapropisms (words that are similar in spelling or sound to, but quite di erent in meaning from, intended words) is described, and results of experiments on using various measures as plug-ins are considered. Suggestions for research directions in the areas of measuring semantic relatedness and intelligent spelling checking are o ered.},
  keywords = {⛔ No DOI found}
}

@inproceedings{camposBiomedicalNamedEntity2012,
  title = {Biomedical {{Named Entity Recognition}}: {{A Survey}} of {{Machine-Learning Tools}}},
  shorttitle = {Biomedical {{Named Entity Recognition}}},
  booktitle = {Theory and Applications for Advanced Text Mining},
  author = {Campos, David and Matos, S{\'e}rgio and Oliveira, Jos{\'e} Lu{\'i}s},
  year = {2012},
  volume = {11},
  pages = {175--195},
  publisher = {IntechOpen},
  address = {Croatia},
  doi = {10.5772/51066},
  urldate = {2024-03-20},
  keywords = {nosource},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2012\\Campos et al. - 2012 - Biomedical Named Entity Recognition A Survey of M2.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\57VLBTUM\\38735.html}
}

@inproceedings{canditoCorpusSequoiaAnnotation2012,
  ids = {canditoCorpusSequoiaAnnotation2012a},
  title = {{Le corpus Sequoia : annotation syntaxique et exploitation pour l'adaptation d'analyseur par pont lexical}},
  shorttitle = {{Le corpus Sequoia}},
  booktitle = {{TALN 2012-19e conf{\'e}rence sur le Traitement Automatique des Langues Naturelles}},
  author = {Candito, Marie and Seddah, Djam{\'e}},
  year = {2012},
  url = {https://inria.hal.science/hal-00698938},
  urldate = {2024-03-21},
  abstract = {Nous pr{\'e}sentons dans cet article la m{\'e}thodologie de constitution et les caract{\'e}ristiques du corpus Sequoia, un corpus en fran{\c c}ais, syntaxiquement annot{\'e} d'apr{\`e}s un sch{\'e}ma d'annotation tr{\`e}s proche de celui du French Treebank (Abeill{\'e} et Barrier, 2004), et librement disponible, en constituants et en d{\'e}pendances. Le corpus comporte des phrases de quatre origines : Europarl fran{\c c}ais, le journal l'Est R{\'e}publicain, Wikip{\'e}dia Fr et des documents de l'Agence Europ{\'e}enne du M{\'e}dicament, pour un total de 3204 phrases et 69246 tokens. En outre, nous pr{\'e}sentons une application de ce corpus : l'{\'e}valuation d'une technique d'adaptation d'analyseurs syntaxiques probabilistes {\`a} des domaines et/ou genres autres que ceux du corpus sur lequel ces analyseurs sont entra{\^i}n{\'e}s. Cette technique utilise des clusters de mots obtenus d'abord par regroupement morphologique {\`a} l'aide d'un lexique, puis par regroupement non supervis{\'e}, et permet une nette am{\'e}lioration de l'analyse des domaines cibles (le corpus Sequoia), tout en pr{\'e}servant le m{\^e}me niveau de performance sur le domaine source (le FTB), ce qui fournit un analyseur multi-domaines, {\`a} la diff{\'e}rence d'autres techniques d'adaptation comme le self-training.},
  langid = {french},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2012\Candito et Seddah - 2012 - Le corpus Sequoia  annotation syntaxique et explo.pdf}
}

@inproceedings{cardonPresentationCampagneEvaluation2020,
  ids = {DEFT2020},
  title = {Pr{\'e}sentation de La Campagne d'{\'e}valuation {{DEFT}} 2020 : Similarit{\'e} Textuelle En Domaine Ouvert et Extraction d'information Pr{\'e}cise Dans Des Cas Cliniques},
  booktitle = {6e Conf{\'e}rence Conjointe Journ{\'e}es d'{{{\'E}tudes}} Sur La Parole ({{JEP}}, 33e {\'E}dition), Traitement Automatique Des Langues Naturelles ({{TALN}}, 27e {\'E}dition), Rencontre Des {\'E}tudiants Chercheurs En Informatique Pour Le Traitement Automatique Des Langues ({{R{\'E}CITAL}}, 22e {\'E}dition). {{Atelier D{\'E}fi}} Fouille de Textes},
  author = {Cardon, R{\'e}mi and Grabar, Natalia and Grouin, Cyril and Hamon, Thierry},
  editor = {Cardon, R{\'e}mi and Grabar, Natalia and Grouin, Cyril and Hamon, Thierry},
  year = {2020},
  pages = {1--13},
  publisher = {ATALA},
  address = {Nancy, France},
  url = {https://hal.archives-ouvertes.fr/hal-02784737},
  keywords = {⛔ No DOI found,Cas cliniques,extraction d'information,nosource,similarit{\'e} textuelle.},
  file = {C:\Users\nhiot\OneDrive\zotero\2020\Cardon et al. - 2020 - Présentation de la campagne d'évaluation DEFT 2020.pdf}
}

@article{carlssonCharacterizationStabilityConvergence2010,
  title = {Characterization, {{Stability}} and {{Convergence}} of {{Hierarchical Clustering Methods}}},
  author = {Carlsson, Gunnar and M{\'e}moli, Facundo},
  editor = {{Microtome Publishing}},
  year = {2010},
  month = aug,
  journal = {Journal of Machine Learning Research},
  volume = {11},
  number = {Apr},
  pages = {1425--1470},
  issn = {1533-7928},
  url = {https://www.jmlr.org/papers/volume11/carlsson10a/carlsson10a.pdf},
  abstract = {We study hierarchical clustering schemes under an axiomatic view. We show that within this framework, one can prove a theorem analogous to one of Kleinberg (2002), in which one obtains an existence and uniqueness theorem instead of a non-existence result. We explore further properties of this unique scheme: stability and convergence are established. We represent dendrograms as ultrametric spaces and use tools from metric geometry, namely the Gromov-Hausdorff distance, to quantify the degree to which perturbations in the input metric space affect the result of hierarchical methods.},
  keywords = {{$\warning$}️ Invalid DOI,⛔ No DOI found,Cluster Analysis,Statistics},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2010\\Carlsson et Mémoli - 2010 - Characterization, Stability and Convergence of Hie3.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\MESX564R\\2010-16571-001.html}
}

@article{chabinConsistentUpdatingDatabases2020,
  ids = {CHL19,CHL20},
  title = {Consistent {{Updating}} of {{Databases}} with {{Marked Nulls}}},
  author = {Chabin, Jacques and {Halfeld-Ferrari}, Mirian and Laurent, Dominique},
  year = {2020},
  month = apr,
  journal = {Knowledge and Information Systems (KAIS)},
  volume = {62},
  number = {4},
  pages = {1571--1609},
  issn = {0219-3116},
  doi = {10.1007/s10115-019-01402-w},
  urldate = {2023-07-05},
  abstract = {This paper revisits the problem of consistency maintenance when insertions or deletions are performed on a valid database containing marked nulls. This problem comes back to light in real-world linked data or RDF databases when blank nodes are associated with null values. This paper proposes solutions for the main problems one has to face when dealing with updates and constraints, namely update determinism, minimal change and leanness of an RDF graph instance. The update semantics is formally introduced and the notion of core is used to ensure a database as small as possible (i.e.~~ the RDF graph leanness). Our algorithms allow the use of constraints such as tuple-generating dependencies, offering a way for solving many practical problems.},
  langid = {english},
  optbibsource = {dblp computer science bibliography, https://dblp.org},
  optdoi = {10.1007/s10115-019-01402-w},
  opttimestamp = {Mon, 04 May 2020 13:22:49 +0200},
  keywords = {Constraints,Logical database,nosource,Null values,RDF,TGD,Updates},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2020\\Chabin et al. - 2020 - Consistent Updating of Databases with Marked Nulls.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\4YE6ECDH\\s10115-019-01402-w.html}
}

@inproceedings{chabinContextdrivenQueryingSystem2018,
  title = {A {{Context-driven Querying System}} for {{Urban Graph Analysis}}},
  booktitle = {Proceedings of the 22nd {{International Database Engineering}} \& {{Applications Symposium}} on - {{IDEAS}} 2018},
  author = {Chabin, Jacques and {Gomes-Jr.}, Luiz and {Halfeld-Ferrari}, Mirian},
  year = {2018},
  series = {{{IDEAS}} 2018},
  pages = {297--301},
  publisher = {ACM Press},
  address = {Villa San Giovanni, Italy},
  doi = {10.1145/3216122.3216148},
  urldate = {2018-12-29},
  abstract = {This paper presents a context-driven query system for urban computing where users are responsible for defining their own restrictions over which datalog-like queries are built. Instead of imposing constraints on databases, our goal is to filter consistent data during the query process. Our query language is able to express aggregates in recursive rules, allowing it to capture network properties typical of graph analysis. This paper presents our query system and analyzes its capabilities using use cases in Urban Computing.},
  isbn = {978-1-4503-6527-7},
  langid = {english},
  keywords = {constraints,data graph,data quality,Query language,smart city},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2018\\Chabin et al. - 2018 - A Context-driven Querying System for Urban Graph A.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\MGXJHLF7\\citation.html;C\:\\Users\\nhiot\\Zotero\\storage\\QEF3INHL\\hal-01837921.html}
}

@inproceedings{chabinGraphRewritingRules2020,
  title = {Graph {{Rewriting Rules}} for {{RDF Database Evolution Management}}},
  booktitle = {Proceedings of the 22nd {{International Conference}} on {{Information Integration}} and {{Web-based Applications}} \& {{Services}}},
  author = {Chabin, Jacques and Eichler, C{\'e}dric and {Halfeld-Ferrari}, Mirian and Hiot, Nicolas},
  year = {2020},
  month = nov,
  series = {{{iiWAS}} '20},
  pages = {134--143},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3428757.3429126},
  urldate = {2022-05-13},
  abstract = {This paper introduces SetUp, a theoretical and applied framework for the management of RDF/S database evolution on the basis of graph rewriting rules. Rewriting rules formalize instance or schema changes, ensuring graph's consistency with respect to given constraints. Constraints considered in this paper are a well known variant of RDF/S semantic, but the approach can be adapted to user-defined constraints. Furthermore, SetUp manages updates by ensuring rule applicability through the generation of side-effects: new updates which guarantee that rule application conditions hold. We provide herein formal validation and experimental evaluation of SetUp.},
  copyright = {All rights reserved},
  isbn = {978-1-4503-8922-8},
  langid = {english},
  keywords = {Constraints,Database Management,Graph rewriting,me,Update},
  file = {C:\Users\nhiot\OneDrive\zotero\2020\Chabin et al. - 2020 - Graph Rewriting Rules for RDF Database Evolution M.pdf}
}

@article{chabinGraphRewritingRules2021,
  title = {Graph Rewriting Rules for {{RDF}} Database Evolution: Optimizing Side-Effect Processing},
  shorttitle = {Graph Rewriting Rules for {{RDF}} Database Evolution},
  author = {Chabin, Jacques and Eichler, C{\'e}dric and {Halfeld-Ferrari}, Mirian and Hiot, Nicolas},
  year = {2021},
  month = aug,
  journal = {International Journal of Web Information Systems},
  volume = {17},
  number = {6},
  pages = {622--644},
  publisher = {Emerald Publishing Limited},
  doi = {10.1108/IJWIS-03-2021-0033},
  copyright = {All rights reserved},
  keywords = {me},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2021\\Chabin et al. - 2021 - Graph rewriting rules for RDF database evolution .pdf;C\:\\Users\\nhiot\\Zotero\\storage\\GNKTXUMA\\html.html;C\:\\Users\\nhiot\\Zotero\\storage\\UP65BUJK\\hal-03329965v1.html}
}

@techreport{chabinGraphRewritingSystem2020,
  type = {Research {{Report}}},
  title = {Graph {{Rewriting System}} for {{Consistent Evolution}} of {{RDF}}/{{S}} Databases},
  author = {Chabin, Jacques and Eichler, C{\'e}dric and {Halfeld-Ferrari}, Mirian and Hiot, Nicolas},
  year = {2020},
  institution = {LIFO, Universit{\'e} d'Orl{\'e}ans, INSA Centre Val de Loire},
  url = {https://hal.science/hal-02560325},
  urldate = {2023-08-03},
  abstract = {This paper investigates the use of graph rewriting rules to model updates-instance or schema changes-on RDF/S databases which are expected to satisfy RDF intrinsic semantic constraints. Such databases being modeled as knowledge graphs, we propose graph rewriting rules formalizing atomic updates whose application transforms the graph and necessarily preserves its consistency. If an update has to be applied when the application conditions of the corresponding rule do not hold, side-effects are generated: they engender new updates in order to ensure the rule applicability. Our system, SetUp, implements our updating approach for RDF/S data and offers a theoretical and applied framework for ensuring consistency when a RDF knowledge graph evolves.},
  copyright = {All rights reserved},
  langid = {english},
  keywords = {me},
  file = {C:\Users\nhiot\OneDrive\zotero\2020\Chabin et al. - 2020 - Graph Rewriting System for Consistent Evolution of.pdf}
}

@misc{chabinIncrementalConsistentUpdating2023,
  title = {Incremental {{Consistent Updating}} of {{Incomplete Databases}}},
  author = {Chabin, Jacques and {Halfeld-Ferrari}, Mirian and Hiot, Nicolas and Laurent, Dominique},
  year = {2023},
  month = feb,
  number = {arXiv:2302.06246},
  eprint = {2302.06246},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2302.06246},
  urldate = {2023-08-07},
  abstract = {Efficient consistency maintenance of incomplete and dynamic real-life databases is a quality label for further data analysis. In prior work, we tackled the generic problem of database updating in the presence of tuple generating constraints from a theoretical viewpoint. The current paper considers the usability of our approach by (a) introducing incremental update routines (instead of the previous from-scratch versions) and (b) removing the restriction that limits the contents of the database to fit in the main memory. In doing so, this paper offers new algorithms, proposes queries and data models inviting discussions on the representation of incompleteness on databases. We also propose implementations under a graph database model and the traditional relational database model. Our experiments show that computation times are similar globally but point to discrepancies in some steps.},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Databases,me,nosource},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2023\\Chabin et al. - 2023 - Incremental Consistent Updating of Incomplete Data.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\WNDR3UKH\\2302.html}
}

@techreport{chabinIncrementalConsistentUpdating2023a,
  title = {Incremental Consistent Updating of Incomplete Databases (Extended Version - Technical Report)},
  author = {Chabin, Jacques and {Halfeld-Ferrari}, Mirian and Hiot, Nicolas and Laurent, Dominique},
  year = {2023},
  institution = {LIFO- Universit{\'e} d'Orl{\'e}ans,},
  url = {https://hal.science/hal-03982841},
  copyright = {All rights reserved},
  keywords = {nosource},
  file = {C:\Users\nhiot\OneDrive\zotero\2023\Chabin et al. - 2023 - Incremental consistent updating of incomplete data2.pdf}
}

@inproceedings{chabinManagingLinkedNulls2023,
  title = {Managing {{Linked Nulls}} in~{{Property Graphs}}: {{Tools}} to~{{Ensure Consistency}} and~{{Reduce Redundancy}}},
  shorttitle = {Managing {{Linked Nulls}} in {{Property Graphs}}},
  booktitle = {Advances in {{Databases}} and {{Information Systems}}},
  author = {Chabin, Jacques and {Halfeld-Ferrari}, Mirian and Hiot, Nicolas and Laurent, Dominique},
  editor = {Abell{\'o}, Alberto and Vassiliadis, Panos and Romero, Oscar and Wrembel, Robert},
  year = {2023},
  month = sep,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {13985},
  pages = {180--194},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-42914-9_13},
  abstract = {Ensuring the provision of consistent and irredundant data sets remains essential to minimize bugs, promote maintainable application code and obtain dependable results in data analytics. A major challenge in achieving consistency is handling incomplete data, i.e., missing information that may be provided later, comes from the fact that, the use of marked (or linked) nulls is required in many applications to express unknown but connected information. In this context, it is well known that maintaining the data consistent and irredundant is not an easy task. This paper proposes a query-driven incremental maintenance approach for consistent and irredundant incomplete databases. Can graph databases improve the efficiency of this operation? How can graph databases manipulate linked nulls? What is the impact of using graph databases on other essential maintenance operations? This paper presents an innovative approach to answering these questions, highlighting the proposal's strengths and weaknesses and offering avenues for further research.},
  copyright = {All rights reserved},
  isbn = {978-3-031-42913-2 978-3-031-42914-9},
  langid = {english},
  keywords = {constraints,graph databases,incomplete data,incremental maintenance,tuple generating dependencies,updates}
}

@techreport{chabinSpecificationSideeffectManagement2020,
  type = {Research {{Report}}},
  title = {Specification of Side-Effect Management Techniques for Semantic Graph Sanitization},
  author = {Chabin, Jacques and Eichler, C{\'e}dric and {Halfeld-Ferrari}, Mirian and Hiot, Nicolas},
  year = {2020},
  number = {D6},
  institution = {LIFO, Universit{\'e} d'Orl{\'e}ans, INSA Centre Val de Loire},
  url = {https://hal.science/hal-02957974},
  abstract = {The goal of the SENDUP project is to propose anonymisation mechanisms for data organized as graphs with an underlying semantic. Such mechanisms trig- gers updates on the database. This deliverable presents the update approach and side-effect management techniques defined in SENDUP. We focus on updates -instance or schema changes- on RDF/S databases which are expected to satisfy RDF intrinsic semantic constraints. We model RDF/S databases as type graphs and use graph rewriting rules to formalize updates. Such rules define both the effect of a graph transformation and its applicability conditions. We propose 19 rules modelling atomic updates and prove that their application necessarily preserves the database's consistency. If an update has to be applied when the application conditions of the corre- sponding rule do not hold, side-effects are generated: they engender new updates in order to ensure the rule applicability. These techniques are implemented in a dedicated software module S1 called SetUp. This deliverable also presents a preliminary experimental validation and evaluation of SetUp.},
  copyright = {All rights reserved},
  langid = {english},
  keywords = {me,nosource},
  file = {C:\Users\nhiot\OneDrive\zotero\2020\Chabin et al. - 2020 - Specification of side-effect management techniques.pdf}
}

@misc{chabinUpdateChase2023,
  title = {{{UpdateChase}}},
  author = {Chabin, Jacques and {Halfeld-Ferrari}, Mirian and Hiot, Dominique and Laurent, Dominique and {Moret-Bailly}, Lucas},
  year = {2023},
  month = jan,
  url = {https://gitlab.com/jacques-chabin/UpdateChase},
  urldate = {2023-07-21},
  abstract = {Impl{\'e}mentation et benchmarks des algorithmes incr{\'e}mentaux pour la mise {\`a} jour coh{\'e}rente d'une base de donn{\'e}es graphe},
  keywords = {me},
  file = {C:\Users\nhiot\Zotero\storage\WPJMI7RA\UpdateChase.html}
}

@unpublished{chabinUsingGraphGrammar2019,
  title = {Using a Graph Grammar to Update a {{RDF}}/{{S}} Document},
  author = {Chabin, Jacques and Eichler, C{\'e}dric and {Halfeld-Ferrari}, Mirian},
  year = {2019},
  langid = {english},
  keywords = {nosource}
}

@inproceedings{chandraOptimalImplementationConjunctive1977,
  ids = {CM77},
  title = {Optimal Implementation of Conjunctive Queries in Relational Data Bases},
  booktitle = {Proceedings of the Ninth Annual {{ACM}} Symposium on {{Theory}} of Computing},
  author = {Chandra, Ashok K. and Merlin, Philip M.},
  year = {1977},
  month = may,
  series = {{{STOC}} '77},
  pages = {77--90},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/800105.803397},
  abstract = {We define the class of conjunctive queries in relational data bases, and the generalized join operator on relations. The generalized join plays an important part in answering conjunctive queries, and it can be implemented using matrix multiplication. It is shown that while answering conjunctive queries is NP complete (general queries are PSPACE complete), one can find an implementation that is within a constant of optimal. The main lemma used to show this is that each conjunctive query has a unique minimal equivalent query (much like minimal finite automata).},
  isbn = {978-1-4503-7409-5},
  annotation = {QID: Q56813555},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1977\\Chandra et Merlin - 1977 - Optimal implementation of conjunctive queries in r.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\6D3X624J\\800105.html}
}

@misc{ChatetteDatasetGenerator2024,
  ids = {simgusSimGusChatette2024},
  title = {Chatette: {{A}} Dataset Generator for {{Rasa NLU}}},
  shorttitle = {Chatette},
  year = {2024},
  month = mar,
  url = {https://github.com/SimGus/Chatette},
  urldate = {2024-03-25},
  abstract = {A powerful dataset generator for Rasa NLU, inspired by Chatito},
  copyright = {MIT License},
  keywords = {botkit,chatbot,chatbots,chatito,cli,dataset-generation,nlg,nlp,nlu,parsing,python,rasa,rasa-nlu,sentence},
  file = {C:\Users\nhiot\Zotero\storage\DYKEDZD8\1.1.5.html}
}

@article{chenEntityrelationshipModelUnified1976,
  title = {The Entity-Relationship Model---toward a Unified View of Data},
  author = {Chen, Peter Pin-Shan},
  year = {1976},
  month = mar,
  journal = {ACM Transactions on Database Systems},
  volume = {1},
  number = {1},
  pages = {9--36},
  issn = {0362-5915, 1557-4644},
  doi = {10.1145/320434.320440},
  urldate = {2023-10-27},
  abstract = {A data model, called the entity-relationship model, is proposed. This model incorporates some of the important semantic information about the real world. A special diagrammatic technique is introduced as a tool for database design. An example of database design and description using the model and the diagrammatic technique is given. Some implications for data integrity, information retrieval, and data manipulation are discussed. The entity-relationship model can be used as a basis for unification of different views of data: the network model, the relational model, and the entity set model. Semantic ambiguities in these models are analyzed. Possible ways to derive their views of data from the entity-relationship model are presented.},
  langid = {english},
  keywords = {Data Base Task Group,data definition and manipulation,data integrity and consistency,data models,database design,entity set model,entity-relationship model,logigcal view of data,network model,relational model,semantics of data},
  annotation = {QID: Q54151498},
  file = {C:\Users\nhiot\OneDrive\zotero\1976\Chen - 1976 - The entity-relationship model—toward a unified vie.pdf}
}

@article{chomskyAlgebraicTheoryContextfree1963,
  title = {The Algebraic Theory of Context-Free Languages},
  author = {Chomsky, Noam and Sch{\"u}tzenberger, Marcel-Paul},
  year = {1963},
  journal = {Studies in Logic and the Foundations of Mathematics},
  volume = {35},
  pages = {118--161},
  doi = {10.1016/S0049-237X(08)72023-8},
  keywords = {nosource}
}

@article{christiansenSurveyAdaptableGrammars1990,
  title = {A Survey of Adaptable Grammars},
  author = {Christiansen, H.},
  year = {1990},
  month = nov,
  journal = {ACM SIGPLAN Notices},
  volume = {25},
  number = {11},
  pages = {35--44},
  issn = {0362-1340, 1558-1160},
  doi = {10.1145/101356.101357},
  urldate = {2023-09-28},
  langid = {english},
  file = {C:\Users\nhiot\OneDrive\zotero\1990\Christiansen - 1990 - A survey of adaptable grammars.pdf}
}

@inproceedings{cimianoText2Onto2005,
  title = {{{Text2Onto}}},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  author = {Cimiano, Philipp and V{\"o}lker, Johanna},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Montoyo, Andr{\'e}s and Mu{\'n}oz, Rafael and M{\'e}tais, Elisabeth},
  year = {2005},
  month = jun,
  volume = {3513},
  pages = {227--238},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/11428817_21},
  urldate = {2024-02-29},
  abstract = {In this paper we present Text2Onto, a framework for ontology learning from textual resources. Three main features distinguish Text2Onto from our earlier framework TextToOnto as well as other state-of-the-art ontology learning frameworks. First, by representing the learned knowledge at a meta-level in the form of instantiated modeling primitives within a so called Probabilistic Ontology Model (POM), we remain independent of a concrete target language while being able to translate the instantiated primitives into any (reasonably expressive) knowledge representation formalism. Second, user interaction is a core aspect of Text2Onto and the fact that the system calculates a confidence for each learned object allows to design sophisticated visualizations of the POM. Third, by incorporating strategies for data-driven change discovery, we avoid processing the whole corpus from scratch each time it changes, only selectively updating the POM according to the corpus changes instead. Besides increasing efficiency in this way, it also allows a user to trace the evolution of the ontology with respect to the changes in the underlying corpus.},
  isbn = {978-3-540-26031-8 978-3-540-32110-1},
  langid = {english},
  keywords = {Explanation Component,Knowledge Representation Language,Mereological Relation,Modeling Primitive,Ontology Learning},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2005\\Cimiano et Völker - 2005 - Text2Onto2.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\EDL9UE4E\\Cimiano et Völker - 2005 - Text2Onto.pdf}
}

@article{coddRelationalModelData1970,
  title = {A Relational Model of Data for Large Shared Data Banks},
  author = {Codd, Edgar F.},
  year = {1970},
  journal = {Communications of the ACM},
  volume = {13},
  number = {6},
  pages = {377--387},
  publisher = {ACM New York, NY, USA},
  doi = {10.1145/362384.362685},
  annotation = {QID: Q32061744},
  file = {C:\Users\nhiot\OneDrive\zotero\1970\Codd - 1970 - A relational model of data for large shared data b.pdf}
}

@article{ComputationGreatestRegular2016,
  title = {Computation of the {{Greatest Regular Equivalence}}},
  year = {2016},
  month = jan,
  journal = {Filomat},
  volume = {30},
  number = {1},
  pages = {179--190},
  publisher = {National Library of Serbia},
  issn = {0354-5180},
  doi = {10.2298/FIL1601179S},
  urldate = {2023-10-31},
  abstract = {The notion of social roles is a centerpiece of most sociological theoretical considerations. Regular equivalences were introduced by White and Reitz in [15] as the least restrictive among the most commonly used definitions of equivalence in social network analysis. In this paper we consider a generalisation of this notion to a bipartite case. We define a pair of regular equivalences on a two-mode social network and we provide an algorithm for computing the greatest pair of regular equivalences.},
  langid = {english},
  file = {C:\Users\nhiot\OneDrive\zotero\2016\2016 - Computation of the Greatest Regular Equivalence.pdf}
}

@inproceedings{consoleCopingIncompleteData2020,
  ids = {CGLT20},
  title = {Coping with {{Incomplete Data}}: {{Recent Advances}}},
  shorttitle = {Coping with {{Incomplete Data}}},
  booktitle = {Proceedings of the 39th {{ACM SIGMOD-SIGACT-SIGAI Symposium}} on {{Principles}} of {{Database Systems}}},
  author = {Console, Marco and Guagliardo, Paolo and Libkin, Leonid and Toussaint, Etienne},
  year = {2020},
  month = jun,
  series = {{{PODS}}'20},
  pages = {33--47},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3375395.3387970},
  urldate = {2023-08-03},
  abstract = {Handling incomplete data in a correct manner is a notoriously hard problem in databases. Theoretical approaches rely on the computationally hard notion of certain answers, while practical solutions rely on ad hoc query evaluation techniques based on three-valued logic. Can we find a middle ground, and produce correct answers efficiently? The paper surveys results of the last few years motivated by this question. We re-examine the notion of certainty itself, and show that it is much more varied than previously thought. We identify cases when certain answers can be computed efficiently and, short of that, provide deterministic and probabilistic approximation schemes for them. We look at the role of three-valued logic as used in SQL query evaluation, and discuss the correctness of the choice, as well as the necessity of such a logic for producing query answers.},
  isbn = {978-1-4503-7108-7},
  keywords = {approximate query answering,certain answers,incomplete information,many-valued logics,naive evaluation,nosource,relational databases},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2020\\Console et al. - 2020 - Coping with Incomplete Data Recent Advances.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\KJF3RWAC\\3375395.html}
}

@article{cowieInformationExtraction2000,
  title = {Information Extraction},
  author = {Cowie, Jim and Wilks, Yorick},
  year = {2000},
  journal = {Handbook of Natural Language Processing},
  volume = {56},
  pages = {57},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2000\\Cowie et Wilks - 2000 - Information extraction.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\43IETTRD\\books.html}
}

@incollection{dailleActesTALN20032003,
  title = {Actes de {{TALN}} 2003 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 2003 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Daille, B{\'e}atrice},
  year = {2003},
  month = jun,
  publisher = {IRIN / ATALA},
  address = {Batz-sur-mer},
  keywords = {nosource}
}

@article{darmoniMLPubMedBaseDonnees,
  title = {{{MLPubMed}}: Une Base de Donn{\'e}es Bibliographique Multi-Lingue},
  shorttitle = {{{MLPubMed}}},
  author = {Darmoni, St{\'e}fan J. and Soualmia, Lina F. and Griffon, Nicolas and Grosjean, Julien and Kerdelhu{\'e}, Ga{\'e}tan and Kergourlay, Ivan and Thirion, Benoit and Dahamna, Badisse},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\_\Darmoni et al. - MLPubMed une base de données bibliographique mult.pdf}
}

@inproceedings{degiacomoDealingInconsistenciesIncompleteness2009,
  title = {Dealing with Inconsistencies and Incompleteness in Database Update (Position Paper)},
  booktitle = {Inconsistency and {{Incompleteness}} in {{Databases}}},
  author = {De Giacomo, Giuseppe and Lenzerini, Maurizio and Poggi, Antonella and Rosati, Riccardo},
  year = {2009},
  month = aug,
  pages = {97},
  url = {https://www.academia.edu/61920314/Dealing\_with\_inconsistencies\_and\_incompleteness\_in\_database\_update\_position\_paper\_},
  urldate = {2024-01-08},
  abstract = {Several areas of research and various application domains have been concerned in the last years with the problem of dealing with incomplete databases. Data integration as well as the Semantic Web are notable examples. Surprisingly, while many research efforts have been focusing on several interesting issues related to incomplete databases, as query answering, not much investigation have been done concerning updates. In this position paper we aim at highlighting some of the issues we are dealing with in our work on updates over incomplete databases. Instance level updates under constraints Our interest in this area stems mainly from the need to deal with updates in Description Logics based ontologies. Description logics (DLs) are logics for expressing the conceptual knowledge about a domain in terms of classes and associations between them [1]. Such logics are currently considered among the most promising formalisms for representing ontologies by the Semantic Web community [2]. DL-based ontologies are often used for accessing data stored in a data layer by means of query answering.},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2009\De Giacomo et al. - 2009 - Dealing with inconsistencies and incompleteness in2.pdf}
}

@inproceedings{degiacomoPracticalUpdateManagement2017,
  title = {Practical {{Update Management}} in {{Ontology-Based Data Access}}},
  booktitle = {The {{Semantic Web}} -- {{ISWC}} 2017},
  author = {De Giacomo, Giuseppe and Lembo, Domenico and Oriol, Xavier and Savo, Domenico Fabio and Teniente, Ernest},
  editor = {{d'Amato}, Claudia and Fernandez, Miriam and Tamma, Valentina and Lecue, Freddy and {Cudr{\'e}-Mauroux}, Philippe and Sequeda, Juan and Lange, Christoph and Heflin, Jeff},
  year = {2017},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {225--242},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-68288-4_14},
  abstract = {Ontology-based Data Access (OBDA) is gaining importance both scientifically and practically. However, little attention has been paid so far to the problem of updating OBDA systems. This is an essential issue if we want to be able to cope with modifications of data both at the ontology and at the source level, while maintaining the independence of the data sources. In this paper, we propose mechanisms to properly handle updates in this context. We show that updating data both at the ontology and source level is first-order rewritable. We also provide a practical implementation of such updating mechanisms based on non-recursive Datalog.},
  isbn = {978-3-319-68288-4},
  langid = {english},
  file = {C:\Users\nhiot\OneDrive\zotero\2017\De Giacomo et al. - 2017 - Practical Update Management in Ontology-Based Data.pdf}
}

@inproceedings{devlinBERTPretrainingDeep2019,
  ids = {devlinBERTPretrainingDeep2019a},
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  booktitle = {Proceedings of {{NAACL-HLT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  eprint = {1810.04805},
  primaryclass = {cs},
  pages = {4171--4186},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1810.04805},
  urldate = {2024-03-20},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\nhiot\OneDrive\zotero\2019\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform2.pdf}
}

@incollection{diasActesTALN20152015,
  title = {Actes de {{TALN}} 2015 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 2015 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Dias, Ga{\"e}l},
  year = {2015},
  month = jun,
  publisher = {HULTECH / ATALA},
  address = {Caen},
  keywords = {nosource}
}

@inproceedings{duchierMetagrammarCompilerNLP2005,
  title = {The {{Metagrammar Compiler}}: {{An NLP Application}} with a {{Multi-paradigm Architecture}}},
  shorttitle = {The {{Metagrammar Compiler}}},
  booktitle = {Multiparadigm {{Programming}} in {{Mozart}}/{{Oz}}},
  author = {Duchier, Denys and Le Roux, Joseph and Parmentier, Yannick},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Van Roy, Peter},
  year = {2005},
  volume = {3389},
  pages = {175--187},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-31845-3_15},
  urldate = {2023-09-29},
  isbn = {978-3-540-25079-1 978-3-540-31845-3},
  file = {C:\Users\nhiot\OneDrive\zotero\2005\Duchier et al. - 2005 - The Metagrammar Compiler An NLP Application with .pdf}
}

@inproceedings{eichlerBackSEmanticNetworks2022,
  title = {Back on {{SEmantic Networks}} of {{Data}}: {{Utility}} and {{Privacy}}},
  shorttitle = {Back on {{SEmantic Networks}} of {{Data}}},
  booktitle = {Rendez-{{Vous}} de La {{Recherche}} et de l'{{Enseignement}} de La {{S{\'e}curit{\'e}}} Des {{Syst{\`e}mes}} d'{{Information RESSI}}},
  author = {Eichler, C{\'e}dric and Chabin, Jacques and Echahed, Rachid and Ferrari, Mirian H. and Hiot, Nicolas and Nguyen, Benjamin and Prost, Fr{\'e}d{\'e}ric},
  year = {2022},
  url = {https://hal.science/hal-03788897/document},
  urldate = {2024-05-01},
  copyright = {All rights reserved},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2022\Eichler et al. - 2022 - Back on SEmantic Networks of Data Utility and Pri.pdf}
}

@inproceedings{elhadadSemEval2015Task142015,
  title = {{{SemEval-2015}} Task 14: {{Analysis}} of Clinical Text},
  booktitle = {Proceedings of the 9th International Workshop on Semantic Evaluation, {{SemEval}}@{{NAACL-HLT}} 2015, Denver, Colorado, {{USA}}, June 4-5, 2015},
  author = {Elhadad, No{\'e}mie and Pradhan, Sameer and Gorman, Sharon Lipsky and Manandhar, Suresh and Chapman, Wendy W. and Savova, Guergana K.},
  editor = {Cer, Daniel M. and Jurgens, David and Nakov, Preslav and Zesch, Torsten},
  year = {2015},
  pages = {303--310},
  publisher = {The Association for Computer Linguistics},
  doi = {10.18653/v1/s15-2051},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/semeval/ElhadadPGMCS15.bib},
  keywords = {nosource},
  timestamp = {Tue, 28 Jan 2020 10:29:10 +0100},
  file = {C:\Users\nhiot\OneDrive\zotero\2015\Elhadad et al. - 2015 - SemEval-2015 task 14 Analysis of clinical text.pdf}
}

@article{everettRegularEquivalenceGeneral1994,
  title = {Regular Equivalence: {{General}} Theory},
  shorttitle = {Regular Equivalence},
  author = {Everett, Martin G. and Borgatti, Stephen P.},
  year = {1994},
  month = may,
  journal = {The Journal of Mathematical Sociology},
  volume = {19},
  number = {1},
  pages = {29--52},
  publisher = {Routledge},
  issn = {0022-250X, 1545-5874},
  doi = {10.1080/0022250x.1994.9990134},
  urldate = {2024-01-12},
  abstract = {The theory of regular equivalence has advanced over the last 15 years on a number of different fronts. Notation and terminology have developed often making it difficult to obtain a coherent view of the area as a whole. This paper attempts to provide a framework in which to develop and explore the general mathematical theory of regular equivalence and to place a number of the more important results into that framework.},
  langid = {english},
  file = {C:\Users\nhiot\OneDrive\zotero\1994\Everett et Borgatti - 1994 - Regular equivalence General theory.pdf}
}

@inproceedings{faderOpenQuestionAnswering2014,
  title = {Open Question Answering over Curated and Extracted Knowledge Bases},
  booktitle = {Proceedings of the 20th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  author = {Fader, Anthony and Zettlemoyer, Luke and Etzioni, Oren},
  year = {2014},
  month = aug,
  pages = {1156--1165},
  publisher = {ACM},
  address = {New York New York USA},
  doi = {10.1145/2623330.2623677},
  urldate = {2024-03-29},
  isbn = {978-1-4503-2956-9},
  langid = {english},
  optbibsource = {dblp computer science bibliography, https://dblp.org},
  optbiburl = {https://dblp.org/rec/conf/kdd/FaderZE14.bib},
  opttimestamp = {Tue, 06 Nov 2018 16:59:36 +0100},
  opturl = {https://doi.org/10.1145/2623330.2623677},
  keywords = {⛔ No DOI found,nosource}
}

@article{faginDataExchangeGetting2005,
  title = {Data Exchange: Getting to the Core},
  shorttitle = {Data Exchange},
  author = {Fagin, Ronald and Kolaitis, Phokion G. and Popa, Lucian},
  year = {2005},
  month = mar,
  journal = {ACM Transactions on Database Systems (TODS)},
  volume = {30},
  number = {1},
  pages = {174--210},
  issn = {0362-5915, 1557-4644},
  doi = {10.1145/1061318.1061323},
  urldate = {2023-08-16},
  abstract = {Data exchange is the problem of taking data structured under a source schema and creating an instance of a target schema that reflects the source data as accurately as possible. Given a source instance, there may be many solutions to the data exchange problem, that is, many target instances that satisfy the constraints of the data exchange problem. In an earlier article, we identified a special class of solutions that we call universal. A universal solution has homomorphisms into every possible solution, and hence is a ``most general possible'' solution. Nonetheless, given a source instance, there may be many universal solutions. This naturally raises the question of whether there is a ``best'' universal solution, and hence a best solution for data exchange. We answer this question by considering the well-known notion of the core of a structure, a notion that was first studied in graph theory, and has also played a role in conjunctive-query processing. The core of a structure is the smallest substructure that is also a homomorphic image of the structure. All universal solutions have the same core (up to isomorphism); we show that this core is also a universal solution, and hence the smallest universal solution. The uniqueness of the core of a universal solution together with its minimality make the core an ideal solution for data exchange. We investigate the computational complexity of producing the core. Well-known results by Chandra and Merlin imply that, unless P = NP, there is no polynomial-time algorithm that, given a structure as input, returns the core of that structure as output. In contrast, in the context of data exchange, we identify natural and fairly broad conditions under which there are polynomial-time algorithms for computing the core of a universal solution. We also analyze the computational complexity of the following decision problem that underlies the computation of cores: given two graphs G and H, is H the core of G? Earlier results imply that this problem is both NP-hard and coNP-hard. Here, we pinpoint its exact complexity by establishing that it is a DP-complete problem. Finally, we show that the core is the best among all universal solutions for answering existential queries, and we propose an alternative semantics for answering queries in data exchange settings.},
  langid = {english},
  keywords = {Certain answers,chase,computational complexity,conjunctive queries,core,data exchange,data integration,dependencies,nosource,query answering,universal solutions},
  annotation = {QID: Q106466848}
}

@inproceedings{faginSemanticsUpdatesDatabases1983,
  ids = {FUV83},
  title = {On the Semantics of Updates in Databases},
  booktitle = {Proceedings of the 2nd {{ACM SIGACT-SIGMOD Symposium}} on {{Principles}} of {{Database Systems}}},
  author = {Fagin, Ronald and Ullman, Jeffrey D. and Vardi, Moshe Y.},
  year = {1983},
  month = mar,
  series = {{{PODS}} '83},
  pages = {352--365},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/588058.588100},
  urldate = {2023-08-03},
  abstract = {We suggest here a methodology for updating databases with integrity constraints and rules for deriving inexphcit information. First we consider the problem of updating arbitrary theories by inserting into them or deleting from them arbitrary sentences. The solution involves two key ideas when replacing an old theory by a new one we wish to minimize the change in the theory, and when there are several theories that involve minimal changes, we look for a new theory that reflects that ambiguity. The methodology is also adapted to updating databases, where different facts can carry different priorities, and to updating user views.},
  isbn = {978-0-89791-097-2},
  optbibsource = {dblp computer science bibliography, http://dblp.org},
  optcrossref = {DBLP:conf/pods/83},
  optdoi = {10.1145/588058.588100},
  opttimestamp = {Wed, 29 Mar 2017 16:45:25 +0200},
  keywords = {nosource},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1983\\Fagin et al. - 1983 - On the semantics of updates in databases.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\7SGAML5K\\588058.html}
}

@article{faginUpdatingLogicalDatabases1986,
  title = {Updating {{Logical Databases}}},
  author = {Fagin, Ronald and Kuper, Gabriel M. and Ullman, Jeffrey D. and Vardi, Moshe Y.},
  year = {1986},
  journal = {Advances in Computing Research},
  volume = {3},
  pages = {1--18},
  doi = {10.21236/ada144937},
  keywords = {nosource},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1986\\Fagin et al. - 1986 - Updating Logical Databases.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\RS48PZZC\\ADA144937.html}
}

@article{fanDependenciesGraphs2019,
  title = {Dependencies for {{Graphs}}},
  author = {Fan, Wenfei and Lu, Ping},
  year = {2019},
  month = jun,
  journal = {ACM Transactions on Database Systems},
  volume = {44},
  number = {2},
  pages = {1--40},
  issn = {0362-5915, 1557-4644},
  doi = {10.1145/3287285},
  urldate = {2024-01-08},
  abstract = {This article proposes a class of dependencies for graphs, referred to as graph entity dependencies (GEDs). A GED is defined as a combination of a graph pattern and an attribute dependency. In a uniform format, GEDs can express graph functional dependencies with constant literals to catch inconsistencies, and keys carrying id literals to identify entities (vertices) in a graph. We revise the chase for GEDs and prove its Church-Rosser property. We characterize GED satisfiability and implication, and establish the complexity of these problems and the validation problem for GEDs, in the presence and absence of constant literals and id literals. We also develop a sound, complete and independent axiom system for finite implication of GEDs. In addition, we extend GEDs with built-in predicates or disjunctions, to strike a balance between the expressive power and complexity. We settle the complexity of the satisfiability, implication, and validation problems for these extensions.},
  langid = {english},
  keywords = {axiom system,built-in predicates,conditional functional dependencies,disjunction,EGDs,Graph dependencies,implication,keys,satisfiability,TGDs,validation},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2019\\Fan et Lu - 2019 - Dependencies for Graphs.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\8BMXGJZI\\3287285.html}
}

@inproceedings{fanFunctionalDependenciesGraphs2016,
  title = {Functional {{Dependencies}} for {{Graphs}}},
  booktitle = {Proceedings of the 2016 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Fan, Wenfei and Wu, Yinghui and Xu, Jingbo},
  year = {2016},
  month = jun,
  series = {{{SIGMOD}} '16},
  pages = {1843--1857},
  publisher = {ACM},
  address = {San Francisco California USA},
  doi = {10.1145/2882903.2915232},
  urldate = {2023-12-07},
  abstract = {We propose a class of functional dependencies for graphs, referred to as GFDs. GFDs capture both attribute-value dependencies and topological structures of entities, and subsume conditional functional dependencies (CFDs) as a special case. We show that the satisfiability and implication problems for GFDs are coNP-complete and NP-complete, respectively, no worse than their CFD counterparts. We also show that the validation problem for GFDs is coNP-complete. Despite the intractability, we develop parallel scalable algorithms for catching violations of GFDs in large-scale graphs. Using real-life and synthetic data, we experimentally verify that GFDs provide an effective approach to detecting inconsistencies in knowledge and social graphs.},
  isbn = {978-1-4503-3531-7},
  langid = {english},
  keywords = {functional dependencies,graphs,implication,satisfiability,validation},
  file = {C:\Users\nhiot\OneDrive\zotero\2016\Fan et al. - 2016 - Functional Dependencies for Graphs.pdf}
}

@inproceedings{fanIncrementalizingGraphAlgorithms2021,
  title = {Incrementalizing {{Graph Algorithms}}},
  booktitle = {Proceedings of the 2021 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Fan, Wenfei and Tian, Chao and Xu, Ruiqi and Yin, Qiang and Yu, Wenyuan and Zhou, Jingren},
  editor = {Li, Guoliang and Li, Zhanhuai and Idreos, Stratos and Srivastava, Divesh},
  year = {2021},
  month = jun,
  series = {{{SIGMOD}} '21},
  pages = {459--471},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3448016.3452796},
  abstract = {Incremental algorithms are important to dynamic graph analyses, but are hard to write and analyze. Few incremental graph algorithms are in place, and even fewer offer performance guarantees. This paper approaches this by proposing to incrementalize existing batch algorithms. We identify a class of incrementalizable algorithms abstracted in a fixpoint model. We show how to deduce an incremental algorithm A{$\Delta$} from such an algorithm A. Moreover, A{$\Delta$} can be made bounded relative to A, i.e., its cost is determined by the sizes of changes to graphs and changes to the affected area that is necessarily checked by batch algorithm A. We provide generic conditions under which a deduced algorithm A{$\Delta$} warrants to be correct and relatively bounded, by adopting the same logic and data structures of A, at most using timestamps as an additional auxiliary structure. Based on these, we show that a variety of graph-centric algorithms can be incrementalized with relative boundedness. Using real-life and synthetic graphs, we experimentally verify the scalability and efficiency of the incrementalized algorithms.},
  isbn = {978-1-4503-8343-1},
  langid = {english},
  keywords = {boundedness,fixpoint algorithm,incrementalization,nosource},
  file = {C:\Users\nhiot\OneDrive\zotero\2021\Fan et al. - 2021 - Incrementalizing Graph Algorithms2.pdf}
}

@article{fanKeysGraphs2015,
  ids = {FFTD15},
  title = {Keys for Graphs},
  author = {Fan, Wenfei and Fan, Zhe and Tian, Chao and Dong, Xin Luna},
  year = {2015},
  month = aug,
  journal = {Proceedings of the VLDB Endowment},
  volume = {8},
  number = {12},
  pages = {1590--1601},
  publisher = {VLDB Endowment},
  issn = {2150-8097},
  doi = {10.14778/2824032.2824056},
  urldate = {2023-08-03},
  abstract = {Keys for graphs aim to uniquely identify entities represented by vertices in a graph. We propose a class of keys that are recursively defined in terms of graph patterns, and are interpreted with subgraph isomorphism. Extending conventional keys for relations and XML, these keys find applications in object identification, knowledge fusion and social network reconciliation. As an application, we study the entity matching problem that, given a graph G and a set {$\Sigma$} of keys, is to find all pairs of entities (vertices) in G that are identified by keys in {$\Sigma$}. We show that the problem is intractable, and cannot be parallelized in logarithmic rounds. Nonetheless, we provide two parallel scalable algorithms for entity matching, in MapReduce and a vertex-centric asynchronous model. Using real-life and synthetic data, we experimentally verify the effectiveness and scalability of the algorithms.},
  file = {C:\Users\nhiot\OneDrive\zotero\2015\Fan et al. - 2015 - Keys for graphs.pdf}
}

@inproceedings{fanLinkingEntitiesRelations2024,
  title = {Linking {{Entities}} across {{Relations}} and {{Graphs}}},
  booktitle = {{{ACM Transactions}} on {{Database Systems}}},
  author = {Fan, Wenfei and Lu, Ping and Pang, Kehan and Jin, Ruochun and Yu, Wenyuan},
  year = {2024},
  month = feb,
  volume = {49},
  pages = {1--50},
  issn = {2375-026X},
  doi = {10.1145/3639363},
  urldate = {2024-04-05},
  abstract = {This article proposes a notion of parametric simulation to link entities across a relational database {$D$} and a graph G. Taking functions and thresholds for measuring vertex closeness, path associations, and important properties as parameters, parametric simulation identifies tuples t in {$D$} and vertices v in G that refer to the same real-world entity, based on both topological and semantic matching. We develop machine learning methods to learn the parameter functions and thresholds. We show that parametric simulation is in quadratic-time by providing such an algorithm. Moreover, we develop an incremental algorithm for parametric simulation; we show that the incremental algorithm is bounded relative to its batch counterpart, i.e., it incurs the minimum cost for incrementalizing the batch algorithm. Putting these together, we develop HER, a parallel system to check whether (t, v) makes a match, find all vertex matches of t in G, and compute all matches across {$D$} and G, all in quadratic-time; moreover, HER supports incremental computation of these in response to updates to {$D$} and G. Using real-life and synthetic data, we empirically verify that HER is accurate with F-measure of 0.94 on average, and is able to scale with database {$D$} and graph G for both batch and incremental computations.},
  langid = {english},
  keywords = {Conferences,Data engineering,Data integration,Entity resolution,Graph data management,Heterogeneous entity resolution,Incremental algorithm,Knowledge graph,Machine learning,Machine learning algorithms,Parallelization,Parametric simulation,Relational database,Relational databases,Relative boundedness,Scalability,Semantics},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2024\\Fan et al. - 2024 - Linking Entities across Relations and Graphs.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\KMSPI24I\\3639363.html}
}

@article{florekLiaisonDivisionPoints1951,
  title = {{Sur la liaison et la division des points d'un ensemble fini}},
  author = {Florek, K. and {\L}ukaszewicz, J. and Perkal, J. and Steinhaus, Hugo and Zubrzycki, S.},
  year = {1951},
  journal = {Colloquium Mathematicum},
  volume = {2},
  number = {3-4},
  pages = {282--285},
  issn = {0010-1354},
  doi = {10.4064/cm-2-3-4-282-285},
  urldate = {2023-04-18},
  langid = {fra},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1951\\Florek et al. - 1951 - Sur la liaison et la division des points d'un ense.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\NWJZTFEF\\209969.html}
}

@article{flourisFormalFoundationsRDF2013,
  ids = {FKAC13},
  title = {Formal Foundations for {{RDF}}/{{S KB}} Evolution},
  author = {Flouris, Giorgos and Konstantinidis, George and Antoniou, Grigoris and Christophides, Vassilis},
  year = {2013},
  month = apr,
  journal = {Knowledge and Information Systems},
  volume = {35},
  number = {1},
  pages = {153--191},
  issn = {0219-1377, 0219-3116},
  doi = {10.1007/s10115-012-0500-2},
  urldate = {2019-06-20},
  langid = {english},
  optbibsource = {dblp computer science bibliography, http://dblp.org},
  optdoi = {10.1007/s10115-012-0500-2},
  opttimestamp = {Tue, 09 Apr 2013 17:54:05 +0200},
  annotation = {QID: Q58198067},
  file = {C:\Users\nhiot\OneDrive\zotero\2013\Flouris et al. - 2013 - Formal foundations for RDFS KB evolution.pdf}
}

@inproceedings{francisCypherEvolvingQuery2018,
  title = {Cypher: {{An Evolving Query Language}} for {{Property Graphs}}},
  shorttitle = {Cypher},
  booktitle = {Proceedings of the 2018 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Francis, Nadime and Green, Alastair and Guagliardo, Paolo and Libkin, Leonid and Lindaaker, Tobias and Marsault, Victor and Plantikow, Stefan and Rydberg, Mats and Selmer, Petra and Taylor, Andr{\'e}s},
  editor = {Das, Gautam and Jermaine, Christopher M. and Bernstein, Philip A.},
  year = {2018},
  month = may,
  series = {{{SIGMOD}} '18},
  pages = {1433--1445},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3183713.3190657},
  abstract = {The Cypher property graph query language is an evolving language, originally designed and implemented as part of the Neo4j graph database, and it is currently used by several commercial database products and researchers. We describe Cypher 9, which is the first version of the language governed by the openCypher Implementers Group. We first introduce the language by example, and describe its uses in industry. We then provide a formal semantic definition of the core read-query features of Cypher, including its variant of the property graph data model, and its ASCII Art graph pattern matching mechanism for expressing subgraphs of interest to an application. We compare the features of Cypher to other property graph query languages, and describe extensions, at an advanced stage of development, which will form part of Cypher 10, turning the language into a compositional language which supports graph projections and multiple named graphs.},
  isbn = {978-1-4503-4703-7},
  keywords = {cypher,formal semantics,formal specification,graph databases,property graphs,query language},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2018\\Francis et al. - 2018 - Cypher An Evolving Query Language for Property Gr.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\ERZNBMM2\\3183713.html}
}

@inproceedings{fredriksonModelInversionAttacks2015,
  title = {Model {{Inversion Attacks}} That {{Exploit Confidence Information}} and {{Basic Countermeasures}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Fredrikson, Matt and Jha, Somesh and Ristenpart, Thomas},
  year = {2015},
  month = oct,
  series = {{{CCS}} '15},
  pages = {1322--1333},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2810103.2813677},
  urldate = {2024-04-07},
  abstract = {Machine-learning (ML) algorithms are increasingly utilized in privacy-sensitive applications such as predicting lifestyle choices, making medical diagnoses, and facial recognition. In a model inversion attack, recently introduced in a case study of linear classifiers in personalized medicine by Fredrikson et al., adversarial access to an ML model is abused to learn sensitive genomic information about individuals. Whether model inversion attacks apply to settings outside theirs, however, is unknown. We develop a new class of model inversion attack that exploits confidence values revealed along with predictions. Our new attacks are applicable in a variety of settings, and we explore two in depth: decision trees for lifestyle surveys as used on machine-learning-as-a-service systems and neural networks for facial recognition. In both cases confidence values are revealed to those with the ability to make prediction queries to models. We experimentally show attacks that are able to estimate whether a respondent in a lifestyle survey admitted to cheating on their significant other and, in the other context, show how to recover recognizable images of people's faces given only their name and access to the ML model. We also initiate experimental exploration of natural countermeasures, investigating a privacy-aware decision tree training algorithm that is a simple variant of CART learning, as well as revealing only rounded confidence values. The lesson that emerges is that one can avoid these kinds of MI attacks with negligible degradation to utility.},
  isbn = {978-1-4503-3832-5},
  keywords = {attacks,machine learning,privacy},
  file = {C:\Users\nhiot\OneDrive\zotero\2015\Fredrikson et al. - 2015 - Model Inversion Attacks that Exploit Confidence In.pdf}
}

@article{freemanSetMeasuresCentrality1977a,
  ids = {freemanSetMeasuresCentrality1977},
  title = {A {{Set}} of {{Measures}} of {{Centrality Based}} on {{Betweenness}}},
  author = {Freeman, Linton C.},
  year = {1977},
  month = mar,
  journal = {Sociometry},
  volume = {40},
  number = {1},
  eprint = {3033543},
  eprinttype = {jstor},
  pages = {35--41},
  publisher = {[American Sociological Association, Sage Publications, Inc.]},
  issn = {0038-0431},
  doi = {10.2307/3033543},
  urldate = {2024-01-12},
  abstract = {A Family of new measures of point and graph centrality based on early intuitions of Bavelas (1948) is introduced. These measures define centrality in terms of the degree to which a point falls on the shortest path between others and therefore has a potential for control of communication. They may be used to index centrality in any large or small network of symmetrical relations, whether connected or unconnected.},
  file = {C:\Users\nhiot\Zotero\storage\X8LFH5V5\Freeman - 1977 - A Set of Measures of Centrality Based on Betweenne.pdf}
}

@inproceedings{friburgerFiniteStateTransducerCascade2001,
  title = {Finite-{{State Transducer Cascade}} to {{Extract Proper Names}} in {{Texts}}},
  booktitle = {Implementation and Application of Automata, 6th International Conference, {{CIAA}} 2001, Pretoria, South Africa, July 23-25, 2001, Revised Papers},
  author = {Friburger, Nathalie and Maurel, Denis},
  editor = {Watson, Bruce W. and Wood, Derick},
  year = {2001},
  month = jul,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {2494},
  pages = {115--124},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-36390-4_10},
  abstract = {This article describes a finite-state cascade for the extraction of person names in texts in French. We extract these proper names in order to categorize and to cluster texts with them. After a finite-state pre-processing (division of the text in sentences, tagging with dictionaries, etc.), a series of finite-state transducers is applied one after the other to the text and locates left and right contexts that indicates the presence of a person name. An evaluation of the results of this extraction is presented.},
  isbn = {978-3-540-36390-3},
  langid = {english},
  keywords = {Compound Word,Coreference Resolution,Input Alphabet,Natural Language Processing,nosource,Output Alphabet},
  file = {C:\Users\nhiot\OneDrive\zotero\2001\Friburger et Maurel - 2001 - Finite-State Transducer Cascade to Extract Proper .pdf}
}

@inproceedings{gaioExtendedNamedEntity2017,
  title = {Extended Named Entity Recognition Using Finite-State Transducers: {{An}} Application to Place Names},
  booktitle = {The Ninth International Conference on Advanced Geographic Information Systems, Applications, and Services ({{GEOProcessing}} 2017)},
  author = {Gaio, Mauro and Moncla, Ludovic},
  year = {2017},
  month = mar,
  address = {Nice, France},
  url = {https://hal.archives-ouvertes.fr/hal-01492994},
  hal_id = {hal-01492994},
  hal_version = {v1},
  keywords = {Geo-information processing,Geo-spatial data mining,Geo-spatial Web Ser- vices and processing},
  file = {C:\Users\nhiot\Zotero\storage\NG6IYJVY\Gaio et Moncla - 2017 - Extended named entity recognition using finite-sta.pdf}
}

@inproceedings{gamalloMappingSyntacticDependencies2002,
  title = {Mapping {{Syntactic Dependencies}} onto {{Semantic Relations}}},
  booktitle = {Proceedings of the {{ECAI}} Workshop on Machine Learning and Natural Language Processing for Ontology Engineering},
  author = {Gamallo, Pablo and Gonz{\'a}lez, Marco and Agustini, Alexandre and Lopes, Gabriel and de Lima, Vera L{\'u}cia Strube},
  year = {2002},
  pages = {15--22},
  url = {https://www.semanticscholar.org/paper/Mapping-Syntactic-Dependencies-onto-Semantic-Gamallo-Gonz\%C3\%A1lez/45ee40319a8b57f2fe7ca3ecc33ee78b97697c92},
  urldate = {2024-04-07},
  abstract = {This paper presents a corpus-based method for extracting semantic relations between words. The method is based on two sequential procedures. First, it automatically classifies syntactic dependencies according to their selection restrictions. Those dependencies that require the same selection restrictions are put together into the same semantic group. Then, interpretation rules are applied on the classified syntactic dependencies, in order to learn the specific semantic relations underlying syntactically related words.},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2002\\Gamallo et al. - 2002 - Mapping syntactic dependencies onto semantic relat.pdf;C\:\\Users\\nhiot\\OneDrive\\zotero\\2002\\Gamallo et al. - 2002 - Mapping Syntactic Dependencies onto Semantic Relat2.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\HY72NMLN\\1571417125955290624.html}
}

@inproceedings{ganesanOpinosisGraphBased2010,
  title = {Opinosis: {{A Graph Based Approach}} to {{Abstractive Summarization}} of {{Highly Redundant Opinions}}},
  shorttitle = {Opinosis},
  booktitle = {Proceedings of the 23rd {{International Conference}} on {{Computational Linguistics}}},
  author = {Ganesan, Kavita and Zhai, ChengXiang and Han, Jiawei},
  year = {2010},
  month = aug,
  series = {{{COLING}} '10},
  pages = {340--348},
  publisher = {Association for Computational Linguistics},
  address = {USA},
  url = {https://www.researchgate.net/profile/Kavita-Ganesan/publication/284446151\_opinosis/links/5653584d08ae1ef92975f73e/opinosis.pdf},
  urldate = {2024-04-04},
  abstract = {We present a novel graph-based summarization framework (Opinosis) that generates concise abstractive summaries of highly redundant opinions. Evaluation results on summarizing user reviews show that Opinosis summaries have better agreement with human summaries compared to the baseline extractive method. The summaries are readable, reasonably well-formed and are informative enough to convey the major opinions.},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\_\Ganesan et al. - Opinosis A Graph-Based Approach to Abstractive Su2.pdf}
}

@inproceedings{garcelonEnrichissementSemantiqueAssocie2014,
  title = {Enrichissement S{\'e}mantique Associ{\'e} {\`a} La D{\'e}tection de La N{\'e}gation et Des Ant{\'e}c{\'e}dents Familiaux Dans Un Entrep{\^o}t de Donn{\'e}es Hospitalier.},
  booktitle = {{{JFIM}}},
  author = {Garcelon, Nicolas and Salomon, R{\'e}mi and Burgun, Anita},
  year = {2014},
  pages = {83--93},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2014\Garcelon et al. - 2014 - Enrichissement sémantique associé à la détection d.pdf}
}

@misc{GENIA,
  title = {{{GENIA}}},
  url = {https://paperswithcode.com/dataset/genia},
  urldate = {2024-03-21},
  abstract = {The GENIA corpus is the primary collection of biomedical literature compiled and annotated within the scope of the GENIA project. The corpus was created to support the development and evaluation of information extraction and text mining systems for the domain of molecular biology. The corpus contains 1,999 Medline abstracts, selected using a PubMed query for the three MeSH terms ``human'', ``blood cells'', and ``transcription factors''. The corpus has been annotated with various levels of linguistic and semantic information. The primary categories of annotation in the GENIA corpus and the corresponding subcorpora are: Part-of-Speech annotation Constituency (phrase structure) syntactic annotation Term annotation Event annotation Relation annotation Coreference annotation},
  langid = {english},
  file = {C:\Users\nhiot\Zotero\storage\KXPYG8T8\genia.html}
}

@incollection{genthialActesTALN19971997,
  title = {Actes de {{TALN}} 1997 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 1997 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Genthial, Damien},
  year = {1997},
  month = jun,
  address = {Grenoble},
  keywords = {nosource}
}

@inproceedings{goasdoueEfficientQueryAnswering2013,
  title = {Efficient Query Answering against Dynamic {{RDF}} Databases},
  booktitle = {Proceedings of the 16th {{International Conference}} on {{Extending Database Technology}}},
  author = {Goasdou{\'e}, Fran{\c c}ois and Manolescu, Ioana and Roati{\c s}, Alexandra},
  year = {2013},
  month = mar,
  series = {{{EDBT}} '13},
  pages = {299--310},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2452376.2452412},
  urldate = {2023-08-03},
  abstract = {A promising method for efficiently querying RDF data consists of translating SPARQL queries into efficient RDBMS-style operations. However, answering SPARQL queries requires handling RDF reasoning, which must be implemented outside the relational engines that do not support it. We introduce the database (DB) fragment of RDF, going beyond the expressive power of previously studied RDF fragments. We devise novel sound and complete techniques for answering Basic Graph Pattern (BGP) queries within the DB fragment of RDF, exploring the two established approaches for handling RDF semantics, namely reformulation and saturation. In particular, we focus on handling database updates within each approach and propose a method for incrementally maintaining the saturation; updates raise specific difficulties due to the rich RDF semantics. Our techniques are designed to be deployed on top of any RDBMS(-style) engine, and we experimentally study their performance trade-offs.},
  isbn = {978-1-4503-1597-5},
  keywords = {query answering,RDF fragments,reasoning},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2013\\Goasdoué et al. - 2013 - Efficient query answering against dynamic RDF data.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\ZEREIB62\\2452376.html}
}

@article{gonzalezlopezdemurillasConnectingDatabasesProcess2019,
  title = {Connecting Databases with Process Mining: A Meta Model and Toolset},
  shorttitle = {Connecting Databases with Process Mining},
  author = {Gonz{\'a}lez L{\'o}pez De Murillas, Eduardo and Reijers, Hajo A. and Aalst, Wil M.},
  year = {2019},
  month = apr,
  journal = {Software and Systems Modeling (SoSyM)},
  volume = {18},
  number = {2},
  pages = {1209--1247},
  issn = {1619-1366, 1619-1374},
  doi = {10.1007/s10270-018-0664-7},
  urldate = {2024-04-26},
  abstract = {Process mining techniques require event logs which, in many cases, are obtained from databases. Obtaining these event logs is not a trivial task and requires substantial domain knowledge. In addition, an extracted event log provides only a single view on the database. To change our view, e.g., to focus on another business process and generate another event log, it is necessary to go back to the source of data. This paper proposes a meta model to integrate both process and data perspectives, relating one to the other. It can be used to generate different views from the database at any moment in a highly flexible way. This approach decouples the data extraction from the application of analysis techniques, enabling the application of process mining in different contexts.},
  langid = {english},
  keywords = {Data schema,Database,Event extraction,Meta model,Process mining},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2019\\González López De Murillas et al. - 2019 - Connecting databases with process mining a meta m.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\6ZNMKSWL\\s10270-018-0664-7.html}
}

@inproceedings{gottlobComputingCoresData2005,
  title = {Computing Cores for Data Exchange: New Algorithms and Practical Solutions},
  shorttitle = {Computing Cores for Data Exchange},
  booktitle = {Proceedings of the Twenty-Fourth {{ACM SIGMOD-SIGACT-SIGART}} Symposium on {{Principles}} of Database Systems},
  author = {Gottlob, Georg},
  year = {2005},
  month = jun,
  series = {{{PODS}} '05},
  pages = {148--159},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1065167.1065187},
  urldate = {2023-08-16},
  abstract = {Data Exchange is the problem of inserting data structured under a source schema into a target schema of different structure (possibly with integrity constraints), while reflecting the source data as accurately as possible. We study computational issues related to data exchange in the setting of Fagin, Kolaitis, and Popa(PODS'03). We use the technique of hypertree decompositions to derive improved algorithms for computing the core of a relational instance with labeled nulls, a problem we show to be fixed-parameter intractable with respect to the block size of the input instances. We show that computing the core of a data exchange problem is tractable for two large and useful classes of target constraints. The first class includes functional dependencies and weakly acyclic inclusion dependencies. The second class consists of full tuple generating dependencies and arbitrary equation generating dependencies. Finally, we show that computing cores is NP-hard in presence of a system-predicate NULL(x), which is true iff x is a null value.},
  isbn = {978-1-59593-062-0},
  keywords = {nosource}
}

@inproceedings{gottlobOntologicalQueriesRewriting2011,
  title = {Ontological Queries: {{Rewriting}} and Optimization},
  shorttitle = {Ontological Queries},
  booktitle = {Proceedings of the 27th International Conference on Data Engineering, {{ICDE}}, Germany},
  author = {Gottlob, Georg and Orsi, Giorgio and Pieris, Andreas},
  year = {2011},
  month = apr,
  pages = {2--13},
  publisher = {IEEE},
  issn = {2375-026X},
  doi = {10.1109/ICDE.2011.5767965},
  urldate = {2023-12-24},
  abstract = {Ontological queries are evaluated against an enterprise ontology rather than directly on a database. The evaluation and optimization of such queries is an intriguing new problem for database research. In this paper we discuss two important aspects of this problem: query rewriting and query optimization. Query rewriting consists of the compilation of an ontological query into an equivalent query against the underlying relational database. The focus here is on soundness and completeness. We review previous results and present a new rewriting algorithm for rather general types of ontological constraints (description logics). In particular, we show how a conjunctive query (CQ) against an enterprise ontology can be compiled into a union of conjunctive queries (UCQ) against the underlying database. Ontological query optimization, in this context, attempts to improve this process so to produce possibly small and cost-effective output UCQ. We review existing optimization methods, and propose an effective new method that works for Linear Datalog{\textpm}, a description logic that encompasses well-known description logics of the DL-Lite family.},
  keywords = {nosource},
  file = {C:\Users\nhiot\OneDrive\zotero\2011\Gottlob et al. - 2011 - Ontological queries Rewriting and optimization.pdf}
}

@article{goyalRecentNamedEntity2018,
  title = {Recent {{Named Entity Recognition}} and {{Classification}} Techniques: {{A}} Systematic Review},
  shorttitle = {Recent {{Named Entity Recognition}} and {{Classification}} Techniques},
  author = {Goyal, Archana and Gupta, Vishal and Kumar, Manish},
  year = {2018},
  month = aug,
  journal = {Computer Science Review},
  volume = {29},
  pages = {21--43},
  publisher = {Elsevier},
  issn = {1574-0137},
  doi = {10.1016/j.cosrev.2018.06.001},
  urldate = {2024-02-13},
  abstract = {Textual information is becoming available in abundance on the web, arising the requirement of techniques and tools to extract the meaningful information. One of such an important information extraction task is Named Entity Recognition and Classification. It is the problem of finding the members of various predetermined classes, such as person, organization, location, date/time, quantities, numbers etc. The concept of named entity extraction was first proposed in Sixth Message Understanding Conference in 1996. Since then, a number of techniques have been developed by many researchers for extracting diversity of entities from different languages and genres of text. Still, there is a growing interest among research community to develop more new approaches to extract diverse named entities which are helpful in various natural language applications. Here we present a survey of developments and progresses made in Named Entity Recognition and Classification research.},
  langid = {english},
  file = {C:\Users\nhiot\Zotero\storage\DETWMZLZ\S1574013717302782.html}
}

@inproceedings{grabarCASFrenchCorpus2018,
  title = {{{CAS}}: {{French Corpus}} with {{Clinical Cases}}},
  shorttitle = {{{CAS}}},
  booktitle = {Proceedings of the {{Ninth International Workshop}} on {{Health Text Mining}} and {{Information Analysis}}},
  author = {Grabar, Natalia and Claveau, Vincent and Dalloux, Cl{\'e}ment},
  year = {2018},
  month = oct,
  pages = {122--128},
  publisher = {Association for Computational Linguistics},
  address = {Brussels, Belgium},
  doi = {10.18653/v1/w18-5614},
  urldate = {2021-05-25},
  abstract = {Textual corpora are extremely important for various NLP applications as they provide information necessary for creating, setting and testing these applications and the corresponding tools. They are also crucial for designing reliable methods and reproducible results. Yet, in some areas, such as the medical area, due to confidentiality or to ethical reasons, it is complicated and even impossible to access textual data representative of those produced in these areas. We propose the CAS corpus built with clinical cases, such as they are reported in the published scientific literature in French. We describe this corpus, currently containing over 397,000 word occurrences, and the existing linguistic and semantic annotations.},
  file = {C:\Users\nhiot\OneDrive\zotero\2018\Grabar et al. - 2018 - CAS French Corpus with Clinical Cases.pdf}
}

@inproceedings{grabarCorpusAnnoteCas2019,
  title = {Corpus Annot{\'e} de Cas Cliniques En Fran{\c c}ais},
  booktitle = {{{TALN}} 2019 - 26e Conference on Traitement Automatique Des Langues Naturelles},
  author = {Grabar, Natalia and Grouin, Cyril and Hamon, Thierry and Claveau, Vincent},
  year = {2019},
  month = jul,
  pages = {1--14},
  address = {Toulouse, France},
  url = {https://hal.archives-ouvertes.fr/hal-02391878},
  hal_id = {hal-02391878},
  hal_version = {v1},
  keywords = {annotations,cas clinique,cat{\'e}gorisation,categorization,clinical case,Clinical corpus,Corpus clinique,extraction d'information,information extraction},
  file = {C:\Users\nhiot\OneDrive\zotero\2019\Grabar et al. - 2019 - Corpus annoté de cas cliniques en français.pdf}
}

@book{grahneProblemIncompleteInformation1991,
  title = {The {{Problem}} of {{Incomplete Information}} in {{Relational Databases}}},
  author = {Grahne, G{\"o}sta},
  year = {1991},
  month = nov,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {554},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-54919-6},
  abstract = {In a relational database the information is recorded as rows in tables. However, in many practical situations the available information is incomplete and the values for some columns are missing. Yet few existing database management systems allow the user to enter null values in the database. This monograph analyses the problems raised by allowing null values in relational databases. The analysis covers semantical, syntactical, and computational aspects. Algorithms for query evaluation, dependency enforcement and updates in the presence of null values are also given. The analysis of the computational complexity of the algorithms suggests that from a practical point of view the database should be stored as Horn tables, which are generalizations of ordinary relations, allowing null values and Horn clause-like restrictions on these null values. Horn tables efficiently support a large class of queries, dependencies and updates.},
  isbn = {978-3-540-46507-2},
  langid = {english},
  lccn = {QA76.9.D3 G69 1991},
  keywords = {⛔ No DOI found,Computers / Artificial Intelligence / General,Computers / Database Administration \& Management,Computers / Information Technology,Computers / Information Theory,Computers / Programming / Algorithms,nosource,Relational databases},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1991\\Grahne - 1991 - The Problem of Incomplete Information in Relationa.pdf;C\:\\Users\\nhiot\\OneDrive\\zotero\\1991\\Grahne - 1991 - The Problem of Incomplete Information in Relationa2.pdf;C\:\\Users\\nhiot\\OneDrive\\zotero\\1991\\Grahne - 1991 - The Problem of Incomplete Information in Relationa3.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\R23IRBKN\\3-540-54919-6_10.html}
}

@inproceedings{grishmanInformationExtractionTechniques1997,
  title = {Information Extraction: {{Techniques}} and Challenges},
  shorttitle = {Information Extraction},
  booktitle = {International Summer School on Information Extraction},
  author = {Grishman, Ralph},
  year = {1997},
  volume = {1299},
  pages = {10--27},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-63438-x_2},
  isbn = {978-3-540-63438-6 978-3-540-69548-6},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1997\\Grishman - 1997 - Information extraction Techniques and challenges.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\PYHSUCCK\\10.html}
}

@inproceedings{grishmanMessageUnderstandingConference61996a,
  ids = {grishmanMessageUnderstandingConference61996},
  title = {Message {{Understanding Conference-6}}: A Brief History},
  shorttitle = {Message {{Understanding Conference-6}}},
  booktitle = {Proceedings of the 16th Conference on {{Computational}} Linguistics - {{Volume}} 1},
  author = {Grishman, Ralph and Sundheim, Beth},
  year = {1996},
  month = aug,
  series = {{{COLING}} '96},
  pages = {466--471},
  publisher = {Association for Computational Linguistics},
  address = {USA},
  doi = {10.3115/992628.992709},
  urldate = {2024-04-05},
  abstract = {We have recently completed the sixth in a series of "Message Understanding Conferences" which are designed to promote and evaluate research in information extraction. MUC-6 introduced several innovations over prior MUCs, most notably in the range of different tasks for which evaluations were conducted. We describe some of the motivations for the new format and briefly discuss some of the results of the evaluations.},
  file = {C:\Users\nhiot\Zotero\storage\W2N92SVM\Grishman et Sundheim - 1996 - Message Understanding Conference-6 a brief histor.pdf}
}

@inproceedings{grossUseFiniteAutomata1989,
  ids = {grossUseFiniteAutomata1987},
  title = {The Use of Finite Automata in the Lexical Representation of Natural Language},
  booktitle = {Electronic {{Dictionaries}} and {{Automata}} in {{Computational Linguistics}}},
  author = {Gross, Maurice},
  editor = {Gross, Maurice and Perrin, Dominique},
  year = {1989},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {34--50},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-51465-1_3},
  isbn = {978-3-540-48140-9},
  langid = {english},
  keywords = {Compound Word,Finite Automaton,Flight Simulator,Oxford English Dictionary,Simple Word},
  file = {C:\Users\nhiot\Zotero\storage\W3UCPU6M\10.html}
}

@inproceedings{grouinClassificationCasCliniques2021,
  ids = {grouinClassificationCasCliniques2021a},
  title = {{Classification de cas cliniques et {\'e}valuation automatique de r{\'e}ponses d'{\'e}tudiants : pr{\'e}sentation de la campagne DEFT 2021 (Clinical cases classification and automatic evaluation of student answers : Presentation of the DEFT 2021 Challenge)}},
  shorttitle = {{Classification de cas cliniques et {\'e}valuation automatique de r{\'e}ponses d'{\'e}tudiants}},
  booktitle = {{Actes de la 28e Conf{\'e}rence sur le Traitement Automatique des Langues Naturelles. Atelier D{\'E}fi Fouille de Textes (DEFT)}},
  author = {Grouin, Cyril and Grabar, Natalia and Illouz, Gabriel},
  year = {2021},
  month = jun,
  pages = {1--13},
  publisher = {ATALA},
  address = {Lille, France},
  url = {https://aclanthology.org/2021.jeptalnrecital-deft.1},
  urldate = {2023-09-22},
  abstract = {Le d{\'e}fi fouille de textes (DEFT) est une campagne d'{\'e}valuation annuelle francophone. Nous pr{\'e}sentons les corpus et baselines {\'e}labor{\'e}es pour trois t{\^a}ches : (i) identifier le profil clinique de patients d{\'e}crits dans des cas cliniques, (ii) {\'e}valuer automatiquement les r{\'e}ponses d'{\'e}tudiants sur des questionnaires en ligne (Moodle) {\`a} partir de la correction de l'enseignant, et (iii) poursuivre une {\'e}valuation de r{\'e}ponses d'{\'e}tudiants {\`a} partir de r{\'e}ponses d{\'e}j{\`a} {\'e}valu{\'e}es par l'enseignant. Les r{\'e}sultats varient de 0,394 {\`a} 0,814 de F-mesure sur la premi{\`e}re t{\^a}che (7 {\'e}quipes), de 0,448 {\`a} 0,682 de pr{\'e}cision sur la deuxi{\`e}me (3 {\'e}quipes), et de 0,133 {\`a} 0,510 de pr{\'e}cision sur la derni{\`e}re (3 {\'e}quipes).},
  langid = {french},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2021\\Grouin et al. - 2021 - Classification de cas cliniques et évaluation auto.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\X22H9BDQ\\2021.jeptalnrecital-deft.1.html}
}

@article{guagliardoCorrectnessSQLQueries2017,
  title = {Correctness of {{SQL Queries}} on {{Databases}} with {{Nulls}}},
  author = {Guagliardo, Paolo and Libkin, Leonid},
  year = {2017},
  month = oct,
  journal = {ACM SIGMOD Record},
  volume = {46},
  number = {3},
  pages = {5--16},
  issn = {0163-5808},
  doi = {10.1145/3156655.3156657},
  urldate = {2023-08-08},
  abstract = {Multiple issues with SQL's handling of nulls have been well documented. Having efficiency as its main goal, SQL disregards the standard notion of correctness on incomplete databases -- certain answers -- due to its high complexity. As a result, the evaluation of SQL queries on databases with nulls may produce answers that are just plain wrong. However, SQL evaluation can be modified, at least for relational algebra queries, to approximate certain answers, i.e., return only correct answers. We examine recently proposed approximation schemes for certain answers and analyze their complexity, both theoretical bounds and real-life behavior},
  file = {C:\Users\nhiot\OneDrive\zotero\2017\Guagliardo et Libkin - 2017 - Correctness of SQL Queries on Databases with Nulls.pdf}
}

@inproceedings{halfeld-ferrariRDFUpdatesConstraints2017,
  title = {{{RDF Updates}} with {{Constraints}}},
  booktitle = {Knowledge {{Engineering}} and {{Semantic Web}} - 8th {{International Conference}}, {{KESW}}, {{Szczecin}}, {{Poland}}, {{Proceedings}}},
  author = {{Halfeld-Ferrari}, Mirian and Hara, Carmem S. and Uber, Flavio R.},
  editor = {R{\'o}{\.z}ewski, Przemys{\l}aw and Lange, Christoph},
  year = {2017},
  series = {Communications in {{Computer}} and {{Information Science}}},
  volume = {786},
  pages = {229--245},
  publisher = {Springer International Publishing},
  address = {Szczecin, Poland},
  doi = {10.1007/978-3-319-69548-8_16},
  abstract = {This paper deals with the problem of updating an RDF database, expected to satisfy user-defined constraints as well as RDF intrinsic semantic constraints. As updates may violate these constraints, side-effects are generated in order to preserve consistency. We investigate the use of nulls (blank nodes) as placeholders for unknown required data as a technique to provide this consistency and to reduce the number of side-effects. Experimental results validate our goals.},
  isbn = {978-3-319-69547-1 978-3-319-69548-8},
  langid = {english},
  keywords = {Constraints,RDF,RDFS,Updates},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2017\\Halfed Ferrari et al. - 2017 - RDF Updates with Constraints.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\KT9SZGTE\\978-3-319-69548-8_16.html}
}

@article{halfeld-ferrariUpdateRulesDatalog1998,
  title = {Update {{Rules}} in {{Datalog Programs}}},
  author = {{Halfeld-Ferrari}, Mirian and Laurent, Dominique and Spyratos, Nicolas},
  year = {1998},
  month = dec,
  journal = {Journal of Logic and Computation},
  volume = {8},
  number = {6},
  pages = {745--775},
  publisher = {OUP},
  issn = {0955-792X, 1465-363X},
  doi = {10.1093/logcom/8.6.745},
  abstract = {We propose a deductive database model containing two kinds of rules: update rules of the form L0{\textleftarrow}L1, where L0 and L1 are literals, and query rules of the form of normal logic program rules. A basic feature of our approach is that new knowledge inputs are always assimilated. Moreover, updates are always deterministic and they preserve database consistency.We consider that update rules have higher priority than query rules, i.e., update rules may generate exceptions to query-driven derivations. We introduce a semantics framework for database update and query answering, based on the well-founded semantics. We also suggest an alternative approach based on extended logic programs and we show that our database model can be defined in terms of non-monotonic formalisms.},
  langid = {english},
  keywords = {Datalog,deductive database,nosource,update},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1998\\Halfeld-Ferrari et al. - 1998 - Update Rules in Datalog Programs.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\7CB956IH\\8142245.html;C\:\\Users\\nhiot\\Zotero\\storage\\B72S7SY9\\8142245.html}
}

@inproceedings{halfeld-ferrariUpdatingRDFDatabases2017,
  title = {Updating {{RDF}}/{{S Databases Under Constraints}}},
  booktitle = {Advances in Databases and Information Systems - 21st European Conference, {{ADBIS}}, Nicosia, Cyprus, Proceedings},
  author = {{Halfeld-Ferrari}, Mirian and Laurent, Dominique},
  editor = {Kirikova, M{\=a}r{\=i}te and N{\o}rv{\aa}g, Kjetil and Papadopoulos, George A.},
  year = {2017},
  volume = {10509},
  pages = {357--371},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-66917-5_24},
  urldate = {2023-12-24},
  isbn = {978-3-319-66916-8 978-3-319-66917-5},
  keywords = {nosource}
}

@inproceedings{hassanGRFusionGraphsFirstClass2018,
  ids = {hassanGRFusionGraphsFirstClass2018a},
  title = {{{GRFusion}}: {{Graphs}} as {{First-Class Citizens}} in {{Main-Memory Relational Database Systems}}},
  shorttitle = {{{GRFusion}}},
  booktitle = {Proceedings of the 2018 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Hassan, Mohamed S. and Kuznetsova, Tatiana and Jeong, Hyun Chai and Aref, Walid G. and Sadoghi, Mohammad},
  year = {2018},
  month = may,
  series = {{{SIGMOD}} '18},
  pages = {1789--1792},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3183713.3193541},
  urldate = {2024-01-11},
  abstract = {The maturity of RDBMSs has motivated academia and industry to invest efforts in leveraging RDBMSs for graph processing, where efficiency is proven for vital graph queries. However, none of these efforts process graphs natively inside the RDBMS, which is particularly challenging due to the impedance mismatch between the relational and the graph models. In this demonstration, we present GRFusion, an in-memory relational database system, where graphs are managed as first-class citizens. GRFusion is realized inside VoltDB. The SQL and query engines of VoltDB are empowered to declaratively define graphs and execute cross-data-model query plans that consist of relational operators and newly-introduced graph operators. Using a social network and a real continental-sized road network covering the entire U.S., we demonstrate the functionality and the performance of GRFusion in evaluating queries that reference both relational tables and graphs seamlessly in the same query execution pipeline. GRFusion shows up to four orders-of-magnitude speed-up in query-time w.r.t. state-of-the-art approaches.},
  isbn = {978-1-4503-4703-7},
  langid = {english},
  keywords = {cross-data-model query plans,graph queries,main-memory databases,nosource,path traversals},
  file = {C:\Users\nhiot\OneDrive\zotero\2018\Hassan et al. - 2018 - GRFusion Graphs as First-Class Citizens in Main-M3.pdf}
}

@article{hazmanSurveyOntologyLearning2011,
  title = {A {{Survey}} of {{Ontology Learning Approaches}}},
  author = {Hazman, Maryam and {El-Beltagy}, Samhaa R. and Rafea, Ahmed},
  year = {2011},
  month = may,
  journal = {International Journal of Computer Applications},
  volume = {22},
  number = {9},
  pages = {36--43},
  issn = {09758887},
  doi = {10.5120/2610-3642},
  urldate = {2024-04-06},
  file = {C:\Users\nhiot\OneDrive\zotero\2011\Hazman et al. - 2011 - A survey of ontology learning approaches2.pdf}
}

@article{hellCoreGraph1992,
  title = {The Core of a Graph},
  author = {Hell, Pavol and Ne{\v s}et{\v r}il, Jaroslav},
  year = {1992},
  month = nov,
  journal = {Discrete Mathematics},
  volume = {109},
  number = {1-3},
  pages = {117--126},
  issn = {0012365X},
  doi = {10.1016/0012-365X(92)90282-K},
  urldate = {2024-01-03},
  langid = {english},
  keywords = {nosource}
}

@book{hinzenOxfordHandbookCompositionality2012,
  title = {The Oxford Handbook of Compositionality},
  editor = {Hinzen, Wolfram and Machery, Edouard and Werning, Markus},
  year = {2012},
  publisher = {Oxford},
  doi = {10.1093/oxfordhb/9780199541072.001.0001},
  keywords = {nosource}
}

@inproceedings{hiotDOINGDEFTUtilisation2021,
  title = {{DOING@DEFT : utilisation de lexiques pour une classification efficace de cas cliniques}},
  shorttitle = {{DOING@DEFT}},
  booktitle = {{Traitement Automatique des Langues Naturelles}},
  author = {Hiot, Nicolas and Minard, Anne-Lyse and Badin, Flora},
  editor = {Denis, Pascal and Grabar, Natalia and Fraisse, Amel and Cardon, R{\'e}mi and Jacquemin, Bernard and Kergosien, Eric and Balvet, Antonio},
  year = {2021},
  pages = {41--53},
  publisher = {ATALA},
  address = {Lille, France},
  url = {https://hal.science/hal-03265924},
  urldate = {2023-09-22},
  abstract = {Nous pr{\'e}sentons dans cet article notre participation {\`a} la t{\^a}che 1 de la campagne d'{\'e}valuation francophone DEFT 2021, sur l'identification du profil clinique du patient. Nous proposons une m{\'e}thode {\'e}volutive et efficace en temps et en ressources pour la classification de documents m{\'e}dicaux pouvant {\^e}tre facilement adapt{\'e}e {\`a} d'autres domaines de recherche. Notre syst{\`e}me a obtenu les meilleures performances sur cette t{\^a}che avec une F-mesure de 0,814.},
  copyright = {All rights reserved},
  hal_id = {hal-03265924},
  hal_version = {v1},
  langid = {french},
  keywords = {⛔ No DOI found,cas clinique,classification.,lexique,me,transducteur fini},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2021\\Hiot et al. - 2021 - DOING@DEFT  utilisation de lexiques pour une clas.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\FHMF4CQI\\hal-03265924.html}
}

@techreport{HL17,
  title = {Updating {{RDF}}/{{S}} Databases under Negative and Tuple-Generating Constraints},
  author = {{Halfeld-Ferrari}, Mirian and Laurent, Dominique},
  year = {2017},
  institution = {LIFO- Universit{\'e} d'Orl{\'e}ans, RR-2017-05},
  url = {https://www.univ-orleans.fr/lifo/rapports.php?lang=en\&sub=sub3},
  keywords = {nosource}
}

@misc{honnibalSpaCyIndustrialstrengthNatural2020,
  title = {{{spaCy}}: {{Industrial-strength Natural Language Processing}} in {{Python}}},
  shorttitle = {{{spaCy}}},
  author = {Honnibal, Matthew and Montani, Ines and Van Landeghem, Sofie and Boyd, Adriane},
  year = {2020},
  doi = {10.5281/zenodo.1212303},
  urldate = {2024-03-21},
  abstract = {💫 Industrial-strength Natural Language Processing (NLP) in Python},
  copyright = {MIT}
}

@article{honnibalSpacyNaturalLanguage2017,
  title = {Spacy 2: {{Natural}} Language Understanding with Bloom Embeddings, Convolutional Neural Networks and Incremental Parsing},
  shorttitle = {{{spaCy}} 2},
  author = {Honnibal, Matthew and Montani, Ines},
  year = {2017},
  journal = {Proceedings of the Association for Computational Linguistics (ACL)},
  volume = {7},
  number = {1},
  pages = {688--697},
  optshorttitle = {spacy 2},
  keywords = {⛔ No DOI found,nosource}
}

@book{hopcroftIntroductionAutomataTheory2007,
  title = {Introduction to Automata Theory, Languages and Computation},
  author = {Hopcroft, John E. and Motwani, Rajeev and Ullman, Jeffrey D.},
  year = {2007},
  edition = {3rd},
  publisher = {Pearson/Addison Wesley},
  address = {Boston},
  isbn = {978-0-321-45536-9 978-0-321-46225-1 978-0-321-45537-6},
  langid = {english},
  lccn = {QA267 .H56 2007},
  keywords = {Computational complexity,Formal languages,Machine theory},
  annotation = {OCLC: ocm69013079\\
QID: Q90418603},
  file = {C:\Users\nhiot\OneDrive\zotero\2007\Hopcroft et al. - 2007 - Introduction to automata theory, languages and com.pdf}
}

@article{huEntityLinkingSymmetrical2019,
  title = {Entity {{Linking}} via {{Symmetrical Attention-Based Neural Network}} and {{Entity Structural Features}}},
  author = {Hu, Shengze and Tan, Zhen and Zeng, Weixin and Ge, Bin and Xiao, Weidong},
  year = {2019},
  month = apr,
  journal = {Symmetry},
  volume = {11},
  number = {4},
  pages = {453},
  publisher = {MDPI},
  issn = {2073-8994},
  doi = {10.3390/sym11040453},
  urldate = {2024-04-06},
  abstract = {In the process of knowledge graph construction, entity linking is a pivotal step, which maps mentions in text to a knowledge base. Existing models only utilize individual information to represent their latent features and ignore the correlation between entities and their mentions. Besides, in the process of entity feature extraction, only partial latent features, i.e., context features, are leveraged to extract latent features, and the pivotal entity structural features are ignored. In this paper, we propose SA-ESF, which leverages the symmetrical Bi-LSTM neural network with the double attention mechanism to calculate the correlation between mentions and entities in two aspects: (1) entity embeddings and mention context features; (2) mention embeddings and entity description features; furthermore, the context features, structural features, and entity ID feature are integrated to represent entity embeddings jointly. Finally, we leverage (1) the similarity score between each mention and its candidate entities and (2) the prior probability to calculate the final ranking results. The experimental results on nine benchmark dataset validate the performance of SA-ESF where the average F1 score is up to 0.866.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2019\\Hu et al. - 2019 - Entity Linking via Symmetrical Attention-Based Neu.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\ZUE6NU3G\\453.html}
}

@article{huNaturalLanguageAggregate2018,
  title = {Natural Language Aggregate Query over {{RDF}} Data},
  author = {Hu, Xin and Dang, Depeng and Yao, Yingting and Ye, Luting},
  year = {2018},
  month = jul,
  journal = {Information Sciences},
  volume = {454--455},
  pages = {363--381},
  issn = {00200255},
  doi = {10.1016/j.ins.2018.04.042},
  urldate = {2024-03-29},
  langid = {english},
  optbibsource = {dblp computer science bibliography, https://dblp.org},
  optbiburl = {https://dblp.org/rec/journals/isci/HuDYY18.bib},
  opttimestamp = {Thu, 16 Apr 2020 14:52:54 +0200},
  opturl = {https://doi.org/10.1016/j.ins.2018.04.042},
  keywords = {nosource},
  file = {C:\Users\nhiot\OneDrive\zotero\2018\Hu et al. - 2018 - Natural language aggregate query over RDF data.pdf}
}

@article{imielinskiIncompleteInformationRelational1984,
  ids = {IL84},
  title = {Incomplete Information in Relational Databases},
  author = {Imielinski, Tomasz and Lipski Jr., Witold},
  year = {1984},
  month = sep,
  journal = {Journal of the ACM (JACM)},
  volume = {31},
  number = {4},
  pages = {761--791},
  publisher = {ACM New York, NY, USA},
  issn = {0004-5411},
  doi = {10.1145/1634.1886},
  optbibsource = {dblp computer science bibliography, http://dblp.org},
  optdoi = {10.1145/1634.1886},
  opttimestamp = {Thu, 26 Jan 2012 17:31:32 +0100},
  keywords = {nosource},
  annotation = {QID: Q56698644},
  file = {C:\Users\nhiot\OneDrive\zotero\1984\Imielinski et Lipski Jr. - 1984 - Incomplete information in relational databases.pdf}
}

@article{jaccardDistributionFloreAlpine1901,
  title = {Distribution de La {{Flore Alpine}} Dans Le {{Bassin}} Des {{Dranses}} et Dans Quelques R{\'e}gions Voisines.},
  author = {Jaccard, Paul},
  year = {1901},
  month = jan,
  journal = {Bulletin de la Societe Vaudoise des Sciences Naturelles},
  volume = {37},
  pages = {241--72},
  doi = {10.5169/seals-266440},
  file = {C:\Users\nhiot\OneDrive\zotero\1901\Jaccard - 1901 - Distribution de la Flore Alpine dans le Bassin des.pdf}
}

@incollection{jardinoActesTALN20052005,
  title = {Actes de {{TALN}} 2005 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 2005 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Jardino, Mich{\`e}le},
  year = {2005},
  month = jun,
  publisher = {LIMSI / ATALA},
  address = {Dourdan},
  keywords = {nosource}
}

@inproceedings{joshiTreeAdjoiningGrammars1985,
  title = {Tree Adjoining Grammars: {{How}} Much Context-Sensitivity Is Required to Provide Reasonable Structural Descriptions?},
  shorttitle = {Tree Adjoining Grammars},
  booktitle = {Natural {{Language Parsing}}: {{Psychological}}, {{Computational}}, and {{Theoretical Perspectives}}},
  author = {Joshi, Aravind K.},
  editor = {Zwicky, Arnold M. and Dowty, David R. and Karttunen, Lauri},
  year = {1985},
  series = {Studies in {{Natural Language Processing}}},
  pages = {206--250},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/CBO9780511597855.007},
  urldate = {2023-10-23},
  abstract = {Since the late 1970s there has been vigorous activity in constructing highly constrained grammatical systems by eliminating the transformational component either totally or partially. There is increasing recognition of the fact that the entire range of dependencies that transformational grammars in their various incarnations have tried to account for can be captured satisfactorily by classes of rules that are nontransformational and at the same time highly constrained in terms of the classes of grammars and languages they define.Two types of dependencies are especially important: subcategorization and filler-gap dependencies. Moreover, these dependencies can be unbounded. One of the motivations for transformations was to account for unbounded dependencies. The so-called nontransformational grammars account for the unbounded dependencies in different ways. In a tree adjoining grammar (TAG) unboundedness is achieved by factoring the dependencies and recursion in a novel and linguistically interesting manner. All dependencies are defined on a finite set of basic structures (trees), which are bounded. Unboundedness is then a corollary of a particular composition operation called adjoining. There are thus no unbounded dependencies in a sense.This factoring of recursion and dependencies is in contrast to transformational grammars (TG), where recursion is defined in the base and the transformations essentially carry out the checking of the dependencies. The phrase linking grammars (PLGs) (Peters and Ritchie, 1982) and the lexical functional grammars (LFGs) (Kaplan and Bresnan, 1983) share this aspect of TGs; that is, recursion builds up a set a structures, some of which are then filtered out by transformations in a TG, by the constraints on linking in a PLG, and by the constraints introduced via the functional structures in an LFG.},
  isbn = {978-0-511-59785-5},
  keywords = {nosource}
}

@book{jurafskySpeechLanguageProcessing2009,
  title = {Speech and Language Processing : An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
  shorttitle = {Speech and {{Language Processing}}},
  author = {Jurafsky, Dan and Martin, James H.},
  year = {2009},
  month = jul,
  volume = {4},
  publisher = {Pearson education, Asia},
  address = {Upper Saddle River, N.J.},
  url = {http://www.amazon.com/Speech-Language-Processing-2nd-Edition/dp/0131873210/ref=pd\_bxgy\_b\_img\_y},
  abstract = {An explosion of Web-based language techniques, merging of distinct fields, availability of phone-based dialogue systems, and much more make this an exciting time in speech and language processing. The first of its kind to thoroughly cover language technology - at all levels and with all modern technologies - this book takes an empirical approach to the subject, based on applying statistical and other machine-learning algorithms to large corporations. Builds each chapter around one or more worked examples demonstrating the main idea of the chapter, usingthe examples to illustrate the relative strengths and weaknesses of various approaches. Adds coverage of statistical sequence labeling, information extraction, question answering and summarization, advanced topics in speech recognition, speech synthesis. Revises coverage of language modeling, formal grammars, statistical parsing, machine translation, and dialog processing. A useful reference for professionals in any of the areas of speech and language processing. -- Book Description from Website.},
  isbn = {978-0-13-187321-6 0-13-187321-0},
  keywords = {language},
  file = {C:\Users\nhiot\OneDrive\zotero\2019\Jurafsky et Martin - 2019 - Speech and language processing  an introduction t.pdf}
}

@article{khadirOntologyLearningGrand2021,
  title = {Ontology Learning: {{Grand}} Tour and Challenges},
  shorttitle = {Ontology Learning},
  author = {Khadir, Ahlem Ch{\'e}rifa and Aliane, Hassina and Guessoum, Ahmed},
  year = {2021},
  month = feb,
  journal = {Computer Science Review},
  volume = {39},
  pages = {100339},
  publisher = {Elsevier},
  issn = {1574-0137},
  doi = {10.1016/j.cosrev.2020.100339},
  urldate = {2024-02-29},
  abstract = {Ontologies are at the core of the semantic web. As knowledge bases, they are very useful resources for many artificial intelligence applications. Ontology learning, as a research area, proposes techniques to automate several tasks of the ontology construction process to simplify the tedious work of manually building ontologies. In this paper we present the state of the art of this field. Different classes of approaches are covered (linguistic, statistical, and machine learning), including some recent ones (deep-learning-based approaches). In addition, some relevant solutions (frameworks), which offer strategies and built-in methods for ontology learning, are presented. A descriptive summary is made to point out the capabilities of the different contributions based on criteria that have to do with the produced ontology components and the degree of automation. We also highlight the challenge of evaluating ontologies to make them reliable, since it is not a trivial task in this field; it actually represents a research area on its own. Finally, we identify some unresolved issues and open questions.},
  keywords = {Deep learning,Linguistic and statistical approaches,Machine learning,Ontologies,Ontology learning},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2021\\Khadir et al. - 2021 - Ontology learning Grand tour and challenges.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\EG5Z6MEJ\\S1574013720304391.html}
}

@article{kimGENIACorpusSemantically2003,
  ids = {kimGENIACorpusSemantically2003a,kimGENIACorpusSemantically2003b},
  title = {{{GENIA}} Corpus: {{A}} Semantically Annotated Corpus for Bio-Textmining},
  author = {Kim, J.-D. and Ohta, T. and Tateisi, Y. and Tsujii, J.},
  year = {2003},
  month = jul,
  journal = {Bioinformatics},
  volume = {19},
  number = {suppl\_1},
  pages = {i180--i182},
  publisher = {Oxford University Press},
  issn = {1367-4811, 1367-4803},
  doi = {10.1093/bioinformatics/btg1023},
  urldate = {2024-03-21},
  abstract = {Motivation: Natural language processing (NLP) methods are regarded as being useful to raise the potential of text mining from biological literature. The lack of an extensively annotated corpus of this literature, however, causes a major bottleneck for applying NLP techniques. GENIA corpus is being developed to provide reference materials to let NLP techniques work for bio-textmining. Results: GENIA corpus version 3.0 consisting of 2000 MEDLINE abstracts has been released with more than 400\,000 words and almost 100\,000 annotations for biological terms.Availability: GENIA corpus is freely available at http://www-tsujii.is.s.u-tokyo.ac.jp/GENIAKeywords: Text Mining, Information Extraction, Corpus, Natural Language Processing, Computational Molecular Biology*To whom correspondence should be addressed.},
  langid = {english},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2003\\Kim et al. - 2003 - GENIA corpus—a semantically annotated corpus for b.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\FZIAQ84V\\227927.html}
}

@article{knuthSemanticsContextfreeLanguages1968,
  title = {Semantics of Context-Free Languages},
  author = {Knuth, Donald E.},
  year = {1968},
  month = jun,
  journal = {Mathematical Systems Theory},
  volume = {2},
  number = {2},
  pages = {127--145},
  issn = {0025-5661, 1433-0490},
  doi = {10.1007/BF01692511},
  urldate = {2023-10-27},
  abstract = {``Meaning'' may be assigned to a string in a context-free language by defining ``attributes'' of the symbols in a derivation tree for that string. The attributes can be defined by functions associated with each production in the grammar. This paper examines the implications of this process when some of the attributes are ``synthesized'', i.e., defined solely in terms of attributes of thedescendants of the corresponding nonterminal symbol, while other attributes are ``inherited'', i.e., defined in terms of attributes of theancestors of the nonterminal symbol. An algorithm is given which detects when such semantic rules could possibly lead to circular definition of some attributes. An example is given of a simple programming language defined with both inherited and synthesized attributes, and the method of definition is compared to other techniques for formal specification of semantics which have appeared in the literature.},
  langid = {english},
  keywords = {Computational Mathematic,Derivation Tree,Formal Specification,Programming Language,Simple Programming},
  file = {C:\Users\nhiot\OneDrive\zotero\1968\Knuth - 1968 - Semantics of context-free languages.pdf}
}

@article{konysKnowledgeRepositoryOntology2019,
  title = {Knowledge {{Repository}} of {{Ontology Learning Tools}} from {{Text}}},
  author = {Konys, Agnieszka},
  year = {2019},
  month = jan,
  journal = {Procedia Computer Science},
  series = {Knowledge-{{Based}} and {{Intelligent Information}} \& {{Engineering Systems}}: {{Proceedings}} of the 23rd {{International Conference KES2019}}},
  volume = {159},
  pages = {1614--1628},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2019.09.332},
  urldate = {2024-02-29},
  abstract = {Ontologies are one of the fundamental elements of the Semantic Web, and they have gained a lot of popularity and recognition because they are viewed as the answer to the need for interoperable semantics in modern information systems. The intermingling of techniques in areas such as natural language processing, information retrieval, machine learning, data mining, and knowledge representation provide a lot of possibilities for development of ontology learning approaches. A rise in focus on the ability to cope with the scale of Web data required for ontology learning forces the potential growth of cross-language research, emphasizing the automatic or semi-automatic generation of the tools dedicated to text mining and information extraction. This paper presents the integration of ontology learning tools from text in the knowledge repository to incorporate the applied techniques and outputs of an ontology learning algorithm into the one complex multifunctional solution. The proposed knowledge repository covers various applicability of existing techniques of learning ontologies from text, and offers competency question-based reasoning mechanism for individuals to specify their profiles of ontology learning tools. The validation stage is also provided in the form of applied reasoning.},
  keywords = {Knowledge repository,Learning techniques,Ontology learning tools,Ontology learninng from text},
  file = {C:\Users\nhiot\OneDrive\zotero\2019\Konys - 2019 - Knowledge Repository of Ontology Learning Tools fr.pdf}
}

@inproceedings{laffertyConditionalRandomFields2001,
  ids = {Lafferty01conditionalrandom},
  title = {Conditional Random Fields: {{Probabilistic}} Models for Segmenting and Labeling Sequence Data},
  shorttitle = {Conditional Random Fields},
  booktitle = {Icml},
  author = {Lafferty, John and McCallum, Andrew and Pereira, Fernando},
  year = {2001},
  volume = {1},
  pages = {3},
  publisher = {Williamstown, MA},
  url = {http://isoft.postech.ac.kr/{\textasciitilde}gblee/Course/CS704/LectureNotes/ConditionalRandomFields.pdf},
  urldate = {2024-03-21},
  keywords = {⛔ No DOI found,nosource},
  file = {C:\Users\nhiot\OneDrive\zotero\2001\Lafferty et al. - 2001 - Conditional random fields Probabilistic models fo.pdf}
}

@inproceedings{langlaisEnrichissementLexiqueBilingue2007,
  title = {{Enrichissement d'un lexique bilingue par analogie}},
  booktitle = {{Actes de la 14{\`e}me conf{\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs}},
  author = {Langlais, Philippe and Patry, Alexandre},
  year = {2007},
  month = jun,
  pages = {101--110},
  publisher = {IRIT / ATALA},
  address = {Toulouse, France},
  url = {https://aclanthology.org/2007.jeptalnrecital-long.9},
  urldate = {2023-10-23},
  abstract = {La pr{\'e}sence de mots inconnus dans les applications langagi{\`e}res repr{\'e}sente un d{\'e}fi de taille bien connu auquel n'{\'e}chappe pas la traduction automatique. Les syst{\`e}mes professionnels de traduction offrent {\`a} cet effet {\`a} leurs utilisateurs la possibilit{\'e} d'enrichir un lexique de base avec de nouvelles entr{\'e}es. R{\'e}cemment, Stroppa et Yvon (2005) d{\'e}montraient l'int{\'e}r{\^e}t du raisonnement par analogie pour l'analyse morphologique d'une langue. Dans cette {\'e}tude, nous montrons que le raisonnement par analogie offre {\'e}galement une r{\'e}ponse adapt{\'e}e au probl{\`e}me de la traduction d'entr{\'e}es lexicales inconnues.},
  langid = {french},
  keywords = {nosource},
  file = {C:\Users\nhiot\OneDrive\zotero\2007\Langlais et Patry - 2007 - Enrichissement d'un lexique bilingue par analogie.pdf}
}

@inproceedings{lausenSPARQLingConstraintsRDF2008,
  title = {{{SPARQLing}} Constraints for {{RDF}}},
  booktitle = {{{EDBT}}, 11th International Conference on Extending Database Technology, France, Proceedings},
  author = {Lausen, Georg and Meier, Michael and Schmidt, Michael},
  year = {2008},
  month = mar,
  series = {{{EDBT}} '08},
  pages = {499--509},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/1353343.1353404},
  urldate = {2023-12-24},
  abstract = {The goal of the Semantic Web is to support semantic interoperability between applications exchanging data on the web. The idea heavily relies on data being made available in machine readable format, using semantic markup languages. In this regard, the W3C has standardized RDF as the basic markup language for the Semantic Web. In contrast to relational databases, where data relationships are implicitly given by schema information as well as primary and foreign key constraints, relationships in semantic markup languages are made explicit. When mapping relational data into RDF, it is desirable to maintain the information implied by the origin constraints. As an improvement over existing approaches, our scheme allows for translating conventional databases into RDF without losing general constraints and vital key information. As much as in the relational model, those information are indispensable for data consistency and, as shown by example, can serve as a basis for semantic query optimization. We underline the practicability of our approach by showing that SPARQL, the most popular query language for RDF, can be used as a constraint language, akin to SQL in the relational context. As a theoretical contribution, we also discuss satisfiability for interesting classes of constraints and combinations thereof.},
  isbn = {978-1-59593-926-5},
  langid = {english},
  keywords = {nosource},
  file = {C:\Users\nhiot\OneDrive\zotero\2008\Lausen et al. - 2008 - SPARQLing constraints for RDF.pdf}
}

@misc{LearningMultilingualNamed2017,
  title = {Learning Multilingual Named Entity Recognition from {{Wikipedia}}},
  year = {2017},
  month = oct,
  publisher = {figshare},
  doi = {10.6084/m9.figshare.5462500.v1},
  urldate = {2024-03-21},
  abstract = {This is the data associated with Joel Nothman, Nicky Ringland, Will Radford, Tara Murphy and James R. Curran (2013), "Learning multilingual named entity recognition from Wikipedia", Artificial Intelligence 194 (DOI: 10.1016/j.artint.2012.03.006). A preprint is included here as wikiner-preprint.pdfThis data was originally available at http://schwa.org/resources (which linked to http://schwa.org/projects/resources/wiki/Wikiner).The .bz2 files are NER training corpora produced as reported in the Artificial Intelligence paper. wp2 and wp3 are differentiated by wp3 using a higher level of link inference. They use a pipe-delimited format that can be converted to CoNLL 2003 format with system2conll.pl.nothman08types.tsv is a manual classification of articles first used in Joel Nothman, James R. Curran and Tara Murphy (2008), "Transforming Wikipedia into Named Entity Training Data", In Proceedings of the Australasian Language Technology Association Workshop 2008. http://aclanthology.coli.uni-saarland.de/pdf/U/U08/U08-1016.pdfpopular.tsv and random.tsv are manual article classifications developed for the Artifiical Intelligence paper based on different strategies for sampling articles from Wikipedia in order to account for Wikipedia's biased distribution (see that paper). scheme.tsv maps these fine-grained labels to coarser annotations including CoNLL 2003-style.wikigold.conll.txt is a manual NER annotation of some Wikipedia text as presented in Dominic Balasuriya and Nicky  Ringland and Joel Nothman and Tara Murphy and James R. Curran (2009), in Proceedings of the 2009 Workshop on The People's Web Meets NLP: Collaboratively Constructed Semantic Resources (http://www.aclweb.org/anthology/W/W09/W09-3302).See also corpora produced similarly in an enhanced version of this work work (Pan et al., "Cross-lingual Name Tagging and Linking for 282 Languages", ACL 2017) at http://nlp.cs.rpi.edu/wikiann/.},
  langid = {english},
  file = {C:\Users\nhiot\Zotero\storage\IDT9KVAN\5462500.html}
}

@inproceedings{libkinIncompleteDataWhat2014,
  title = {Incomplete Data: What Went Wrong, and How to Fix It},
  shorttitle = {Incomplete Data},
  booktitle = {Proceedings of the 33rd {{ACM SIGMOD-SIGACT-SIGART}} Symposium on {{Principles}} of Database Systems},
  author = {Libkin, Leonid},
  year = {2014},
  month = jun,
  series = {{{PODS}} '14},
  pages = {1--13},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2594538.2594561},
  urldate = {2023-08-03},
  abstract = {Incomplete data is ubiquitous: the more data we accumulate and the more widespread tools for integrating and exchanging data become, the more instances of incompleteness we have. And yet the subject is poorly handled by both practice and theory. Many queries for which students get full marks in their undergraduate courses will not work correctly in the presence of incomplete data, but these ways of evaluating queries are cast in stone -- SQL standard. We have many theoretical results on handling incomplete data but they are, by and large, about showing high complexity bounds, and thus are often dismissed by practitioners. Even worse, we have a basic theoretical notion of what it means to answer queries over incomplete data, and yet this is not at all what practical systems do. Is there a way out of this predicament? Can we have a theory of incompleteness that will appeal to theoreticians and practitioners alike, by explaining incompleteness and being at the same time implementable and useful for applications? After giving a critique of both the practice and the theory of handling incompleteness in databases, the paper outlines a possible way out of this crisis. The key idea is to combine three hitherto used approaches to incompleteness: one based on certain answers and representation systems, one based on viewing incomplete databases as logical theories, and one based on orderings expressing relative value of information.},
  isbn = {978-1-4503-2375-8},
  keywords = {incomplete information,query evaluation},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2014\\Libkin - 2014 - Incomplete data what went wrong, and how to fix i.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\VN5CSSYT\\2594538.html}
}

@article{libkinSQLThreeValuedLogic2016,
  title = {{{SQL}}'s {{Three-Valued Logic}} and {{Certain Answers}}},
  author = {Libkin, Leonid},
  year = {2016},
  journal = {ACM Transactions on Database Systems (TODS)},
  volume = {41},
  number = {1},
  pages = {1--28},
  publisher = {ACM New York, NY, USA},
  issn = {0362-5915},
  doi = {10.1145/2877206},
  abstract = {The goal of the article is to bridge the difference between theoretical and practical approaches to answering queries over databases with nulls. Theoretical research has long ago identified the notion of correctness of query answering over incomplete data: one needs to find certain answers, which are true regardless of how incomplete information is interpreted. This serves as the notion of correctness of query answering, but carries a huge complexity tag. In practice, on the other hand, query answering must be very efficient, and to achieve this, SQL uses three-valued logic for evaluating queries on databases with nulls. Due to the complexity mismatch, the two approaches cannot coincide, but perhaps they are related in some way. For instance, does SQL always produce answers we can be certain about? This is not so: SQL's and certain answers semantics could be totally unrelated. We show, however, that a slight modification of the three-valued semantics for relational calculus queries can provide the required certainty guarantees. The key point of the new scheme is to fully utilize the three-valued semantics, and classify answers not into certain or noncertain, as was done before, but rather into certainly true, certainly false, or unknown. This yields relatively small changes to the evaluation procedure, which we consider at the level of both declarative (relational calculus) and procedural (relational algebra) queries. These new evaluation procedures give us certainty guarantees even for queries returning tuples with null values.},
  keywords = {certain answers,incomplete information,Null values,query evaluation,three-valued logic},
  file = {C:\Users\nhiot\OneDrive\zotero\2016\Libkin - 2016 - SQL’s Three-Valued Logic and Certain Answers.pdf}
}

@inproceedings{liEfficientOnePassEndtoEnd2020,
  title = {Efficient {{One-Pass End-to-End Entity Linking}} for {{Questions}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Li, Belinda Z. and Min, Sewon and Iyer, Srinivasan and Mehdad, Yashar and Yih, Wen-tau},
  year = {2020},
  month = oct,
  eprint = {2010.02413},
  primaryclass = {cs},
  pages = {6433--6441},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.emnlp-main.522},
  urldate = {2024-04-05},
  abstract = {We present ELQ, a fast end-to-end entity linking model for questions, which uses a biencoder to jointly perform mention detection and linking in one pass. Evaluated on WebQSP and GraphQuestions with extended annotations that cover multiple entities per question, ELQ outperforms the previous state of the art by a large margin of +12.7\% and +19.6\% F1, respectively. With a very fast inference time (1.57 examples/s on a single CPU), ELQ can be useful for downstream question answering systems. In a proof-of-concept experiment, we demonstrate that using ELQ significantly improves the downstream QA performance of GraphRetriever (arXiv:1911.03868). Code and data available at https://github.com/facebookresearch/BLINK/tree/master/elq},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2020\\Li et al. - 2020 - Efficient One-Pass End-to-End Entity Linking for Q2.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\F2IWAI7E\\2010.html}
}

@article{linkArithmeticTheoryConsistency2002,
  title = {Towards an {{Arithmetic Theory}} of {{Consistency Enforcement}} Based on {{Preservation}} of {$\delta$}-Constraints},
  author = {Link, Sebastian and Schewe, Klaus-Dieter},
  year = {2002},
  month = jan,
  journal = {Electronic Notes in Theoretical Computer Science},
  series = {{{CATS}}'02, {{Computing}}: The {{Australasian Theory Symposium}}},
  volume = {61},
  pages = {64--83},
  issn = {1571-0661},
  doi = {10.1016/S1571-0661(04)00306-8},
  urldate = {2023-08-16},
  abstract = {Consistency Enforcement provides an alternative theory to common verification techniques within formal specification languages. We consider specifications in the form of guarded commands. The basic idea is then to replace a program specification S by its greatest consistent specialization (GCS) SI which is provably consistent with respect to a given static constraint I, preserves the effects of S according to a specialization order and is maximal with these properties. The theory has been shown to provide several strengths. In particular, the enforcement process for a huge class of complex specifications can be reduced to its basic components. Moreover, the result can be obtained sequentially and is independent from the order of the given constraints. In addition, arithmetic logic has been used to show that GCSs can be efficiently computed for a reasonably large class of program specifications and invariants. However, all results have been achieved with respect to the underlying specialization order. The simplicity of this order reveals some obvious weaknesses. In this paper, we show how the specialization order can be replaced by the notion of {$\delta$}-constraints. Specialization of a program specification S turns out to be equivalent to the preservation of all {$\delta$}-constraints on the underlying state space of S. Obviously, this enables us to weaken the specialization order towards the preservation of certain {$\delta$}-constraints. We define maximal consistent effect preservers (MCEs), show that these are closely related to GCSs and prove that MCEs can be obtained sequentially and independently from the order of a given set of static constraints. This backs up the conjecture that the notion of MCEs leads towards a tailored theory of consistency enforcement.},
  keywords = {arithmetic logic,consistency,constraints,formal specifications,GCS,guarded commands,MCE},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2002\\Link et Schewe - 2002 - Towards an Arithmetic Theory of Consistency Enforc.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\LBEYDF3M\\S1571066104003068.html}
}

@article{linkArithmeticTheoryConsistency2002a,
  title = {An Arithmetic Theory of Consistency Enforcement},
  author = {Link, Sebastian and Schewe, Klaus-Dieter},
  year = {2002},
  journal = {Acta Cybern.},
  volume = {15},
  number = {3},
  pages = {379--416},
  keywords = {⛔ No DOI found,nosource}
}

@article{linSimilarityMeasureText2014,
  title = {A Similarity Measure for Text Classification and Clustering},
  author = {Lin, Yung-Shen and Jiang, Jung-Yi and Lee, Shie-Jue},
  year = {2014},
  month = jul,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {26},
  number = {7},
  pages = {1575--1590},
  issn = {1558-2191},
  doi = {10.1109/tkde.2013.19},
  abstract = {Measuring the similarity between documents is an important operation in the text processing field. In this paper, a new similarity measure is proposed. To compute the similarity between two documents with respect to a feature, the proposed measure takes the following three cases into account: a) The feature appears in both documents, b) the feature appears in only one document, and c) the feature appears in none of the documents. For the first case, the similarity increases as the difference between the two involved feature values decreases. Furthermore, the contribution of the difference is normally scaled. For the second case, a fixed value is contributed to the similarity. For the last case, the feature has no contribution to the similarity. The proposed measure is extended to gauge the similarity between two sets of documents. The effectiveness of our measure is evaluated on several real-world data sets for text classification and clustering problems. The results show that the performance obtained by the proposed measure is better than that achieved by other measures.},
  keywords = {accuracy,Approximation methods,classifiers,clustering algorithms,Clustering algorithms,Document classification,document clustering,Educational institutions,entropy,Euclidean distance,Text processing,Vectors},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2014\\Lin et al. - 2014 - A similarity measure for text classification and c.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\EEYQAHWW\\6420834.html}
}

@article{lipscombMedicalSubjectHeadings2000,
  title = {Medical {{Subject Headings}} ({{MeSH}})},
  author = {Lipscomb, Carolyn E.},
  year = {2000},
  month = jul,
  journal = {Bulletin of the Medical Library Association},
  volume = {88},
  number = {3},
  pages = {265--266},
  publisher = {Medical Library Association},
  issn = {0025-7338},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC35238/},
  urldate = {2023-09-25},
  langid = {english},
  pmcid = {PMC35238},
  pmid = {10928714},
  keywords = {{History, 20th Century},⛔ No DOI found,MEDLARS,MEDLINE,National Library of Medicine (U.S.),Subject Headings,Unified Medical Language System,United States},
  annotation = {QID: Q30895814},
  file = {C:\Users\nhiot\OneDrive\zotero\2000\Lipscomb - 2000 - Medical Subject Headings (MeSH).pdf}
}

@article{liuDBpediaBasedEntityLinking2018,
  title = {{{DBpedia-Based Entity Linking}} via {{Greedy Search}} and {{Adjusted Monte Carlo Random Walk}}},
  author = {Liu, Ming and Chen, Lei and Liu, Bingquan and Zheng, Guidong and Zhang, Xiaoming},
  year = {2018},
  month = apr,
  journal = {ACM Transactions on Information Systems},
  volume = {36},
  number = {2},
  pages = {1--34},
  issn = {1046-8188, 1558-2868},
  doi = {10.1145/3086703},
  urldate = {2024-04-05},
  abstract = {Facing a large amount of entities appearing on the web, entity linking has recently become useful. It assigns an entity from a resource to one name mention to help users grasp the meaning of this name mention. Unfortunately, many possible entities can be assigned to one name mention. Apparently, the usually co-occurring name mentions are related and can be considered together to determine their best assignments. This approach is called collective entity linking and is often conducted based on entity graph. However, traditional collective entity linking methods either consume much time due to the large scale of entity graph or obtain low accuracy due to simplifying graph. To improve both accuracy and efficiency, this article proposes a novel collective entity linking algorithm. It first constructs an entity graph by connecting any two related entities, and then a probability-based objective function is proposed on this graph to ensure the high accuracy of the linking result. Via this function, we convert entity linking to the process of finding the nodes with the highest PageRank Values. Greedy search and an adjusted Monte Carlo random walk are proposed to fulfill this work. Experimental results demonstrate that our algorithm performs much better than traditional linking methods.},
  langid = {english}
}

@inproceedings{maedcheTexttoontoOntologyExtraction2001,
  title = {The Text-to-onto Ontology Extraction and Maintenance Environment},
  booktitle = {Proceedings of the {{ICDM-Workshop}} on {{Integrating Data Mining}} and {{Knowledge Management}}, {{San Jose}}, {{California}}, {{USA}}},
  author = {Maedche, Alexander and Volz, Raphael},
  year = {2001},
  keywords = {⛔ No DOI found}
}

@phdthesis{mahfoudhAdaptationOntologiesAvec2015,
  title = {{Adaptation d'ontologies avec les grammaires de graphes typ{\'e}s : {\'e}volution et fusion}},
  shorttitle = {{Adaptation d'ontologies avec les grammaires de graphes typ{\'e}s}},
  author = {Mahfoudh, Mariem},
  year = {2015},
  month = may,
  url = {https://tel.archives-ouvertes.fr/tel-01528579},
  urldate = {2019-03-07},
  abstract = {{\'E}tant une repr{\'e}sentation formelle et explicite des connaissances d'un domaine, les ontologies font r{\'e}guli{\`e}rement l'objet de nombreux changements et ont ainsi besoin d'{\^e}tre constamment adapt{\'e}es pour notamment pouvoir {\^e}tre r{\'e}utilis{\'e}es et r{\'e}pondre aux nouveaux besoins. Leur r{\'e}utilisation peut prendre diff{\'e}rentes formes ({\'e}volution, alignement, fusion, etc.), et pr{\'e}sente plusieurs verrous scientifiques. L'un des plus importants est la pr{\'e}servation de la consistance de l'ontologie lors de son changement. Afin d'y r{\'e}pondre, nous nous int{\'e}ressons dans cette th{\`e}se {\`a} {\'e}tudier les changements ontologiques et proposons un cadre formel capable de faire {\'e}voluer et de fusionner des ontologies sans affecter leur consistance. Premi{\`e}rement, nous proposons TGGOnto (Typed Graph Grammars for Ontologies), un nouveau formalisme permettant la repr{\'e}sentation des ontologies et leurs changements par les grammaires de graphes typ{\'e}s. Un couplage entre ces deux formalismes est d{\'e}fini afin de profiter des concepts des grammaires de graphes, notamment les NAC (Negative Application Conditions), pour la pr{\'e}servation de la consistance de l'ontologie adapt{\'e}e.Deuxi{\`e}mement, nous proposons EvOGG (Evolving Ontologies with Graph Grammars), une approche d'{\'e}volution d'ontologies qui se base sur le formalisme GGTOnto et traite les inconsistances d'une mani{\`e}re a priori. Nous nous int{\'e}ressons aux ontologies OWL et nous traitons {\`a} la fois : (1) l'enrichissement d'ontologies en {\'e}tudiant leur niveau structurel et (2) le peuplement d'ontologies en {\'e}tudiant les changements qui affectent les individus et leurs assertions. L'approche EvOGG d{\'e}finit des changements ontologiques de diff{\'e}rents types ({\'e}l{\'e}mentaires, compos{\'e}es et complexes) et assure leur impl{\'e}mentation par l'approche alg{\'e}brique de transformation de graphes, SPO (Simple PushOut). Troisi{\`e}mement, nous proposons GROM (Graph Rewriting for Ontology Merging), une approche de fusion d'ontologies capable d'{\'e}viter les redondances de donn{\'e}es et de diminuer les conflits dans le r{\'e}sultat de fusion. L'approche propos{\'e}e se d{\'e}compose en trois {\'e}tapes : (1) la recherche de similarit{\'e} entre concepts en se basant sur des techniques syntaxiques, structurelles et s{\'e}mantiques ; (2) la fusion d'ontologies par l'approche alg{\'e}brique SPO ; (3) l'adaptation de l'ontologie globale r{\'e}sultante par le biais des r{\`e}gles de r{\'e}{\'e}criture de graphes.Afin de valider les travaux men{\'e}s dans cette th{\`e}se, nous avons d{\'e}velopp{\'e} plusieurs outils open source bas{\'e}s sur l'outil AGG (Attributed Graph Grammar). Ces outils ont {\'e}t{\'e} appliqu{\'e}s sur un ensemble d'ontologies, essentiellement sur celles d{\'e}velopp{\'e}es dans le cadre du projet europ{\'e}en CCAlps (Creatives Companies in Alpine Space) qui a financ{\'e} les travaux de cette th{\`e}se.},
  langid = {french},
  school = {Universit{\'e} de Haute Alsace-Mulhouse},
  keywords = {nosource},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2015\\Mahfoudh - 2015 - Adaptation d'ontologies avec les grammaires de gra.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\ZHAL55QA\\tel-01528579.html}
}

@article{mahfoudhAlgebraicGraphTransformations2015,
  title = {Algebraic Graph Transformations for Formalizing Ontology Changes and Evolving Ontologies},
  author = {Mahfoudh, Mariem and Forestier, Germain and Thiry, Laurent and Hassenforder, Michel},
  year = {2015},
  journal = {Knowledge-Based Systems},
  volume = {73},
  pages = {212--226},
  issn = {0950-7051},
  doi = {10.1016/j.knosys.2014.10.007},
  keywords = {AGG,Algebraic graph transformations,Consistency,nosource,Ontology evolution,Typed Graph Grammars},
  annotation = {QID: Q114826529},
  file = {C:\Users\nhiot\OneDrive\zotero\2015\Mahfoudh et al. - 2015 - Algebraic graph transformations for formalizing on.pdf}
}

@article{maierTestingImplicationsData1979,
  title = {Testing Implications of Data Dependencies},
  author = {Maier, David and Mendelzon, Alberto O. and Sagiv, Yehoshua},
  year = {1979},
  month = dec,
  journal = {ACM Transactions on Database Systems (TODS)},
  volume = {4},
  number = {4},
  pages = {455--469},
  publisher = {ACM New York, NY, USA},
  issn = {0362-5915},
  doi = {10.1145/320107.320115},
  urldate = {2023-08-08},
  abstract = {Presented is a computation method---the chase---for testing implication of data dependencies by a set of data dependencies. The chase operates on tableaux similar to those of Aho, Sagiv, and Ullman. The chase includes previous tableau computation methods as special cases. By interpreting tableaux alternately as mappings or as templates for relations, it is possible to test implication of join dependencies (including multivalued dependencies) and functional dependencies by a set of dependencies.},
  keywords = {chase,data dependencies,functional dependencies,join dependencies,multivalued dependencies,relational databases,tableaux},
  annotation = {QID: Q114614050},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1979\\Maier et al. - 1979 - Testing implications of data dependencies.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\5DK3BYZS\\320107.html}
}

@inproceedings{maillotConsistencyEvaluationRDF2014,
  title = {Consistency {{Evaluation}} of {{RDF Data}}: {{How Data}} and {{Updates}} Are {{Relevant}}},
  shorttitle = {Consistency {{Evaluation}} of {{RDF Data}}},
  booktitle = {Tenth International Conference on Signal-Image Technology and Internet-Based Systems, {{SITIS}} 2014, Marrakech, Morocco, November 23-27, 2014},
  author = {Maillot, Pierre and Raimbault, Thomas and Genest, David and Loiseau, St{\'e}phane},
  year = {2014},
  month = nov,
  pages = {187--193},
  publisher = {IEEE},
  doi = {10.1109/SITIS.2014.39},
  abstract = {Trust and quality maintenance have always been problematic in the Semantic Web RDF bases. Numerous propositions to address these problems of data integration have been made, either based on ontologies or on additional metadata. However ontologies suffer from a adaptation speed slower than the data evolution speed and metadata requires ad-hoc manipulations of data by addition of extra-data. In this article we propose an original approach, based exclusively on data from the base, to evaluate the consistency of a candidate update to a RDF base, and finally to know if this update is relevant to the base. Our approach is inspired by case-based reasoning and uses similarity evaluation and query relaxation methods to compare a candidate update to the data from the base. If the modifications of a candidate update make the target part of the base more similar to other part (s) of the base, then this candidate update is considered consistent with the base and can be applied.},
  keywords = {Case-based reasoning,Cognition,Consistency,Context,Data Integration,Databases,Ontologies,Ontology,Resource description framework,Semantic Web,Similarity,Weight measurement},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2014\\Maillot et al. - 2014 - Consistency Evaluation of RDF Data How Data and U.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\7463EG8S\\7081546.html}
}

@article{maliFACTDMFrameworkAutomated2024,
  title = {{{FACT-DM}}: {{A Framework}} for {{Automated Cost-Based Data Model Transformation}}},
  shorttitle = {{{FACT-DM}}},
  author = {Mali, Jihane and Ahvar, Shohreh and Atigui, Faten and Azough, Ahmed and Travers, Nicolas},
  year = {2024},
  url = {https://openproceedings.org/2024/conf/edbt/paper-244.pdf},
  urldate = {2024-04-26},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2024\Mali et al. - 2024 - FACT-DM A Framework for Automated Cost-Based Data.pdf}
}

@incollection{maurelActesTALN20012001,
  title = {Actes de {{TALN}} 2001 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 2001 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Maurel, Denis},
  year = {2001},
  month = jul,
  publisher = {Universit{\'e} de Tours / ATALA},
  address = {Tours},
  keywords = {nosource}
}

@inproceedings{mccloskySelfTrainingBiomedicalParsing2008,
  title = {Self-{{Training}} for {{Biomedical Parsing}}},
  booktitle = {Proceedings of {{ACL-08}}: {{HLT}}, {{Short Papers}}},
  author = {McClosky, David and Charniak, Eugene},
  editor = {Moore, Johanna D. and Teufel, Simone and Allan, James and Furui, Sadaoki},
  year = {2008},
  month = jun,
  pages = {101--104},
  publisher = {Association for Computational Linguistics},
  address = {Columbus, Ohio},
  url = {https://aclanthology.org/P08-2026},
  urldate = {2024-03-21},
  file = {C:\Users\nhiot\Zotero\storage\Y83PTDGH\McClosky et Charniak - 2008 - Self-Training for Biomedical Parsing.pdf}
}

@inproceedings{mihalceaTextRankBringingOrder2004,
  ids = {mihalceaTextrankBringingOrder2004},
  title = {{{TextRank}}: {{Bringing Order}} into {{Text}}},
  shorttitle = {{{TextRank}}},
  booktitle = {Proceedings of the 2004 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Mihalcea, Rada and Tarau, Paul},
  editor = {Lin, Dekang and Wu, Dekai},
  year = {2004},
  month = jul,
  pages = {404--411},
  publisher = {Association for Computational Linguistics},
  address = {Barcelona, Spain},
  url = {https://aclanthology.org/W04-3252},
  urldate = {2024-03-22},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2004\\Mihalcea et Tarau - 2004 - Textrank Bringing order into text.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\NVUULF2E\\Mihalcea et Tarau - 2004 - TextRank Bringing Order into Text.pdf}
}

@inproceedings{mihovDirectConstructionMinimal2001,
  title = {Direct {{Construction}} of {{Minimal Acyclic Subsequential Transducers}}},
  booktitle = {Implementation and Application of Automata, 5th International Conference, {{CIAA}} 2000, London, Ontario, Canada, July 24-25, 2000, Revised Papers},
  author = {Mihov, Stoyan and Maurel, Denis},
  year = {2001},
  series = {Lecture Notes in Computer Science},
  volume = {2088},
  pages = {217--229},
  publisher = {Springer},
  doi = {10.1007/3-540-44674-5_18},
  abstract = {This paper presents an algorithm for direct building of minimal acyclic subsequential transducer, which represents a finite relation given as a sorted list of words with their outputs. The algorithm constructs the minimal transducer directly without constructing intermediate tree-like or pseudo-minimal transducers. In NLP applications our algorithm provides significantly better efficiency than the other algorithms building minimal transducer for large-scale natural language dictionaries. Some experimental comparisons are presented at the end of the paper.},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2001\\Mihov et Maurel - 2001 - Direct Construction of Minimal Acyclic Subsequenti.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\5XISYCIH\\summary.html}
}

@inproceedings{minardDOINGDEFTCascade2020,
  title = {{DOING@ DEFT: cascade de CRF pour l'annotation d'entit{\'e}s cliniques imbriqu{\'e}es}},
  shorttitle = {{DOING@DEFT}},
  booktitle = {{6e conf{\'e}rence conjointe Journ{\'e}es d'{\'E}tudes sur la Parole (JEP, 33e {\'e}dition), Traitement Automatique des Langues Naturelles (TALN, 27e {\'e}dition), Rencontre des {\'E}tudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (R{\'E}CITAL, 22e {\'e}dition). Atelier D{\'E}fi Fouille de Textes}},
  author = {Minard, Anne-Lyse and Roques, Andr{\'e}ane and Hiot, Nicolas and {Halfeld-Ferrari}, Mirian and Savary, Agata},
  year = {2020},
  month = jun,
  pages = {66--78},
  publisher = {ATALA et AFCP},
  address = {Nancy, France},
  url = {https://hal.science/hal-02784743},
  urldate = {2023-08-07},
  abstract = {Cet article pr{\'e}sente le syst{\`e}me d{\'e}velopp{\'e} par l'{\'e}quipe DOING pour la campagne d'{\'e}valuation DEFT 2020 portant sur la similarit{\'e} s{\'e}mantique et l'extraction d'information fine. L'{\'e}quipe a particip{\'e} uniquement {\`a} la t{\^a}che 3 : ``extraction d'information''. Nous avons utilis{\'e} une cascade de CRF pour annoter les diff{\'e}rentes informations {\`a} rep{\'e}rer. Nous nous sommes concentr{\'e}s sur la question de l'imbrication des entit{\'e}s et de la pertinence d'un type d'entit{\'e} pour apprendre {\`a} reconna{\^i}tre un autre. Nous avons {\'e}galement test{\'e} l'utilisation d'une ressource externe, MedDRA, pour am{\'e}liorer les performances du syst{\`e}me et d'un pipeline plus complexe mais ne g{\'e}rant pas l'imbrication des entit{\'e}s. Nous avons soumis 3 runs et nous obtenons en moyenne sur toutes les classes des F-mesures de 0,64, 0,65 et 0,61.},
  copyright = {All rights reserved},
  hal_id = {hal-02784743},
  hal_version = {v3},
  langid = {french},
  keywords = {⛔ No DOI found,apprentissage automatique,cas cliniques,CRF.,entit{\'e}s cliniques,entit{\'e}s imbriqu{\'e}es,extraction d'information fine,me,nosource},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2020\\Minard et al. - 2020 - DOING@ DEFT cascade de CRF pour l'annotation d'en.pdf;C\:\\Users\\nhiot\\OneDrive\\zotero\\2020\\Minard et al. - 2020 - DOING@ DEFT cascade de CRF pour l'annotation d'en2.pdf;C\:\\Users\\nhiot\\OneDrive\\zotero\\2020\\Minard et al. - 2020 - DOING@ DEFT cascade de CRF pour l'annotation d'en3.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\DY6IJYRR\\hal-02784743.html}
}

@inproceedings{mintzDistantSupervisionRelation2009,
  title = {Distant Supervision for Relation Extraction without Labeled Data},
  booktitle = {Proceedings of the {{Joint Conference}} of the 47th {{Annual Meeting}} of the {{ACL}} and the 4th {{International Joint Conference}} on {{Natural Language Processing}} of the {{AFNLP}}: {{Volume}} 2 - {{ACL-IJCNLP}} '09},
  author = {Mintz, Mike and Bills, Steven and Snow, Rion and Jurafsky, Dan},
  year = {2009},
  volume = {2},
  pages = {1003},
  publisher = {Association for Computational Linguistics},
  address = {Suntec, Singapore},
  doi = {10.3115/1690219.1690287},
  urldate = {2021-02-16},
  abstract = {Modern models of relation extraction for tasks like ACE are based on supervised learning of relations from small hand-labeled corpora. We investigate an alternative paradigm that does not require labeled corpora, avoiding the domain dependence of ACEstyle algorithms, and allowing the use of corpora of any size. Our experiments use Freebase, a large semantic database of several thousand relations, to provide distant supervision. For each pair of entities that appears in some Freebase relation, we find all sentences containing those entities in a large unlabeled corpus and extract textual features to train a relation classifier. Our algorithm combines the advantages of supervised IE (combining 400,000 noisy pattern features in a probabilistic classifier) and unsupervised IE (extracting large numbers of relations from large corpora of any domain). Our model is able to extract 10,000 instances of 102 relations at a precision of 67.6\%. We also analyze feature performance, showing that syntactic parse features are particularly helpful for relations that are ambiguous or lexically distant in their expression.},
  isbn = {978-1-932432-46-6},
  langid = {english},
  file = {C:\Users\nhiot\OneDrive\zotero\2009\Mintz et al. - 2009 - Distant supervision for relation extraction withou.pdf}
}

@inproceedings{mondalMedicalEntityLinking2019,
  ids = {mondalMedicalEntityLinking2019a},
  title = {Medical {{Entity Linking}} Using {{Triplet Network}}},
  booktitle = {Proceedings of the 2nd {{Clinical Natural Language Processing Workshop}}},
  author = {Mondal, Ishani and Purkayastha, Sukannya and Sarkar, Sudeshna and Goyal, Pawan and Pillai, Jitesh and Bhattacharyya, Amitava and Gattu, Mahanandeeshwar},
  editor = {Rumshisky, Anna and Roberts, Kirk and Bethard, Steven and Naumann, Tristan},
  year = {2019},
  month = jun,
  eprint = {2012.11164},
  primaryclass = {cs},
  pages = {95--100},
  publisher = {Association for Computational Linguistics},
  address = {Minneapolis, Minnesota, USA},
  doi = {10.18653/v1/W19-1912},
  urldate = {2024-04-05},
  abstract = {Entity linking (or Normalization) is an essential task in text mining that maps the entity mentions in the medical text to standard entities in a given Knowledge Base (KB). This task is of great importance in the medical domain. It can also be used for merging different medical and clinical ontologies. In this paper, we center around the problem of disease linking or normalization. This task is executed in two phases: candidate generation and candidate scoring. In this paper, we present an approach to rank the candidate Knowledge Base entries based on their similarity with disease mention. We make use of the Triplet Network for candidate ranking. While the existing methods have used carefully generated sieves and external resources for candidate generation, we introduce a robust and portable candidate generation scheme that does not make use of the hand-crafted rules. Experimental results on the standard benchmark NCBI disease dataset demonstrate that our system outperforms the prior methods by a significant margin.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2019\\Mondal et al. - 2019 - Medical Entity Linking using Triplet Network.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\HXSGIE8Y\\2012.html}
}

@inproceedings{moranteLearningScopeNegation2008,
  title = {Learning the Scope of Negation in Biomedical Texts},
  booktitle = {Proceedings of the {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} - {{EMNLP}} '08},
  author = {Morante, Roser and Liekens, Anthony and Daelemans, Walter},
  year = {2008},
  pages = {715},
  publisher = {Association for Computational Linguistics},
  address = {Honolulu, Hawaii},
  doi = {10.3115/1613715.1613805},
  urldate = {2023-10-06},
  abstract = {In this paper we present a machine learning system that finds the scope of negation in biomedical texts. The system consists of two memory-based engines, one that decides if the tokens in a sentence are negation signals, and another that finds the full scope of these negation signals. Our approach to negation detection differs in two main aspects from existing research on negation. First, we focus on finding the scope of negation signals, instead of determining whether a term is negated or not. Second, we apply supervised machine learning techniques, whereas most existing systems apply rule-based algorithms. As far as we know, this way of approaching the negation scope finding task is novel.},
  langid = {english},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2008\\Morante et al. - 2008 - Learning the scope of negation in biomedical texts.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\Z6Q4NUMF\\learning-the-scope-of-negation-Q3PJj6.html}
}

@inproceedings{morinAutomaticAcquisitionSemantic1999,
  title = {Automatic Acquisition of Semantic Relations between Terms from Technical Corpora},
  booktitle = {Proc. of the {{Fifth International Congress}} on {{Terminology}} and {{Knowledge Engineering-TKE}}'99},
  author = {Morin, Emmanuel},
  year = {1999},
  keywords = {⛔ No DOI found}
}

@article{nasarNamedEntityRecognition2021,
  title = {Named {{Entity Recognition}} and {{Relation Extraction}}: {{State-of-the-Art}}},
  shorttitle = {Named {{Entity Recognition}} and {{Relation Extraction}}},
  author = {Nasar, Zara and Jaffry, Syed Waqar and Malik, Muhammad Kamran},
  year = {2021},
  month = feb,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {1},
  pages = {1--39},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3445965},
  urldate = {2024-03-20},
  abstract = {With the advent of Web 2.0, there exist many online platforms that result in massive textual-data production. With ever-increasing textual data at hand, it is of immense importance to extract information nuggets from this data. One approach towards effective harnessing of this unstructured textual data could be its transformation into structured text. Hence, this study aims to present an overview of approaches that can be applied to extract key insights from textual data in a structured way. For this, Named Entity Recognition and Relation Extraction are being majorly addressed in this review study. The former deals with identification of named entities, and the latter deals with problem of extracting relation between set of entities. This study covers early approaches as well as the developments made up till now using machine learning models. Survey findings conclude that deep-learning-based hybrid and joint models are currently governing the state-of-the-art. It is also observed that annotated benchmark datasets for various textual-data generators such as Twitter and other social forums are not available. This scarcity of dataset has resulted into relatively less progress in these domains. Additionally, the majority of the state-of-the-art techniques are offline and computationally expensive. Last, with increasing focus on deep-learning frameworks, there is need to understand and explain the under-going processes in deep architectures.},
  langid = {english},
  keywords = {deep learning,Information extraction,joint modeling,named entity recognition,relation extraction},
  file = {C:\Users\nhiot\OneDrive\zotero\2022\Nasar et al. - 2022 - Named Entity Recognition and Relation Extraction .pdf}
}

@incollection{navarro-almanzaAutomatedOntologyExtraction2020,
  ids = {navarro-almanzaAutomatedOntologyExtraction2020a},
  title = {Automated {{Ontology Extraction}} from {{Unstructured Texts}} Using {{Deep Learning}}},
  booktitle = {Intuitionistic and {{Type-2 Fuzzy Logic Enhancements}} in {{Neural}} and {{Optimization Algorithms}}: {{Theory}} and {{Applications}}},
  author = {{Navarro-Almanza}, Ra{\'u}l and {Ju{\'a}rez-Ram{\'i}rez}, Reyes and Licea, Guillermo and Castro, Juan R.},
  editor = {Castillo, Oscar and Melin, Patricia and Kacprzyk, Janusz},
  year = {2020},
  volume = {862},
  pages = {727--755},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-35445-9_50},
  urldate = {2024-04-06},
  isbn = {978-3-030-35444-2 978-3-030-35445-9},
  langid = {english}
}

@article{navigliWordSenseDisambiguation2024,
  title = {Word {{Sense Disambiguation}}: {{A Survey}}},
  shorttitle = {Word {{Sense Disambiguation}}},
  author = {Navigli, Roberto},
  year = {2024},
  journal = {ACM computing surveys (CSUR)},
  volume = {41},
  number = {2},
  pages = {1--69},
  issn = {0360-0300},
  doi = {10/dhsjt2},
  urldate = {2019-06-20},
  abstract = {Word sense disambiguation (WSD) is the ability to identify the meaning of words in context in a computational manner. WSD is considered an AI-complete problem, that is, a task whose solution is at least as hard as the most difficult problems in artificial intelligence. We introduce the reader to the motivations for solving the ambiguity of words and provide a description of the task. We overview supervised, unsupervised, and knowledge-based approaches. The assessment of WSD systems is discussed in the context of the Senseval/Semeval campaigns, aiming at the objective evaluation of systems participating in several different disambiguation tasks. Finally, applications, open problems, and future directions are discussed.},
  langid = {english},
  keywords = {lexical ambiguity,lexical semantics,semantic annotation,sense annotation,Word sense disambiguation,word sense discrimination,WSD},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2009\\Navigli - 2009 - Word Sense Disambiguation A Survey.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\YSBAZJAG\\citation.html}
}

@incollection{nazarenkoActesTALN20092009,
  title = {Actes de {{TALN}} 2009 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 2009 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Nazarenko, Adeline and Poibeau, Thierry},
  year = {2009},
  month = jun,
  publisher = {LIPN / ATALA},
  address = {Senlis},
  keywords = {nosource}
}

@book{needhamGraphAlgorithmsPractical2019,
  title = {Graph {{Algorithms}}: {{Practical Examples}} in {{Apache Spark}} and {{Neo4j}}},
  shorttitle = {Graph {{Algorithms}}},
  author = {Needham, Mark and Hodler, Amy E.},
  year = {2019},
  month = may,
  publisher = {"O'Reilly Media, Inc."},
  abstract = {Discover how graph algorithms can help you leverage the relationships within your data to develop more intelligent solutions and enhance your machine learning models. You'll learn how graph analytics are uniquely suited to unfold complex structures and reveal difficult-to-find patterns lurking in your data. Whether you are trying to build dynamic network models or forecast real-world behavior, this book illustrates how graph algorithms deliver value---from finding vulnerabilities and bottlenecks to detecting communities and improving machine learning predictions.This practical book walks you through hands-on examples of how to use graph algorithms in Apache Spark and Neo4j---two of the most common choices for graph analytics. Also included: sample code and tips for over 20 practical graph algorithms that cover optimal pathfinding, importance through centrality, and community detection.Learn how graph analytics vary from conventional statistical analysisUnderstand how classic graph algorithms work, and how they are appliedGet guidance on which algorithms to use for different types of questionsExplore algorithm examples with working code and sample datasets from Spark and Neo4jSee how connected feature extraction can increase machine learning accuracy and precisionWalk through creating an ML workflow for link prediction combining Neo4j and Spark},
  googlebooks = {yYWZDwAAQBAJ},
  isbn = {978-1-4920-4765-0},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / General,Computers / Computer Graphics,Computers / Intelligence (AI) \& Semantics,Computers / Mathematical \& Statistical Software,Computers / Programming / Algorithms,Computers / Software Development \& Engineering / Computer Graphics,Mathematics / Graphic Methods},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2019\\Needham et Hodler - 2019 - Graph Algorithms Practical Examples in Apache Spa.epub;C\:\\Users\\nhiot\\OneDrive\\zotero\\2019\\Needham et Hodler - 2019 - Graph Algorithms Practical Examples in Apache Spa2.pdf}
}

@inproceedings{neumannScispaCyFastRobust2019,
  title = {{{ScispaCy}}: {{Fast}} and {{Robust Models}} for {{Biomedical Natural Language Processing}}},
  shorttitle = {{{ScispaCy}}},
  booktitle = {Proceedings of the 18th {{BioNLP Workshop}} and {{Shared Task}}},
  author = {Neumann, Mark and King, Daniel and Beltagy, Iz and Ammar, Waleed},
  editor = {{Demner-Fushman}, Dina and Cohen, Kevin Bretonnel and Ananiadou, Sophia and Tsujii, Junichi},
  year = {2019},
  month = aug,
  eprint = {1902.07669},
  primaryclass = {cs},
  pages = {319--327},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/W19-5034},
  urldate = {2024-03-21},
  abstract = {Despite recent advances in natural language processing, many statistical models for processing text perform extremely poorly under domain shift. Processing biomedical and clinical text is a critically important application area of natural language processing, for which there are few robust, practical, publicly available models. This paper describes scispaCy, a new Python library and models for practical biomedical/scientific text processing, which heavily leverages the spaCy library. We detail the performance of two packages of models released in scispaCy and demonstrate their robustness on several tasks and datasets. Models and code are available at https://allenai.github.io/scispacy/.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  annotation = {QID: Q101248419},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2019\\Neumann et al. - 2019 - ScispaCy Fast and Robust Models for Biomedical Na4.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\D6RZYE87\\1902.html}
}

@article{nikolaouQueryingIncompleteInformation2016,
  title = {Querying Incomplete Information in {{RDF}} with {{SPARQL}}},
  author = {Nikolaou, Charalampos and Koubarakis, Manolis},
  year = {2016},
  month = aug,
  journal = {Artificial Intelligence},
  volume = {237},
  pages = {138--171},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2016.04.005},
  urldate = {2023-08-08},
  abstract = {Incomplete information has been studied in-depth in relational databases and knowledge representation. In the context of the Web, incomplete information issues have been studied in detail for XML, but very few papers exist that do the same for RDF. In this paper we make the first general proposal for extending RDF with the ability to represent property values that exist but are unknown or partially known using constraints. Following ideas from incomplete information literature, we develop a semantics for this extension of RDF, called RDFi, and study query evaluation for SPARQL. We transfer the concept of representation systems from incomplete information in relational databases to the case of RDFi and identify two very important fragments of SPARQL that can be used to define a representation system for RDFi. The first corresponds to the monotone fragment of graph patterns that uses only the operators AND, UNION, and FILTER. The second corresponds to the well-designed graph patterns, that is, a fragment that uses only operators AND, FILTER, and OPT, and enjoys interesting properties that make query evaluation efficient. We prove that each of the two fragments can be used to define a representation system for CONSTRUCT queries without blank nodes in their templates. We also define the fundamental concept of certain answers to SPARQL queries over RDFi databases and present an algorithm for its computation. Then, we present complexity results for computing certain answers by considering equality, temporal, and spatial constraint languages and the class of CONSTRUCT queries of our representation systems. Finally, we demonstrate the usefulness of RDFi in geospatial Semantic Web applications by giving a number of examples and comparing the modeling capabilities of RDFi with related formalisms found in the literature.},
  langid = {english},
  keywords = {Incomplete information,RDF,Semantic Web,SPARQL},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2016\\Nikolaou et Koubarakis - 2016 - Querying incomplete information in RDF with SPARQL.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\N8CIQSEH\\S0004370216300467.html}
}

@inproceedings{nivreIncrementalityDeterministicDependency2004,
  title = {Incrementality in Deterministic Dependency Parsing},
  booktitle = {Proceedings of the {{Workshop}} on {{Incremental Parsing}}: {{Bringing Engineering}} and {{Cognition Together}}},
  author = {Nivre, Joakim},
  year = {2004},
  month = jul,
  series = {{{IncrementParsing}} '04},
  pages = {50--57},
  publisher = {Association for Computational Linguistics},
  address = {Barcelona, Spain},
  doi = {10.3115/1613148.1613156},
  urldate = {2024-04-07},
  abstract = {Deterministic dependency parsing is a robust and efficient approach to syntactic parsing of unrestricted natural language text. In this paper, we analyze its potential for incremental processing and conclude that strict incrementality is not achievable within this framework. However, we also show that it is possible to minimize the number of structures that require non-incremental processing by choosing an optimal parsing algorithm. This claim is substantiated with experimental evidence showing that the algorithm achieves incremental parsing for 68.9\% of the input when tested on a random sample of Swedish text. When restricted to sentences that are accepted by the parser, the degree of incrementality increases to 87.9\%.},
  langid = {english},
  file = {C:\Users\nhiot\OneDrive\zotero\2004\Nivre - 2004 - Incrementality in deterministic dependency parsing.pdf}
}

@inproceedings{nivreUniversalGrammarNatural2015,
  title = {Towards a {{Universal Grammar}} for {{Natural Language Processing}}},
  booktitle = {Computational {{Linguistics}} and {{Intelligent Text Processing}}},
  author = {Nivre, Joakim},
  editor = {Gelbukh, Alexander},
  year = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {9041},
  pages = {3--16},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-18111-0_1},
  abstract = {Universal Dependencies is a recent initiative to develop cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. In this paper, I outline the motivation behind the initiative and explain how the basic design principles follow from these requirements. I then discuss the different components of the annotation standard, including principles for word segmentation, morphological annotation, and syntactic annotation. I conclude with some thoughts on the challenges that lie ahead.},
  isbn = {978-3-319-18110-3 978-3-319-18111-0},
  langid = {english},
  keywords = {Computational Linguistics,Content Word,Function Word,Natural Language Processing,Word Segmentation},
  file = {C\:\\Users\\nhiot\\Zotero\\storage\\P9MTPHQF\\Nivre - 2015 - Towards a Universal Grammar for Natural Language P.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\USRYHAHU\\Nivre - 2015 - Towards a Universal Grammar for Natural Language P.pdf}
}

@article{nothmanLearningMultilingualNamed2013,
  ids = {nothmanLearningMultilingualNamed2013a},
  title = {Learning Multilingual Named Entity Recognition from {{Wikipedia}}},
  author = {Nothman, Joel and Ringland, Nicky and Radford, Will and Murphy, Tara and Curran, James R.},
  year = {2013},
  month = jan,
  journal = {Artificial Intelligence},
  series = {Artificial {{Intelligence}}, {{Wikipedia}} and {{Semi-Structured Resources}}},
  volume = {194},
  pages = {151--175},
  publisher = {Elsevier},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2012.03.006},
  urldate = {2024-03-21},
  abstract = {We automatically create enormous, free and multilingual silver-standard training annotations for named entity recognition (ner) by exploiting the text and structure of Wikipedia. Most ner systems rely on statistical models of annotated data to identify and classify names of people, locations and organisations in text. This dependence on expensive annotation is the knowledge bottleneck our work overcomes. We first classify each Wikipedia article into named entity (ne) types, training and evaluating on 7200 manually-labelled Wikipedia articles across nine languages. Our cross-lingual approach achieves up to 95\% accuracy. We transform the links between articles into ne annotations by projecting the target article's classifications onto the anchor text. This approach yields reasonable annotations, but does not immediately compete with existing gold-standard data. By inferring additional links and heuristically tweaking the Wikipedia corpora, we better align our automatic annotations to gold standards. We annotate millions of words in nine languages, evaluating English, German, Spanish, Dutch and Russian Wikipedia-trained models against conll shared task data and other gold-standard corpora. Our approach outperforms other approaches to automatic ne annotation (Richman and Schone, 2008 [61], Mika et al., 2008 [46]) competes with gold-standard training when tested on an evaluation corpus from a different source; and performs 10\% better than newswire-trained models on manually-annotated Wikipedia text.},
  keywords = {Annotated corpora,Information extraction,Named entity recognition,Semi-structured resources,Semi-supervised learning,Wikipedia},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2013\\Nothman et al. - 2013 - Learning multilingual named entity recognition fro.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\XE4CSF9N\\S0004370212000276.html}
}

@inproceedings{onetChaseProcedureIts2013,
  title = {The {{Chase Procedure}} and Its {{Applications}} in {{Data Exchange}}},
  booktitle = {Dagstuhl {{Follow-Ups}}},
  author = {Onet, Adrian},
  year = {2013},
  volume = {5},
  pages = {1--37},
  publisher = {Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik},
  doi = {10.4230/DFU.VOL5.10452.1},
  urldate = {2024-01-03},
  abstract = {The initial and basic role of the chase procedure was to test logical implication between sets of dependencies in order to determine equivalence of database instances known to satisfy a given set of dependencies and to determine query equivalence under database constrains. Recently the chase procedure has experienced a revival due to its application in data exchange. In this chapter we review the chase algorithm and its properties as well as its application in data exchange.},
  copyright = {https://creativecommons.org/licenses/by/3.0/legalcode},
  langid = {english},
  keywords = {{000 Computer science, knowledge, general works},Computer Science,nosource},
  file = {C:\Users\nhiot\OneDrive\zotero\2013\Onet - 2013 - The Chase Procedure and its Applications in Data E.pdf}
}

@techreport{pagePageRankCitationRanking1999,
  type = {Technical {{Report}}},
  title = {The {{PageRank Citation Ranking}}: {{Bringing Order}} to the {{Web}}.},
  shorttitle = {The {{PageRank}} Citation Ranking},
  author = {Page, Lawrence and Brin, Sergey and Motwani, Rajeev and Winograd, Terry},
  year = {1999},
  month = nov,
  number = {1999-66},
  pages = {1--14},
  institution = {Stanford InfoLab},
  url = {http://ilpubs.stanford.edu:8090/422/},
  abstract = {The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes PageRank, a mathod for rating Web pages objectively and mechanically, effectively measuring the human interest and attention devoted to them. We compare PageRank to an idealized random Web surfer. We show how to efficiently compute PageRank for large numbers of pages. And, we show how to apply PageRank to search and to user navigation.},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\1999\Page et al. - 1999 - The PageRank Citation Ranking Bringing Order to t.pdf}
}

@article{papenbrockFunctionalDependencyDiscovery2015,
  title = {Functional Dependency Discovery: An Experimental Evaluation of Seven Algorithms},
  shorttitle = {Functional Dependency Discovery},
  author = {Papenbrock, Thorsten and Ehrlich, Jens and Marten, Jannik and Neubert, Tommy and Rudolph, Jan-Peer and Sch{\"o}nberg, Martin and Zwiener, Jakob and Naumann, Felix},
  year = {2015},
  month = jun,
  journal = {Proceedings of the VLDB Endowment},
  volume = {8},
  number = {10},
  pages = {1082--1093},
  issn = {2150-8097},
  doi = {10.14778/2794367.2794377},
  urldate = {2024-04-08},
  abstract = {Functional dependencies are important metadata used for schema normalization, data cleansing and many other tasks. The efficient discovery of functional dependencies in tables is a well-known challenge in database research and has seen several approaches. Because no comprehensive comparison between these algorithms exist at the time, it is hard to choose the best algorithm for a given dataset. In this experimental paper, we describe, evaluate, and compare the seven most cited and most important algorithms, all solving this same problem. First, we classify the algorithms into three different categories, explaining their commonalities. We then describe all algorithms with their main ideas. The descriptions provide additional details where the original papers were ambiguous or incomplete. Our evaluation of careful re-implementations of all algorithms spans a broad test space including synthetic and real-world data. We show that all functional dependency algorithms optimize for certain data characteristics and provide hints on when to choose which algorithm. In summary, however, all current approaches scale surprisingly poorly, showing potential for future research.},
  file = {C:\Users\nhiot\OneDrive\zotero\2015\Papenbrock et al. - 2015 - Functional dependency discovery an experimental e.pdf}
}

@inproceedings{passosLexiconInfusedPhrase2014,
  ids = {passosLexiconInfusedPhrase,passosLexiconInfusedPhrase2014a,passosLexiconInfusedPhrasea,passosLexiconInfusedPhraseb},
  title = {Lexicon {{Infused Phrase Embeddings}} for {{Named Entity Resolution}}},
  booktitle = {Proceedings of the {{Eighteenth Conference}} on {{Computational Natural Language Learning}}},
  author = {Passos, Alexandre and Kumar, Vineet and Mccallum, Andrew},
  year = {2014},
  pages = {78--86},
  publisher = {Citeseer},
  doi = {10.3115/v1/W14-1609},
  urldate = {2024-03-20},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2014\\Passos et al. - 2014 - Lexicon Infused Phrase Embeddings for Named Entity3.pdf;C\:\\Users\\nhiot\\OneDrive\\zotero\\2014\\Passos et al. - 2014 - Lexicon Infused Phrase Embeddings for Named Entity4.pdf}
}

@article{patel-schneiderUsingDescriptionLogics2015,
  title = {Using {{Description Logics}} for {{RDF Constraint Checking}} and {{Closed-World Recognition}}},
  author = {{Patel-Schneider}, Peter},
  year = {2015},
  month = feb,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {29},
  number = {1},
  pages = {247--253},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v29i1.9177},
  urldate = {2023-12-27},
  abstract = {RDF and Description Logics work in an open-world setting where absence of information is not information about absence.  Nevertheless, Description Logic axioms can be interpreted in a closed-world setting and in this setting they can be used for both constraint checking and closed-world recognition against information sources.  When the information sources are expressed in well-behaved RDF or RDFS (i.e., RDF graphs interpreted in the RDF or RDFS semantics) this constraint checking and closed-world recognition is simple to describe.  Further this constraint checking can be implemented as SPARQL querying and thus effectively performed.},
  copyright = {Copyright (c)},
  langid = {english},
  keywords = {Constraints,nosource},
  file = {C:\Users\nhiot\OneDrive\zotero\2015\Patel-Schneider - 2015 - Using Description Logics for RDF Constraint Checki.pdf}
}

@book{patelAppliedNaturalLanguage2021,
  ids = {patelAppliedNaturalLanguage},
  title = {Applied {{Natural Language Processing}} in the {{Enterprise}}},
  author = {Patel, Ankur A. and Arasanipalai, Ajay Uppili},
  year = {2021},
  month = may,
  publisher = {O'Reilly Media, Inc.},
  url = {https://books.google.fr/books?hl=fr\&lr=\&id=5dktEAAAQBAJ\&oi=fnd\&pg=PR2\&dq=Explosion+AI\%27s+NLP+library:+spaCy+\&ots=19cpr4BVQR\&sig=s5m8nE0RvH8gD3hU4qhpT7z-7AU},
  abstract = {NLP has exploded in popularity over the last few years. But while Google, Facebook, OpenAI, and others continue to release larger language models, many teams still struggle with building NLP applications that live up to the hype. This hands-on guide helps you get up to speed on the latest and most promising trends in NLP.With a basic understanding of machine learning and some Python experience, you'll learn how to build, train, and deploy models for real-world applications in your organization. Authors Ankur Patel and Ajay Uppili Arasanipalai guide you through the process using code and examples that highlight the best practices in modern NLP.Use state-of-the-art NLP models such as BERT and GPT-3 to solve NLP tasks such as named entity recognition, text classification, semantic search, and reading comprehensionTrain NLP models with performance comparable or superior to that of out-of-the-box systemsLearn about Transformer architecture and modern tricks like transfer learning that have taken the NLP world by stormBecome familiar with the tools of the trade, including spaCy, Hugging Face, and fast.aiBuild core parts of the NLP pipeline--including tokenizers, embeddings, and language models--from scratch using Python and PyTorchTake your models out of Jupyter notebooks and learn how to deploy, monitor, and maintain them in production},
  isbn = {978-1-4920-6254-7},
  langid = {english},
  keywords = {⛔ No DOI found,Computers / Artificial Intelligence / General,Computers / Artificial Intelligence / Natural Language Processing,Computers / Business \& Productivity Software / Business Intelligence,Computers / Data Science / Data Analytics,Computers / Data Science / General,Computers / Data Science / Machine Learning,Computers / Machine Theory}
}

@inproceedings{pathakEzDISupervisedNLP2015,
  title = {{{ezDI}}: {{A}} Supervised {{NLP}} System for Clinical Narrative Analysis},
  booktitle = {Proceedings of the 9th International Workshop on Semantic Evaluation, {{SemEval}}@{{NAACL-HLT}} 2015, Denver, Colorado, {{USA}}, June 4-5, 2015},
  author = {Pathak, Parth and Patel, Pinal and Panchal, Vishal and Soni, Sagar and Dani, Kinjal and Patel, Amrish and Choudhary, Narayan},
  editor = {Cer, Daniel M. and Jurgens, David and Nakov, Preslav and Zesch, Torsten},
  year = {2015},
  pages = {412--416},
  publisher = {The Association for Computer Linguistics},
  doi = {10.18653/v1/s15-2071},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/semeval/PathakPPSDPC15.bib},
  keywords = {nosource},
  timestamp = {Tue, 28 Jan 2020 10:29:20 +0100},
  file = {C:\Users\nhiot\OneDrive\zotero\2015\Pathak et al. - 2015 - ezDI A supervised NLP system for clinical narrati.pdf}
}

@inproceedings{pereiraParsingDeduction1983,
  title = {Parsing as Deduction},
  booktitle = {21st Annual Meeting of the Association for Computational Linguistics},
  author = {Pereira, Fernando C. N. and Warren, David H. D.},
  year = {1983},
  month = jun,
  pages = {137--144},
  publisher = {Association for Computational Linguistics},
  address = {Cambridge, Massachusetts, USA},
  doi = {10.3115/981311.981338},
  keywords = {nosource}
}

@inproceedings{pichlerComplexityEvaluatingTuple2011,
  title = {The Complexity of Evaluating Tuple Generating Dependencies},
  booktitle = {Proceedings of the 14th {{International Conference}} on {{Database Theory}}},
  author = {Pichler, Reinhard and Skritek, Sebastian},
  year = {2011},
  month = mar,
  series = {{{ICDT}} '11},
  pages = {244--255},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1938551.1938583},
  urldate = {2023-08-16},
  abstract = {Dependencies have played an important role in database design for many years. More recently, they have also turned out to be central to data integration and data exchange. In this work we concentrate on tuple generating dependencies (tgds) which enforce the presence of certain tuples in a database instance if certain other tuples are already present. Previous complexity results in data integration and data exchange mainly referred to the data complexity. In this work, we study the query complexity and combined complexity of a fundamental problem related to tgds, namely checking if a given tgd is satisfied by a database instance. We also address an important variant of this problem which deals with updates (by inserts or deletes) of a database: Here we have to check if all previously satisfied tgds are still satisfied after an update. We show that the query complexity and combined complexity of these problems are much higher than the data complexity. However, we also prove sufficient conditions on the tgds to reduce this high complexity.},
  isbn = {978-1-4503-0529-7},
  file = {C:\Users\nhiot\OneDrive\zotero\2011\Pichler et Skritek - 2011 - The complexity of evaluating tuple generating depe.pdf}
}

@incollection{pierrelActesTALN20022002,
  title = {Actes de {{TALN}} 2002 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 2002 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Pierrel, Jean-Marie},
  year = {2002},
  month = jun,
  publisher = {ATILF / ATALA},
  address = {Nancy},
  keywords = {nosource}
}

@inproceedings{pokornyGraphDatabasesTheir2015,
  title = {Graph {{Databases}}: {{Their Power}} and {{Limitations}}},
  shorttitle = {Graph {{Databases}}},
  booktitle = {Computer {{Information Systems}} and {{Industrial Management}}: 14th {{IFIP TC}} 8 {{International Conference}}, {{CISIM}} 2015, {{Warsaw}}, {{Poland}}, {{September}} 24-26, 2015, {{Proceedings}} 14},
  author = {Pokorn{\'y}, Jaroslav},
  editor = {Saeed, Khalid and Homenda, Wladyslaw},
  year = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {58--69},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-24369-6_5},
  abstract = {Real world data offers a lot of possibilities to be represented as graphs. As a result we obtain undirected or directed graphs, multigraphs and hypergraphs, labelled or weighted graphs and their variants. A development of graph modelling brings also new approaches, e.g., considering constraints. Processing graphs in a database way can be done in many different ways. Some graphs can be represented as JSON or XML structures and processed by their native database tools. More generally, a graph database is specified as any storage system that provides index-free adjacency, i.e. an explicit graph structure. Graph database technology contains some technological features inherent to traditional databases, e.g. ACID properties and availability. Use cases of graph databases like Neo4j, OrientDB, InfiniteGraph, FlockDB, AllegroGraph, and others, document that graph databases are becoming a common means for any connected data. In Big Data era, important questions are connected with scalability for large graphs as well as scaling for read/write operations. For example, scaling graph data by distributing it in a network is much more difficult than scaling simpler data models and is still a work in progress. Still a challenge is pattern matching in graphs providing, in principle, an arbitrarily complex identity function. Mining complete frequent patterns from graph databases is also challenging since supporting operations are computationally costly. In this paper, we discuss recent advances and limitations in these areas as well as future directions.},
  isbn = {978-3-319-24369-6},
  langid = {english},
  keywords = {Big graphs,Graph database,Graph querying,Graph scalability,Graph storage},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2015\\Pokorný - 2015 - Graph Databases Their Power and Limitations.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\Z5XWGHCZ\\978-3-319-24369-6_5.html}
}

@phdthesis{pollardGeneralizedPhraseStructure1984,
  title = {Generalized Phrase Structure Grammars, Head Grammars, and Natural Language},
  author = {Pollard, Carl},
  year = {1984},
  school = {Stanford University, CA},
  keywords = {nosource}
}

@misc{porterSnowballLanguageStemming2001,
  title = {Snowball: {{A}} Language for Stemming Algorithms},
  shorttitle = {Snowball},
  author = {Porter, Martin F.},
  year = {2001},
  month = oct,
  url = {https://snowball.tartarus.org/texts/introduction},
  keywords = {⛔ No DOI found,NaturalLanguage(Processing) dipl\textsubscript{l}iteratur information\textsubscript{r}etrieval stemming},
  file = {C:\Users\nhiot\Zotero\storage\JHUK5JZ3\introduction.html}
}

@inproceedings{pradhanTaskShAReCLEF2013,
  title = {Task 1: {{ShARe}}/{{CLEF eHealth}} Evaluation Lab 2013},
  booktitle = {Working Notes for {{CLEF}} 2013 Conference , Valencia, Spain, September 23-26, 2013},
  author = {Pradhan, Sameer and Elhadad, No{\'e}mie and South, Brett R. and Mart{\'i}nez, David and Christensen, Lee M. and Vogel, Amy and Suominen, Hanna and Chapman, Wendy W. and Savova, Guergana K.},
  editor = {Forner, Pamela and Navigli, Roberto and Tufis, Dan and Ferro, Nicola},
  year = {2013},
  series = {{{CEUR}} Workshop Proceedings},
  volume = {1179},
  publisher = {CEUR-WS.org},
  url = {http://ceur-ws.org/Vol-1179/CLEF2013wn-CLEFeHealth-PradhanEt2013.pdf},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/clef/PradhanESMCVSCS13.bib},
  keywords = {⛔ No DOI found,nosource},
  timestamp = {Wed, 12 Feb 2020 16:44:31 +0100},
  file = {C:\Users\nhiot\OneDrive\zotero\2013\Pradhan et al. - 2013 - Task 1 ShAReCLEF eHealth evaluation lab 2013.pdf}
}

@inproceedings{raadDetectionLiensIdentite2017,
  title = {D{\'e}tection de Liens d'identit{\'e} Contextuels Dans Une Base de Connaissances.},
  booktitle = {{{IC}} 2017 - 28es {{Journ{\'e}es}} Francophones d'{{Ing{\'e}nierie}} Des {{Connaissances}}},
  author = {Raad, Joe and Pernelle, Nathalie and Sa{\"i}s, Fatiha},
  editor = {Roussey, Catherine},
  year = {2017},
  month = jul,
  series = {Actes {{IC}} 2017 28es {{Journ{\'e}es}} Francophones d'{{Ing{\'e}nierie}} Des {{Connaissances}}},
  pages = {56--67},
  address = {Caen, France},
  url = {https://hal.archives-ouvertes.fr/hal-01570053},
  urldate = {2019-03-04},
  abstract = {De nombreuses applications du Web de donn{\'e}es exploitent des liens d'identit{\'e}s d{\'e}clar{\'e}s {\`a} l'aide du constructeur owl :sameAs. Cependant, diff{\'e}rentes {\'e}tudes ont montr{\'e} qu'une utilisation abusive de ces liens peut conduire {\`a} des inf{\'e}rences erron{\'e}es ou contradictoires. Dans ce papier nous proposons de calculer des liens d'identit{\'e}s contextuels qui permettent d'expliciter les contextes dans lesquels ces liens sont valides. La notion de contexte que nous proposons est repr{\'e}sent{\'e}e en se basant sur l'ontologie de domaine dans laquelle les instances sont repr{\'e}sent{\'e}es. Nous avons exp{\'e}riment{\'e} cette approche dans le domaine des donn{\'e}es scientifiques o{\`u} les {\'e}l{\'e}ments d{\'e}crivant les exp{\'e}riences partagent rarement un lien d'identit{\'e} tel que d{\'e}fini par owl :sameAs.},
  keywords = {Bases de connaissances,Contextes,Enrichissement,Liage de donn{\'e}es,Ontologies},
  file = {C:\Users\nhiot\OneDrive\zotero\2017\Raad et al. - 2017 - Détection de liens d'identité contextuels dans une.pdf}
}

@article{raadDetectionLiensIdentite2018,
  title = {D{\'e}tection de Liens d'identit{\'e} Erron{\'e}s En Utilisant La D{\'e}tection de Communaut{\'e}s Dans Les Graphes d'identit{\'e}},
  author = {Raad, Joe and BECK, Wouter and Pernelle, Nathalie and Sais, Fatiha and Harmelen, Frank},
  year = {2018},
  month = aug,
  journal = {Ing{\'e}nierie des syst{\`e}mes d'information},
  volume = {23},
  number = {3-4},
  pages = {61--88},
  doi = {10.3166/isi.23.3-4.61-88},
  file = {C:\Users\nhiot\OneDrive\zotero\2018\Raad et al. - 2018 - Détection de liens d’identité erronés en utilisant2.pdf}
}

@inproceedings{radfordImprovingLanguageUnderstanding2018,
  title = {Improving {{Language Understanding}} by {{Generative Pre-Training}}},
  author = {Radford, Alec and Narasimhan, Karthik},
  year = {2018},
  publisher = {OpenAI},
  url = {https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035},
  urldate = {2024-05-01},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2018\Radford et Narasimhan - 2018 - Improving Language Understanding by Generative Pre.pdf}
}

@inproceedings{ramshawTextChunkingUsing1995,
  ids = {ramshaw-marcus-1995-text,ramshawTextChunkingUsing1995a},
  title = {Text {{Chunking Using Transformation-Based Learning}}},
  booktitle = {Third {{Workshop}} on {{Very Large Corpora}}},
  author = {Ramshaw, Lance A. and Marcus, Mitchell P.},
  editor = {Ide, Nancy and V{\'e}ronis, Jean and Armstrong, Susan and Church, Kenneth and Isabelle, Pierre and Manzi, Sandra and Tzoukermann, Evelyne and Yarowsky, David},
  year = {1995},
  month = may,
  series = {Text, {{Speech}} and {{Language Technology}}},
  volume = {11},
  eprint = {cmp-lg/9505040},
  pages = {157--176},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-017-2390-9_10},
  urldate = {2024-03-22},
  abstract = {Eric Brill introduced transformation-based learning and showed that it can do part-of-speech tagging with fairly high accuracy. The same method can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive ``baseNP'' chunks. For this purpose, it is convenient to view chunking as a tagging problem by encoding the chunk structure in new tags attached to each word. In automatic tests using Treebank-derived data, this technique achieved recall and precision rates of roughly 92\% for baseNP chunks and 88\% for somewhat more complex chunks that partition the sentence. Some interesting adaptations to the transformation-based learning approach are also suggested by this application.},
  archiveprefix = {arxiv},
  isbn = {978-90-481-5349-7 978-94-017-2390-9},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1995\\Ramshaw et Marcus - 1995 - Text Chunking Using Transformation-Based Learning.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\ZWF7ZGYA\\9505040.html}
}

@inproceedings{rauExtractingCompanyNames1991,
  title = {Extracting Company Names from Text},
  booktitle = {The {{Seventh IEEE Conference}} on {{Artificial Intelligence Application}}},
  author = {Rau, L. F.},
  year = {1991},
  volume = {1},
  pages = {29--32},
  publisher = {IEEE},
  doi = {10.1109/CAIA.1991.120841},
  urldate = {2024-03-20},
  keywords = {⛔ No DOI found}
}

@misc{REDACCorpusTexte2008,
  title = {{{REDAC}} : {{Corpus}} Texte {{Wikip{\'e}diaFR2008}}},
  year = {2008},
  month = jun,
  url = {http://redac.univ-tlse2.fr/corpus/wikipedia.html},
  urldate = {2023-10-06},
  abstract = {Le corpus Wikip{\'e}dia-FR a {\'e}t{\'e} constitu{\'e} {\`a} partir du dump de la version fran{\c c}aise de l'encyclop{\'e}die Wikip{\'e}dia du 18/06/2008. Ce dump correspond {\`a} la version << HTML statique >> disponible {\`a} l'adresse : http://dumps.wikimedia.org/. Les traitements appliqu{\'e}s {\`a} ce dump sont minimaux et ont consist{\'e} {\`a} extraire les parties textuelles des articles. Les sommaires num{\'e}rot{\'e}s de d{\'e}but d'articles ont {\'e}t{\'e} supprim{\'e}s, ainsi que les sections << voir aussi >> (liens vers des r{\'e}f{\'e}rences externes). Les parties << notes >> ont {\'e}t{\'e} conserv{\'e}es. Le corpus a {\'e}t{\'e} {\'e}tiquet{\'e} morphosyntaxiquement avec TreeTagger, de l'Universit{\'e} de Stuttgart.},
  file = {C:\Users\nhiot\Zotero\storage\XEYQ9D4E\wikipedia.html}
}

@inproceedings{reiterLogicalReconstructionRelational1989,
  title = {Towards a {{Logical Reconstruction}} of {{Relational Database Theory}}},
  booktitle = {Readings in {{Artificial Intelligence}} and {{Databases}}},
  author = {Reiter, Raymond},
  editor = {Mylopolous, John and Brodie, Michael},
  year = {1989},
  month = jan,
  pages = {301--327},
  publisher = {Elsevier},
  address = {San Francisco (CA)},
  doi = {10.1016/B978-0-934613-53-8.50025-X},
  urldate = {2023-08-07},
  abstract = {Insofar as database theory can be said to owe a debt to logic, the currency on loan is model theoretic in the sense that a database can be viewed as a particular kind of first order interpretation, and query evaluation is a process of truth Junctional evaluation of first order formulae with respect to this interpretation. It is this model theoretic paradigm which leads, for example, to many valued propositional logies for databases with null values. In this chapter I argue that a proof theoretic view of databases is possible, and indeed much more fruitful. Specifically, I show how relational databases can be seen as special theories of first order logic, namely theories incorporating the following assumptions:1.The domain closure assumption. The individuals occurring in the database are all and only the existing individuals.2.The unique name assumption. Individuals with distinct names are distinct.3.The closed world assumption. The only possible instances of a relation are those implied by the database. It will follow that a proof theoretic paradigm for relational databases provides a correct treatment of:1.Query evaluation for databases that have incomplete information, including null values.2.Integrity constraints and their enforcement.3.Conceptual modelling and the extension of the relational model to incorporate more real world semantics.},
  isbn = {978-0-934613-53-8},
  langid = {english},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1989\\Reiter - 1989 - Towards a Logical Reconstruction of Relational Dat.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\4VPCJ9R4\\B978093461353850025X.html}
}

@article{reiterSoundSometimesComplete1986,
  title = {A Sound and Sometimes Complete Query Evaluation Algorithm for Relational Databases with Null Values},
  author = {Reiter, Raymond},
  year = {1986},
  month = apr,
  journal = {Journal of the ACM (JACM)},
  volume = {33},
  number = {2},
  pages = {349--370},
  issn = {0004-5411},
  doi = {10.1145/5383.5388},
  abstract = {A sound and, in certain cases, complete method is described for evaluating queries in relational databases with null values where these nulls represent existing but unknown individuals. The soundness and completeness results are proved relative to a formalization of such databases as suitable theories of first-order logic. Because the algorithm conforms to the relational algebra, it may easily be incorporated into existing relational systems.},
  keywords = {nosource},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1986\\Reiter - 1986 - A sound and sometimes complete query evaluation al.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\J9PHZZNY\\5383.html}
}

@misc{ReSEauxNumeriquesDonnees,
  title = {{r{\'e}SEaux Num{\'e}riques de Donn{\'e}es s{\'e}mantiques: Utilit{\'e} et vie Priv{\'e}e}},
  shorttitle = {{r{\'e}SEaux Num{\'e}riques de Donn{\'e}es s{\'e}mantiques}},
  journal = {Agence nationale de la recherche},
  url = {https://anr.fr/Projet-ANR-18-CE23-0010},
  urldate = {2024-02-06},
  abstract = {La quantit{\'e} de donn{\'e}es produites par les particuliers et les entreprises a explos{\'e} durant les derni{\`e}res d{\'e}cennies. Leur exploitation offre des opportunit{\'e}s mais questionne le respect de la vie priv{\'e}e. Tandis que les concepts de donn{\'e}es li{\'e}es et ouvertes gagnent en importance, le grand public exprime une m{\'e}fiance croissante vis-{\`a}-vis de l'exploitation des donn{\'e}es personnelles. Cela conduit {\`a} un nouveau d{\'e}fi : comment pr{\'e}server la vie priv{\'e}e tout en fournissant des donn{\'e}es utilisables?},
  langid = {french},
  file = {C:\Users\nhiot\Zotero\storage\QTUVRMLG\Projet-ANR-18-CE23-0010.html}
}

@inproceedings{ribeiroWhyShouldTrust2016,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2016},
  month = aug,
  pages = {1135--1144},
  publisher = {ACM},
  address = {San Francisco California USA},
  doi = {10.1145/2939672.2939778},
  urldate = {2024-04-08},
  isbn = {978-1-4503-4232-2},
  langid = {english},
  file = {C:\Users\nhiot\OneDrive\zotero\2016\Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf}
}

@book{robinsonGraphDatabasesNew2015,
  title = {Graph {{Databases}}: {{New Opportunities}} for {{Connected Data}}},
  shorttitle = {Graph {{Databases}}},
  author = {Robinson, Ian and Webber, Jim and Eifrem, Emil},
  year = {2015},
  month = jun,
  publisher = {"O'Reilly Media, Inc."},
  abstract = {Discover how graph databases can help you manage and query highly connected data. With this practical book, you'll learn how to design and implement a graph database that brings the power of graphs to bear on a broad range of problem domains. Whether you want to speed up your response to user queries or build a database that can adapt as your business evolves, this book shows you how to apply the schema-free graph model to real-world problems.This second edition includes new code samples and diagrams, using the latest Neo4j syntax, as well as information on new functionality. Learn how different organizations are using graph databases to outperform their competitors. With this book's data modeling, query, and code examples, you'll quickly be able to implement your own solution.Model data with the Cypher query language and property graph modelLearn best practices and common pitfalls when modeling with graphsPlan and implement a graph database solution in test-driven fashionExplore real-world examples to learn how and why organizations use a graph databaseUnderstand common patterns and components of graph database architectureUse analytical techniques and algorithms to mine graph database information},
  googlebooks = {RTvcCQAAQBAJ},
  isbn = {978-1-4919-3086-1},
  langid = {english},
  keywords = {Computers / Data Science / Data Analytics,Computers / Data Science / Data Modeling \& Design,Computers / Data Science / Data Visualization,Computers / Data Science / Data Warehousing,Computers / Data Science / General},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2015\\Robinson et al. - 2015 - Graph Databases New Opportunities for Connected D.epub;C\:\\Users\\nhiot\\OneDrive\\zotero\\2015\\Robinson et al. - 2015 - Graph Databases New Opportunities for Connected D.pdf}
}

@inproceedings{sakorFalconEntityRelation2020,
  title = {Falcon 2.0: {{An Entity}} and {{Relation Linking Tool}} over {{Wikidata}}},
  shorttitle = {Falcon 2.0},
  booktitle = {Proceedings of the 29th {{ACM International Conference}} on {{Information}} \& {{Knowledge Management}}},
  author = {Sakor, Ahmad and Singh, Kuldeep and Patel, Anery and Vidal, Maria-Esther},
  year = {2020},
  month = oct,
  series = {{{CIKM}} '20},
  eprint = {1912.11270},
  primaryclass = {cs},
  pages = {3141--3148},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3340531.3412777},
  urldate = {2024-04-05},
  abstract = {The Natural Language Processing (NLP) community has significantly contributed to the solutions for entity and relation recognition from a natural language text, and possibly linking them to proper matches in Knowledge Graphs (KGs). Considering Wikidata as the background KG, there are still limited tools to link knowledge within the text to Wikidata. In this paper, we present Falcon 2.0, the first joint entity and relation linking tool over Wikidata. It receives a short natural language text in the English language and outputs a ranked list of entities and relations annotated with the proper candidates in Wikidata. The candidates are represented by their Internationalized Resource Identifier (IRI) in Wikidata. Falcon 2.0 resorts to the English language model for the recognition task (e.g., N-Gram tiling and N-Gram splitting), and then an optimization approach for the linking task. We have empirically studied the performance of Falcon 2.0 on Wikidata and concluded that it outperforms all the existing baselines. Falcon 2.0 is open source and can be reused by the community; all the required instructions of Falcon 2.0 are well-documented at our GitHub repository (https://github.com/SDM-TIB/falcon2.0). We also demonstrate an online API, which can be run without any technical expertise. Falcon 2.0 and its background knowledge bases are available as resources at https://labs.tib.eu/falcon/falcon2/.},
  archiveprefix = {arxiv},
  isbn = {978-1-4503-6859-9},
  langid = {english},
  keywords = {background knowledge,Computer Science - Computation and Language,dbpedia,english morphology,entity linking,nlp,relation linking,wikidata},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2020\\Sakor et al. - 2020 - Falcon 2.0 An Entity and Relation Linking Tool ov2.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\LALSVBFJ\\1912.html}
}

@book{sakuraiTheoryApplicationsAdvanced2012,
  title = {Theory and {{Applications}} for {{Advanced Text Mining}}},
  author = {Sakurai, Shigeaki},
  year = {2012},
  month = nov,
  publisher = {BoD -- Books on Demand},
  abstract = {Due to the growth of computer technologies and web technologies, we can easily collect and store large amounts of text data. We can believe that the data include useful knowledge. Text mining techniques have been studied aggressively in order to extract the knowledge from the data since late 1990s. Even if many important techniques have been developed, the text mining research field continues to expand for the needs arising from various application fields. This book is composed of 9 chapters introducing advanced text mining techniques. They are various techniques from relation extraction to under or less resourced language. I believe that this book will give new knowledge in the text mining field and help many readers open their new research fields.},
  googlebooks = {EfqdDwAAQBAJ},
  isbn = {978-953-51-0852-8},
  langid = {english},
  keywords = {Computers / General,Computers / Human-Computer Interaction (HCI)}
}

@inproceedings{savaryRelationExtractionClinical2022,
  title = {Relation {{Extraction}} from {{Clinical Cases}} for a {{Knowledge Graph}}},
  booktitle = {European {{Conference}} on {{Advances}} in {{Databases}} and {{Information Systems}}},
  author = {Savary, Agata and Silvanovich, Alena and Minard, Anne-Lyse and Hiot, Nicolas and {Halfeld-Ferrari}, Mirian},
  editor = {Chiusano, Silvia and Cerquitelli, Tania and Wrembel, Robert and N{\o}rv{\aa}g, Kjetil and Catania, Barbara and {Vargas-Solar}, Genoveva and Zumpano, Ester},
  year = {2022},
  month = aug,
  series = {Communications in {{Computer}} and {{Information Science}}},
  volume = {1652},
  pages = {353--365},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-15743-1_33},
  abstract = {We describe a system for automatic extraction of semantic relations between entities in a medical corpus of clinical cases. It builds upon a previously developed module for entity extraction and upon a morphosyntactic parser. It uses experimentally designed rules based on syntactic dependencies and trigger words, as well as on sequencing and nesting of entities of particular types. The results obtained on a small corpus are promising. Our larger perspective is transforming information extracted from medical texts into knowledge graphs.},
  copyright = {All rights reserved},
  isbn = {978-3-031-15742-4 978-3-031-15743-1},
  langid = {english},
  keywords = {me,nosource},
  file = {C:\Users\nhiot\OneDrive\zotero\2022\Savary et al. - 2022 - Relation Extraction from Clinical Cases for a Know.pdf}
}

@article{scheweLimitationsRuleTriggering1998,
  ids = {ScT98},
  title = {Limitations of Rule Triggering Systems for Integrity Maintenance in the Context of Transition Specifications},
  author = {Schewe, Klaus-Dieter and Thalheim, Bernhard},
  year = {1998},
  month = jan,
  journal = {Acta Cybernetica},
  volume = {13},
  number = {3},
  pages = {277--304},
  publisher = {University of Szeged},
  issn = {2676-993X},
  doi = {10.1007/3-540-63699-4_12},
  urldate = {2023-08-08},
  abstract = {Integrity Maintenance is considered one of the major application fields of rule triggering systems (RTSs). In the case of a given integrity constraint being violated by a database transition these systems trigger repairing actions. Then it is necessary to guarantee the termination of the RTS, its determinacy and the consistency of final states. Transition specifications provide some kind of dynamic semantics requiring certasin effects on database states to occur. In the context of transition specifications integrity maintenance has to cope with the additional problem of effect preservation. Limitations of RTSs with respect to this extended problems are investigated. It will be shown that for any set of constraints there exist non-repairable transitions, which depend on the closure of the constraint set. This implies that integrity maintenance by RTSs is only possible, if the constraint implication problem is decidable. Even if unrepairable transitions are excluded, this does not prevent the RTS to produce undesired behaviour. Analyzing the behaviour of RTSs leads to the definition of critical paths in associated rule hypergraphs and the requirement of such paths being absent. It will be shown that this requirement can be satisfied if the underlying set of constraints is stratified, but this notion turns out to be too strong to be also necessary. A sufficient and necessary condition for the absence of critical paths is obtained, if sets of constraints are required to be locally stratified.},
  copyright = {Copyright (c)},
  langid = {english},
  optbibsource = {dblp computer science bibliography, https://dblp.org},
  opttimestamp = {Tue, 09 Jul 2013 18:10:34 +0200},
  keywords = {nosource},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1998\\Schewe et Thalheim - 1998 - Limitations of rule triggering systems for integri2.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\QNHWPH6A\\294150.html}
}

@article{scheweTheoryConsistencyEnforcement1999,
  title = {Towards a Theory of Consistency Enforcement},
  author = {Schewe, Klaus-Dieter and Thalheim, Bernhard},
  year = {1999},
  month = feb,
  journal = {Acta Informatica},
  volume = {36},
  number = {2},
  pages = {97--141},
  issn = {1432-0525},
  doi = {10.1007/s002360050155},
  urldate = {2023-08-16},
  abstract = {State oriented specifications with invariants occur in almost all formal specification languages. Hence the problem is to prove the consistency of the specified operations with respect to the invariants. Whilst the problem seems to be easily solvable in predicative specifications, it usually requires sophisticated verification efforts, when specifications in the style of Dijkstra's guarded commands as e.g. in the specification language B are used. As an alternative consistency enforcement will be discussed in this paper. The basic idea is to replace inconsistent operations by new consistent ones preserving at the same time the intention of the old one. More precisely, this can be formalized by consistent spezializations, where specialization is a specific partial order on operations defined via predicate transformers. It will be shown that greatest consistent specializations (GCSs) always exist and are compatible with conjunctions of invariants. Then under certain mild restrictions the general construction of such GCSs is possible. Precisely, given the GCSs of simple basic assignments the GCS of a complex operation results from replacing involved assignments by their GCSs and the investigation of a guard. In general, GCS construction can be embedded in refinement calculi and therefore strengthens the systematic development of correct programs.},
  langid = {english},
  keywords = {Formal Specification,General Construction,Partial Order,Specification Language,Systematic Development},
  annotation = {QID: Q57376473},
  file = {C:\Users\nhiot\OneDrive\zotero\1999\Schewe et Thalheim - 1999 - Towards a theory of consistency enforcement.pdf}
}

@misc{SENDUPSEmanticNetwork,
  title = {{SENDUP -- SEmantic Network of Data: Utility and Privacy}},
  shorttitle = {{SENDUP -- SEmantic Network of Data}},
  url = {https://www.univ-orleans.fr/lifo/evenements/sendup-project/},
  urldate = {2024-02-06},
  langid = {french},
  file = {C:\Users\nhiot\Zotero\storage\STM6E5CY\sendup-project.html}
}

@inproceedings{seretanCollocationTranslationBased2007,
  title = {{Collocation translation based on sentence alignment and parsing}},
  booktitle = {{Actes de la 14{\`e}me conf{\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs}},
  author = {Seretan, Violeta and Wehrli, {\'E}ric},
  year = {2007},
  month = jun,
  pages = {375--384},
  publisher = {IRIT / ATALA},
  address = {Toulouse, France},
  url = {https://aclanthology.org/2007.jeptalnrecital-long.37},
  urldate = {2023-10-23},
  abstract = {Bien que de nombreux efforts aient {\'e}t{\'e} d{\'e}ploy{\'e}s pour extraire des collocations {\`a} partir de corpus de textes, seule une minorit{\'e} de travaux se pr{\'e}occupent aussi de rendre le r{\'e}sultat de l'extraction pr{\^e}t {\`a} {\^e}tre utilis{\'e} dans les applications TAL qui pourraient en b{\'e}n{\'e}ficier, telles que la traduction automatique. Cet article d{\'e}crit une m{\'e}thode pr{\'e}cise d'identification de la traduction des collocations dans un corpus parall{\`e}le, qui pr{\'e}sente les avantages suivants : elle peut traiter des collocation flexibles (et pas seulement fig{\'e}es) ; elle a besoin de ressources limit{\'e}es et d'un pouvoir de calcul raisonnable (pas d'alignement complet, pas d'entra{\^i}nement) ; elle peut {\^e}tre appliqu{\'e}e {\`a} plusieurs paires des langues et fonctionne m{\^e}me en l'absence de dictionnaires bilingues. La m{\'e}thode est bas{\'e}e sur l'information syntaxique provenant du parseur multilingue Fips. L'{\'e}valuation effectu{\'e}e sur 4000 collocations de type verbe-objet correspondant {\`a} plusieurs paires de langues a montr{\'e} une pr{\'e}cision moyenne de 89.8\% et une couverture satisfaisante (70.9\%). Ces r{\'e}sultats sont sup{\'e}rieurs {\`a} ceux enregistr{\'e}s dans l'{\'e}valuation d'autres m{\'e}thodes de traduction de collocations.},
  langid = {french},
  keywords = {nosource},
  file = {C:\Users\nhiot\OneDrive\zotero\2007\Seretan et Wehrli - 2007 - Collocation translation based on sentence alignmen.pdf}
}

@inproceedings{shenProbabilisticModelLinking2014,
  title = {A Probabilistic Model for Linking Named Entities in Web Text with Heterogeneous Information Networks},
  booktitle = {Proceedings of the 2014 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Shen, Wei and Han, Jiawei and Wang, Jianyong},
  year = {2014},
  month = jun,
  series = {{{SIGMOD}} '14},
  pages = {1199--1210},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2588555.2593676},
  urldate = {2024-04-05},
  abstract = {Heterogeneous information networks that consist of multi-type, interconnected objects are becoming ubiquitous and increasingly popular, such as social media networks and bibliographic networks. The task to link named entity mentions detected from the unstructured Web text with their corresponding entities existing in a heterogeneous information network is of practical importance for the problem of information network population and enrichment. This task is challenging due to name ambiguity and limited knowledge existing in the information network. Most existing entity linking methods focus on linking entities with Wikipedia or Wikipedia-derived knowledge bases (e.g., YAGO), and are largely dependent on the special features associated with Wikipedia (e.g., Wikipedia articles or Wikipedia-based relatedness measures). Since heterogeneous information networks do not have such features, these previous methods cannot be applied to our task. In this paper, we propose SHINE, the first probabilistic model to link the named entities in Web text with a heterogeneous information network to the best of our knowledge. Our model consists of two components: the entity popularity model that captures the popularity of an entity, and the entity object model that captures the distribution of multi-type objects appearing in the textual context of an entity, which is generated using meta-path constrained random walks over networks. As different meta-paths express diverse semantic meanings and lead to various distributions over objects, different paths have different weights in entity linking. We propose an effective iterative approach to automatically learning the weights for each meta-path based on the expectation-maximization (EM) algorithm without requiring any training data. Experimental results on a real world data set demonstrate the effectiveness and efficiency of our proposed model in comparison with the baselines.},
  isbn = {978-1-4503-2376-5},
  langid = {english},
  keywords = {domain-specific entity linking,entity linking,heterogeneous information networks},
  file = {C:\Users\nhiot\OneDrive\zotero\2014\Shen et al. - 2014 - A probabilistic model for linking named entities i.pdf}
}

@phdthesis{sirangeloRepresentingQueryingIncomplete2014,
  type = {{{HDR}}},
  title = {Representing and {{Querying Incomplete Information}}: A {{Data Interoperability Perspective}}},
  shorttitle = {Representing and {{Querying Incomplete Information}}},
  author = {Sirangelo, Cristina},
  year = {2014},
  month = dec,
  address = {Cachan},
  url = {https://tel.archives-ouvertes.fr/tel-01092547},
  urldate = {2023-08-07},
  abstract = {This habilitation thesis presents some of my most recent work, which has been done in collaboration with several other people. In particular this thesis concentrates on our contributions to the study of incomplete information in the context of data interoperability. In this scenario data is heterogenous and decentralized, needs to be integrated from several sources and exchanged between different applications. Incompleteness, i.e. the presence of ``missing'' or ``unknown'' portions of data, is naturally generated in data exchange and integration, due to data heterogeneity. The management of incomplete information poses new challenges in this context.The focus of our study is the development of models of incomplete information suitable to data interoperability tasks, and the study of techniques for efficiently querying several forms of incompleteness.},
  langid = {english},
  school = {Ecole Normale Sup{\'e}rieure de Cachan},
  keywords = {nosource},
  file = {C:\Users\nhiot\OneDrive\zotero\2014\Sirangelo - 2014 - Representing and Querying Incomplete Information .pdf}
}

@article{skavantzosNormalizingPropertyGraphs2023,
  ids = {skavantzosphilippNormalizingPropertyGraphs2023},
  title = {Normalizing {{Property Graphs}}},
  author = {Skavantzos, Philipp and Link, Sebastian},
  year = {2023},
  month = aug,
  journal = {Proceedings of the VLDB Endowment},
  volume = {16},
  number = {11},
  pages = {3031--3043},
  publisher = {VLDB Endowment},
  issn = {2150-8097},
  doi = {10.14778/3611479.3611506},
  urldate = {2023-10-18},
  abstract = {Normalization aims at minimizing sources of potential data inconsistency and costs of update maintenance incurred by data redundancy. For relational databases, different classes of dependencies cause data redundancy and have resulted in proposals such as Third, Boyce-Codd, Fourth and Fifth Normal Form. Features of more advanced data models make it challenging to extend achievements from the relational model to missing, non-atomic, or uncertain data. We initiate research on the normalization of graph data, starting with a class of functional dependencies tailored to property graphs. We show that this class captures important semantics of applications, constitutes a rich source of data redundancy, its implication problem can be decided in linear time, and facilitates the normalization of property graphs flexibly tailored to their labels and properties that are targeted by applications. We normalize property graphs into Boyce-Codd Normal Form without loss of data and dependencies whenever possible for the target labels and properties, but guarantee Third Normal Form in general. Experiments on real-world property graphs quantify and qualify various benefits of graph normalization: 1) removing redundant property values as sources of inconsistent data, 2) detecting inconsistency as violation of functional dependencies, 3) reducing update overheads by orders of magnitude, and 4) significant speed ups of aggregate queries.},
  langid = {english},
  file = {C:\Users\nhiot\OneDrive\zotero\2023\Skavantzos et Link - 2023 - Normalizing Property Graphs.pdf}
}

@inproceedings{songPrivacyRisksSecuring2019,
  title = {Privacy {{Risks}} of {{Securing Machine Learning Models}} against {{Adversarial Examples}}},
  booktitle = {Proceedings of the 2019 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}},
  author = {Song, Liwei and Shokri, Reza and Mittal, Prateek},
  year = {2019},
  month = nov,
  series = {{{CCS}} '19},
  pages = {241--257},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3319535.3354211},
  urldate = {2024-04-07},
  abstract = {The arms race between attacks and defenses for machine learning models has come to a forefront in recent years, in both the security community and the privacy community. However, one big limitation of previous research is that the security domain and the privacy domain have typically been considered separately. It is thus unclear whether the defense methods in one domain will have any unexpected impact on the other domain. In this paper, we take a step towards resolving this limitation by combining the two domains. In particular, we measure the success of membership inference attacks against six state-of-the-art defense methods that mitigate the risk of adversarial examples (i.e., evasion attacks). Membership inference attacks determine whether or not an individual data record has been part of a model's training set. The accuracy of such attacks reflects the information leakage of training algorithms about individual members of the training set. Adversarial defense methods against adversarial examples influence the model's decision boundaries such that model predictions remain unchanged for a small area around each input. However, this objective is optimized on training data. Thus, individual data records in the training set have a significant influence on robust models. This makes the models more vulnerable to inference attacks. To perform the membership inference attacks, we leverage the existing inference methods that exploit model predictions. We also propose two new inference methods that exploit structural properties of robust models on adversarially perturbed data. Our experimental evaluation demonstrates that compared with the natural training (undefended) approach, adversarial defense methods can indeed increase the target model's risk against membership inference attacks. When using adversarial defenses to train the robust models, the membership inference advantage increases by up to 4.5 times compared to the naturally undefended models. Beyond revealing the privacy risks of adversarial defenses, we further investigate the factors, such as model capacity, that influence the membership information leakage.},
  isbn = {978-1-4503-6747-9},
  keywords = {adversarial examples and defenses,machine learning,membership inference attacks},
  file = {C:\Users\nhiot\OneDrive\zotero\2019\Song et al. - 2019 - Privacy Risks of Securing Machine Learning Models .pdf}
}

@misc{SpaCy101Everything,
  title = {{{spaCy}} 101: {{Everything}} You Need to Know {$\cdot$} {{spaCy Usage Documentation}}},
  shorttitle = {{{spaCy}} 101},
  journal = {spaCy 101: Everything you need to know},
  url = {https://spacy.io/usage/spacy-101},
  urldate = {2024-03-21},
  abstract = {The most important concepts, explained in simple terms},
  langid = {english},
  file = {C:\Users\nhiot\Zotero\storage\WYT35G4P\spacy-101.html}
}

@article{sparckjonesStatisticalInterpretationTerm1972,
  ids = {sparckjonesStatisticalInterpretationTerm1972a,sparckjonesStatisticalInterpretationTerm2004},
  title = {A Statistical Interpretation of Term Specificity and Its Application in Retrieval},
  author = {Sparck Jones, Karen},
  year = {1972},
  month = jan,
  journal = {Journal of Documentation},
  volume = {28},
  number = {1},
  pages = {11--21},
  publisher = {MCB UP Ltd},
  issn = {0022-0418},
  doi = {10.1108/eb026526},
  urldate = {2023-09-22},
  abstract = {The exhaustivity of document descriptions and the specificity of index terms are usually regarded as independent. It is suggested that specificity should be interpreted statistically, as a function of term use rather than of term meaning. The effects on retrieval of variations in term specificity are examined, experiments with three test collections showing in particular that frequently-occurring terms are required for good overall performance. It is argued that terms should be weighted according to collection frequency, so that matches on less frequent, more specific, terms are of greater value than matches on frequent terms. Results for the test collections show that considerable improvements in performance are obtained with this very simple procedure.},
  keywords = {nosource},
  annotation = {QID: Q54296753},
  file = {C:\Users\nhiot\OneDrive\zotero\1972\Sparck Jones - 1972 - A statistical interpretation of term specificity a.pdf}
}

@inproceedings{steinmetzNaturalLanguageQuestions2019,
  title = {From {{Natural Language Questions}} to {{SPARQL Queries}}: {{A Pattern-based Approach}}},
  shorttitle = {From {{Natural Language Questions}} to {{SPARQL Queries}}},
  booktitle = {{{BTW}} 2019},
  author = {Steinmetz, Nadine and Arning, Ann-Katrin and Sattler, Kai-Uwe},
  year = {2019},
  pages = {289--308},
  publisher = {Gesellschaft f{\"u}r Informatik, Bonn},
  doi = {10/ggdnr4},
  urldate = {2019-11-29},
  abstract = {Linked Data knowledge bases are valuable sources of knowledge which give insights, reveal facts about various relationships and provide a large amount of metadata in well-structured form. Although the format of semantic information -- namely as RDF(S) -- is kept simple by representing each fact as a triple of subject, property and object, the access to the knowledge is only available using SPARQL queries on the data. Therefore, Question Answering (QA) systems provide a user-friendly way to access any type of knowledge base and especially for Linked Data sources to get insight into the semantic information. As RDF(S) knowledge bases are usually structured in the same way and provide per se semantic metadata about the contained information, we provide a novel approach that is independent from the underlying knowledge base. Thus, the main contribution of our proposed approach constitutes the simple replaceability of the underlying knowledge base. The algorithm is based on general question and query patterns and only accesses the knowledge base for the actual query generation and execution. This paper presents the proposed approach and an evaluation in comparison to state-of-the-art Linked Data approaches for challenges of QA systems.},
  isbn = {978-3-88579-683-1},
  langid = {english},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2019\\Steinmetz et al. - 2019 - From Natural Language Questions to SPARQL Queries.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\PZUKHUTP\\21702.html}
}

@inproceedings{strubellEnergyPolicyConsiderations2019,
  title = {Energy and {{Policy Considerations}} for {{Deep Learning}} in {{NLP}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  year = {2019},
  month = jun,
  pages = {3645--3650},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/P19-1355},
  urldate = {2023-10-23},
  abstract = {Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.},
  keywords = {Computer Science - Computation and Language},
  annotation = {QID: Q64512333},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2019\\Strubell et al. - 2019 - Energy and Policy Considerations for Deep Learning2.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\IYNS5HUA\\1906.html}
}

@inproceedings{tellierHowSplitRecursive2008,
  title = {How to Split Recursive Automata},
  booktitle = {Grammatical Inference: {{Algorithms}} and Applications. 9th International Colloquium, {{ICGI}} 2008 Saint-Malo, France, September 22-24, 2008 Proceedings},
  author = {Tellier, Isabelle},
  editor = {Clark, Alexander and Coste, Fran{\c c}ois and Miclet, Laurent},
  year = {2008},
  series = {{{LNAI}}},
  volume = {5278},
  pages = {200--212},
  publisher = {Springer},
  doi = {10.1007/978-3-540-88009-7_16},
  hal_id = {inria-00341770},
  hal_version = {v1},
  keywords = {categorial grammars,grammatical inference,recursive automata},
  file = {C:\Users\nhiot\Zotero\storage\5W74HWHS\Tellier - 2008 - How to split recursive automata.pdf}
}

@article{utamaEndtoendNeuralNatural2018,
  title = {An {{End-to-end Neural Natural Language Interface}} for {{Databases}}},
  author = {Utama, Prasetya and Weir, Nathaniel and Basik, Fuat and Binnig, Carsten and Cetintemel, Ugur and H{\"a}ttasch, Benjamin and Ilkhechi, Amir and Ramaswamy, Shekar and Usta, Arif},
  year = {2018},
  month = apr,
  journal = {arXiv preprint arXiv:1804.00401},
  eprint = {1804.00401},
  url = {http://arxiv.org/abs/1804.00401},
  urldate = {2019-12-03},
  abstract = {The ability to extract insights from new data sets is critical for decision making. Visual interactive tools play an important role in data exploration since they provide non-technical users with an effective way to visually compose queries and comprehend the results. Natural language has recently gained traction as an alternative query interface to databases with the potential to enable non-expert users to formulate complex questions and information needs efficiently and effectively. However, understanding natural language questions and translating them accurately to SQL is a challenging task, and thus Natural Language Interfaces for Databases (NLIDBs) have not yet made their way into practical tools and commercial products. In this paper, we present DBPal, a novel data exploration tool with a natural language interface. DBPal leverages recent advances in deep models to make query understanding more robust in the following ways: First, DBPal uses a deep model to translate natural language statements to SQL, making the translation process more robust to paraphrasing and other linguistic variations. Second, to support the users in phrasing questions without knowing the database schema and the query features, DBPal provides a learned auto-completion model that suggests partial query extensions to users during query formulation and thus helps to write complex queries.},
  archiveprefix = {arxiv},
  optabstract = {The ability to extract insights from new data sets is critical for decision making. Visual interactive tools play an important role in data exploration since they provide non-technical users with an effective way to visually compose queries and comprehend the results. Natural language has recently gained traction as an alternative query interface to databases with the potential to enable non-expert users to formulate complex questions and information needs efficiently and effectively. However, understanding natural language questions and translating them accurately to SQL is a challenging task, and thus Natural Language Interfaces for Databases (NLIDBs) have not yet made their way into practical tools and commercial products. In this paper, we present DBPal, a novel data exploration tool with a natural language interface. DBPal leverages recent advances in deep models to make query understanding more robust in the following ways: First, DBPal uses a deep model to translate natural language statements to SQL, making the translation process more robust to paraphrasing and other linguistic variations. Second, to support the users in phrasing questions without knowing the database schema and the query features, DBPal provides a learned auto-completion model that suggests partial query extensions to users during query formulation and thus helps to write complex queries.},
  optkeywords = {Computer Science - Computation and Language, Computer Science - Databases, Computer Science - Human-Computer Interaction},
  optmonth = {04},
  opturl = {http://arxiv.org/abs/1804.00401},
  opturldate = {2019-12-03},
  keywords = {⛔ No DOI found,Computer Science - Computation and Language,Computer Science - Databases,Computer Science - Human-Computer Interaction},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2018\\Utama et al. - 2018 - An End-to-end Neural Natural Language Interface fo.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\3WER3DB7\\1804.html}
}

@misc{vargas-solarConversationalDataExploration2023,
  ids = {vargas-solarConversationalDataExploration2023b,vargas-solarConversationalDataExploration2023c},
  title = {Conversational {{Data Exploration}}: {{A Game-Changer}} for {{Designing Data Science Pipelines}}},
  shorttitle = {Conversational {{Data Exploration}}},
  author = {{Vargas-Solar}, Genoveva and Cerquitelli, Tania and {Espinosa-Oviedo}, Javier A. and Cheval, Fran{\c c}ois and Buchaille, Anthelme and Polgar, Luca},
  year = {2023},
  month = nov,
  journal = {arXiv e-prints},
  number = {arXiv:2311.06695},
  eprint = {2311.06695},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.06695},
  urldate = {2024-03-25},
  abstract = {This paper proposes a conversational approach implemented by the system Chatin for driving an intuitive data exploration experience. Our work aims to unlock the full potential of data analytics and artificial intelligence with a new generation of data science solutions. Chatin is a cutting-edge tool that democratises access to AI-driven solutions, empowering non-technical users from various disciplines to explore data and extract knowledge from it.},
  archiveprefix = {arxiv},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Databases,Computer Science - Human-Computer Interaction,Conversational Analysis,Data Science Pipelines},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2023\\Vargas-Solar et al. - 2023 - Conversational Data Exploration A Game-Changer fo.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\6QZFS7A7\\2311.html;C\:\\Users\\nhiot\\Zotero\\storage\\QFXINSZ2\\hal-04299052.html}
}

@misc{vargas-solarTextKnowledgeGraphs2023,
  ids = {vargas-solarTextKnowledgeGraphs2023a},
  title = {From {{Text}} to {{Knowledge}} with {{Graphs}}: Modelling, Querying and Exploiting Textual Content},
  shorttitle = {From {{Text}} to {{Knowledge}} with {{Graphs}}},
  author = {{Vargas-Solar}, Genoveva and Alves, Mirian Halfeld Ferrari and Forst, Anne-Lyse Minard},
  year = {2023},
  month = oct,
  number = {arXiv:2310.06122},
  eprint = {2310.06122},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.06122},
  urldate = {2024-03-25},
  abstract = {This paper highlights the challenges, current trends, and open issues related to the representation, querying and analytics of content extracted from texts. The internet contains vast text-based information on various subjects, including commercial documents, medical records, scientific experiments, engineering tests, and events that impact urban and natural environments. Extracting knowledge from this text involves understanding the nuances of natural language and accurately representing the content without losing information. This allows knowledge to be accessed, inferred, or discovered. To achieve this, combining results from various fields, such as linguistics, natural language processing, knowledge representation, data storage, querying, and analytics, is necessary. The vision in this paper is that graphs can be a well-suited text content representation once annotated and the right querying and analytics techniques are applied. This paper discusses this hypothesis from the perspective of linguistics, natural language processing, graph models and databases and artificial intelligence provided by the panellists of the DOING session in the MADICS Symposium 2022.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Databases},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2023\\Vargas-Solar et al. - 2023 - From Text to Knowledge with Graphs modelling, que.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\MW99I9FL\\2310.html}
}

@inproceedings{vargas-solarTranslatingDataScience2023,
  title = {Translating Data Science Queries from Natural Language into Graph Analytics Queries Using {{NLDS-QL}}},
  booktitle = {Workshops of the {{EDBT}}/{{ICDT}} 2023 {{Joint Conference}}},
  author = {{Vargas-Solar}, Genoveva and Dao, Karim and {Espinosa-Oviedo}, Javier},
  year = {2023},
  month = mar,
  number = {3379},
  address = {Ioannina, Greece},
  url = {https://hal.science/hal-04272051},
  urldate = {2024-03-25},
  abstract = {This paper introduces NLDS-QL 1 , a translator of data science questions expressed in natural language (NL) into data science queries on graph databases. Our translator is based on a simplified NL described by a grammar that specifies sentences combining keywords to refer to operations on graphs with the vocabulary of the graph schema. This paper shows NLDS-QL in action within a scenario to explore and analyse a graph base with patient diagnoses generated with the open-source Synthea.},
  keywords = {⛔ No DOI found,data science queries,graph analytics,graph stores,natural language processing},
  file = {C:\Users\nhiot\OneDrive\zotero\2023\Vargas-Solar et al. - 2023 - Translating data science queries from natural lang2.pdf}
}

@book{vukoticNeo4jAction2015,
  title = {Neo4j in Action},
  author = {Vukotic, Aleksa and Watt, Nicki and Abedrabbo, Tareq and Fox, Dominic and Partner, Jonas},
  year = {2015},
  volume = {22},
  publisher = {Manning Shelter Island},
  file = {C:\Users\nhiot\OneDrive\zotero\2015\Vukotic et al. - 2015 - Neo4j in action.pdf}
}

@article{wangComparisonWordEmbeddings2018,
  title = {A Comparison of Word Embeddings for the Biomedical Natural Language Processing},
  author = {Wang, Yanshan and Liu, Sijia and Afzal, Naveed and {Rastegar-Mojarad}, Majid and Wang, Liwei and Shen, Feichen and Kingsbury, Paul and Liu, Hongfang},
  year = {2018},
  month = nov,
  journal = {Journal of Biomedical Informatics},
  volume = {87},
  pages = {12--20},
  issn = {1532-0464},
  doi = {10.1016/j.jbi.2018.09.008},
  urldate = {2023-12-07},
  abstract = {Background Word embeddings have been prevalently used in biomedical Natural Language Processing (NLP) applications due to the ability of the vector representations being able to capture useful semantic properties and linguistic relationships between words. Different textual resources (e.g., Wikipedia and biomedical literature corpus) have been utilized in biomedical NLP to train word embeddings and these word embeddings have been commonly leveraged as feature input to downstream machine learning models. However, there has been little work on evaluating the word embeddings trained from different textual resources. Methods In this study, we empirically evaluated word embeddings trained from four different corpora, namely clinical notes, biomedical publications, Wikipedia, and news. For the former two resources, we trained word embeddings using unstructured electronic health record (EHR) data available at Mayo Clinic and articles (MedLit) from PubMed Central, respectively. For the latter two resources, we used publicly available pre-trained word embeddings, GloVe and Google News. The evaluation was done qualitatively and quantitatively. For the qualitative evaluation, we randomly selected medical terms from three categories (i.e., disorder, symptom, and drug), and manually inspected the five most similar words computed by embeddings for each term. We also analyzed the word embeddings through a 2-dimensional visualization plot of 377 medical terms. For the quantitative evaluation, we conducted both intrinsic and extrinsic evaluation. For the intrinsic evaluation, we evaluated the word embeddings' ability to capture medical semantics by measruing the semantic similarity between medical terms using four published datasets: Pedersen's dataset, Hliaoutakis's dataset, MayoSRS, and UMNSRS. For the extrinsic evaluation, we applied word embeddings to multiple downstream biomedical NLP applications, including clinical information extraction (IE), biomedical information retrieval (IR), and relation extraction (RE), with data from shared tasks. Results The qualitative evaluation shows that the word embeddings trained from EHR and MedLit can find more similar medical terms than those trained from GloVe and Google News. The intrinsic quantitative evaluation verifies that the semantic similarity captured by the word embeddings trained from EHR is closer to human experts' judgments on all four tested datasets. The extrinsic quantitative evaluation shows that the word embeddings trained on EHR achieved the best F1 score of 0.900 for the clinical IE task; no word embeddings improved the performance for the biomedical IR task; and the word embeddings trained on Google News had the best overall F1 score of 0.790 for the RE task. Conclusion Based on the evaluation results, we can draw the following conclusions. First, the word embeddings trained from EHR and MedLit can capture the semantics of medical terms better, and find semantically relevant medical terms closer to human experts' judgments than those trained from GloVe and Google News. Second, there does not exist a consistent global ranking of word embeddings for all downstream biomedical NLP applications. However, adding word embeddings as extra features will improve results on most downstream tasks. Finally, the word embeddings trained from the biomedical domain corpora do not necessarily have better performance than those trained from the general domain corpora for any downstream biomedical NLP task.},
  keywords = {Information extraction,Information retrieval,Machine learning,Natural language processing,Word embeddings},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2018\\Wang et al. - 2018 - A comparison of word embeddings for the biomedical.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\2KRG9QP2\\S1532046418301825.html}
}

@inproceedings{wangCrossdomainNaturalLanguage2019,
  title = {A Cross-Domain Natural Language Interface to Databases Using Adversarial Text Method},
  booktitle = {Database},
  author = {Wang, Wenlu},
  year = {2019},
  volume = {1},
  pages = {4},
  abstract = {A natural language interface (NLI) to databases is an interface that supports natural language queries to be executed by database management systems (DBMS). However, most NLIs are domain specific due to the complexity of the natural language questions, and an NLI trained on one domain is hard to be transferred another due to the discrepancies between di↵erent ontology. Inspired by the idea of stripping domain-specific information out of natural language questions, we propose a cross-domain NLI with a general purpose question tagging strategy and a multi-language neural translation model. Our question tagging strategy is able to extract the ``skeleton'' of the question that represents its semantic structure for any domain. With question tagging, every domain will be handled equally with a single multi-language neural translation model. Our preliminary experiments show that our multi-domain model has excellent cross-domain transferability.},
  langid = {english},
  optabstract = {A natural language interface (NLI) to databases is an interface that supports natural language queries to be executed by database management systems (DBMS). However, most NLIs are domain specific due to the complexity of the natural language questions, and an NLI trained on one domain is hard to be transferred another due to the discrepancies between di?erent ontology. Inspired by the idea of stripping domain-specific information out of natural language questions, we propose a cross-domain NLI with a general purpose question tagging strategy and a multi-language neural translation model. Our question tagging strategy is able to extract the skeleton of the question that represents its semantic structure for any domain. With question tagging, every domain will be handled equally with a single multi-language neural translation model. Our preliminary experiments show that our multi-domain model has excellent cross-domain transferability.},
  optlanguage = {en},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2019\Wang - 2019 - A cross-domain natural language interface to datab.pdf}
}

@article{watrobskiOntologyLearningMethods2020,
  ids = {watrobskiOntologyLearningMethods2020a},
  title = {Ontology Learning Methods from Text-an Extensive Knowledge-Based Approach},
  author = {W{\k a}tr{\'o}bski, Jaros{\l}aw},
  year = {2020},
  month = jan,
  journal = {Procedia Computer Science},
  series = {Knowledge-{{Based}} and {{Intelligent Information}} \& {{Engineering Systems}}: {{Proceedings}} of the 24th {{International Conference KES2020}}},
  volume = {176},
  pages = {3356--3368},
  publisher = {Elsevier},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2020.09.061},
  urldate = {2024-04-04},
  abstract = {Ontologies are a key element of the Semantic Web. They aim to capture basic knowledge by providing appropriate terms and formal relationships between them, so that they can be used in a machine-processable manner. Accordingly they enable automatic aggregation and practical use as well as unexpected reuse of distributed data sources. Ontologies may come from many different sources, pursuing different goals and quality criteria. However, performed manually ontology construction is a very complex and tedious task, thus many methods proposed offer automatic or semi-automatic way for ontology construction. Many of the methods have their own, specific features. Therefore, this paper proposes an extensive knowledge-based approach covering the domain of ontology learning methods from text. This work aims to collect the knowledge of available approaches for ontology learning and the prominent differences between them, drawing on best practices in ontology engineering. The proposed approach refers to methods and aims to enrich knowledge in the field of ontology learning (OL). In this paper, the author's ontology contains a set of various types of methods with main techniques used, and the necessary features in the miscellaneous approaches. The proposed an extensive knowledge-based approach uses a reasoning mechanism based on competency questions for individual approaches to determine their ontology learning method profiles. The validation stage has also been carried out. At the same time, it is an extension of the previous study in the form of a repository of knowledge about OL tools. In addition, the combination of both ontologies: tools and methods aim to provide a more efficient OL solution from text.},
  keywords = {Domain ontology learning,Methods for ontology leaninf from text,Ontology integration,Ontology learning from text},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2020\\Wątróbski - 2020 - Ontology learning methods from text-an extensive k.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\UKUXXCCZ\\S1877050920319566.html}
}

@incollection{wehrliActesTALN20002000,
  title = {Actes de {{TALN}} 2000 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 2000 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Wehrli, Eric and Rajman, Martin},
  year = {2000},
  month = oct,
  publisher = {EPFL / ATALA},
  address = {Lausanne},
  keywords = {nosource}
}

@inproceedings{weichselbraunMiningLeveragingBackground2018,
  title = {Mining and {{Leveraging Background Knowledge}} for {{Improving Named Entity Linking}}},
  booktitle = {Proceedings of the 8th {{International Conference}} on {{Web Intelligence}}, {{Mining}} and {{Semantics}}},
  author = {Weichselbraun, Albert and Kuntschik, Philipp and Bra{\c s}oveanu, Adrian M.P.},
  year = {2018},
  month = jun,
  series = {{{WIMS}} '18},
  pages = {1--11},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3227609.3227670},
  urldate = {2024-04-06},
  abstract = {Knowledge-rich Information Extraction (IE) methods aspire towards combining classical IE with background knowledge obtained from third-party resources. Linked Open Data repositories that encode billions of machine readable facts from sources such as Wikipedia play a pivotal role in this development. The recent growth of Linked Data adoption for Information Extraction tasks has shed light on many data quality issues in these data sources that seriously challenge their usefulness such as completeness, timeliness and semantic correctness. Information Extraction methods are, therefore, faced with problems such as name variance and type confusability. If multiple linked data sources are used in parallel, additional concerns regarding link stability and entity mappings emerge. This paper develops methods for integrating Linked Data into Named Entity Linking methods and addresses challenges in regard to mining knowledge from Linked Data, mitigating data quality issues, and adapting algorithms to leverage this knowledge. Finally, we apply these methods to Recognyze, a graph-based Named Entity Linking (NEL) system, and provide a comprehensive evaluation which compares its performance to other well-known NEL systems, demonstrating the impact of the suggested methods on its own entity linking performance.},
  isbn = {978-1-4503-5489-9},
  langid = {english},
  keywords = {Knowledge-rich Information Extraction,Linked Data Quality Information Extraction,Named Entity Linking,Natural Language Processing,Semantic Technologies},
  file = {C:\Users\nhiot\OneDrive\zotero\2018\Weichselbraun et al. - 2018 - Mining and Leveraging Background Knowledge for Imp2.pdf}
}

@article{weischedelOntoNotesLargeTraining2011,
  ids = {marcusOntoNotesLargeTraining,ralphOntoNotesLargeTraining2011,weischedelOntoNotesLargeTraining,weischedelOntoNotesLargeTraining2011a},
  title = {{{OntoNotes}}: {{A}} Large Training Corpus for Enhanced Processing},
  shorttitle = {{{OntoNotes}}},
  author = {Weischedel, Ralph and Hovy, Eduard and Marcus, Mitchell and Palmer, Martha and Belvin, Robert and Pradhan, Sameer and Ramshaw, Lance and Xue, Nianwen},
  year = {2011},
  journal = {Handbook of Natural Language Processing and Machine Translation},
  pages = {59},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2011\Weischedel et al. - 2011 - OntoNotes A large training corpus for enhanced pr.pdf}
}

@misc{weischedelralphOntoNotesRelease2013,
  title = {{{OntoNotes Release}} 5.0},
  author = {{Weischedel, Ralph} and {Palmer, Martha} and {Marcus, Mitchell} and {Hovy, Eduard} and {Pradhan, Sameer} and {Ramshaw, Lance} and {Xue, Nianwen} and {Taylor, Ann} and {Kaufman, Jeff} and {Franchini, Michelle} and {El-Bachouti, Mohammed} and {Belvin, Robert} and {Houston, Ann}},
  year = {2013},
  month = oct,
  pages = {2806280 KB},
  doi = {10.35111/XMHB-2B84},
  urldate = {2024-03-21},
  abstract = {{$<$}h3{$>$}Introduction{$<$}/h3{$><$}br{$>$}  {$<$}p{$>$}OntoNotes Release 5.0 is the final release of the OntoNotes project, a collaborative effort between {$<$}a href="http://www.bbn.com/" rel="nofollow"{$>$}BBN Technologies{$<$}/a{$>$}, the {$<$}a href="http://www.colorado.edu/" rel="nofollow"{$>$}University of Colorado{$<$}/a{$>$}, the {$<$}a href="http://www.upenn.edu/" rel="nofollow"{$>$}University of Pennsylvania{$<$}/a{$>$} and the {$<$}a href="http://www.isi.edu/home" rel="nofollow"{$>$}University of Southern Californias Information Sciences Institute{$<$}/a{$>$}. The goal of the project was to annotate a large corpus comprising various genres of text (news, conversational telephone speech, weblogs, usenet newsgroups, broadcast, talk shows) in three languages (English, Chinese, and Arabic) with structural information (syntax and predicate argument structure) and shallow semantics (word sense linked to an ontology and coreference).{$<$}/p{$><$}br{$>$}  {$<$}p{$>$}OntoNotes Release 5.0 contains the content of earlier releases -- OntoNotes Release 1.0 {$<$}a href="http://catalog.ldc.upenn.edu/LDC2007T21" rel="nofollow"{$>$}LDC2007T21{$<$}/a{$>$}, OntoNotes Release 2.0 {$<$}a href="http://catalog.ldc.upenn.edu/LDC2008T04" rel="nofollow"{$>$}LDC2008T04{$<$}/a{$>$}, OntoNotes Release 3.0 {$<$}a href="http://catalog.ldc.upenn.edu/LDC2009T24" rel="nofollow"{$>$}LDC2009T24{$<$}/a{$>$} and OntoNotes Release 4.0 {$<$}a href="http://catalog.ldc.upenn.edu/LDC2011T03" rel="nofollow"{$>$}LDC2011T03{$<$}/a{$>$} -- and adds source data from and/or additional annotations for, newswire (News), broadcast news (BN), broadcast conversation (BC), telephone conversation (Tele) and web data (Web) in English and Chinese and newswire data in Arabic. Also contained is English pivot text (Old Testament and New Testament text). This cumulative publication consists of 2.9 million words with counts shown in the table below.{$<$}/p{$><$}br{$>$}  {$<$}table{$><$}br{$>$}  {$<$}tbody{$><$}br{$>$}  {$<$}tr{$><$}br{$>$}  {$<$}td{$>\&$}nbsp;{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}Arabic{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}English{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}Chinese{$<$}/td{$><$}br{$>$}  {$<$}/tr{$><$}br{$>$}  {$<$}tr{$><$}br{$>$}  {$<$}td{$>$}News{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}300k{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}625k{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}250k{$<$}/td{$><$}br{$>$}  {$<$}/tr{$><$}br{$>$}  {$<$}tr{$><$}br{$>$}  {$<$}td{$>$}BN{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}n/a{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}200k{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}250k{$<$}/td{$><$}br{$>$}  {$<$}/tr{$><$}br{$>$}  {$<$}tr{$><$}br{$>$}  {$<$}td{$>$}BC{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}n/a{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}200k{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}150k{$<$}/td{$><$}br{$>$}  {$<$}/tr{$><$}br{$>$}  {$<$}tr{$><$}br{$>$}  {$<$}td{$>$}Web{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}n/a{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}300k{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}150k{$<$}/td{$><$}br{$>$}  {$<$}/tr{$><$}br{$>$}  {$<$}tr{$><$}br{$>$}  {$<$}td{$>$}Tele{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}n/a{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}120k{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}100k{$<$}/td{$><$}br{$>$}  {$<$}/tr{$><$}br{$>$}  {$<$}tr{$><$}br{$>$}  {$<$}td{$>$}Pivot{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}n/a{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}n/a{$<$}/td{$><$}br{$>$}  {$<$}td{$>$}300{$<$}/td{$><$}br{$>$}  {$<$}/tr{$><$}br{$>$}  {$<$}/tbody{$><$}br{$>$}  {$<$}/table{$><$}br{$>$}  {$<$}p{$>\&$}nbsp;{$<$}/p{$><$}br{$>$}  {$<$}p{$>$}The OntoNotes project built on two time-tested resources, following the {$<$}a href="http://catalog.ldc.upenn.edu/LDC99T42" rel="nofollow"{$>$}Penn Treebank{$<$}/a{$>$} for syntax and the {$<$}a href="http://catalog.ldc.upenn.edu/LDC2004T14" rel="nofollow"{$>$}Penn PropBank{$<$}/a{$>$} for predicate-argument structure. Its semantic representation includes word sense disambiguation for nouns and verbs, with some word senses connected to an ontology, and coreference.{$<$}/p{$><$}br{$>$}  {$<$}h3{$>$}Data{$<$}/h3{$><$}br{$>$}  {$<$}p{$>$}Documents describing the annotation guidelines and the routines for deriving various views of the data from the database are included in the documentation directory of this release. The annotation is provided both in separate text files for each annotation layer (Treebank, PropBank, word sense, etc.) and in the form of an integrated relational database (ontonotes-v5.0.sql.gz) with a Python API to provide convenient cross-layer access.{$<$}/p{$><$}br{$>$}  {$<$}p{$>$}It is a known issue that this release contains some non-validating XML files. The included tools, however, use a non-validating XML parser to parse the .xml files and load the appropriate values.{$<$}/p{$><$}br{$>$}  {$<$}h3{$>$}Tools{$<$}/h3{$><$}br{$>$}  {$<$}p{$>$}This release includes OntoNotes DB Tool v0.999 beta, the tool used to assemble the database from the original annotation files. It can be found in the directory tools/ontonotes-db-tool-v0.999b. This tool can be used to derive various views of the data from the database, and it provides an API that can implement new queries or views. Licensing information for the OntoNotes DB Tool package is included in its source directory.{$<$}/p{$><$}br{$>$}  {$<$}h3{$>$}Samples{$<$}/h3{$><$}br{$>$}  {$<$}p{$>$}Please view these samples:{$<$}/p{$><$}br{$>$}  {$<$}ul{$><$}br{$>$}  {$<$}li{$><$}a href="desc/addenda/LDC2013T19.cmn.jpg" rel="nofollow"{$>$}Chinese{$<$}/a{$><$}/li{$><$}br{$>$}  {$<$}li{$><$}a href="desc/addenda/LDC2013T19.ara.jpg" rel="nofollow"{$>$}Arabic{$<$}/a{$><$}/li{$><$}br{$>$}  {$<$}li{$><$}a href="desc/addenda/LDC2013T19.eng.jpg" rel="nofollow"{$>$}English{$<$}/a{$><$}/li{$><$}br{$>$}  {$<$}/ul{$><$}br{$>$}  {$<$}h3{$>$}Updates{$<$}/h3{$><$}br{$>$}  {$<$}p{$>$}Additional documentation was added on December 11, 2014\&nbsp; and is included in downloads after that date.\&nbsp;{$<$}/p{$><$}br{$>$}  {$<$}h3{$>$}Acknowledgment{$<$}/h3{$><$}br{$>$}  {$<$}p{$>$}This work is supported in part by the Defense Advanced Research Projects Agency, GALE Program Grant No. HR0011-06-1-003. The content of this publication does not necessarily reflect the position or policy of the Government, and no official endorsement should be inferred.{$<$}/p{$><$}/br{$>$}  Portions {\copyright} 2006 Abu Dhabi TV, {\copyright} 2006 Agence France Presse, {\copyright} 2006 Al-Ahram, {\copyright} 2006 Al Alam News Channel, {\copyright} 2006 Al Arabiya, {\copyright} 2006 Al Hayat, {\copyright} 2006 Al Iraqiyah, {\copyright} 2006 Al Quds-Al Arabi, {\copyright} 2006 Anhui TV, {\copyright} 2002, 2006 An Nahar, {\copyright} 2006 Asharq-al-Awsat, {\copyright} 2010 Bible League International, {\copyright} 2005 Cable News Network, LP, LLLP, {\copyright} 2000-2001 China Broadcasting System, {\copyright} 2000-2001, 2005-2006 China Central TV, {\copyright} 2006 China Military Online, {\copyright} 2000-2001 China National Radio, {\copyright} 2006 Chinanews.com, {\copyright} 2000-2001 China Television System, {\copyright} 1989 Dow Jones \& Company, Inc., {\copyright} 2006 Dubai TV, {\copyright} 2006 Guangming Daily, {\copyright} 2006 Kuwait TV, {\copyright} 2005-2006 National Broadcasting Company, Inc., {\copyright} 2006 New Tang Dynasty TV, {\copyright} 2006 Nile TV, {\copyright} 2006 Oman TV, {\copyright} 2006 PAC Ltd, {\copyright} 2006 Peoples Daily Online, {\copyright} 2005-2006 Phoenix TV, {\copyright} 2000-2001 Sinorama Magazine, {\copyright} 2006 Syria TV, {\copyright} 1996-1998, 2006 Xinhua News Agency, {\copyright} 1996, 1997, 2005, 2007, 2008, 2009, 2011, 2013 Trustees of the University of Pennsylvania}
}

@article{wengMedicalSubdomainClassification2017,
  ids = {weng2017medical},
  title = {Medical Subdomain Classification of Clinical Notes Using a Machine Learning-Based Natural Language Processing Approach},
  author = {Weng, Wei-Hung and Wagholikar, Kavishwar B. and McCray, Alexa T. and Szolovits, Peter and Chueh, Henry C.},
  year = {2017},
  month = dec,
  journal = {BMC Medical Informatics and Decision Making},
  volume = {17},
  number = {1},
  pages = {1--13},
  publisher = {BioMed Central},
  issn = {1472-6947},
  doi = {10.1186/s12911-017-0556-8},
  urldate = {2021-05-28},
  abstract = {The medical subdomain of a clinical note, such as cardiology or neurology, is useful content-derived metadata for developing machine learning downstream applications. To classify the medical subdomain of a note accurately, we have constructed a machine learning-based natural language processing (NLP) pipeline and developed medical subdomain classifiers based on the content of the note.},
  keywords = {{Medical Decision Making, Computer-assisted},Deep Learning,Distributed Representation,Machine Learning,Natural Language Processing,Unified Medical Language System},
  annotation = {QID: Q45943393},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2017\\Weng et al. - 2017 - Medical subdomain classification of clinical notes.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\SHS3H2R3\\s12911-017-0556-8.html}
}

@article{whiteGraphSemigroupHomomorphisms1983,
  title = {Graph and Semigroup Homomorphisms on Networks of Relations},
  author = {White, Douglas R. and Reitz, Karl P.},
  year = {1983},
  month = jun,
  journal = {Social Networks},
  volume = {5},
  number = {2},
  pages = {193--234},
  issn = {0378-8733},
  doi = {10.1016/0378-8733(83)90025-4},
  urldate = {2023-10-31},
  abstract = {The algebraic definitions presented here are motivated by our search for an adequate formalization of the concepts of social roles as regularities in social network patterns. The theorems represent significant homomorphic reductions of social networks which are possible using these definitions to capture the role structure of a network. The concepts build directly on the pioneering work of S.F. Nadel (1957) and the pathbreaking approach to blockmodeling introduced by Lorrain and White (1971) and refined in subsequent years (White, Boorman and Breiger 1976;Boorman and White 1976; Arabie, Boorman and Levitt, 1978; Sailer, 1978). Blockmodeling is one of the predominant techniques for deriving structural models of social networks. When a network is represented by a directed multigraph, a blockmodel of the multigraph can be characterized as mapping points and edges onto their images in a reduced multigraph. The relations in a network or multigraph can also be composed to form a semigroup. In the first part of the paper we examine ``graph'' homomorphisms, or homomorphic mappings of the points or actors in a network. A family of basic concepts of role equivalence are introduced, and theorems presented to show the structure preserving properties of their various induced homomorphisms. This extends the ``classic'' approach to blockmodeling via the equivalence of positions. Lorrain and White (1971), Pattison (1980), Boyd, 1980, Boyd, 1982, and most recently Bonacich (1982) have explored the topic taken up in the second part of this paper, namely the homomorphic reduction of the semigroup of relations on a network, and the relation between semigroup and graph homomorphisms. Our approach allows us a significant beginning in reducing the complexity of a multigraph by collapsing relations which play a similar ``role'' in the network.},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1983\\White et Reitz - 1983 - Graph and semigroup homomorphisms on networks of r.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\9HE8RHYA\\0378873383900254.html}
}

@article{winslettModelbasedApproachUpdating1988,
  title = {A Model-Based Approach to Updating Databases with Incomplete Information},
  author = {Winslett, Marianne},
  year = {1988},
  month = jun,
  journal = {ACM Transactions on Database Systems (TODS)},
  volume = {13},
  number = {2},
  pages = {167--196},
  publisher = {ACM New York, NY, USA},
  issn = {0362-5915},
  doi = {10.1145/42338.42386},
  urldate = {2023-08-03},
  abstract = {Suppose one wishes to construct, use, and maintain a database of facts about the real world, even though the state of that world is only partially known. In the artificial intelligence domain, this problem arises when an agent has a base set of beliefs that reflect partial knowledge about the world, and then tries to incorporate new, possibly contradictory knowledge into this set of beliefs. In the database domain, one facet of this situation is the well-known null values problem. We choose to represent such a database as a logical theory, and view the models of the theory as representing possible states of the world that are consistent with all known information. How can new information be incorporated into the database? For example, given the new information that ``b or c is true,'' how can one get rid of all outdated information about b and c, add the new information, and yet in the process not disturb any other information in the database? In current-day database management systems, the difficult and tedious burden of determining exactly what to add and remove from the database is placed on the user. The goal of our research was to relieve users of that burden, by equipping the database management system with update algorithms that can automatically determine what to add and remove from the database. Under our approach, new information about the state of the world is input to the database management system as a well-formed formula that the state of the world is now known to satisfy. We have constructed database update algorithms to interpret this update formula and incorporate the new information represented by the formula into the database without further assistance from the user. In this paper we show how to embed the incomplete database and the incoming information in the language of mathematical logic, explain the semantics of our update operators, and discuss the algorithms that implement these operators.},
  langid = {english},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1988\\Winslett - 1988 - A model-based approach to updating databases with .pdf;C\:\\Users\\nhiot\\Zotero\\storage\\IP27ELLK\\42338.html}
}

@book{winslettUpdatingLogicalDatabases1990,
  title = {Updating Logical Databases},
  author = {Winslett, Marianne Southall},
  year = {1990},
  series = {Cambridge Tracts in Theoretical Computer Science},
  number = {9},
  publisher = {Cambridge University Press},
  address = {Cambridge ; New York},
  isbn = {978-0-521-37371-5},
  lccn = {QA76.9.D3 W567 1990},
  keywords = {Database management}
}

@article{wishartDrugBankKnowledgebaseDrugs2008,
  title = {{{DrugBank}}: A Knowledgebase for Drugs, Drug Actions and Drug Targets},
  shorttitle = {{{DrugBank}}},
  author = {Wishart, David S. and Knox, Craig and Guo, An Chi and Cheng, Dean and Shrivastava, Savita and Tzur, Dan and Gautam, Bijaya and Hassanali, Murtaza},
  year = {2008},
  journal = {Nucleic acids research},
  volume = {36},
  number = {suppl\_1},
  pages = {D901--D906},
  publisher = {Oxford University Press},
  doi = {10.1093/nar/gkm958},
  annotation = {QID: Q24650300},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2008\\Wishart et al. - 2008 - DrugBank a knowledgebase for drugs, drug actions .pdf;C\:\\Users\\nhiot\\Zotero\\storage\\ESEGG5Y5\\2508024.html;C\:\\Users\\nhiot\\Zotero\\storage\\PZDDAR83\\2508024.html}
}

@article{wishartDrugBankMajorUpdate2018,
  title = {{{DrugBank}} 5.0: A Major Update to the {{DrugBank}} Database for 2018},
  shorttitle = {{{DrugBank}} 5.0},
  author = {Wishart, David S. and Feunang, Yannick D. and Guo, An C. and Lo, Elvis J. and Marcu, Ana and Grant, Jason R. and Sajed, Tanvir and Johnson, Daniel and Li, Carin and Sayeeda, Zinat},
  year = {2018},
  journal = {Nucleic acids research},
  volume = {46},
  number = {D1},
  pages = {D1074--D1082},
  publisher = {Oxford University Press},
  doi = {10.1093/nar/gkx1037},
  annotation = {QID: Q47128239},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2018\\Wishart et al. - 2018 - DrugBank 5.0 a major update to the DrugBank datab.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\6JDTJNUQ\\4602867.html}
}

@article{wongOntologyLearningText2012,
  ids = {wongOntologyLearningText2011},
  title = {Ontology {{Learning}} from {{Text}}: {{A Look Back}} and into the {{Future}}},
  shorttitle = {Ontology {{Learning}} from {{Text}}},
  author = {Wong, Wilson and Liu, Wei and Bennamoun, Mohammed},
  year = {2012},
  month = aug,
  journal = {ACM Computing Surveys},
  volume = {44},
  number = {4},
  pages = {1--36},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/2333112.2333115},
  urldate = {2024-04-06},
  abstract = {Ontologies are often viewed as the answer to the need for interoperable semantics in modern information systems. The explosion of textual information on the Read/Write Web coupled with the increasing demand for ontologies to power the Semantic Web have made (semi-)automatic ontology learning from text a very promising research area. This together with the advanced state in related areas, such as natural language processing, have fueled research into ontology learning over the past decade. This survey looks at how far we have come since the turn of the millennium and discusses the remaining challenges that will define the research directions in this area in the near future.},
  langid = {english},
  file = {C:\Users\nhiot\OneDrive\zotero\2012\Wong et al. - 2012 - Ontology learning from text A look back and into .pdf}
}

@inproceedings{xuMirrorNaturalLanguage2023,
  title = {Mirror: {{A Natural Language Interface}} for {{Data Querying}}, {{Summarization}}, and {{Visualization}}},
  shorttitle = {Mirror},
  booktitle = {Companion {{Proceedings}} of the {{ACM Web Conference}} 2023},
  author = {Xu, Canwen and McAuley, Julian and Wang, Penghan},
  year = {2023},
  month = apr,
  series = {{{WWW}} '23 {{Companion}}},
  eprint = {2303.08697},
  primaryclass = {cs},
  pages = {49--52},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3543873.3587309},
  urldate = {2024-03-25},
  abstract = {We present Mirror, an open-source platform for data exploration and analysis powered by large language models. Mirror offers an intuitive natural language interface for querying databases, and automatically generates executable SQL commands to retrieve relevant data and summarize it in natural language. In addition, users can preview and manually edit the generated SQL commands to ensure the accuracy of their queries. Mirror also generates visualizations to facilitate understanding of the data. Designed with flexibility and human input in mind, Mirror is suitable for both experienced data analysts and non-technical professionals looking to gain insights from their data.},
  archiveprefix = {arxiv},
  isbn = {978-1-4503-9419-2},
  langid = {english},
  keywords = {automatic data analysis,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Databases,Computer Science - Software Engineering,natural language interface,pretrained language model,semantic parsing},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2023\\Xu et al. - 2023 - Mirror A Natural Language Interface for Data Quer2.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\H8LEB38D\\2303.html}
}

@inproceedings{zafarFormalQueryGeneration2018,
  title = {Formal {{Query Generation}} for {{Question Answering}} over {{Knowledge Bases}}},
  booktitle = {European {{Semantic Web Conference}}},
  author = {Zafar, Hamid and Napolitano, Giulio and Lehmann, Jens},
  editor = {Gangemi, Aldo and Navigli, Roberto and Vidal, Maria-Esther and Hitzler, Pascal and Troncy, Rapha{\"e}l and Hollink, Laura and Tordai, Anna and Alam, Mehwish},
  year = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {714--728},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10/ggdnr8},
  abstract = {Question answering (QA) systems often consist of several components such as Named Entity Disambiguation (NED), Relation Extraction (RE), and Query Generation (QG). In this paper, we focus on the QG process of a QA pipeline on a large-scale Knowledge Base (KB), with noisy annotations and complex sentence structures. We therefore propose SQG, a SPARQL Query Generator with modular architecture, enabling easy integration with other components for the construction of a fully functional QA pipeline. SQG can be used on large open-domain KBs and handle noisy inputs by discovering a minimal subgraph based on uncertain inputs, that it receives from the NED and RE components. This ability allows SQG to consider a set of candidate entities/relations, as opposed to the most probable ones, which leads to a significant boost in the performance of the QG component. The captured subgraph covers multiple candidate walks, which correspond to SPARQL queries. To enhance the accuracy, we present a ranking model based on Tree-LSTM that takes into account the syntactical structure of the question and the tree representation of the candidate queries to find the one representing the correct intention behind the question. SQG outperforms the baseline systems and achieves a macro F1-measure of 75\% on the LC-QuAD dataset.},
  isbn = {978-3-319-93417-4},
  langid = {english},
  optabstract = {Question answering (QA) systems often consist of several components such as Named Entity Disambiguation (NED), Relation Extraction (RE), and Query Generation (QG). In this paper, we focus on the QG process of a QA pipeline on a large-scale Knowledge Base (KB), with noisy annotations and complex sentence structures. We therefore propose SQG, a SPARQL Query Generator with modular architecture, enabling easy integration with other components for the construction of a fully functional QA pipeline. SQG can be used on large open-domain KBs and handle noisy inputs by discovering a minimal subgraph based on uncertain inputs, that it receives from the NED and RE components. This ability allows SQG to consider a set of candidate entities/relations, as opposed to the most probable ones, which leads to a significant boost in the performance of the QG component. The captured subgraph covers multiple candidate walks, which correspond to SPARQL queries. To enhance the accuracy, we present a ranking model based on Tree-LSTM that takes into account the syntactical structure of the question and the tree representation of the candidate queries to find the one representing the correct intention behind the question. SQG outperforms the baseline systems and achieves a macro F1-measure of 75\% on the LC-QuAD dataset.},
  optaddress = {Cham},
  optdoi = {10/ggdnr8},
  opteditor = {Gangemi, Aldo and Navigli, Roberto and Vidal, Maria-Esther and Hitzler, Pascal and Troncy, Rapha{\"e}l and Hollink, Laura and Tordai, Anna and Alam, Mehwish},
  optlanguage = {en},
  optpublisher = {Springer International Publishing},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2018\\Zafar et al. - 2018 - Formal Query Generation for Question Answering ove.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\IBX2KWLD\\978-3-319-93417-4_46.html}
}

@article{zanioloDatabaseRelationsNull1984,
  ids = {zanioloDatabaseRelationsNull1984a},
  title = {Database Relations with Null Values},
  author = {Zaniolo, Carlo},
  year = {1984},
  month = feb,
  journal = {Journal of Computer and System Sciences},
  volume = {28},
  number = {1},
  pages = {142--166},
  issn = {0022-0000},
  doi = {10.1016/0022-0000(84)90080-1},
  urldate = {2023-08-08},
  abstract = {A new formal approach is proposed for modeling incomplete database information by means of null values. The basis of our approach is an interpretation of nulls which obviates the need for more than one type of null. The conceptual soundness of this approach is demonstrated by generalizing the formal framework of the relational data model to include null values. In particular, the set-theoretical properties of relations with nulls are studied and the definitions of set inclusion, set union, and set difference are generalized. A simple and efficient strategy for evaluating queries in the presence of nulls is provided. The operators of relational algebra are then generalized accordingly. Finally, the deep-rooted logical and computational problems of previous approaches are reviewed to emphasize the superior practicability of the solution.},
  langid = {english},
  keywords = {nosource},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1984\\Zaniolo - 1984 - Database relations with null values.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\N6ACRI36\\0022000084900801.html}
}

@inproceedings{zelleLearningSemanticGrammars1993,
  title = {Learning Semantic Grammars with Constructive Inductive Logic Programming},
  booktitle = {Proceedings of the Eleventh National Conference on {{Artificial}} Intelligence},
  author = {Zelle, John M. and Mooney, Raymond J.},
  year = {1993},
  month = jul,
  series = {{{AAAI}}'93},
  pages = {817--822},
  publisher = {AAAI Press},
  address = {Washington, D.C.},
  urldate = {2024-04-07},
  abstract = {Automating the construction of semantic grammars is a difficult and interesting problem for machine learning. This paper shows how the semantic-grammar acquisition problem can be viewed as the learning of search-control heuristics in a logic program. Appropriate control rules are learned using a new first-order induction algorithm that automatically invents useful syntactic and semantic categories. Empirical results show that the learned parsers generalize well to novel sentences and out-perform previous approaches based on connectionist techniques.},
  isbn = {978-0-262-51071-4},
  keywords = {{$\warning$}️ Invalid DOI,⛔ No DOI found},
  file = {C:\Users\nhiot\Zotero\storage\KA57LBDC\Zelle et Mooney - 1993 - Learning semantic grammars with constructive induc.pdf}
}

@article{zhangRecentAdvancesMethods2013,
  title = {Recent Advances in Methods of Lexical Semantic Relatedness - {{A}} Survey},
  author = {Zhang, Ziqi and Gentile, Anna and Ciravegna, Fabio},
  year = {2013},
  month = oct,
  journal = {Natural Language Engineering},
  volume = {19},
  doi = {10.1017/S1351324912000125},
  abstract = {Measuring lexical semantic relatedness is an important task in Natural Language Processing (NLP). It is often a prerequisite to many complex NLP tasks. Despite an extensive amount of work dedicated to this area of research, there is a lack of an up-to-date survey in the field. This paper aims to address this issue with a study that is focused on four perspectives: (i) a comparative analysis of background information resources that are essential for measuring lexical semantic relatedness; (ii) a review of the literature with a focus on recent methods that are not covered in previous surveys; (iii) discussion of the studies in the biomedical domain where novel methods have been introduced but inadequately communicated across the domain boundaries; and (iv) an evaluation of lexical semantic relatedness methods and a discussion of useful lessons for the development and application of such methods. In addition, we discuss a number of issues in this field and suggest future research directions. It is believed that this work will be a valuable reference to researchers of lexical semantic relatedness and substantially support the research activities in this field.},
  file = {C:\Users\nhiot\OneDrive\zotero\2013\Zhang et al. - 2013 - Recent advances in methods of lexical semantic rel.pdf}
}

@article{zhangSimpleFastAlgorithms1989,
  title = {Simple {{Fast Algorithms}} for the {{Editing Distance}} between {{Trees}} and {{Related Problems}}},
  author = {Zhang, Kaizhong and Shasha, Dennis},
  year = {1989},
  month = dec,
  journal = {SIAM Journal on Computing},
  volume = {18},
  number = {6},
  pages = {1245--1262},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0097-5397},
  doi = {10/br6wtq},
  urldate = {2021-12-01},
  abstract = {Ordered labeled trees are trees in which the left-to-right order among siblings is significant. The distance between two ordered trees is considered to be the weighted number of edit operations (insert, delete, and modify) to transform one tree to another. The problem of approximate tree matching is also considered. Specifically, algorithms are designed to answer the following kinds of questions:1. What is the distance between two trees? 2. What is the minimum distance between \$T\_1 \$ and \$T\_2 \$ when zero or more subtrees can be removed from \$T\_2 \$? 3. Let the pruning of a tree at node n mean removing all the descendants of node n. The analogous question for prunings as for subtrees is answered. A dynamic programming algorithm is presented to solve the three questions in sequential time \$O({\textbar}T\_1 {\textbar} {\textbackslash}times {\textbar}T\_2 {\textbar} {\textbackslash}times {\textbackslash}min (\{{\textbackslash}textit\{depth\}\}(T\_1 ),\{{\textbackslash}textit\{leaves\}\}(T\_1 )) {\textbackslash}times {\textbackslash}min (\{{\textbackslash}textit\{depth\}\}(T\_2 ),\{{\textbackslash}textit\{leaves\}\}(T\_2 )))\$ and space \$O({\textbar}T\_1 {\textbar} {\textbackslash}times {\textbar}T\_2 {\textbar})\$ compared with \$O({\textbar}T\_1 {\textbar} {\textbackslash}times {\textbar}T\_2 {\textbar} {\textbackslash}times (\{{\textbackslash}textit\{depth\}\}(T\_1 )){\textasciicircum}2 {\textbackslash}times (\{{\textbackslash}textit\{depth\}\}(T\_2 )){\textasciicircum}2 )\$ for the best previous published algorithm due to Tai [J. Assoc. Comput. Mach., 26 (1979), pp, 422-433]. Further, the algorithm presented here can be parallelized to give time \$O({\textbar}T\_1 {\textbar} {\textbackslash}times {\textbar}T\_2 {\textbar})\$.},
  keywords = {68P05,68Q20,68Q25,68R10,dynamic programming,editing distance,parallel algorithm,pattern recognition,trees},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1989\\Zhang et Shasha - 1989 - Simple Fast Algorithms for the Editing Distance be.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\D6JLBYXQ\\0218082.html}
}

@article{zhengQuestionAnsweringKnowledge2018,
  title = {Question Answering over Knowledge Graphs: Question Understanding via Template Decomposition},
  shorttitle = {Question Answering over Knowledge Graphs},
  author = {Zheng, Weiguo and Yu, Jeffrey Xu and Zou, Lei and Cheng, Hong},
  year = {2018},
  month = jul,
  journal = {Proceedings of the VLDB Endowment},
  volume = {11},
  number = {11},
  pages = {1373--1386},
  issn = {2150-8097},
  doi = {10/gf3t2s},
  urldate = {2020-02-13},
  abstract = {The gap between unstructured natural language and structured data makes it challenging to build a system that supports using natural language to query large knowledge graphs. Many existing methods construct a structured query for the input question based on a syntactic parser. Once the input question is parsed incorrectly, a false structured query will be generated, which may result in false or incomplete answers. The problem gets worse especially for complex questions. In this paper, we propose a novel systematic method to understand natural language questions by using a large number of binary templates rather than semantic parsers. As sufficient templates are critical in the procedure, we present a low-cost approach that can build a huge number of templates automatically. To reduce the search space, we carefully devise an index to facilitate the online template decomposition. Moreover, we design effective strategies to perform the two-level disambiguations (i.e., entity-level ambiguity and structure-level ambiguity) by considering the query semantics. Extensive experiments over several benchmarks demonstrate that our proposed approach is effective as it significantly outperforms state-of-the-art methods in terms of both precision and recall.},
  optabstract = {The gap between unstructured natural language and structured data makes it challenging to build a system that supports using natural language to query large knowledge graphs. Many existing methods construct a structured query for the input question based on a syntactic parser. Once the input question is parsed incorrectly, a false structured query will be generated, which may result in false or incomplete answers. The problem gets worse especially for complex questions. In this paper, we propose a novel systematic method to understand natural language questions by using a large number of binary templates rather than semantic parsers. As sufficient templates are critical in the procedure, we present a low-cost approach that can build a huge number of templates automatically. To reduce the search space, we carefully devise an index to facilitate the online template decomposition. Moreover, we design effective strategies to perform the two-level disambiguations (i.e., entity-level ambiguity and structure-level ambiguity) by considering the query semantics. Extensive experiments over several benchmarks demonstrate that our proposed approach is effective as it significantly outperforms state-of-the-art methods in terms of both precision and recall.},
  optdoi = {10/gf3t2s},
  optmonth = {07},
  optnumber = {11},
  optshorttitle = {Question answering over knowledge graphs},
  opturl = {https://doi.org/10.14778/3236187.3236192},
  opturldate = {2020-02-13},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2018\\Zheng et al. - 2018 - Question answering over knowledge graphs question.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\97IHVNSX\\3236187.html}
}

@book{zipfPsychobiologyLanguage1935,
  title = {The Psycho-Biology of Language},
  author = {Zipf, G. K.},
  year = {1935},
  series = {The Psycho-Biology of Language},
  pages = {ix, 336},
  publisher = {Houghton, Mifflin},
  address = {Oxford, England},
  abstract = {Frequency counts of phonemes, morphemes, and words in samples of written discourse in diverse languages are presented in support of the generalization that the more complex any speech element, the less frequently does it occur. Thus, the greater the frequency of occurrence of words, the less tends to be their average length, and the smaller also is the number of different words. The relation between frequency and number of different words is said to be expressed by the formula ab2 = k, in which a represents the number of different words of a given frequency and b the frequency. The relationship between the magnitude of speech elements and their frequency is attributed to the operation of a "law" of linguistic change: that as the frequency of phonemes or of linguistic forms increases, their magnitude decreases. There is thus a tendency to "maintain an equilibrium" between length and frequency, and this tendency rests upon an "underlying law of economy." Human beings strive to maintain an "emotional equilibrium" between variety and repetitiveness of environmental factors and behavior. A speaker's discourse must represent a compromise between variety and repetitiveness adapted to the hearer's "tolerable limits of change in maintaining emotional equilibrium." This accounts for the maintenance of the relationship ab2 = k; the exponent of b expresses this "rate of variegation." (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C:\Users\nhiot\Zotero\storage\S2VNTYX9\1935-04756-000.html}
}

@inproceedings{zouNaturalLanguageQuestion2014,
  title = {Natural Language Question Answering over {{RDF}}: A Graph Data Driven Approach},
  shorttitle = {Natural Language Question Answering over {{RDF}}},
  booktitle = {Proceedings of the 2014 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Zou, Lei and Huang, Ruizhe and Wang, Haixun and Yu, Jeffrey Xu and He, Wenqiang and Zhao, Dongyan},
  year = {2014},
  month = jun,
  pages = {313--324},
  publisher = {ACM},
  address = {Snowbird Utah USA},
  doi = {10.1145/2588555.2610525},
  urldate = {2024-03-29},
  isbn = {978-1-4503-2376-5},
  langid = {english},
  optbibsource = {dblp computer science bibliography, https://dblp.org},
  optbiburl = {https://dblp.org/rec/conf/sigmod/ZouHWYHZ14.bib},
  opteditor = {Curtis E. Dyreson and Feifei Li and M. Tamer {\"O}zsu},
  opttimestamp = {Wed, 06 Mar 2019 07:05:24 +0100},
  opturl = {https://doi.org/10.1145/2588555.2610525},
  keywords = {⛔ No DOI found,nosource}
}

@incollection{zweigenbaumActesTALN19981998,
  title = {Actes de {{TALN}} 1998 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 1998 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Zweigenbaum, Pierre},
  year = {1998},
  month = jun,
  publisher = {ATALA},
  address = {Paris},
  keywords = {nosource}
}
