@book{abiteboulFoundationsDatabases1995,
  title = {Foundations of {{Databases}}},
  shorttitle = {Foundations of {{Databases}}},
  author = {Abiteboul, Serge and Hull, Richard and Vianu, Victor},
  year = {1995},
  edition = {1st},
  volume = {8},
  publisher = {{Addison-Wesley Longman Publishing Co., Inc.}},
  address = {{USA}},
  abstract = {This work presents comprehensive coverage of the foundations and theory of database systems. It is a reference to both classical material and advanced topics, bringing together many subjects including up-to-date coverage of object-oriented and logic databases. Numerous exercises are provided at three levels of difficulty. The book is intended for use by database professionals at all levels of experience, and graduate and senior level students in Advanced Theory of Databases.},
  googlebooks = {HN9QAAAAMAAJ},
  isbn = {978-0-201-53771-0},
  langid = {english},
  keywords = {\#nosource},
  file = {C:\Users\nhiot\Zotero\storage\KKV3MZ2R\abiteboul95.html}
}

@inproceedings{abiteboulUpdateSemanticsIncomplete1985,
  title = {Update Semantics for Incomplete Databases},
  booktitle = {Proceedings of the 11th International Conference on {{Very Large Data Bases}} - {{Volume}} 11},
  author = {Abiteboul, Serge and Grahne, G{\"o}sta},
  editor = {Pirotte, Alain and Vassiliou, Yannis},
  year = {1985},
  month = aug,
  series = {{{VLDB}} '85},
  pages = {1--12},
  publisher = {{VLDB Endowment}},
  address = {{Stockholm, Sweden}},
  url = {https://dblp.org/rec/conf/vldb/AbiteboulG85},
  abstract = {A database containing some incomplete information is viewed as a set of possible states of the real world. The semantics of updates is given based on simple set operations on the set of states. Some basic results concerning the capabilities of known models of incomplete databases to handle updates are exhibited.},
  isbn = {0-934613-17-6},
  keywords = {\#nosource,⛔ No DOI found},
  file = {C:\Users\nhiot\Zotero\storage\3SJCDNV4\1286760.html}
}

@article{ahoEfficientOptimizationClass1979,
  title = {Efficient Optimization of a Class of Relational Expressions},
  author = {Aho, Alfred V. and Sagiv, Yehoshua and Ullman, Jeffrey D.},
  year = {1979},
  journal = {ACM Trans. Database Syst.},
  volume = {4},
  number = {4},
  pages = {435--454},
  publisher = {{ACM New York, NY, USA}},
  issn = {0362-5915},
  doi = {10.1145/320107.320112},
  abstract = {The design of several database query languages has been influenced by Codd's relational algebra. This paper discusses the difficulty of optimizing queries based on the relational algebra operations select, project, and join. A matrix, called a tableau, is proposed as a useful device for representing the value of a query, and optimization of queries is couched in terms of finding a minimal tableau equivalent to a given one. Functional dependencies can be used to imply additional equivalences among tableaux. Although the optimization problem is NP-complete, a polynomial time algorithm exists to optimize tableaux that correspond to an important subclass of queries.},
  keywords = {equivalence of queries,NP-completeness,query optimization,relational algebra,relational database,tableaux},
  file = {C:\Users\nhiot\OneDrive\zotero\1979\Aho et al. - 1979 - Efficient optimization of a class of relational ex.pdf}
}

@article{ahoTheoryJoinsRelational1979,
  title = {The Theory of Joins in Relational Databases},
  author = {Aho, Alfred V. and Beeri, Catriel and Ullman, Jeffrey D.},
  year = {1979},
  journal = {ACM Transactions on Database Systems (TODS)},
  volume = {4},
  number = {3},
  pages = {297--314},
  publisher = {{ACM New York, NY, USA}},
  doi = {10.1145/320083.320091},
  abstract = {Answering queries in a relational database often requires that the natural join of two or more relations be computed. However, not all joins are semantically meaningful. This paper gives an efficient algorithm to determine whether the join of several relations is semantically meaningful (lossless) and an efficient algorithm to determine whether a set of relations has a subset with a lossy join. These algorithms assume that all data dependencies are functional. Similar techniques also apply to the case where data dependencies are multivalued.},
  file = {C:\Users\nhiot\OneDrive\zotero\1979\Aho et al. - 1979 - The theory of joins in relational databases.pdf}
}

@inproceedings{amaviNaturalLanguageQuerying2020,
  title = {Natural {{Language Querying System Through Entity Enrichment}}},
  booktitle = {{{ADBIS}}, {{TPDL}} and {{EDA}} 2020 {{Common Workshops}} and {{Doctoral Consortium}}: {{International Workshops}}: {{DOING}}, {{MADEISD}}, {{SKG}}, {{BBIGAP}}, {{SIMPDA}}, {{AIMinScience}} 2020 and {{Doctoral Consortium}}, {{Lyon}}, {{France}}, {{August}} 25\textendash 27, 2020, {{Proceedings}} 24},
  author = {Amavi, Joshua and Halfed Ferrari, Mirian and Hiot, Nicolas},
  editor = {Bellatreche, Ladjel and Bielikov{\'a}, M{\'a}ria and Boussa{\"i}d, Omar and Catania, Barbara and Darmont, J{\'e}r{\^o}me and Demidova, Elena and Duchateau, Fabien and Hall, Mark and Mer{\v c}un, Tanja and Novikov, Boris and Papatheodorou, Christos and Risse, Thomas and Romero, Oscar and Sautot, Lucile and Talens, Guilaine and Wrembel, Robert and {\v Z}umer, Maja},
  year = {2020},
  month = aug,
  series = {Communications in {{Computer}} and {{Information Science}}},
  volume = {1260},
  pages = {36--48},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-55814-7_3},
  abstract = {This paper focuses on a domain expert querying system over databases. It presents a solution designed for a French enterprise interested in offering a natural language interface for its clients. The approach, based on entity enrichment, aims at translating natural language queries into database queries. In this paper, the database is treated through a logical paradigm, suggesting the adaptability of our approach to different database models. The good precision of our method is shown through some preliminary experiments.},
  copyright = {All rights reserved},
  hal_id = {hal-02959502},
  hal_version = {v1},
  isbn = {978-3-030-55814-7},
  langid = {english},
  keywords = {\#nosource,Database query,me,NLI,NLP,Question answering},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2020\\Amavi et al. - 2020 - Natural Language Querying System Through Entity En.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\XGTK6R7N\\DOING13.mp4;C\:\\Users\\nhiot\\Zotero\\storage\\MT5B2A5X\\978-3-030-55814-7_3.html}
}

@incollection{amsiliActesTALN19991999,
  title = {Actes de {{TALN}} 1999 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 1999 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Amsili, Pascal},
  year = {1999},
  month = jul,
  publisher = {{ATALA}},
  address = {{Carg\`ese}},
  keywords = {\#nosource}
}

@inproceedings{anglesPGKeysKeysProperty2021,
  title = {{{PG-Keys}}: {{Keys}} for Property Graphs},
  shorttitle = {Pg-Keys},
  booktitle = {Proceedings of the 2021 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Angles, Renzo and Bonifati, Angela and Dumbrava, Stefania and Fletcher, George and Hare, Keith W. and Hidders, Jan and Lee, Victor E. and Li, Bei and Libkin, Leonid and Martens, Wim and Murlak, Filip and Perryman, Josh and Savkovic, Ognjen and Schmidt, Michael and Sequeda, Juan F. and Staworko, Slawek and Tomaszuk, Dominik},
  year = {2021},
  pages = {2423--2436},
  publisher = {{ACM}},
  doi = {10.1145/3448016.3457561},
  keywords = {\#nosource},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2021\\Angles et al. - 2021 - PG-Keys Keys for property graphs.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\7YULBJER\\3448016.html}
}

@book{baderGraphPartitioningGraph2013,
  title = {Graph {{Partitioning}} and {{Graph Clustering}}},
  author = {Bader, David and Meyerhenke, Henning and Sanders, Peter and Wagner, Dorothea},
  year = {2013},
  month = jan,
  series = {Contemporary {{Mathematics}}},
  volume = {588},
  publisher = {{American Mathematical Society}},
  issn = {0271-4132, 1098-3627},
  doi = {10.1090/conm/588},
  urldate = {2023-10-31},
  abstract = {Advancing research. Creating connections.},
  isbn = {978-0-8218-9038-7 978-0-8218-9868-0 978-0-8218-9869-7},
  langid = {english}
}

@book{barrasaBuildingKnowledgeGraphs2023,
  title = {Building {{Knowledge Graphs}}: {{A Practitioner}}'s {{Guide}}},
  author = {Barrasa, Jesus and Webber, Jim},
  year = {2023},
  month = jun,
  publisher = {{"O'Reilly Media, Inc."}},
  abstract = {Incredibly useful, knowledge graphs help organizations keep track of medical research, cybersecurity threat intelligence, GDPR compliance, web user engagement, and much more. They do so by storing interlinked descriptions of entities\textemdash objects, events, situations, or abstract concepts\textemdash and encoding the underlying information. How do you create a knowledge graph? And how do you move it from theory into production?Using hands-on examples, this practical book shows data scientists and data engineers how to build their own knowledge graphs. Authors Jes\'us Barrasa and Jim Webber from Neo4j illustrate common patterns for building knowledge graphs that solve many of today's pressing knowledge management problems. You'll quickly discover how these graphs become increasingly useful as you add data and augment them with algorithms and machine learning.Learn the organizing principles necessary to build a knowledge graphExplore how graph databases serve as a foundation for knowledge graphsUnderstand how to import structured and unstructured data into your graphFollow examples to build integration-and-search knowledge graphsLearn what pattern detection knowledge graphs help you accomplishExplore dependency knowledge graphs through examplesUse examples of natural language knowledge graphs and chatbotsUse graph algorithms and ML to gain insight into connected data},
  googlebooks = {6MTGEAAAQBAJ},
  isbn = {978-1-09-812706-0},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / Natural Language Processing,Computers / Data Science / Data Modeling \& Design,Computers / Data Science / Data Visualization,Computers / Data Science / General,Computers / Data Science / Machine Learning},
  file = {C:\Users\nhiot\OneDrive\zotero\2023\Barrasa et Webber - 2023 - Building Knowledge Graphs A Practitioner's Guide.pdf}
}

@inproceedings{barretGenericAbstractionsData2021,
  ids = {barretGenericAbstractionsData2021a},
  title = {Toward {{Generic Abstractions}} for {{Data}} of {{Any Model}}},
  booktitle = {{{BDA}} 2021-{{Informal}} Publication Only},
  author = {Barret, Nelly and Manolescu, Ioana and Upadhyay, Prajna},
  year = {2021},
  month = oct,
  url = {https://hal.inria.fr/hal-03344041},
  urldate = {2021-10-26},
  abstract = {Digital data sharing leads to unprecedented opportunities to develop data-driven systems for supporting economic activities, the social and political life, and science. Many open-access datasets are RDF graphs, but others are CSV files, Neo4J property graphs, JSON or XML documents, etc. Potential users need to understand a dataset in order to decide if it is useful for their goal. While some datasets come with a schema and/or documentation, this is not always the case. Data summarization or schema inference tools have been proposed, specializing in XML, or JSON, or the RDF data models. In this work, we present a dataset abstraction approach, which () applies on relational, CSV, XML, JSON, RDF or Property Graph data; () computes an abstraction meant for humans (as opposed to a schema meant for a parser); () integrates Information Extraction data profiling, to also classify dataset content among a set of categories of interest to the user. Our abstractions are conceptually close to an Entity-Relationship diagram, if one allows nested and possibly heterogeneous structure within entities.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2021\\Barret et al. - 2021 - Toward Generic Abstractions for Data of Any Model.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\NWGV8NRL\\hal-03344041.html}
}

@incollection{benamaraActesTALN20072007,
  title = {Actes de {{TALN}} 2007 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 2007 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Benamara, Farah and Hatout, Nabil and Muller, Philippe and Ozdowska, Sylwia},
  year = {2007},
  month = jun,
  publisher = {{IRIT / ATALA}},
  address = {{Toulouse}},
  keywords = {\#nosource}
}

@inproceedings{bernhardApprentissageNonSupervise2007,
  title = {{Apprentissage non supervis\'e de familles morphologiques par classification ascendante hi\'erarchique}},
  booktitle = {{Actes de la 14\`eme conf\'erence sur le Traitement Automatique des Langues Naturelles. Articles longs}},
  author = {Bernhard, Delphine},
  year = {2007},
  month = jun,
  pages = {345--354},
  publisher = {{IRIT / ATALA}},
  address = {{Toulouse, France}},
  url = {https://aclanthology.org/2007.jeptalnrecital-long.34},
  urldate = {2023-10-23},
  abstract = {Cet article pr\'esente un syst\`eme d'acquisition de familles morphologiques qui proc\`ede par apprentissage non supervis\'e \`a partir de listes de mots extraites de corpus de textes. L'approche consiste \`a former des familles par groupements successifs, similairement aux m\'ethodes de classification ascendante hi\'erarchique. Les crit\`eres de regroupement reposent sur la similarit\'e graphique des mots ainsi que sur des listes de pr\'efixes et de paires de suffixes acquises automatiquement \`a partir des corpus trait\'es. Les r\'esultats obtenus pour des corpus de textes de sp\'ecialit\'e en fran\c{c}ais et en anglais sont \'evalu\'es \`a l'aide de la base CELEX et de listes de r\'ef\'erence construites manuellement. L'\'evaluation d\'emontre les bonnes performances du syst\`eme, ind\'ependamment de la langue, et ce malgr\'e la technicit\'e et la complexit\'e morphologique du vocabulaire trait\'e.},
  hal_id = {hal-00800342},
  langid = {french},
  keywords = {\#nosource},
  file = {C:\Users\nhiot\OneDrive\zotero\2007\Bernhard - 2007 - Apprentissage non supervisé de familles morphologi.pdf}
}

@incollection{bigiActesTALN20142014,
  title = {Actes de {{TALN}} 2014 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 2014 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Bigi, Brigitte},
  year = {2014},
  month = jul,
  publisher = {{LPL / ATALA}},
  address = {{Marseille}},
  keywords = {\#nosource}
}

@incollection{blacheActesTALN20042004,
  title = {Actes de {{TALN}} 2004 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 2004 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Blache, Philippe},
  year = {2004},
  month = apr,
  publisher = {{LPL / ATALA}},
  address = {{F\`es, Maroc}},
  keywords = {\#nosource}
}

@inproceedings{blancoIssuesDetectingNegation2011,
  ids = {blancoIssuesDetectingNegation2011a},
  title = {Some {{Issues}} on {{Detecting Negation}} from {{Text}}},
  booktitle = {Twenty-{{Fourth International FLAIRS Conference}}},
  author = {Blanco, Eduardo and Moldovan, Dan},
  year = {2011},
  publisher = {{Citeseer}},
  url = {https://citeseerx.ist.psu.edu/document?repid=rep1\&type=pdf\&doi=1a29aca86eb2850a4d6df547b99356a806d8f040},
  urldate = {2023-10-06},
  abstract = {Negation is present in all human languages and it is used to reverse the polarity of parts of a statement. It is a complex phenomenon that interacts with many other aspects of language. Besides the direct meaning, negated statements often carry a latent positive meaning. Negation can be interpreted in terms of its scope and focus. This paper explores the importance of both scope and focus to capture the meaning of negated statements. Some issues on detecting negation from text are outlined, the forms in which negation occurs are depicted and heuristics to detect its scope and focus are proposed.},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2011\Blanco et Moldovan - 2011 - Some issues on detecting negation from text.pdf}
}

@book{bonifatiQueryingGraphs2018,
  title = {Querying {{Graphs}}},
  author = {Bonifati, Angela and Fletcher, George and Voigt, Hannes and Yakovets, Nikolay and Jagadish, H. V.},
  year = {2018},
  series = {Synthesis {{Lectures}} on {{Data Management}}},
  volume = {10},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-01864-0},
  abstract = {Graph data modeling and querying arises in many practical application domains such as social and biological networks where the primary focus is on concepts and their relationships and the rich patterns in these complex webs of interconnectivity. In this book, we present a concise unified view on the basic challenges which arise over the complete life cycle of formulating and processing queries on graph databases. To that purpose, we present all major concepts relevant to this life cycle, formulated in terms of a common and unifying ground: the property graph data model\textemdash the pre-dominant data model adopted by modern graph database systems. We aim especially to give a coherent and in-depth perspective on current graph querying and an outlook for future developments. Our presentation is self-contained, covering the relevant topics from: graph data models, graph query languages and graph query specification, graph constraints, and graph query processing. We conclude by indicating major open research challenges towards the next generation of graph data management systems.},
  isbn = {978-3-031-00736-1 978-3-031-01864-0},
  langid = {english},
  keywords = {\#nosource,Computers / Information Technology,Computers / Information Theory,Computers / Internet / General,Computers / Networking / General,Computers / Online Services,Computers / Programming / Algorithms},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2018\\Bonifati et al. - 2018 - Querying Graphs.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\9CCKMBG8\\978-3-031-01864-0.html}
}

@article{bousquetPERTOMedProjectExploiting2010,
  ids = {bousquetPERTOMedProjectExploiting2010a},
  title = {The {{PERTOMed Project}}: {{Exploiting}} and Validating Terminological Resources of Comparable {{Russian-French-English}} Corpora within Pharmacovigilance.},
  shorttitle = {The {{PERTOMed Project}}},
  author = {Bousquet, Cedric and {Zimina-Poirot}, Maria},
  year = {2010},
  month = feb,
  journal = {Terminology in Everyday Life},
  volume = {13},
  pages = {213--232},
  publisher = {{John Benjamins Publishing}},
  doi = {10.1075/tlrp.13.15bou},
  urldate = {2023-09-25},
  abstract = {The PERTOMed project is a pluri-disciplinary research initiative undertaken by several institutions in France. Applications considered within the part of the project described in this article concern pharmacovigilance and adverse drug reactions. We had multiple objectives: to create a specialized Russian Internet corpus; to test new tools and methods for term extraction from comparable multilingual texts and to build terminological resources including Russian. A trilingual Russian-French-English lexicon resulting form this work is freely available from the PERTOMed server.},
  langid = {english},
  file = {C:\Users\nhiot\OneDrive\zotero\2010\Bousquet et Zimina-Poirot - 2010 - The PERTOMed Project Exploiting and validating te.pdf}
}

@inproceedings{bravoSemanticallyCorrectQuery2006,
  title = {Semantically {{Correct Query Answers}} in the {{Presence}} of {{Null Values}}},
  booktitle = {Current {{Trends}} in {{Database Technology}} \textendash{} {{EDBT}} 2006},
  author = {Bravo, Loreto and Bertossi, Leopoldo},
  editor = {Grust, Torsten and H{\"o}pfner, Hagen and Illarramendi, Arantza and Jablonski, Stefan and Mesiti, Marco and M{\"u}ller, Sascha and Patranjan, Paula-Lavinia and Sattler, Kai-Uwe and Spiliopoulou, Myra and Wijsen, Jef},
  year = {2006},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {336--357},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11896548_27},
  abstract = {For several reasons a database may not satisfy a given set of integrity constraints (ICs), but most likely most of the information in it is still consistent with those ICs; and could be retrieved when queries are answered. Consistent answers to queries wrt a set of ICs have been characterized as answers that can be obtained from every possible minimally repaired consistent version of the original database. In this paper we consider databases that contain null values and are also repaired, if necessary, using null values. For this purpose, we propose first a precise semantics for IC satisfaction in a database with null values that is compatible with the way null values are treated in commercial database management systems. Next, a precise notion of repair is introduced that privileges the introduction of null values when repairing foreign key constraints, in such a way that these new values do not create an infinite cycle of new inconsistencies. Finally, we analyze how to specify this kind of repairs of a database that contains null values using disjunctive logic programs with stable model semantics.},
  isbn = {978-3-540-46790-8},
  langid = {english},
  keywords = {Database Instance,Disjunctive Logic Program,Integrity Constraint,Query Answer,Stable Model Semantic},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2006\\Bravo et Bertossi - 2006 - Semantically Correct Query Answers in the Presence2.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\CMVWCZF5\\semantically-correct-query-answers-in-z9VL6b.html}
}

@article{brinAnatomyLargescaleHypertextual1998,
  title = {The Anatomy of a Large-Scale Hypertextual {{Web}} Search Engine},
  author = {Brin, Sergey and Page, Lawrence},
  year = {1998},
  month = apr,
  journal = {Computer Networks and ISDN Systems},
  series = {Proceedings of the {{Seventh International World Wide Web Conference}}},
  volume = {30},
  number = {1-7},
  pages = {107--117},
  publisher = {{Elsevier}},
  issn = {0169-7552},
  doi = {10/dgjh27},
  urldate = {2023-09-22},
  abstract = {In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/ To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of Web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the Web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and Web proliferation, creating a Web search engine today is very different from three years ago. This paper provides an in-depth description of our large-scale Web search engine \textemdash{} the first such detailed public description we know of to date. Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want.},
  keywords = {Google,Information retrieval,PageRank,Search engines,World Wide Web},
  annotation = {QID: Q55970547},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1998\\Brin et Page - 1998 - The anatomy of a large-scale hypertextual Web sear2.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\5FZIELHS\\S016975529800110X.html}
}

@inproceedings{briscoeRobustAccurateStatistical2002,
  title = {Robust Accurate Statistical Annotation of General Text},
  booktitle = {Proceedings of the Third International Conference on Language Resources and Evaluation, {{LREC}} 2002, May 29-31, 2002, Las Palmas, Canary Islands, Spain},
  author = {Briscoe, Ted and Carroll, John A.},
  year = {2002},
  publisher = {{European Language Resources Association}},
  url = {http://www.lrec-conf.org/proceedings/lrec2002/sumarios/250.htm},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/lrec/BriscoeC02.bib},
  keywords = {\#nosource,⛔ No DOI found},
  timestamp = {Mon, 19 Aug 2019 15:23:48 +0200}
}

@article{brownMedicalDictionaryRegulatory1999,
  title = {The {{Medical Dictionary}} for {{Regulatory Activities}} ({{MedDRA}})},
  author = {Brown, Elliot G. and Wood, Louise and Wood, Sue},
  year = {1999},
  month = feb,
  journal = {Drug-Safety},
  volume = {20},
  number = {2},
  pages = {109--117},
  publisher = {{Springer}},
  issn = {1179-1942},
  doi = {10/czv6mb},
  abstract = {The International Conference on Harmonisation has agreed upon the structure and content of the Medical Dictionary for Regulatory Activities (MedDRA) version 2.0 which should become available in the early part of 1999.},
  langid = {english},
  file = {C:\Users\nhiot\Zotero\storage\FJ536H56\00002018-199920020-00002.html}
}

@inproceedings{cardonPresentationCampagneEvaluation2020,
  ids = {DEFT2020},
  title = {Pr\'esentation de La Campagne d'\'evaluation {{DEFT}} 2020 : Similarit\'e Textuelle En Domaine Ouvert et Extraction d'information Pr\'ecise Dans Des Cas Cliniques},
  booktitle = {6e Conf\'erence Conjointe Journ\'ees d'{{\'Etudes}} Sur La Parole ({{JEP}}, 33e \'Edition), Traitement Automatique Des Langues Naturelles ({{TALN}}, 27e \'Edition), Rencontre Des \'Etudiants Chercheurs En Informatique Pour Le Traitement Automatique Des Langues ({{R\'ECITAL}}, 22e \'Edition). {{Atelier D\'Efi}} Fouille de Textes},
  author = {Cardon, R{\'e}mi and Grabar, Natalia and Grouin, Cyril and Hamon, Thierry},
  editor = {Cardon, R{\'e}mi and Grabar, Natalia and Grouin, Cyril and Hamon, Thierry},
  year = {2020},
  pages = {1--13},
  publisher = {{ATALA}},
  address = {{Nancy, France}},
  url = {https://hal.archives-ouvertes.fr/hal-02784737},
  keywords = {\#nosource,⛔ No DOI found,Cas cliniques,extraction d'information,similarit\'e textuelle.}
}

@article{carlssonCharacterizationStabilityConvergence2010,
  ids = {carlssonCharacterizationStabilityConvergence2010a},
  title = {Characterization, Stability and Convergence of Hierarchical Clustering Methods},
  author = {Carlsson, Gunnar and M{\'e}moli, Facundo},
  year = {2010},
  journal = {Journal of Machine Learning Research},
  volume = {11},
  number = {Apr},
  pages = {1425--1470},
  publisher = {{Microtome Publishing}},
  address = {{US}},
  issn = {1533-7928},
  url = {https://www.jmlr.org/papers/volume11/carlsson10a/carlsson10a.pdf},
  abstract = {We study hierarchical clustering schemes under an axiomatic view. We show that within this framework, one can prove a theorem analogous to one of Kleinberg (2002), in which one obtains an existence and uniqueness theorem instead of a non-existence result. We explore further properties of this unique scheme: stability and convergence are established. We represent dendrograms as ultrametric spaces and use tools from metric geometry, namely the Gromov-Hausdorff distance, to quantify the degree to which perturbations in the input metric space affect the result of hierarchical methods. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {⛔ No DOI found,Cluster Analysis,Statistics},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2010\\Carlsson et Mémoli - 2010 - Characterization, stability and convergence of hie.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\MESX564R\\2010-16571-001.html}
}

@article{chabinConsistentUpdatingDatabases2020,
  title = {Consistent {{Updating}} of {{Databases}} with {{Marked Nulls}}},
  author = {Chabin, Jacques and {Halfeld-Ferrari}, Mirian and Laurent, Dominique},
  year = {2020},
  month = apr,
  journal = {Knowl. Inf. Syst.},
  volume = {62},
  number = {4},
  pages = {1571--1609},
  issn = {0219-3116},
  doi = {10.1007/s10115-019-01402-w},
  urldate = {2023-07-05},
  abstract = {This paper revisits the problem of consistency maintenance when insertions or deletions are performed on a valid database containing marked nulls. This problem comes back to light in real-world linked data or RDF databases when blank nodes are associated with null values. This paper proposes solutions for the main problems one has to face when dealing with updates and constraints, namely update determinism, minimal change and leanness of an RDF graph instance. The update semantics is formally introduced and the notion of core is used to ensure a database as small as possible (i.e.~~ the RDF graph leanness). Our algorithms allow the use of constraints such as tuple-generating dependencies, offering a way for solving many practical problems.},
  langid = {english},
  keywords = {\#nosource,Constraints,Logical database,Null values,RDF,TGD,Updates},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2020\\Chabin et al. - 2020 - Consistent Updating of Databases with Marked Nulls.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\4YE6ECDH\\s10115-019-01402-w.html}
}

@inproceedings{chabinContextdrivenQueryingSystem2018,
  title = {A {{Context-driven Querying System}} for {{Urban Graph Analysis}}},
  booktitle = {Proceedings of the 22nd {{International Database Engineering}} \& {{Applications Symposium}} on - {{IDEAS}} 2018},
  author = {Chabin, Jacques and {Gomes-Jr.}, Luiz and {Halfeld-Ferrari}, Mirian},
  year = {2018},
  series = {{{IDEAS}} 2018},
  pages = {297--301},
  publisher = {{ACM Press}},
  address = {{Villa San Giovanni, Italy}},
  doi = {10.1145/3216122.3216148},
  urldate = {2018-12-29},
  abstract = {This paper presents a context-driven query system for urban computing where users are responsible for defining their own restrictions over which datalog-like queries are built. Instead of imposing constraints on databases, our goal is to filter consistent data during the query process. Our query language is able to express aggregates in recursive rules, allowing it to capture network properties typical of graph analysis. This paper presents our query system and analyzes its capabilities using use cases in Urban Computing.},
  isbn = {978-1-4503-6527-7},
  langid = {english},
  keywords = {constraints,data graph,data quality,Query language,smart city},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2018\\Chabin et al. - 2018 - A Context-driven Querying System for Urban Graph A.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\MGXJHLF7\\citation.html;C\:\\Users\\nhiot\\Zotero\\storage\\QEF3INHL\\hal-01837921.html}
}

@inproceedings{chabinGraphRewritingRules2020,
  title = {Graph {{Rewriting Rules}} for {{RDF Database Evolution Management}}},
  booktitle = {Proceedings of the 22nd {{International Conference}} on {{Information Integration}} and {{Web-based Applications}} \& {{Services}}},
  author = {Chabin, Jacques and Eichler, C{\'e}dric and Halfed Ferrari, Mirian and Hiot, Nicolas},
  year = {2020},
  month = nov,
  series = {{{iiWAS}} '20},
  pages = {134--143},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3428757.3429126},
  urldate = {2022-05-13},
  abstract = {This paper introduces SetUp, a theoretical and applied framework for the management of RDF/S database evolution on the basis of graph rewriting rules. Rewriting rules formalize instance or schema changes, ensuring graph's consistency with respect to given constraints. Constraints considered in this paper are a well known variant of RDF/S semantic, but the approach can be adapted to user-defined constraints. Furthermore, SetUp manages updates by ensuring rule applicability through the generation of side-effects: new updates which guarantee that rule application conditions hold. We provide herein formal validation and experimental evaluation of SetUp.},
  isbn = {978-1-4503-8922-8},
  keywords = {Constraints,Database Management,Graph rewriting,me,Update},
  file = {C:\Users\nhiot\OneDrive\zotero\2020\Chabin et al. - 2020 - Graph Rewriting Rules for RDF Database Evolution M.pdf}
}

@article{chabinGraphRewritingRules2021,
  title = {Graph Rewriting Rules for {{RDF}} Database Evolution: Optimizing Side-Effect Processing},
  shorttitle = {Graph Rewriting Rules for {{RDF}} Database Evolution},
  author = {Chabin, Jacques and Eichler, C{\'e}dric and Halfed Ferrari, Mirian and Hiot, Nicolas},
  year = {2021},
  month = aug,
  journal = {International Journal of Web Information Systems},
  volume = {17},
  number = {6},
  pages = {622--644},
  publisher = {{Emerald Publishing Limited}},
  doi = {10.1108/IJWIS-03-2021-0033},
  keywords = {me},
  file = {C\:\\Users\\nhiot\\Zotero\\storage\\GNKTXUMA\\html.html;C\:\\Users\\nhiot\\Zotero\\storage\\UP65BUJK\\hal-03329965v1.html}
}

@techreport{chabinGraphRewritingSystem2020,
  type = {Research {{Report}}},
  title = {Graph {{Rewriting System}} for {{Consistent Evolution}} of {{RDF}}/{{S}} Databases},
  author = {Chabin, Jacques and Eichler, C{\'e}dric and Halfed Ferrari, Mirian and Hiot, Nicolas},
  year = {2020},
  institution = {{LIFO, Universit\'e d'Orl\'eans, INSA Centre Val de Loire}},
  url = {https://hal.science/hal-02560325},
  urldate = {2023-08-03},
  abstract = {This paper investigates the use of graph rewriting rules to model updates-instance or schema changes-on RDF/S databases which are expected to satisfy RDF intrinsic semantic constraints. Such databases being modeled as knowledge graphs, we propose graph rewriting rules formalizing atomic updates whose application transforms the graph and necessarily preserves its consistency. If an update has to be applied when the application conditions of the corresponding rule do not hold, side-effects are generated: they engender new updates in order to ensure the rule applicability. Our system, SetUp, implements our updating approach for RDF/S data and offers a theoretical and applied framework for ensuring consistency when a RDF knowledge graph evolves.},
  copyright = {All rights reserved},
  keywords = {me},
  file = {C:\Users\nhiot\OneDrive\zotero\2020\Chabin et al. - 2020 - Graph Rewriting System for Consistent Evolution of.pdf}
}

@misc{chabinIncrementalConsistentUpdating2023,
  title = {Incremental {{Consistent Updating}} of {{Incomplete Databases}}},
  author = {Chabin, Jacques and Ferrari, Mirian Halfeld and Hiot, Nicolas and Laurent, Dominique},
  year = {2023},
  month = feb,
  number = {arXiv:2302.06246},
  eprint = {2302.06246},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.06246},
  urldate = {2023-08-07},
  abstract = {Efficient consistency maintenance of incomplete and dynamic real-life databases is a quality label for further data analysis. In prior work, we tackled the generic problem of database updating in the presence of tuple generating constraints from a theoretical viewpoint. The current paper considers the usability of our approach by (a) introducing incremental update routines (instead of the previous from-scratch versions) and (b) removing the restriction that limits the contents of the database to fit in the main memory. In doing so, this paper offers new algorithms, proposes queries and data models inviting discussions on the representation of incompleteness on databases. We also propose implementations under a graph database model and the traditional relational database model. Our experiments show that computation times are similar globally but point to discrepancies in some steps.},
  archiveprefix = {arxiv},
  keywords = {\#nosource,⛔ No DOI found,Computer Science - Databases,me},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2023\\Chabin et al. - 2023 - Incremental Consistent Updating of Incomplete Data.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\WNDR3UKH\\2302.html}
}

@techreport{chabinSpecificationSideeffectManagement2020,
  type = {Research {{Report}}},
  title = {Specification of Side-Effect Management Techniques for Semantic Graph Sanitization},
  author = {Chabin, Jacques and Eichler, C{\'e}dric and Halfed Ferrari, Mirian and Hiot, Nicolas},
  year = {2020},
  number = {D6},
  institution = {{LIFO, Universit\'e d'Orl\'eans, INSA Centre Val de Loire}},
  url = {https://hal.science/hal-02957974},
  abstract = {The goal of the SENDUP project is to propose anonymisation mechanisms for data organized as graphs with an underlying semantic. Such mechanisms trig- gers updates on the database. This deliverable presents the update approach and side-effect management techniques defined in SENDUP. We focus on updates -instance or schema changes- on RDF/S databases which are expected to satisfy RDF intrinsic semantic constraints. We model RDF/S databases as type graphs and use graph rewriting rules to formalize updates. Such rules define both the effect of a graph transformation and its applicability conditions. We propose 19 rules modelling atomic updates and prove that their application necessarily preserves the database's consistency. If an update has to be applied when the application conditions of the corre- sponding rule do not hold, side-effects are generated: they engender new updates in order to ensure the rule applicability. These techniques are implemented in a dedicated software module S1 called SetUp. This deliverable also presents a preliminary experimental validation and evaluation of SetUp.},
  keywords = {\#nosource,me},
  file = {C:\Users\nhiot\OneDrive\zotero\2020\Chabin et al. - 2020 - Specification of side-effect management techniques.pdf}
}

@unpublished{chabinUsingGraphGrammar2019,
  title = {Using a Graph Grammar to Update a {{RDF}}/{{S}} Document},
  author = {Chabin, Jacques and Eichler, C{\'e}dric and Halfeld Ferrari, Mirian},
  year = {2019},
  langid = {english},
  keywords = {\#nosource}
}

@inproceedings{chandraOptimalImplementationConjunctive1977,
  title = {Optimal Implementation of Conjunctive Queries in Relational Data Bases},
  booktitle = {Proceedings of the Ninth Annual {{ACM}} Symposium on {{Theory}} of Computing},
  author = {Chandra, Ashok K. and Merlin, Philip M.},
  year = {1977},
  month = may,
  series = {{{STOC}} '77},
  pages = {77--90},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/800105.803397},
  abstract = {We define the class of conjunctive queries in relational data bases, and the generalized join operator on relations. The generalized join plays an important part in answering conjunctive queries, and it can be implemented using matrix multiplication. It is shown that while answering conjunctive queries is NP complete (general queries are PSPACE complete), one can find an implementation that is within a constant of optimal. The main lemma used to show this is that each conjunctive query has a unique minimal equivalent query (much like minimal finite automata).},
  isbn = {978-1-4503-7409-5},
  annotation = {QID: Q56813555},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1977\\Chandra et Merlin - 1977 - Optimal implementation of conjunctive queries in r.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\6D3X624J\\800105.html}
}

@article{chenEntityrelationshipModelUnified1976,
  title = {The Entity-Relationship Model\textemdash toward a Unified View of Data},
  author = {Chen, Peter Pin-Shan},
  year = {1976},
  month = mar,
  journal = {ACM Trans. Database Syst.},
  volume = {1},
  number = {1},
  pages = {9--36},
  issn = {0362-5915, 1557-4644},
  doi = {10.1145/320434.320440},
  urldate = {2023-10-27},
  abstract = {A data model, called the entity-relationship model, is proposed. This model incorporates some of the important semantic information about the real world. A special diagrammatic technique is introduced as a tool for database design. An example of database design and description using the model and the diagrammatic technique is given. Some implications for data integrity, information retrieval, and data manipulation are discussed. The entity-relationship model can be used as a basis for unification of different views of data: the network model, the relational model, and the entity set model. Semantic ambiguities in these models are analyzed. Possible ways to derive their views of data from the entity-relationship model are presented.},
  langid = {english},
  keywords = {Data Base Task Group,data definition and manipulation,data integrity and consistency,data models,database design,entity set model,entity-relationship model,logigcal view of data,network model,relational model,semantics of data},
  annotation = {QID: Q54151498},
  file = {C:\Users\nhiot\OneDrive\zotero\1976\Chen - 1976 - The entity-relationship model—toward a unified vie.pdf}
}

@article{chomskyAlgebraicTheoryContextfree1963,
  title = {The Algebraic Theory of Context-Free Languages},
  author = {Chomsky, Noam and Sch{\"u}tzenberger, Marcel-Paul},
  year = {1963},
  journal = {Studies in Logic and the Foundations of Mathematics},
  volume = {35},
  pages = {118--161},
  doi = {10.1016/S0049-237X(08)72023-8},
  keywords = {\#nosource}
}

@article{christiansenSurveyAdaptableGrammars1990,
  title = {A Survey of Adaptable Grammars},
  author = {Christiansen, H.},
  year = {1990},
  month = nov,
  journal = {SIGPLAN Not.},
  volume = {25},
  number = {11},
  pages = {35--44},
  issn = {0362-1340, 1558-1160},
  doi = {10.1145/101356.101357},
  urldate = {2023-09-28},
  langid = {english},
  file = {C:\Users\nhiot\Zotero\storage\K36JRM7H\Christiansen - 1990 - A survey of adaptable grammars.pdf}
}

@article{coddRelationalModelData1970,
  title = {A Relational Model of Data for Large Shared Data Banks},
  author = {Codd, Edgar F.},
  year = {1970},
  journal = {Communications of the ACM},
  volume = {13},
  number = {6},
  pages = {377--387},
  publisher = {{ACM New York, NY, USA}},
  doi = {10.1145/362384.362685},
  annotation = {QID: Q32061744},
  file = {C:\Users\nhiot\OneDrive\zotero\1970\Codd - 1970 - A relational model of data for large shared data b.pdf}
}

@article{ComputationGreatestRegular2016,
  title = {Computation of the {{Greatest Regular Equivalence}}},
  year = {2016},
  month = jan,
  journal = {Filomat},
  volume = {30},
  number = {1},
  pages = {179--190},
  publisher = {{National Library of Serbia}},
  issn = {0354-5180},
  doi = {10.2298/FIL1601179S},
  urldate = {2023-10-31},
  abstract = {The notion of social roles is a centerpiece of most sociological theoretical considerations. Regular equivalences were introduced by White and Reitz in [15] as the least restrictive among the most commonly used definitions of equivalence in social network analysis. In this paper we consider a generalisation of this notion to a bipartite case. We define a pair of regular equivalences on a two-mode social network and we provide an algorithm for computing the greatest pair of regular equivalences.},
  langid = {english},
  file = {C:\Users\nhiot\Zotero\storage\HKTCFBBE\2016 - Computation of the Greatest Regular Equivalence.pdf}
}

@inproceedings{consoleCopingIncompleteData2020,
  title = {Coping with {{Incomplete Data}}: {{Recent Advances}}},
  shorttitle = {Coping with {{Incomplete Data}}},
  booktitle = {Proceedings of the 39th {{ACM SIGMOD-SIGACT-SIGAI Symposium}} on {{Principles}} of {{Database Systems}}},
  author = {Console, Marco and Guagliardo, Paolo and Libkin, Leonid and Toussaint, Etienne},
  year = {2020},
  month = jun,
  series = {{{PODS}}'20},
  pages = {33--47},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3375395.3387970},
  urldate = {2023-08-03},
  abstract = {Handling incomplete data in a correct manner is a notoriously hard problem in databases. Theoretical approaches rely on the computationally hard notion of certain answers, while practical solutions rely on ad hoc query evaluation techniques based on three-valued logic. Can we find a middle ground, and produce correct answers efficiently? The paper surveys results of the last few years motivated by this question. We re-examine the notion of certainty itself, and show that it is much more varied than previously thought. We identify cases when certain answers can be computed efficiently and, short of that, provide deterministic and probabilistic approximation schemes for them. We look at the role of three-valued logic as used in SQL query evaluation, and discuss the correctness of the choice, as well as the necessity of such a logic for producing query answers.},
  isbn = {978-1-4503-7108-7},
  keywords = {\#nosource,approximate query answering,certain answers,incomplete information,many-valued logics,naive evaluation,relational databases},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2020\\Console et al. - 2020 - Coping with Incomplete Data Recent Advances.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\KJF3RWAC\\3375395.html}
}

@article{cowieInformationExtraction2000,
  title = {Information Extraction},
  author = {Cowie, Jim and Wilks, Yorick},
  year = {2000},
  journal = {Handbook of Natural Language Processing},
  volume = {56},
  pages = {57},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2000\\Cowie et Wilks - 2000 - Information extraction.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\43IETTRD\\books.html}
}

@incollection{dailleActesTALN20032003,
  title = {Actes de {{TALN}} 2003 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 2003 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Daille, B{\'e}atrice},
  year = {2003},
  month = jun,
  publisher = {{IRIN / ATALA}},
  address = {{Batz-sur-mer}},
  keywords = {\#nosource}
}

@article{darmoniMLPubMedBaseDonnees,
  title = {{{MLPubMed}}: Une Base de Donn\'ees Bibliographique Multi-Lingue},
  shorttitle = {{{MLPubMed}}},
  author = {Darmoni, St{\'e}fan J. and Soualmia, Lina F. and Griffon, Nicolas and Grosjean, Julien and Kerdelhu{\'e}, Ga{\'e}tan and Kergourlay, Ivan and Thirion, Benoit and Dahamna, Badisse},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\_\Darmoni et al. - MLPubMed une base de données bibliographique mult.pdf}
}

@article{degiacomoDealingInconsistenciesIncompleteness2009,
  title = {Dealing with Inconsistencies and Incompleteness in Database Update (Position Paper)},
  author = {De Giacomo, Giuseppe and Lenzerini, Maurizio and Poggi, Antonella and Rosati, Riccardo},
  year = {2009},
  month = aug,
  url = {https://www.academia.edu/61920314/Dealing\_with\_inconsistencies\_and\_incompleteness\_in\_database\_update\_position\_paper\_},
  abstract = {Several areas of research and various application domains have been concerned in the last years with the problem of dealing with incomplete databases. Data integration as well as the Semantic Web are notable examples. Surprisingly, while many research efforts have been focusing on several interesting issues related to incomplete databases, as query answering, not much investigation have been done concerning updates. In this position paper we aim at highlighting some of the issues we are dealing with in our work on updates over incomplete databases. Instance level updates under constraints Our interest in this area stems mainly from the need to deal with updates in Description Logics based ontologies. Description logics (DLs) are logics for expressing the conceptual knowledge about a domain in terms of classes and associations between them [1]. Such logics are currently considered among the most promising formalisms for representing ontologies by the Semantic Web community [2]. DL-based ontologies are often used for accessing data stored in a data layer by means of query answering.},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2009\De Giacomo et al. - 2009 - Dealing with inconsistencies and incompleteness in.pdf}
}

@inproceedings{degiacomoPracticalUpdateManagement2017,
  title = {Practical {{Update Management}} in {{Ontology-Based Data Access}}},
  booktitle = {The {{Semantic Web}} \textendash{} {{ISWC}} 2017},
  author = {De Giacomo, Giuseppe and Lembo, Domenico and Oriol, Xavier and Savo, Domenico Fabio and Teniente, Ernest},
  editor = {{d'Amato}, Claudia and Fernandez, Miriam and Tamma, Valentina and Lecue, Freddy and {Cudr{\'e}-Mauroux}, Philippe and Sequeda, Juan and Lange, Christoph and Heflin, Jeff},
  year = {2017},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {225--242},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-68288-4_14},
  abstract = {Ontology-based Data Access (OBDA) is gaining importance both scientifically and practically. However, little attention has been paid so far to the problem of updating OBDA systems. This is an essential issue if we want to be able to cope with modifications of data both at the ontology and at the source level, while maintaining the independence of the data sources. In this paper, we propose mechanisms to properly handle updates in this context. We show that updating data both at the ontology and source level is first-order rewritable. We also provide a practical implementation of such updating mechanisms based on non-recursive Datalog.},
  isbn = {978-3-319-68288-4},
  langid = {english},
  file = {C:\Users\nhiot\OneDrive\zotero\2017\De Giacomo et al. - 2017 - Practical Update Management in Ontology-Based Data.pdf}
}

@incollection{diasActesTALN20152015,
  title = {Actes de {{TALN}} 2015 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 2015 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Dias, Ga{\"e}l},
  year = {2015},
  month = jun,
  publisher = {{HULTECH / ATALA}},
  address = {{Caen}},
  keywords = {\#nosource}
}

@inproceedings{duchierMetagrammarCompilerNLP2005,
  title = {The {{Metagrammar Compiler}}: {{An NLP Application}} with a {{Multi-paradigm Architecture}}},
  shorttitle = {The {{Metagrammar Compiler}}},
  booktitle = {Multiparadigm {{Programming}} in {{Mozart}}/{{Oz}}},
  author = {Duchier, Denys and Le Roux, Joseph and Parmentier, Yannick},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Van Roy, Peter},
  year = {2005},
  volume = {3389},
  pages = {175--187},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-31845-3_15},
  urldate = {2023-09-29},
  isbn = {978-3-540-25079-1 978-3-540-31845-3},
  file = {C:\Users\nhiot\OneDrive\zotero\2005\Duchier et al. - 2005 - The Metagrammar Compiler An NLP Application with .pdf}
}

@inproceedings{elhadadSemEval2015Task142015,
  title = {{{SemEval-2015}} Task 14: {{Analysis}} of Clinical Text},
  booktitle = {Proceedings of the 9th International Workshop on Semantic Evaluation, {{SemEval}}@{{NAACL-HLT}} 2015, Denver, Colorado, {{USA}}, June 4-5, 2015},
  author = {Elhadad, No{\'e}mie and Pradhan, Sameer and Gorman, Sharon Lipsky and Manandhar, Suresh and Chapman, Wendy W. and Savova, Guergana K.},
  editor = {Cer, Daniel M. and Jurgens, David and Nakov, Preslav and Zesch, Torsten},
  year = {2015},
  pages = {303--310},
  publisher = {{The Association for Computer Linguistics}},
  doi = {10.18653/v1/s15-2051},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/semeval/ElhadadPGMCS15.bib},
  keywords = {\#nosource},
  timestamp = {Tue, 28 Jan 2020 10:29:10 +0100}
}

@article{faginDataExchangeGetting2005,
  title = {Data Exchange: Getting to the Core},
  shorttitle = {Data Exchange},
  author = {Fagin, Ronald and Kolaitis, Phokion G. and Popa, Lucian},
  year = {2005},
  month = mar,
  journal = {ACM Trans. Database Syst.},
  volume = {30},
  number = {1},
  pages = {174--210},
  publisher = {{ACM New York, NY, USA}},
  issn = {0362-5915, 1557-4644},
  doi = {10.1145/1061318.1061323},
  urldate = {2023-08-16},
  abstract = {Data exchange is the problem of taking data structured under a source schema and creating an instance of a target schema that reflects the source data as accurately as possible. Given a source instance, there may be many solutions to the data exchange problem, that is, many target instances that satisfy the constraints of the data exchange problem. In an earlier article, we identified a special class of solutions that we call universal. A universal solution has homomorphisms into every possible solution, and hence is a ``most general possible'' solution. Nonetheless, given a source instance, there may be many universal solutions. This naturally raises the question of whether there is a ``best'' universal solution, and hence a best solution for data exchange. We answer this question by considering the well-known notion of the core of a structure, a notion that was first studied in graph theory, and has also played a role in conjunctive-query processing. The core of a structure is the smallest substructure that is also a homomorphic image of the structure. All universal solutions have the same core (up to isomorphism); we show that this core is also a universal solution, and hence the smallest universal solution. The uniqueness of the core of a universal solution together with its minimality make the core an ideal solution for data exchange. We investigate the computational complexity of producing the core. Well-known results by Chandra and Merlin imply that, unless P = NP, there is no polynomial-time algorithm that, given a structure as input, returns the core of that structure as output. In contrast, in the context of data exchange, we identify natural and fairly broad conditions under which there are polynomial-time algorithms for computing the core of a universal solution. We also analyze the computational complexity of the following decision problem that underlies the computation of cores: given two graphs G and H, is H the core of G? Earlier results imply that this problem is both NP-hard and coNP-hard. Here, we pinpoint its exact complexity by establishing that it is a DP-complete problem. Finally, we show that the core is the best among all universal solutions for answering existential queries, and we propose an alternative semantics for answering queries in data exchange settings.},
  langid = {english},
  optbibsource = {dblp computer science bibliography, http://dblp.org},
  optbiburl = {http://dblp.uni-trier.de/rec/bib/journals/tods/FaginKP05},
  optdoi = {10.1145/1061318.1061323},
  opttimestamp = {Thu, 09 Feb 2006 13:53:18 +0100},
  opturl = {http://doi.acm.org/10.1145/1061318.1061323},
  keywords = {\#nosource,Certain answers,chase,computational complexity,conjunctive queries,core,data exchange,data integration,dependencies,query answering,universal solutions},
  annotation = {QID: Q106466848}
}

@inproceedings{faginSemanticsUpdatesDatabases1983,
  title = {On the Semantics of Updates in Databases},
  booktitle = {Proceedings of the 2nd {{ACM SIGACT-SIGMOD Symposium}} on {{Principles}} of {{Database Systems}}},
  author = {Fagin, Ronald and Ullman, Jeffrey D. and Vardi, Moshe Y.},
  year = {1983},
  month = mar,
  series = {{{PODS}} '83},
  pages = {352--365},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/588058.588100},
  urldate = {2023-08-03},
  abstract = {We suggest here a methodology for updating databases with integrity constraints and rules for deriving inexphcit information. First we consider the problem of updating arbitrary theories by inserting into them or deleting from them arbitrary sentences. The solution involves two key ideas when replacing an old theory by a new one we wish to minimize the change in the theory, and when there are several theories that involve minimal changes, we look for a new theory that reflects that ambiguity. The methodology is also adapted to updating databases, where different facts can carry different priorities, and to updating user views.},
  isbn = {978-0-89791-097-2},
  keywords = {\#nosource},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1983\\Fagin et al. - 1983 - On the semantics of updates in databases.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\7SGAML5K\\588058.html}
}

@article{faginUpdatingLogicalDatabases1986,
  title = {Updating {{Logical Databases}}},
  author = {Fagin, Ronald and Kuper, Gabriel M. and Ullman, Jeffrey D. and Vardi, Moshe Y.},
  year = {1986},
  journal = {Adv. Comput. Res.},
  volume = {3},
  pages = {1--18},
  doi = {10.21236/ada144937},
  keywords = {\#nosource},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1986\\Fagin et al. - 1986 - Updating Logical Databases.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\RS48PZZC\\ADA144937.html}
}

@article{fanDependenciesGraphs2019,
  title = {Dependencies for {{Graphs}}},
  author = {Fan, Wenfei and Lu, Ping},
  year = {2019},
  month = feb,
  journal = {ACM Trans. Database Syst.},
  volume = {44},
  number = {2},
  pages = {1--40},
  publisher = {{ACM New York, NY, USA}},
  issn = {0362-5915},
  doi = {10.1145/3287285},
  urldate = {2023-08-03},
  abstract = {This article proposes a class of dependencies for graphs, referred to as graph entity dependencies (GEDs). A GED is defined as a combination of a graph pattern and an attribute dependency. In a uniform format, GEDs can express graph functional dependencies with constant literals to catch inconsistencies, and keys carrying id literals to identify entities (vertices) in a graph. We revise the chase for GEDs and prove its Church-Rosser property. We characterize GED satisfiability and implication, and establish the complexity of these problems and the validation problem for GEDs, in the presence and absence of constant literals and id literals. We also develop a sound, complete and independent axiom system for finite implication of GEDs. In addition, we extend GEDs with built-in predicates or disjunctions, to strike a balance between the expressive power and complexity. We settle the complexity of the satisfiability, implication, and validation problems for these extensions.},
  keywords = {axiom system,built-in predicates,conditional functional dependencies,disjunction,EGDs,Graph dependencies,implication,keys,satisfiability,TGDs,validation},
  file = {C\:\\Users\\nhiot\\Zotero\\storage\\42IFQCS2\\3287285.html;C\:\\Users\\nhiot\\Zotero\\storage\\ULM587AG\\3287285.html}
}

@article{fanKeysGraphs2015,
  title = {Keys for Graphs},
  author = {Fan, Wenfei and Fan, Zhe and Tian, Chao and Dong, Xin Luna},
  year = {2015},
  month = aug,
  journal = {Proc. VLDB Endow.},
  volume = {8},
  number = {12},
  pages = {1590--1601},
  publisher = {{VLDB Endowment}},
  issn = {2150-8097},
  doi = {10.14778/2824032.2824056},
  urldate = {2023-08-03},
  abstract = {Keys for graphs aim to uniquely identify entities represented by vertices in a graph. We propose a class of keys that are recursively defined in terms of graph patterns, and are interpreted with subgraph isomorphism. Extending conventional keys for relations and XML, these keys find applications in object identification, knowledge fusion and social network reconciliation. As an application, we study the entity matching problem that, given a graph G and a set {$\Sigma$} of keys, is to find all pairs of entities (vertices) in G that are identified by keys in {$\Sigma$}. We show that the problem is intractable, and cannot be parallelized in logarithmic rounds. Nonetheless, we provide two parallel scalable algorithms for entity matching, in MapReduce and a vertex-centric asynchronous model. Using real-life and synthetic data, we experimentally verify the effectiveness and scalability of the algorithms.},
  file = {C:\Users\nhiot\OneDrive\zotero\2015\Fan et al. - 2015 - Keys for graphs.pdf}
}

@article{florekLiaisonDivisionPoints1951,
  title = {{Sur la liaison et la division des points d'un ensemble fini}},
  author = {Florek, K. and {\L}ukaszewicz, J. and Perkal, J. and Steinhaus, Hugo and Zubrzycki, S.},
  year = {1951},
  journal = {Colloquium Mathematicum},
  volume = {2},
  number = {3-4},
  pages = {282--285},
  issn = {0010-1354},
  doi = {10.4064/cm-2-3-4-282-285},
  urldate = {2023-04-18},
  langid = {fra},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1951\\Florek et al. - 1951 - Sur la liaison et la division des points d'un ense.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\NWJZTFEF\\209969.html}
}

@article{flourisFormalFoundationsRDF2013,
  title = {Formal Foundations for {{RDF}}/{{S KB}} Evolution},
  author = {Flouris, Giorgos and Konstantinidis, George and Antoniou, Grigoris and Christophides, Vassilis},
  year = {2013},
  month = apr,
  journal = {Knowl. Inf. Syst.},
  volume = {35},
  number = {1},
  pages = {153--191},
  issn = {0219-1377, 0219-3116},
  doi = {10.1007/s10115-012-0500-2},
  urldate = {2019-06-20},
  langid = {english},
  annotation = {QID: Q58198067},
  file = {C:\Users\nhiot\OneDrive\zotero\2013\Flouris et al. - 2013 - Formal foundations for RDFS KB evolution.pdf}
}

@inproceedings{friburgerFiniteStateTransducerCascade2001,
  title = {Finite-{{State Transducer Cascade}} to {{Extract Proper Names}} in {{Texts}}},
  booktitle = {Implementation and Application of Automata, 6th International Conference, {{CIAA}} 2001, Pretoria, South Africa, July 23-25, 2001, Revised Papers},
  author = {Friburger, Nathalie and Maurel, Denis},
  editor = {Watson, Bruce W. and Wood, Derick},
  year = {2001},
  month = jul,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {2494},
  pages = {115--124},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-36390-4_10},
  abstract = {This article describes a finite-state cascade for the extraction of person names in texts in French. We extract these proper names in order to categorize and to cluster texts with them. After a finite-state pre-processing (division of the text in sentences, tagging with dictionaries, etc.), a series of finite-state transducers is applied one after the other to the text and locates left and right contexts that indicates the presence of a person name. An evaluation of the results of this extraction is presented.},
  isbn = {978-3-540-36390-3},
  langid = {english},
  keywords = {\#nosource,Compound Word,Coreference Resolution,Input Alphabet,Natural Language Processing,Output Alphabet},
  file = {C:\Users\nhiot\OneDrive\zotero\2001\Friburger et Maurel - 2001 - Finite-State Transducer Cascade to Extract Proper .pdf}
}

@inproceedings{gaioExtendedNamedEntity2017,
  title = {Extended Named Entity Recognition Using Finite-State Transducers: {{An}} Application to Place Names},
  booktitle = {The Ninth International Conference on Advanced Geographic Information Systems, Applications, and Services ({{GEOProcessing}} 2017)},
  author = {Gaio, Mauro and Moncla, Ludovic},
  year = {2017},
  month = mar,
  address = {{Nice, France}},
  url = {https://hal.archives-ouvertes.fr/hal-01492994},
  hal_id = {hal-01492994},
  hal_version = {v1},
  keywords = {Geo-information processing,Geo-spatial data mining,Geo-spatial Web Ser- vices and processing}
}

@inproceedings{garcelonEnrichissementSemantiqueAssocie2014,
  title = {Enrichissement S\'emantique Associ\'e \`a La D\'etection de La N\'egation et Des Ant\'ec\'edents Familiaux Dans Un Entrep\^ot de Donn\'ees Hospitalier.},
  booktitle = {{{JFIM}}},
  author = {Garcelon, Nicolas and Salomon, R{\'e}mi and Burgun, Anita},
  year = {2014},
  pages = {83--93},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2014\Garcelon et al. - 2014 - Enrichissement sémantique associé à la détection d.pdf}
}

@incollection{genthialActesTALN19971997,
  title = {Actes de {{TALN}} 1997 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 1997 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Genthial, Damien},
  year = {1997},
  month = jun,
  address = {{Grenoble}},
  keywords = {\#nosource}
}

@inproceedings{goasdoueEfficientQueryAnswering2013,
  title = {Efficient Query Answering against Dynamic {{RDF}} Databases},
  booktitle = {Proceedings of the 16th {{International Conference}} on {{Extending Database Technology}}},
  author = {Goasdou{\'e}, Fran{\c c}ois and Manolescu, Ioana and Roati{\c s}, Alexandra},
  year = {2013},
  month = mar,
  series = {{{EDBT}} '13},
  pages = {299--310},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2452376.2452412},
  urldate = {2023-08-03},
  abstract = {A promising method for efficiently querying RDF data consists of translating SPARQL queries into efficient RDBMS-style operations. However, answering SPARQL queries requires handling RDF reasoning, which must be implemented outside the relational engines that do not support it. We introduce the database (DB) fragment of RDF, going beyond the expressive power of previously studied RDF fragments. We devise novel sound and complete techniques for answering Basic Graph Pattern (BGP) queries within the DB fragment of RDF, exploring the two established approaches for handling RDF semantics, namely reformulation and saturation. In particular, we focus on handling database updates within each approach and propose a method for incrementally maintaining the saturation; updates raise specific difficulties due to the rich RDF semantics. Our techniques are designed to be deployed on top of any RDBMS(-style) engine, and we experimentally study their performance trade-offs.},
  isbn = {978-1-4503-1597-5},
  keywords = {query answering,RDF fragments,reasoning},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2013\\Goasdoué et al. - 2013 - Efficient query answering against dynamic RDF data.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\ZEREIB62\\2452376.html}
}

@inproceedings{gottlobComputingCoresData2005,
  title = {Computing Cores for Data Exchange: New Algorithms and Practical Solutions},
  shorttitle = {Computing Cores for Data Exchange},
  booktitle = {Proceedings of the Twenty-Fourth {{ACM SIGMOD-SIGACT-SIGART}} Symposium on {{Principles}} of Database Systems},
  author = {Gottlob, Georg},
  year = {2005},
  month = jun,
  series = {{{PODS}} '05},
  pages = {148--159},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1065167.1065187},
  urldate = {2023-08-16},
  abstract = {Data Exchange is the problem of inserting data structured under a source schema into a target schema of different structure (possibly with integrity constraints), while reflecting the source data as accurately as possible. We study computational issues related to data exchange in the setting of Fagin, Kolaitis, and Popa(PODS'03). We use the technique of hypertree decompositions to derive improved algorithms for computing the core of a relational instance with labeled nulls, a problem we show to be fixed-parameter intractable with respect to the block size of the input instances. We show that computing the core of a data exchange problem is tractable for two large and useful classes of target constraints. The first class includes functional dependencies and weakly acyclic inclusion dependencies. The second class consists of full tuple generating dependencies and arbitrary equation generating dependencies. Finally, we show that computing cores is NP-hard in presence of a system-predicate NULL(x), which is true iff x is a null value.},
  isbn = {978-1-59593-062-0},
  keywords = {\#nosource}
}

@inproceedings{grabarCASFrenchCorpus2018,
  title = {{{CAS}}: {{French Corpus}} with {{Clinical Cases}}},
  shorttitle = {{{CAS}}},
  booktitle = {Proceedings of the {{Ninth International Workshop}} on {{Health Text Mining}} and {{Information Analysis}}},
  author = {Grabar, Natalia and Claveau, Vincent and Dalloux, Cl{\'e}ment},
  year = {2018},
  month = oct,
  pages = {122--128},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10/ggnqh8},
  urldate = {2021-05-25},
  abstract = {Textual corpora are extremely important for various NLP applications as they provide information necessary for creating, setting and testing these applications and the corresponding tools. They are also crucial for designing reliable methods and reproducible results. Yet, in some areas, such as the medical area, due to confidentiality or to ethical reasons, it is complicated and even impossible to access textual data representative of those produced in these areas. We propose the CAS corpus built with clinical cases, such as they are reported in the published scientific literature in French. We describe this corpus, currently containing over 397,000 word occurrences, and the existing linguistic and semantic annotations.},
  file = {C:\Users\nhiot\OneDrive\zotero\2018\Grabar et al. - 2018 - CAS French Corpus with Clinical Cases.pdf}
}

@inproceedings{grabarCorpusAnnoteCas2019,
  title = {Corpus Annot\'e de Cas Cliniques En Fran\c{c}ais},
  booktitle = {{{TALN}} 2019 - 26e Conference on Traitement Automatique Des Langues Naturelles},
  author = {Grabar, Natalia and Grouin, Cyril and Hamon, Thierry and Claveau, Vincent},
  year = {2019},
  month = jul,
  pages = {1--14},
  address = {{Toulouse, France}},
  url = {https://hal.archives-ouvertes.fr/hal-02391878},
  hal_id = {hal-02391878},
  hal_version = {v1},
  keywords = {annotations,cas clinique,cat\'egorisation,categorization,clinical case,Clinical corpus,Corpus clinique,extraction d'information,information extraction}
}

@book{grahneProblemIncompleteInformation1991,
  title = {The {{Problem}} of {{Incomplete Information}} in {{Relational Databases}}},
  author = {Grahne, G{\"o}sta},
  year = {1991},
  month = nov,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {554},
  publisher = {{Springer Science \& Business Media}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-54919-6},
  abstract = {In a relational database the information is recorded as rows in tables. However, in many practical situations the available information is incomplete and the values for some columns are missing. Yet few existing database management systems allow the user to enter null values in the database. This monograph analyses the problems raised by allowing null values in relational databases. The analysis covers semantical, syntactical, and computational aspects. Algorithms for query evaluation, dependency enforcement and updates in the presence of null values are also given. The analysis of the computational complexity of the algorithms suggests that from a practical point of view the database should be stored as Horn tables, which are generalizations of ordinary relations, allowing null values and Horn clause-like restrictions on these null values. Horn tables efficiently support a large class of queries, dependencies and updates.},
  isbn = {978-3-540-54919-2 978-3-540-46507-2},
  langid = {english},
  keywords = {\#nosource,⛔ No DOI found,Computers / Artificial Intelligence / General,Computers / Database Administration \& Management,Computers / Information Technology,Computers / Information Theory,Computers / Programming / Algorithms},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1991\\Grahne - 1991 - The Problem of Incomplete Information in Relationa.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\R23IRBKN\\3-540-54919-6_10.html}
}

@inproceedings{grishmanInformationExtractionTechniques1997,
  title = {Information Extraction: {{Techniques}} and Challenges},
  shorttitle = {Information Extraction},
  booktitle = {International Summer School on Information Extraction},
  author = {Grishman, Ralph},
  year = {1997},
  volume = {1299},
  pages = {10--27},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-63438-x_2},
  isbn = {978-3-540-63438-6 978-3-540-69548-6},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1997\\Grishman - 1997 - Information extraction Techniques and challenges.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\PYHSUCCK\\10.html}
}

@inproceedings{grossUseFiniteAutomata1989,
  ids = {grossUseFiniteAutomata1987},
  title = {The Use of Finite Automata in the Lexical Representation of Natural Language},
  booktitle = {Electronic {{Dictionaries}} and {{Automata}} in {{Computational Linguistics}}},
  author = {Gross, Maurice},
  editor = {Gross, Maurice and Perrin, Dominique},
  year = {1989},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {34--50},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10/bgcnzw},
  isbn = {978-3-540-48140-9},
  langid = {english},
  keywords = {⛔ No DOI found,Compound Word,Finite Automaton,Flight Simulator,Oxford English Dictionary,Simple Word},
  file = {C:\Users\nhiot\Zotero\storage\W3UCPU6M\10.html}
}

@inproceedings{grouinClassificationCasCliniques2021,
  ids = {grouinClassificationCasCliniques2021a},
  title = {{Classification de cas cliniques et \'evaluation automatique de r\'eponses d'\'etudiants : pr\'esentation de la campagne DEFT 2021 (Clinical cases classification and automatic evaluation of student answers : Presentation of the DEFT 2021 Challenge)}},
  shorttitle = {{Classification de cas cliniques et \'evaluation automatique de r\'eponses d'\'etudiants}},
  booktitle = {{Actes de la 28e Conf\'erence sur le Traitement Automatique des Langues Naturelles. Atelier D\'Efi Fouille de Textes (DEFT)}},
  author = {Grouin, Cyril and Grabar, Natalia and Illouz, Gabriel},
  year = {2021},
  month = jun,
  pages = {1--13},
  publisher = {{ATALA}},
  address = {{Lille, France}},
  url = {https://aclanthology.org/2021.jeptalnrecital-deft.1},
  urldate = {2023-09-22},
  abstract = {Le d\'efi fouille de textes (DEFT) est une campagne d'\'evaluation annuelle francophone. Nous pr\'esentons les corpus et baselines \'elabor\'ees pour trois t\^aches : (i) identifier le profil clinique de patients d\'ecrits dans des cas cliniques, (ii) \'evaluer automatiquement les r\'eponses d'\'etudiants sur des questionnaires en ligne (Moodle) \`a partir de la correction de l'enseignant, et (iii) poursuivre une \'evaluation de r\'eponses d'\'etudiants \`a partir de r\'eponses d\'ej\`a \'evalu\'ees par l'enseignant. Les r\'esultats varient de 0,394 \`a 0,814 de F-mesure sur la premi\`ere t\^ache (7 \'equipes), de 0,448 \`a 0,682 de pr\'ecision sur la deuxi\`eme (3 \'equipes), et de 0,133 \`a 0,510 de pr\'ecision sur la derni\`ere (3 \'equipes).},
  langid = {french},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2021\\Grouin et al. - 2021 - Classification de cas cliniques et évaluation auto.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\X22H9BDQ\\2021.jeptalnrecital-deft.1.html}
}

@article{guagliardoCorrectnessSQLQueries2017,
  title = {Correctness of {{SQL Queries}} on {{Databases}} with {{Nulls}}},
  author = {Guagliardo, Paolo and Libkin, Leonid},
  year = {2017},
  month = oct,
  journal = {SIGMOD Rec.},
  volume = {46},
  number = {3},
  pages = {5--16},
  issn = {0163-5808},
  doi = {10.1145/3156655.3156657},
  urldate = {2023-08-08},
  abstract = {Multiple issues with SQL's handling of nulls have been well documented. Having efficiency as its main goal, SQL disregards the standard notion of correctness on incomplete databases -- certain answers -- due to its high complexity. As a result, the evaluation of SQL queries on databases with nulls may produce answers that are just plain wrong. However, SQL evaluation can be modified, at least for relational algebra queries, to approximate certain answers, i.e., return only correct answers. We examine recently proposed approximation schemes for certain answers and analyze their complexity, both theoretical bounds and real-life behavior},
  file = {C:\Users\nhiot\OneDrive\zotero\2017\Guagliardo et Libkin - 2017 - Correctness of SQL Queries on Databases with Nulls.pdf}
}

@inproceedings{halfedferrariRDFUpdatesConstraints2017,
  title = {{{RDF Updates}} with {{Constraints}}},
  booktitle = {Knowledge {{Engineering}} and {{Semantic Web}} - 8th {{International Conference}}, {{KESW}}, {{Szczecin}}, {{Poland}}, {{Proceedings}}},
  author = {Halfed Ferrari, Mirian and Hara, Carmem S. and Uber, Flavio R.},
  editor = {R{\'o}{\.z}ewski, Przemys{\l}aw and Lange, Christoph},
  year = {2017},
  series = {Communications in {{Computer}} and {{Information Science}}},
  volume = {786},
  pages = {229--245},
  publisher = {{Springer International Publishing}},
  address = {{Szczecin, Poland}},
  doi = {10.1007/978-3-319-69548-8_16},
  abstract = {This paper deals with the problem of updating an RDF database, expected to satisfy user-defined constraints as well as RDF intrinsic semantic constraints. As updates may violate these constraints, side-effects are generated in order to preserve consistency. We investigate the use of nulls (blank nodes) as placeholders for unknown required data as a technique to provide this consistency and to reduce the number of side-effects. Experimental results validate our goals.},
  isbn = {978-3-319-69547-1 978-3-319-69548-8},
  langid = {english},
  keywords = {Constraints,RDF,RDFS,Updates},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2017\\Halfed Ferrari et al. - 2017 - RDF Updates with Constraints.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\KT9SZGTE\\978-3-319-69548-8_16.html}
}

@article{halfeldferrariUpdateRulesDatalog1998,
  title = {Update {{Rules}} in {{Datalog Programs}}},
  author = {Halfeld Ferrari, Mirian and Laurent, Dominique and Spyratos, Nicolas},
  year = {1998},
  month = dec,
  journal = {Journal of Logic and Computation},
  volume = {8},
  number = {6},
  pages = {745--775},
  publisher = {{OUP}},
  issn = {0955-792X, 1465-363X},
  doi = {10.1093/logcom/8.6.745},
  abstract = {We propose a deductive database model containing two kinds of rules: update rules of the form L0{$\leftarrow$}L1, where L0 and L1 are literals, and query rules of the form of normal logic program rules. A basic feature of our approach is that new knowledge inputs are always assimilated. Moreover, updates are always deterministic and they preserve database consistency.We consider that update rules have higher priority than query rules, i.e., update rules may generate exceptions to query-driven derivations. We introduce a semantics framework for database update and query answering, based on the well-founded semantics. We also suggest an alternative approach based on extended logic programs and we show that our database model can be defined in terms of non-monotonic formalisms.},
  langid = {english},
  keywords = {\#nosource,Datalog,deductive database,update},
  file = {C\:\\Users\\nhiot\\Zotero\\storage\\7CB956IH\\8142245.html;C\:\\Users\\nhiot\\Zotero\\storage\\B72S7SY9\\8142245.html}
}

@book{hinzenOxfordHandbookCompositionality2012,
  title = {The Oxford Handbook of Compositionality},
  editor = {Hinzen, Wolfram and Machery, Edouard and Werning, Markus},
  year = {2012},
  publisher = {{Oxford}},
  doi = {10.1093/oxfordhb/9780199541072.001.0001},
  keywords = {\#nosource}
}

@inproceedings{hiotDOINGDEFTUtilisation2021,
  title = {{DOING@DEFT : utilisation de lexiques pour une classification efficace de cas cliniques}},
  shorttitle = {{DOING@DEFT}},
  booktitle = {{Traitement Automatique des Langues Naturelles}},
  author = {Hiot, Nicolas and Minard, Anne-Lyse and Badin, Flora},
  editor = {Denis, Pascal and Grabar, Natalia and Fraisse, Amel and Cardon, R{\'e}mi and Jacquemin, Bernard and Kergosien, Eric and Balvet, Antonio},
  year = {2021},
  pages = {41--53},
  publisher = {{ATALA}},
  address = {{Lille, France}},
  url = {https://hal.science/hal-03265924},
  urldate = {2023-09-22},
  abstract = {Nous pr\'esentons dans cet article notre participation \`a la t\^ache 1 de la campagne d'\'evaluation francophone DEFT 2021, sur l'identification du profil clinique du patient. Nous proposons une m\'ethode \'evolutive et efficace en temps et en ressources pour la classification de documents m\'edicaux pouvant \^etre facilement adapt\'ee \`a d'autres domaines de recherche. Notre syst\`eme a obtenu les meilleures performances sur cette t\^ache avec une F-mesure de 0,814.},
  copyright = {All rights reserved},
  hal_id = {hal-03265924},
  hal_version = {v1},
  langid = {french},
  pdf = {https://hal.archives-ouvertes.fr/hal-03265924/file/75.pdf},
  keywords = {⛔ No DOI found,cas clinique,classification.,lexique,me,transducteur fini},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2021\\Hiot et al. - 2021 - DOING@DEFT  utilisation de lexiques pour une clas.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\FHMF4CQI\\hal-03265924.html}
}

@misc{hiotUpdateChase2023,
  title = {{{UpdateChase}}},
  author = {Hiot, Nicolas and {Moret-Bailly}, Lucas and Chabin, Jacques},
  year = {2023},
  month = jan,
  url = {https://gitlab.com/jacques-chabin/UpdateChase},
  urldate = {2023-07-21},
  abstract = {Impl\'ementation et benchmarks des algorithmes incr\'ementaux pour la mise \`a jour coh\'erente d'une base de donn\'ees graphe},
  keywords = {me},
  file = {C:\Users\nhiot\Zotero\storage\WPJMI7RA\UpdateChase.html}
}

@article{imielinskiIncompleteInformationRelational1984,
  title = {Incomplete Information in Relational Databases},
  author = {Imielinski, Tomasz and Lipski Jr., Witold},
  year = {1984},
  month = sep,
  journal = {J. ACM},
  volume = {31},
  number = {4},
  pages = {761--791},
  publisher = {{ACM New York, NY, USA}},
  issn = {0004-5411},
  doi = {10.1145/1634.1886},
  keywords = {\#nosource},
  annotation = {QID: Q56698644},
  file = {C:\Users\nhiot\OneDrive\zotero\1984\Imielinski et Lipski Jr. - 1984 - Incomplete information in relational databases.pdf}
}

@article{jaccardDistributionFloreAlpine1901,
  title = {Distribution de La {{Flore Alpine}} Dans Le {{Bassin}} Des {{Dranses}} et Dans Quelques R\'egions Voisines.},
  author = {Jaccard, Paul},
  year = {1901},
  month = jan,
  journal = {Bulletin de la Societe Vaudoise des Sciences Naturelles},
  volume = {37},
  pages = {241--72},
  doi = {10/ghh3sp},
  file = {C:\Users\nhiot\OneDrive\zotero\1901\Jaccard - 1901 - Distribution de la Flore Alpine dans le Bassin des.pdf}
}

@incollection{jardinoActesTALN20052005,
  title = {Actes de {{TALN}} 2005 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 2005 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Jardino, Mich{\`e}le},
  year = {2005},
  month = jun,
  publisher = {{LIMSI / ATALA}},
  address = {{Dourdan}},
  keywords = {\#nosource}
}

@inproceedings{joshiTreeAdjoiningGrammars1985,
  title = {Tree Adjoining Grammars: {{How}} Much Context-Sensitivity Is Required to Provide Reasonable Structural Descriptions?},
  shorttitle = {Tree Adjoining Grammars},
  booktitle = {Natural {{Language Parsing}}: {{Psychological}}, {{Computational}}, and {{Theoretical Perspectives}}},
  author = {Joshi, Aravind K.},
  editor = {Zwicky, Arnold M. and Dowty, David R. and Karttunen, Lauri},
  year = {1985},
  series = {Studies in {{Natural Language Processing}}},
  pages = {206--250},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9780511597855.007},
  urldate = {2023-10-23},
  abstract = {Since the late 1970s there has been vigorous activity in constructing highly constrained grammatical systems by eliminating the transformational component either totally or partially. There is increasing recognition of the fact that the entire range of dependencies that transformational grammars in their various incarnations have tried to account for can be captured satisfactorily by classes of rules that are nontransformational and at the same time highly constrained in terms of the classes of grammars and languages they define.Two types of dependencies are especially important: subcategorization and filler-gap dependencies. Moreover, these dependencies can be unbounded. One of the motivations for transformations was to account for unbounded dependencies. The so-called nontransformational grammars account for the unbounded dependencies in different ways. In a tree adjoining grammar (TAG) unboundedness is achieved by factoring the dependencies and recursion in a novel and linguistically interesting manner. All dependencies are defined on a finite set of basic structures (trees), which are bounded. Unboundedness is then a corollary of a particular composition operation called adjoining. There are thus no unbounded dependencies in a sense.This factoring of recursion and dependencies is in contrast to transformational grammars (TG), where recursion is defined in the base and the transformations essentially carry out the checking of the dependencies. The phrase linking grammars (PLGs) (Peters and Ritchie, 1982) and the lexical functional grammars (LFGs) (Kaplan and Bresnan, 1983) share this aspect of TGs; that is, recursion builds up a set a structures, some of which are then filtered out by transformations in a TG, by the constraints on linking in a PLG, and by the constraints introduced via the functional structures in an LFG.},
  isbn = {978-0-511-59785-5},
  keywords = {\#nosource}
}

@article{knuthSemanticsContextfreeLanguages1968,
  title = {Semantics of Context-Free Languages},
  author = {Knuth, Donald E.},
  year = {1968},
  month = jun,
  journal = {Math. Systems Theory},
  volume = {2},
  number = {2},
  pages = {127--145},
  issn = {0025-5661, 1433-0490},
  doi = {10.1007/BF01692511},
  urldate = {2023-10-27},
  abstract = {``Meaning'' may be assigned to a string in a context-free language by defining ``attributes'' of the symbols in a derivation tree for that string. The attributes can be defined by functions associated with each production in the grammar. This paper examines the implications of this process when some of the attributes are ``synthesized'', i.e., defined solely in terms of attributes of thedescendants of the corresponding nonterminal symbol, while other attributes are ``inherited'', i.e., defined in terms of attributes of theancestors of the nonterminal symbol. An algorithm is given which detects when such semantic rules could possibly lead to circular definition of some attributes. An example is given of a simple programming language defined with both inherited and synthesized attributes, and the method of definition is compared to other techniques for formal specification of semantics which have appeared in the literature.},
  langid = {english},
  keywords = {Computational Mathematic,Derivation Tree,Formal Specification,Programming Language,Simple Programming},
  file = {C:\Users\nhiot\OneDrive\zotero\1968\Knuth - 1968 - Semantics of context-free languages.pdf}
}

@inproceedings{langlaisEnrichissementLexiqueBilingue2007,
  title = {{Enrichissement d'un lexique bilingue par analogie}},
  booktitle = {{Actes de la 14\`eme conf\'erence sur le Traitement Automatique des Langues Naturelles. Articles longs}},
  author = {Langlais, Philippe and Patry, Alexandre},
  year = {2007},
  month = jun,
  pages = {101--110},
  publisher = {{IRIT / ATALA}},
  address = {{Toulouse, France}},
  url = {https://aclanthology.org/2007.jeptalnrecital-long.9},
  urldate = {2023-10-23},
  abstract = {La pr\'esence de mots inconnus dans les applications langagi\`eres repr\'esente un d\'efi de taille bien connu auquel n'\'echappe pas la traduction automatique. Les syst\`emes professionnels de traduction offrent \`a cet effet \`a leurs utilisateurs la possibilit\'e d'enrichir un lexique de base avec de nouvelles entr\'ees. R\'ecemment, Stroppa et Yvon (2005) d\'emontraient l'int\'er\^et du raisonnement par analogie pour l'analyse morphologique d'une langue. Dans cette \'etude, nous montrons que le raisonnement par analogie offre \'egalement une r\'eponse adapt\'ee au probl\`eme de la traduction d'entr\'ees lexicales inconnues.},
  langid = {french},
  keywords = {\#nosource},
  file = {C:\Users\nhiot\Zotero\storage\KWCKY7NE\Langlais et Patry - 2007 - Enrichissement d'un lexique bilingue par analogie.pdf}
}

@inproceedings{libkinIncompleteDataWhat2014,
  title = {Incomplete Data: What Went Wrong, and How to Fix It},
  shorttitle = {Incomplete Data},
  booktitle = {Proceedings of the 33rd {{ACM SIGMOD-SIGACT-SIGART}} Symposium on {{Principles}} of Database Systems},
  author = {Libkin, Leonid},
  year = {2014},
  month = jun,
  series = {{{PODS}} '14},
  pages = {1--13},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2594538.2594561},
  urldate = {2023-08-03},
  abstract = {Incomplete data is ubiquitous: the more data we accumulate and the more widespread tools for integrating and exchanging data become, the more instances of incompleteness we have. And yet the subject is poorly handled by both practice and theory. Many queries for which students get full marks in their undergraduate courses will not work correctly in the presence of incomplete data, but these ways of evaluating queries are cast in stone -- SQL standard. We have many theoretical results on handling incomplete data but they are, by and large, about showing high complexity bounds, and thus are often dismissed by practitioners. Even worse, we have a basic theoretical notion of what it means to answer queries over incomplete data, and yet this is not at all what practical systems do. Is there a way out of this predicament? Can we have a theory of incompleteness that will appeal to theoreticians and practitioners alike, by explaining incompleteness and being at the same time implementable and useful for applications? After giving a critique of both the practice and the theory of handling incompleteness in databases, the paper outlines a possible way out of this crisis. The key idea is to combine three hitherto used approaches to incompleteness: one based on certain answers and representation systems, one based on viewing incomplete databases as logical theories, and one based on orderings expressing relative value of information.},
  isbn = {978-1-4503-2375-8},
  keywords = {incomplete information,query evaluation},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2014\\Libkin - 2014 - Incomplete data what went wrong, and how to fix i.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\VN5CSSYT\\2594538.html}
}

@article{libkinSQLThreeValuedLogic2016,
  title = {{{SQL}}'s {{Three-Valued Logic}} and {{Certain Answers}}},
  author = {Libkin, Leonid},
  year = {2016},
  journal = {ACM Trans. Database Syst.},
  volume = {41},
  number = {1},
  pages = {1--28},
  publisher = {{ACM New York, NY, USA}},
  issn = {0362-5915},
  doi = {10.1145/2877206},
  abstract = {The goal of the article is to bridge the difference between theoretical and practical approaches to answering queries over databases with nulls. Theoretical research has long ago identified the notion of correctness of query answering over incomplete data: one needs to find certain answers, which are true regardless of how incomplete information is interpreted. This serves as the notion of correctness of query answering, but carries a huge complexity tag. In practice, on the other hand, query answering must be very efficient, and to achieve this, SQL uses three-valued logic for evaluating queries on databases with nulls. Due to the complexity mismatch, the two approaches cannot coincide, but perhaps they are related in some way. For instance, does SQL always produce answers we can be certain about? This is not so: SQL's and certain answers semantics could be totally unrelated. We show, however, that a slight modification of the three-valued semantics for relational calculus queries can provide the required certainty guarantees. The key point of the new scheme is to fully utilize the three-valued semantics, and classify answers not into certain or noncertain, as was done before, but rather into certainly true, certainly false, or unknown. This yields relatively small changes to the evaluation procedure, which we consider at the level of both declarative (relational calculus) and procedural (relational algebra) queries. These new evaluation procedures give us certainty guarantees even for queries returning tuples with null values.},
  keywords = {certain answers,incomplete information,Null values,query evaluation,three-valued logic},
  file = {C:\Users\nhiot\OneDrive\zotero\2016\Libkin - 2016 - SQL’s Three-Valued Logic and Certain Answers.pdf}
}

@article{linkArithmeticTheoryConsistency2002,
  title = {Towards an {{Arithmetic Theory}} of {{Consistency Enforcement}} Based on {{Preservation}} of {$\delta$}-Constraints},
  author = {Link, Sebastian and Schewe, Klaus-Dieter},
  year = {2002},
  month = jan,
  journal = {Electronic Notes in Theoretical Computer Science},
  series = {{{CATS}}'02, {{Computing}}: The {{Australasian Theory Symposium}}},
  volume = {61},
  pages = {64--83},
  issn = {1571-0661},
  doi = {10.1016/S1571-0661(04)00306-8},
  urldate = {2023-08-16},
  abstract = {Consistency Enforcement provides an alternative theory to common verification techniques within formal specification languages. We consider specifications in the form of guarded commands. The basic idea is then to replace a program specification S by its greatest consistent specialization (GCS) SI which is provably consistent with respect to a given static constraint I, preserves the effects of S according to a specialization order and is maximal with these properties. The theory has been shown to provide several strengths. In particular, the enforcement process for a huge class of complex specifications can be reduced to its basic components. Moreover, the result can be obtained sequentially and is independent from the order of the given constraints. In addition, arithmetic logic has been used to show that GCSs can be efficiently computed for a reasonably large class of program specifications and invariants. However, all results have been achieved with respect to the underlying specialization order. The simplicity of this order reveals some obvious weaknesses. In this paper, we show how the specialization order can be replaced by the notion of {$\delta$}-constraints. Specialization of a program specification S turns out to be equivalent to the preservation of all {$\delta$}-constraints on the underlying state space of S. Obviously, this enables us to weaken the specialization order towards the preservation of certain {$\delta$}-constraints. We define maximal consistent effect preservers (MCEs), show that these are closely related to GCSs and prove that MCEs can be obtained sequentially and independently from the order of a given set of static constraints. This backs up the conjecture that the notion of MCEs leads towards a tailored theory of consistency enforcement.},
  keywords = {arithmetic logic,consistency,constraints,formal specifications,GCS,guarded commands,MCE},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2002\\Link et Schewe - 2002 - Towards an Arithmetic Theory of Consistency Enforc.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\LBEYDF3M\\S1571066104003068.html}
}

@article{linSimilarityMeasureText2014,
  title = {A Similarity Measure for Text Classification and Clustering},
  author = {Lin, Yung-Shen and Jiang, Jung-Yi and Lee, Shie-Jue},
  year = {2014},
  month = jul,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {26},
  number = {7},
  pages = {1575--1590},
  issn = {1558-2191},
  doi = {10/f6cs35},
  abstract = {Measuring the similarity between documents is an important operation in the text processing field. In this paper, a new similarity measure is proposed. To compute the similarity between two documents with respect to a feature, the proposed measure takes the following three cases into account: a) The feature appears in both documents, b) the feature appears in only one document, and c) the feature appears in none of the documents. For the first case, the similarity increases as the difference between the two involved feature values decreases. Furthermore, the contribution of the difference is normally scaled. For the second case, a fixed value is contributed to the similarity. For the last case, the feature has no contribution to the similarity. The proposed measure is extended to gauge the similarity between two sets of documents. The effectiveness of our measure is evaluated on several real-world data sets for text classification and clustering problems. The results show that the performance obtained by the proposed measure is better than that achieved by other measures.},
  keywords = {accuracy,Approximation methods,classifiers,clustering algorithms,Clustering algorithms,Document classification,document clustering,Educational institutions,entropy,Euclidean distance,Text processing,Vectors},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2014\\Lin et al. - 2014 - A similarity measure for text classification and c.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\EEYQAHWW\\6420834.html}
}

@article{lipscombMedicalSubjectHeadings2000,
  title = {Medical {{Subject Headings}} ({{MeSH}})},
  author = {Lipscomb, Carolyn E.},
  year = {2000},
  month = jul,
  journal = {Bull Med Libr Assoc},
  volume = {88},
  number = {3},
  pages = {265--266},
  publisher = {{Medical Library Association}},
  issn = {0025-7338},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC35238/},
  urldate = {2023-09-25},
  langid = {english},
  pmcid = {PMC35238},
  pmid = {10928714},
  keywords = {⛔ No DOI found},
  annotation = {QID: Q30895814},
  file = {C:\Users\nhiot\OneDrive\zotero\2000\Lipscomb - 2000 - Medical Subject Headings (MeSH).pdf}
}

@phdthesis{mahfoudhAdaptationOntologiesAvec2015,
  title = {{Adaptation d'ontologies avec les grammaires de graphes typ\'es : \'evolution et fusion}},
  shorttitle = {{Adaptation d'ontologies avec les grammaires de graphes typ\'es}},
  author = {Mahfoudh, Mariem},
  year = {2015},
  month = may,
  url = {https://tel.archives-ouvertes.fr/tel-01528579},
  urldate = {2019-03-07},
  abstract = {\'Etant une repr\'esentation formelle et explicite des connaissances d'un domaine, les ontologies font r\'eguli\`erement l'objet de nombreux changements et ont ainsi besoin d'\^etre constamment adapt\'ees pour notamment pouvoir \^etre r\'eutilis\'ees et r\'epondre aux nouveaux besoins. Leur r\'eutilisation peut prendre diff\'erentes formes (\'evolution, alignement, fusion, etc.), et pr\'esente plusieurs verrous scientifiques. L'un des plus importants est la pr\'eservation de la consistance de l'ontologie lors de son changement. Afin d'y r\'epondre, nous nous int\'eressons dans cette th\`ese \`a \'etudier les changements ontologiques et proposons un cadre formel capable de faire \'evoluer et de fusionner des ontologies sans affecter leur consistance. Premi\`erement, nous proposons TGGOnto (Typed Graph Grammars for Ontologies), un nouveau formalisme permettant la repr\'esentation des ontologies et leurs changements par les grammaires de graphes typ\'es. Un couplage entre ces deux formalismes est d\'efini afin de profiter des concepts des grammaires de graphes, notamment les NAC (Negative Application Conditions), pour la pr\'eservation de la consistance de l'ontologie adapt\'ee.Deuxi\`emement, nous proposons EvOGG (Evolving Ontologies with Graph Grammars), une approche d'\'evolution d'ontologies qui se base sur le formalisme GGTOnto et traite les inconsistances d'une mani\`ere a priori. Nous nous int\'eressons aux ontologies OWL et nous traitons \`a la fois : (1) l'enrichissement d'ontologies en \'etudiant leur niveau structurel et (2) le peuplement d'ontologies en \'etudiant les changements qui affectent les individus et leurs assertions. L'approche EvOGG d\'efinit des changements ontologiques de diff\'erents types (\'el\'ementaires, compos\'ees et complexes) et assure leur impl\'ementation par l'approche alg\'ebrique de transformation de graphes, SPO (Simple PushOut). Troisi\`emement, nous proposons GROM (Graph Rewriting for Ontology Merging), une approche de fusion d'ontologies capable d'\'eviter les redondances de donn\'ees et de diminuer les conflits dans le r\'esultat de fusion. L'approche propos\'ee se d\'ecompose en trois \'etapes : (1) la recherche de similarit\'e entre concepts en se basant sur des techniques syntaxiques, structurelles et s\'emantiques ; (2) la fusion d'ontologies par l'approche alg\'ebrique SPO ; (3) l'adaptation de l'ontologie globale r\'esultante par le biais des r\`egles de r\'e\'ecriture de graphes.Afin de valider les travaux men\'es dans cette th\`ese, nous avons d\'evelopp\'e plusieurs outils open source bas\'es sur l'outil AGG (Attributed Graph Grammar). Ces outils ont \'et\'e appliqu\'es sur un ensemble d'ontologies, essentiellement sur celles d\'evelopp\'ees dans le cadre du projet europ\'een CCAlps (Creatives Companies in Alpine Space) qui a financ\'e les travaux de cette th\`ese.},
  langid = {french},
  school = {Universit\'e de Haute Alsace-Mulhouse},
  keywords = {\#nosource},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2015\\Mahfoudh - 2015 - Adaptation d'ontologies avec les grammaires de gra.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\ZHAL55QA\\tel-01528579.html}
}

@article{mahfoudhAlgebraicGraphTransformations2015,
  title = {Algebraic Graph Transformations for Formalizing Ontology Changes and Evolving Ontologies},
  author = {Mahfoudh, Mariem and Forestier, Germain and Thiry, Laurent and Hassenforder, Michel},
  year = {2015},
  journal = {Knowledge-Based Systems},
  volume = {73},
  pages = {212--226},
  issn = {0950-7051},
  doi = {10.1016/j.knosys.2014.10.007},
  keywords = {\#nosource,AGG,Algebraic graph transformations,Consistency,Ontology evolution,Typed Graph Grammars},
  annotation = {QID: Q114826529},
  file = {C:\Users\nhiot\OneDrive\zotero\2015\Mahfoudh et al. - 2015 - Algebraic graph transformations for formalizing on.pdf}
}

@article{maierTestingImplicationsData1979,
  title = {Testing Implications of Data Dependencies},
  author = {Maier, David and Mendelzon, Alberto O. and Sagiv, Yehoshua},
  year = {1979},
  month = dec,
  journal = {ACM Trans. Database Syst.},
  volume = {4},
  number = {4},
  pages = {455--469},
  publisher = {{ACM New York, NY, USA}},
  issn = {0362-5915},
  doi = {10.1145/320107.320115},
  urldate = {2023-08-08},
  abstract = {Presented is a computation method\textemdash the chase\textemdash for testing implication of data dependencies by a set of data dependencies. The chase operates on tableaux similar to those of Aho, Sagiv, and Ullman. The chase includes previous tableau computation methods as special cases. By interpreting tableaux alternately as mappings or as templates for relations, it is possible to test implication of join dependencies (including multivalued dependencies) and functional dependencies by a set of dependencies.},
  keywords = {chase,data dependencies,functional dependencies,join dependencies,multivalued dependencies,relational databases,tableaux},
  annotation = {QID: Q114614050},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1979\\Maier et al. - 1979 - Testing implications of data dependencies.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\5DK3BYZS\\320107.html}
}

@inproceedings{maillotConsistencyEvaluationRDF2014,
  title = {Consistency {{Evaluation}} of {{RDF Data}}: {{How Data}} and {{Updates}} Are {{Relevant}}},
  shorttitle = {Consistency {{Evaluation}} of {{RDF Data}}},
  booktitle = {Tenth International Conference on Signal-Image Technology and Internet-Based Systems, {{SITIS}} 2014, Marrakech, Morocco, November 23-27, 2014},
  author = {Maillot, Pierre and Raimbault, Thomas and Genest, David and Loiseau, St{\'e}phane},
  year = {2014},
  month = nov,
  pages = {187--193},
  publisher = {{IEEE}},
  doi = {10.1109/SITIS.2014.39},
  abstract = {Trust and quality maintenance have always been problematic in the Semantic Web RDF bases. Numerous propositions to address these problems of data integration have been made, either based on ontologies or on additional metadata. However ontologies suffer from a adaptation speed slower than the data evolution speed and metadata requires ad-hoc manipulations of data by addition of extra-data. In this article we propose an original approach, based exclusively on data from the base, to evaluate the consistency of a candidate update to a RDF base, and finally to know if this update is relevant to the base. Our approach is inspired by case-based reasoning and uses similarity evaluation and query relaxation methods to compare a candidate update to the data from the base. If the modifications of a candidate update make the target part of the base more similar to other part (s) of the base, then this candidate update is considered consistent with the base and can be applied.},
  keywords = {Case-based reasoning,Cognition,Consistency,Context,Data Integration,Databases,Ontologies,Ontology,Resource description framework,Semantic Web,Similarity,Weight measurement},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2014\\Maillot et al. - 2014 - Consistency Evaluation of RDF Data How Data and U.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\7463EG8S\\7081546.html}
}

@incollection{maurelActesTALN20012001,
  title = {Actes de {{TALN}} 2001 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 2001 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Maurel, Denis},
  year = {2001},
  month = jul,
  publisher = {{Universit\'e de Tours / ATALA}},
  address = {{Tours}},
  keywords = {\#nosource}
}

@inproceedings{mihalceaTextrankBringingOrder2004,
  title = {Textrank: {{Bringing}} Order into Text},
  shorttitle = {Textrank},
  booktitle = {Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing},
  author = {Mihalcea, Rada and Tarau, Paul},
  year = {2004},
  pages = {404--411},
  keywords = {⛔ No DOI found},
  file = {C:\Users\nhiot\OneDrive\zotero\2004\Mihalcea et Tarau - 2004 - Textrank Bringing order into text.pdf}
}

@inproceedings{mihovDirectConstructionMinimal2001,
  title = {Direct {{Construction}} of {{Minimal Acyclic Subsequential Transducers}}},
  booktitle = {Implementation and Application of Automata, 5th International Conference, {{CIAA}} 2000, London, Ontario, Canada, July 24-25, 2000, Revised Papers},
  author = {Mihov, Stoyan and Maurel, Denis},
  year = {2001},
  series = {Lecture Notes in Computer Science},
  volume = {2088},
  pages = {217--229},
  publisher = {{Springer}},
  doi = {10/fb729c},
  abstract = {This paper presents an algorithm for direct building of minimal acyclic subsequential transducer, which represents a finite relation given as a sorted list of words with their outputs. The algorithm constructs the minimal transducer directly without constructing intermediate tree-like or pseudo-minimal transducers. In NLP applications our algorithm provides significantly better efficiency than the other algorithms building minimal transducer for large-scale natural language dictionaries. Some experimental comparisons are presented at the end of the paper.},
  keywords = {⛔ No DOI found},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2001\\Mihov et Maurel - 2001 - Direct Construction of Minimal Acyclic Subsequenti.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\5XISYCIH\\summary.html}
}

@inproceedings{minardDOINGDEFTCascade2020,
  title = {{DOING@DEFT : cascade de CRF pour l'annotation d'entit\'es cliniques imbriqu\'ees}},
  shorttitle = {{DOING@DEFT}},
  booktitle = {{Actes de la 6e conf\'erence conjointe Journ\'ees d'\'Etudes sur la Parole (JEP, 33e \'edition), Traitement Automatique des Langues Naturelles (TALN, 27e \'edition), Rencontre des \'Etudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (R\'ECITAL, 22e \'edition). Atelier D\'Efi Fouille de Textes}},
  author = {Minard, Anne-Lyse and Roques, Andr{\'e}ane and Hiot, Nicolas and Halfeld Ferrari Alves, Mirian and Savary, Agata},
  year = {2020},
  month = jun,
  pages = {66--78},
  publisher = {{ATALA et AFCP}},
  address = {{Nancy, France}},
  url = {https://hal.archives-ouvertes.fr/hal-02784743},
  urldate = {2023-08-07},
  abstract = {Cet article pr\'esente le syst\`eme d\'evelopp\'e par l'\'equipe DOING pour la campagne d'\'evaluation DEFT 2020 portant sur la similarit\'e s\'emantique et l'extraction d'information fine. L'\'equipe a particip\'e uniquement \`a la t\^ache 3 : ``extraction d'information''. Nous avons utilis\'e une cascade de CRF pour annoter les diff\'erentes informations \`a rep\'erer. Nous nous sommes concentr\'es sur la question de l'imbrication des entit\'es et de la pertinence d'un type d'entit\'e pour apprendre \`a reconna\^itre un autre. Nous avons \'egalement test\'e l'utilisation d'une ressource externe, MedDRA, pour am\'eliorer les performances du syst\`eme et d'un pipeline plus complexe mais ne g\'erant pas l'imbrication des entit\'es. Nous avons soumis 3 runs et nous obtenons en moyenne sur toutes les classes des F-mesures de 0,64, 0,65 et 0,61.},
  copyright = {All rights reserved},
  hal_id = {hal-02784743},
  hal_version = {v3},
  langid = {french},
  pdf = {https://hal.archives-ouvertes.fr/hal-02784743v3/file/212.pdf},
  keywords = {⛔ No DOI found,apprentissage automatique,cas cliniques,CRF.,entit\'es cliniques,entit\'es imbriqu\'ees,extraction d'information fine,me},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2020\\Minard et al. - 2020 - DOING@DEFT  cascade de CRF pour l'annotation d'en.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\DY6IJYRR\\hal-02784743.html}
}

@inproceedings{minardDOINGDEFTCascade2020a,
  title = {{{DOING}}@{{DEFT}} : Cascade de {{CRF}} Pour l'annotation d'entit\'es Cliniques Imbriqu\'ees ({{DOING}}@{{DEFT}} : Cascade of {{CRF}} for the Annotation of Nested Clinical Entities)},
  booktitle = {Actes de La 6e Conf\'erence Conjointe Journ\'ees d'{{\'Etudes}} Sur La Parole ({{JEP}}, 33e \'Edition), Traitement Automatique Des Langues Naturelles ({{TALN}}, 27e \'Edition), Rencontre Des \'Etudiants Chercheurs En Informatique Pour Le Traitement Automatique Des Langues ({{R\'ECITAL}}, 22e \'Edition). {{Atelier D\'Efi}} Fouille de Textes, Nancy, France, June 8-19, 2020},
  author = {Minard, Anne-Lyse and Roques, Andr{\'e}ane and Hiot, Nicolas and Alves, Mirian Halfeld Ferrari and Savary, Agata},
  editor = {Cardon, R{\'e}mi and Grabar, Natalia and Grouin, Cyril and Hamon, Thierry},
  year = {2020},
  pages = {66--78},
  publisher = {{ATALA et AFCP}},
  url = {https://www.aclweb.org/anthology/2020.jeptalnrecital-deft.7/},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/taln/MinardRHAS20.bib},
  keywords = {\#nosource,⛔ No DOI found},
  timestamp = {Tue, 15 Dec 2020 17:40:18 +0100}
}

@inproceedings{moranteLearningScopeNegation2008,
  title = {Learning the Scope of Negation in Biomedical Texts},
  booktitle = {Proceedings of the {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} - {{EMNLP}} '08},
  author = {Morante, Roser and Liekens, Anthony and Daelemans, Walter},
  year = {2008},
  pages = {715},
  publisher = {{Association for Computational Linguistics}},
  address = {{Honolulu, Hawaii}},
  doi = {10.3115/1613715.1613805},
  urldate = {2023-10-06},
  abstract = {In this paper we present a machine learning system that finds the scope of negation in biomedical texts. The system consists of two memory-based engines, one that decides if the tokens in a sentence are negation signals, and another that finds the full scope of these negation signals. Our approach to negation detection differs in two main aspects from existing research on negation. First, we focus on finding the scope of negation signals, instead of determining whether a term is negated or not. Second, we apply supervised machine learning techniques, whereas most existing systems apply rule-based algorithms. As far as we know, this way of approaching the negation scope finding task is novel.},
  langid = {english},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2008\\Morante et al. - 2008 - Learning the scope of negation in biomedical texts.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\Z6Q4NUMF\\learning-the-scope-of-negation-Q3PJj6.html}
}

@incollection{nazarenkoActesTALN20092009,
  title = {Actes de {{TALN}} 2009 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 2009 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Nazarenko, Adeline and Poibeau, Thierry},
  year = {2009},
  month = jun,
  publisher = {{LIPN / ATALA}},
  address = {{Senlis}},
  keywords = {\#nosource}
}

@book{needhamGraphAlgorithmsPractical2019,
  title = {Graph {{Algorithms}}: {{Practical Examples}} in {{Apache Spark}} and {{Neo4j}}},
  shorttitle = {Graph {{Algorithms}}},
  author = {Needham, Mark and Hodler, Amy E.},
  year = {2019},
  month = may,
  publisher = {{"O'Reilly Media, Inc."}},
  abstract = {Discover how graph algorithms can help you leverage the relationships within your data to develop more intelligent solutions and enhance your machine learning models. You'll learn how graph analytics are uniquely suited to unfold complex structures and reveal difficult-to-find patterns lurking in your data. Whether you are trying to build dynamic network models or forecast real-world behavior, this book illustrates how graph algorithms deliver value\textemdash from finding vulnerabilities and bottlenecks to detecting communities and improving machine learning predictions.This practical book walks you through hands-on examples of how to use graph algorithms in Apache Spark and Neo4j\textemdash two of the most common choices for graph analytics. Also included: sample code and tips for over 20 practical graph algorithms that cover optimal pathfinding, importance through centrality, and community detection.Learn how graph analytics vary from conventional statistical analysisUnderstand how classic graph algorithms work, and how they are appliedGet guidance on which algorithms to use for different types of questionsExplore algorithm examples with working code and sample datasets from Spark and Neo4jSee how connected feature extraction can increase machine learning accuracy and precisionWalk through creating an ML workflow for link prediction combining Neo4j and Spark},
  googlebooks = {yYWZDwAAQBAJ},
  isbn = {978-1-4920-4765-0},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / General,Computers / Computer Graphics,Computers / Intelligence (AI) \& Semantics,Computers / Mathematical \& Statistical Software,Computers / Programming / Algorithms,Computers / Software Development \& Engineering / Computer Graphics,Mathematics / Graphic Methods},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2019\\Needham et Hodler - 2019 - Graph Algorithms Practical Examples in Apache Spa.epub;C\:\\Users\\nhiot\\OneDrive\\zotero\\2019\\Needham et Hodler - 2019 - Graph Algorithms Practical Examples in Apache Spa2.pdf}
}

@article{nikolaouQueryingIncompleteInformation2016,
  title = {Querying Incomplete Information in {{RDF}} with {{SPARQL}}},
  author = {Nikolaou, Charalampos and Koubarakis, Manolis},
  year = {2016},
  month = aug,
  journal = {Artificial Intelligence},
  volume = {237},
  pages = {138--171},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2016.04.005},
  urldate = {2023-08-08},
  abstract = {Incomplete information has been studied in-depth in relational databases and knowledge representation. In the context of the Web, incomplete information issues have been studied in detail for XML, but very few papers exist that do the same for RDF. In this paper we make the first general proposal for extending RDF with the ability to represent property values that exist but are unknown or partially known using constraints. Following ideas from incomplete information literature, we develop a semantics for this extension of RDF, called RDFi, and study query evaluation for SPARQL. We transfer the concept of representation systems from incomplete information in relational databases to the case of RDFi and identify two very important fragments of SPARQL that can be used to define a representation system for RDFi. The first corresponds to the monotone fragment of graph patterns that uses only the operators AND, UNION, and FILTER. The second corresponds to the well-designed graph patterns, that is, a fragment that uses only operators AND, FILTER, and OPT, and enjoys interesting properties that make query evaluation efficient. We prove that each of the two fragments can be used to define a representation system for CONSTRUCT queries without blank nodes in their templates. We also define the fundamental concept of certain answers to SPARQL queries over RDFi databases and present an algorithm for its computation. Then, we present complexity results for computing certain answers by considering equality, temporal, and spatial constraint languages and the class of CONSTRUCT queries of our representation systems. Finally, we demonstrate the usefulness of RDFi in geospatial Semantic Web applications by giving a number of examples and comparing the modeling capabilities of RDFi with related formalisms found in the literature.},
  langid = {english},
  keywords = {Incomplete information,RDF,Semantic Web,SPARQL},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2016\\Nikolaou et Koubarakis - 2016 - Querying incomplete information in RDF with SPARQL.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\N8CIQSEH\\S0004370216300467.html}
}

@inproceedings{pathakEzDISupervisedNLP2015,
  title = {{{ezDI}}: {{A}} Supervised {{NLP}} System for Clinical Narrative Analysis},
  booktitle = {Proceedings of the 9th International Workshop on Semantic Evaluation, {{SemEval}}@{{NAACL-HLT}} 2015, Denver, Colorado, {{USA}}, June 4-5, 2015},
  author = {Pathak, Parth and Patel, Pinal and Panchal, Vishal and Soni, Sagar and Dani, Kinjal and Patel, Amrish and Choudhary, Narayan},
  editor = {Cer, Daniel M. and Jurgens, David and Nakov, Preslav and Zesch, Torsten},
  year = {2015},
  pages = {412--416},
  publisher = {{The Association for Computer Linguistics}},
  doi = {10.18653/v1/s15-2071},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/semeval/PathakPPSDPC15.bib},
  keywords = {\#nosource},
  timestamp = {Tue, 28 Jan 2020 10:29:20 +0100}
}

@inproceedings{pereiraParsingDeduction1983,
  title = {Parsing as Deduction},
  booktitle = {21st Annual Meeting of the Association for Computational Linguistics},
  author = {Pereira, Fernando C. N. and Warren, David H. D.},
  year = {1983},
  month = jun,
  pages = {137--144},
  publisher = {{Association for Computational Linguistics}},
  address = {{Cambridge, Massachusetts, USA}},
  doi = {10.3115/981311.981338},
  keywords = {\#nosource}
}

@inproceedings{pichlerComplexityEvaluatingTuple2011,
  title = {The Complexity of Evaluating Tuple Generating Dependencies},
  booktitle = {Proceedings of the 14th {{International Conference}} on {{Database Theory}}},
  author = {Pichler, Reinhard and Skritek, Sebastian},
  year = {2011},
  month = mar,
  series = {{{ICDT}} '11},
  pages = {244--255},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1938551.1938583},
  urldate = {2023-08-16},
  abstract = {Dependencies have played an important role in database design for many years. More recently, they have also turned out to be central to data integration and data exchange. In this work we concentrate on tuple generating dependencies (tgds) which enforce the presence of certain tuples in a database instance if certain other tuples are already present. Previous complexity results in data integration and data exchange mainly referred to the data complexity. In this work, we study the query complexity and combined complexity of a fundamental problem related to tgds, namely checking if a given tgd is satisfied by a database instance. We also address an important variant of this problem which deals with updates (by inserts or deletes) of a database: Here we have to check if all previously satisfied tgds are still satisfied after an update. We show that the query complexity and combined complexity of these problems are much higher than the data complexity. However, we also prove sufficient conditions on the tgds to reduce this high complexity.},
  isbn = {978-1-4503-0529-7},
  file = {C:\Users\nhiot\OneDrive\zotero\2011\Pichler et Skritek - 2011 - The complexity of evaluating tuple generating depe.pdf}
}

@incollection{pierrelActesTALN20022002,
  title = {Actes de {{TALN}} 2002 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 2002 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Pierrel, Jean-Marie},
  year = {2002},
  month = jun,
  publisher = {{ATILF / ATALA}},
  address = {{Nancy}},
  keywords = {\#nosource}
}

@inproceedings{pokornyGraphDatabasesTheir2015,
  title = {Graph {{Databases}}: {{Their Power}} and {{Limitations}}},
  shorttitle = {Graph {{Databases}}},
  booktitle = {Computer {{Information Systems}} and {{Industrial Management}}: 14th {{IFIP TC}} 8 {{International Conference}}, {{CISIM}} 2015, {{Warsaw}}, {{Poland}}, {{September}} 24-26, 2015, {{Proceedings}} 14},
  author = {Pokorn{\'y}, Jaroslav},
  editor = {Saeed, Khalid and Homenda, Wladyslaw},
  year = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {58--69},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-24369-6_5},
  abstract = {Real world data offers a lot of possibilities to be represented as graphs. As a result we obtain undirected or directed graphs, multigraphs and hypergraphs, labelled or weighted graphs and their variants. A development of graph modelling brings also new approaches, e.g., considering constraints. Processing graphs in a database way can be done in many different ways. Some graphs can be represented as JSON or XML structures and processed by their native database tools. More generally, a graph database is specified as any storage system that provides index-free adjacency, i.e. an explicit graph structure. Graph database technology contains some technological features inherent to traditional databases, e.g. ACID properties and availability. Use cases of graph databases like Neo4j, OrientDB, InfiniteGraph, FlockDB, AllegroGraph, and others, document that graph databases are becoming a common means for any connected data. In Big Data era, important questions are connected with scalability for large graphs as well as scaling for read/write operations. For example, scaling graph data by distributing it in a network is much more difficult than scaling simpler data models and is still a work in progress. Still a challenge is pattern matching in graphs providing, in principle, an arbitrarily complex identity function. Mining complete frequent patterns from graph databases is also challenging since supporting operations are computationally costly. In this paper, we discuss recent advances and limitations in these areas as well as future directions.},
  isbn = {978-3-319-24369-6},
  langid = {english},
  keywords = {Big graphs,Graph database,Graph querying,Graph scalability,Graph storage},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2015\\Pokorný - 2015 - Graph Databases Their Power and Limitations.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\Z5XWGHCZ\\978-3-319-24369-6_5.html}
}

@phdthesis{pollardGeneralizedPhraseStructure1984,
  title = {Generalized Phrase Structure Grammars, Head Grammars, and Natural Language},
  author = {Pollard, Carl},
  year = {1984},
  school = {Stanford University, CA},
  keywords = {\#nosource}
}

@inproceedings{porterSnowballLanguageStemming2001,
  title = {Snowball: {{A}} Language for Stemming Algorithms},
  shorttitle = {Snowball},
  author = {Porter, Martin F.},
  year = {2001},
  month = oct,
  url = {http://snowball.tartarus.org/texts/introduction.html},
  added-at = {2008-03-11T15:53:08.000+0100},
  biburl = {https://www.bibsonomy.org/bibtex/2a0334d9771cea49e3d12e5d71180300f/danielt},
  howpublished = {Published online},
  interhash = {a9213c856a8b690e9ebe255096d91a83},
  intrahash = {a0334d9771cea49e3d12e5d71180300f},
  keywords = {NaturalLanguage(Processing) dipl\textsubscript{l}iteratur information\textsubscript{r}etrieval stemming},
  timestamp = {2008-03-25T12:26:47.000+0100},
  file = {C:\Users\nhiot\Zotero\storage\JHUK5JZ3\introduction.html}
}

@inproceedings{pradhanTaskShAReCLEF2013,
  title = {Task 1: {{ShARe}}/{{CLEF eHealth}} Evaluation Lab 2013},
  booktitle = {Working Notes for {{CLEF}} 2013 Conference , Valencia, Spain, September 23-26, 2013},
  author = {Pradhan, Sameer and Elhadad, No{\'e}mie and South, Brett R. and Mart{\'i}nez, David and Christensen, Lee M. and Vogel, Amy and Suominen, Hanna and Chapman, Wendy W. and Savova, Guergana K.},
  editor = {Forner, Pamela and Navigli, Roberto and Tufis, Dan and Ferro, Nicola},
  year = {2013},
  series = {{{CEUR}} Workshop Proceedings},
  volume = {1179},
  publisher = {{CEUR-WS.org}},
  url = {http://ceur-ws.org/Vol-1179/CLEF2013wn-CLEFeHealth-PradhanEt2013.pdf},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/clef/PradhanESMCVSCS13.bib},
  keywords = {\#nosource,⛔ No DOI found},
  timestamp = {Wed, 12 Feb 2020 16:44:31 +0100}
}

@inproceedings{raadDetectionLiensIdentite2017,
  title = {D\'etection de Liens d'identit\'e Contextuels Dans Une Base de Connaissances.},
  booktitle = {{{IC}} 2017 - 28es {{Journ\'ees}} Francophones d'{{Ing\'enierie}} Des {{Connaissances}}},
  author = {Raad, Joe and Pernelle, Nathalie and Sa{\"i}s, Fatiha},
  editor = {Roussey, Catherine},
  year = {2017},
  month = jul,
  series = {Actes {{IC}} 2017 28es {{Journ\'ees}} Francophones d'{{Ing\'enierie}} Des {{Connaissances}}},
  pages = {56--67},
  address = {{Caen, France}},
  url = {https://hal.archives-ouvertes.fr/hal-01570053},
  urldate = {2019-03-04},
  abstract = {De nombreuses applications du Web de donn\'ees exploitent des liens d'identit\'es d\'eclar\'es \`a l'aide du constructeur owl :sameAs. Cependant, diff\'erentes \'etudes ont montr\'e qu'une utilisation abusive de ces liens peut conduire \`a des inf\'erences erron\'ees ou contradictoires. Dans ce papier nous proposons de calculer des liens d'identit\'es contextuels qui permettent d'expliciter les contextes dans lesquels ces liens sont valides. La notion de contexte que nous proposons est repr\'esent\'ee en se basant sur l'ontologie de domaine dans laquelle les instances sont repr\'esent\'ees. Nous avons exp\'eriment\'e cette approche dans le domaine des donn\'ees scientifiques o\`u les \'el\'ements d\'ecrivant les exp\'eriences partagent rarement un lien d'identit\'e tel que d\'efini par owl :sameAs.},
  keywords = {Bases de connaissances,Contextes,Enrichissement,Liage de donn\'ees,Ontologies},
  file = {C:\Users\nhiot\OneDrive\zotero\2017\Raad et al. - 2017 - Détection de liens d'identité contextuels dans une.pdf}
}

@article{raadDetectionLiensIdentite2018,
  title = {D\'etection de Liens d'identit\'e Erron\'es En Utilisant La D\'etection de Communaut\'es Dans Les Graphes d'identit\'e},
  author = {Raad, Joe and BECK, Wouter and Pernelle, Nathalie and Sais, Fatiha and Harmelen, Frank},
  year = {2018},
  month = aug,
  journal = {Ing\'enierie des syst\`emes d'information},
  volume = {23},
  number = {3-4},
  pages = {61--88},
  doi = {10.3166/isi.23.3-4.61-88},
  file = {C:\Users\nhiot\OneDrive\zotero\2018\Raad et al. - 2018 - Détection de liens d’identité erronés en utilisant2.pdf}
}

@misc{REDACCorpusTexte,
  title = {{{REDAC}} : {{Corpus}} Texte {{Wikip\'ediaFR2008}}},
  url = {http://redac.univ-tlse2.fr/corpus/wikipedia.html},
  urldate = {2023-10-06},
  file = {C:\Users\nhiot\Zotero\storage\XEYQ9D4E\wikipedia.html}
}

@inproceedings{reiterLogicalReconstructionRelational1989,
  title = {Towards a {{Logical Reconstruction}} of {{Relational Database Theory}}},
  booktitle = {Readings in {{Artificial Intelligence}} and {{Databases}}},
  author = {Reiter, Raymond},
  editor = {Mylopolous, John and Brodie, Michael},
  year = {1989},
  month = jan,
  pages = {301--327},
  publisher = {{Elsevier}},
  address = {{San Francisco (CA)}},
  doi = {10.1016/B978-0-934613-53-8.50025-X},
  urldate = {2023-08-07},
  abstract = {Insofar as database theory can be said to owe a debt to logic, the currency on loan is model theoretic in the sense that a database can be viewed as a particular kind of first order interpretation, and query evaluation is a process of truth Junctional evaluation of first order formulae with respect to this interpretation. It is this model theoretic paradigm which leads, for example, to many valued propositional logies for databases with null values. In this chapter I argue that a proof theoretic view of databases is possible, and indeed much more fruitful. Specifically, I show how relational databases can be seen as special theories of first order logic, namely theories incorporating the following assumptions:1.The domain closure assumption. The individuals occurring in the database are all and only the existing individuals.2.The unique name assumption. Individuals with distinct names are distinct.3.The closed world assumption. The only possible instances of a relation are those implied by the database. It will follow that a proof theoretic paradigm for relational databases provides a correct treatment of:1.Query evaluation for databases that have incomplete information, including null values.2.Integrity constraints and their enforcement.3.Conceptual modelling and the extension of the relational model to incorporate more real world semantics.},
  isbn = {978-0-934613-53-8},
  langid = {english},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1989\\Reiter - 1989 - Towards a Logical Reconstruction of Relational Dat.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\4VPCJ9R4\\B978093461353850025X.html}
}

@article{reiterSoundSometimesComplete1986,
  title = {A Sound and Sometimes Complete Query Evaluation Algorithm for Relational Databases with Null Values},
  author = {Reiter, Raymond},
  year = {1986},
  month = apr,
  journal = {Journal of the ACM (JACM)},
  volume = {33},
  number = {2},
  pages = {349--370},
  publisher = {{ACM New York, NY, USA}},
  issn = {0004-5411},
  doi = {10.1145/5383.5388},
  abstract = {A sound and, in certain cases, complete method is described for evaluating queries in relational databases with null values where these nulls represent existing but unknown individuals. The soundness and completeness results are proved relative to a formalization of such databases as suitable theories of first-order logic. Because the algorithm conforms to the relational algebra, it may easily be incorporated into existing relational systems.},
  keywords = {\#nosource},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1986\\Reiter - 1986 - A sound and sometimes complete query evaluation al.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\J9PHZZNY\\5383.html}
}

@book{robinsonGraphDatabasesNew2015,
  title = {Graph {{Databases}}: {{New Opportunities}} for {{Connected Data}}},
  shorttitle = {Graph {{Databases}}},
  author = {Robinson, Ian and Webber, Jim and Eifrem, Emil},
  year = {2015},
  month = jun,
  publisher = {{"O'Reilly Media, Inc."}},
  abstract = {Discover how graph databases can help you manage and query highly connected data. With this practical book, you'll learn how to design and implement a graph database that brings the power of graphs to bear on a broad range of problem domains. Whether you want to speed up your response to user queries or build a database that can adapt as your business evolves, this book shows you how to apply the schema-free graph model to real-world problems.This second edition includes new code samples and diagrams, using the latest Neo4j syntax, as well as information on new functionality. Learn how different organizations are using graph databases to outperform their competitors. With this book's data modeling, query, and code examples, you'll quickly be able to implement your own solution.Model data with the Cypher query language and property graph modelLearn best practices and common pitfalls when modeling with graphsPlan and implement a graph database solution in test-driven fashionExplore real-world examples to learn how and why organizations use a graph databaseUnderstand common patterns and components of graph database architectureUse analytical techniques and algorithms to mine graph database information},
  googlebooks = {RTvcCQAAQBAJ},
  isbn = {978-1-4919-3086-1},
  langid = {english},
  keywords = {Computers / Data Science / Data Analytics,Computers / Data Science / Data Modeling \& Design,Computers / Data Science / Data Visualization,Computers / Data Science / Data Warehousing,Computers / Data Science / General},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2015\\Robinson et al. - 2015 - Graph Databases New Opportunities for Connected D.epub;C\:\\Users\\nhiot\\OneDrive\\zotero\\2015\\Robinson et al. - 2015 - Graph Databases New Opportunities for Connected D.pdf}
}

@inproceedings{savaryRelationExtractionClinical2022,
  title = {Relation {{Extraction}} from~{{Clinical Cases}} for~a~{{Knowledge Graph}}},
  booktitle = {European {{Conference}} on {{Advances}} in {{Databases}} and {{Information Systems}}},
  author = {Savary, Agata and Silvanovich, Alena and Minard, Anne-Lyse and Hiot, Nicolas and Halfeld Ferrari, Mirian},
  editor = {Chiusano, Silvia and Cerquitelli, Tania and Wrembel, Robert and N{\o}rv{\aa}g, Kjetil and Catania, Barbara and {Vargas-Solar}, Genoveva and Zumpano, Ester},
  year = {2022},
  series = {Communications in {{Computer}} and {{Information Science}}},
  pages = {353--365},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-15743-1_33},
  abstract = {We describe a system for automatic extraction of semantic relations between entities in a medical corpus of clinical cases. It builds upon a previously developed module for entity extraction and upon a morphosyntactic parser. It uses experimentally designed rules based on syntactic dependencies and trigger words, as well as on sequencing and nesting of entities of particular types. The results obtained on a small corpus are promising. Our larger perspective is transforming information extracted from medical texts into knowledge graphs.},
  isbn = {978-3-031-15743-1},
  langid = {english},
  keywords = {\#nosource,me},
  file = {C:\Users\nhiot\OneDrive\zotero\2022\Savary et al. - 2022 - Relation Extraction from Clinical Cases for a Know.pdf}
}

@article{scheweLimitationsRuleTriggering1998,
  title = {Limitations of Rule Triggering Systems for Integrity Maintenance in the Context of Transition Specifications},
  author = {Schewe, Klaus-Dieter and Thalheim, Bernhard},
  year = {1998},
  month = jan,
  journal = {Acta Cybernetica},
  volume = {13},
  number = {3},
  pages = {277--304},
  publisher = {{University of Szeged}},
  issn = {2676-993X},
  doi = {10.1007/3-540-63699-4_12},
  urldate = {2023-08-08},
  abstract = {Integrity Maintenance is considered one of the major application fields of rule triggering systems (RTSs). In the case of a given integrity constraint being violated by a database transition these systems trigger repairing actions. Then it is necessary to guarantee the termination of the RTS, its determinacy and the consistency of final states. Transition specifications provide some kind of dynamic semantics requiring certasin effects on database states to occur. In the context of transition specifications integrity maintenance has to cope with the additional problem of effect preservation. Limitations of RTSs with respect to this extended problems are investigated. It will be shown that for any set of constraints there exist non-repairable transitions, which depend on the closure of the constraint set. This implies that integrity maintenance by RTSs is only possible, if the constraint implication problem is decidable. Even if unrepairable transitions are excluded, this does not prevent the RTS to produce undesired behaviour. Analyzing the behaviour of RTSs leads to the definition of critical paths in associated rule hypergraphs and the requirement of such paths being absent. It will be shown that this requirement can be satisfied if the underlying set of constraints is stratified, but this notion turns out to be too strong to be also necessary. A sufficient and necessary condition for the absence of critical paths is obtained, if sets of constraints are required to be locally stratified.},
  copyright = {Copyright (c)},
  langid = {english},
  keywords = {\#nosource},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1998\\Schewe et Thalheim - 1998 - Limitations of rule triggering systems for integri2.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\QNHWPH6A\\294150.html}
}

@article{scheweTheoryConsistencyEnforcement1999,
  title = {Towards a Theory of Consistency Enforcement},
  author = {Schewe, Klaus-Dieter and Thalheim, Bernhard},
  year = {1999},
  month = feb,
  journal = {Acta Informatica},
  volume = {36},
  number = {2},
  pages = {97--141},
  issn = {1432-0525},
  doi = {10.1007/s002360050155},
  urldate = {2023-08-16},
  abstract = {State oriented specifications with invariants occur in almost all formal specification languages. Hence the problem is to prove the consistency of the specified operations with respect to the invariants. Whilst the problem seems to be easily solvable in predicative specifications, it usually requires sophisticated verification efforts, when specifications in the style of Dijkstra's guarded commands as e.g. in the specification language B are used. As an alternative consistency enforcement will be discussed in this paper. The basic idea is to replace inconsistent operations by new consistent ones preserving at the same time the intention of the old one. More precisely, this can be formalized by consistent spezializations, where specialization is a specific partial order on operations defined via predicate transformers. It will be shown that greatest consistent specializations (GCSs) always exist and are compatible with conjunctions of invariants. Then under certain mild restrictions the general construction of such GCSs is possible. Precisely, given the GCSs of simple basic assignments the GCS of a complex operation results from replacing involved assignments by their GCSs and the investigation of a guard. In general, GCS construction can be embedded in refinement calculi and therefore strengthens the systematic development of correct programs.},
  langid = {english},
  keywords = {Formal Specification,General Construction,Partial Order,Specification Language,Systematic Development},
  annotation = {QID: Q57376473},
  file = {C:\Users\nhiot\OneDrive\zotero\1999\Schewe et Thalheim - 1999 - Towards a theory of consistency enforcement.pdf}
}

@inproceedings{seretanCollocationTranslationBased2007,
  title = {{Collocation translation based on sentence alignment and parsing}},
  booktitle = {{Actes de la 14\`eme conf\'erence sur le Traitement Automatique des Langues Naturelles. Articles longs}},
  author = {Seretan, Violeta and Wehrli, {\'E}ric},
  year = {2007},
  month = jun,
  pages = {375--384},
  publisher = {{IRIT / ATALA}},
  address = {{Toulouse, France}},
  url = {https://aclanthology.org/2007.jeptalnrecital-long.37},
  urldate = {2023-10-23},
  abstract = {Bien que de nombreux efforts aient \'et\'e d\'eploy\'es pour extraire des collocations \`a partir de corpus de textes, seule une minorit\'e de travaux se pr\'eoccupent aussi de rendre le r\'esultat de l'extraction pr\^et \`a \^etre utilis\'e dans les applications TAL qui pourraient en b\'en\'eficier, telles que la traduction automatique. Cet article d\'ecrit une m\'ethode pr\'ecise d'identification de la traduction des collocations dans un corpus parall\`ele, qui pr\'esente les avantages suivants : elle peut traiter des collocation flexibles (et pas seulement fig\'ees) ; elle a besoin de ressources limit\'ees et d'un pouvoir de calcul raisonnable (pas d'alignement complet, pas d'entra\^inement) ; elle peut \^etre appliqu\'ee \`a plusieurs paires des langues et fonctionne m\^eme en l'absence de dictionnaires bilingues. La m\'ethode est bas\'ee sur l'information syntaxique provenant du parseur multilingue Fips. L'\'evaluation effectu\'ee sur 4000 collocations de type verbe-objet correspondant \`a plusieurs paires de langues a montr\'e une pr\'ecision moyenne de 89.8\% et une couverture satisfaisante (70.9\%). Ces r\'esultats sont sup\'erieurs \`a ceux enregistr\'es dans l'\'evaluation d'autres m\'ethodes de traduction de collocations.},
  langid = {french},
  keywords = {\#nosource},
  file = {C:\Users\nhiot\OneDrive\zotero\2007\Seretan et Wehrli - 2007 - Collocation translation based on sentence alignmen.pdf}
}

@phdthesis{sirangeloRepresentingQueryingIncomplete2014,
  type = {{{HDR}}},
  title = {Representing and {{Querying Incomplete Information}}: A {{Data Interoperability Perspective}}},
  shorttitle = {Representing and {{Querying Incomplete Information}}},
  author = {Sirangelo, Cristina},
  year = {2014},
  month = dec,
  address = {{Cachan}},
  url = {https://tel.archives-ouvertes.fr/tel-01092547},
  urldate = {2023-08-07},
  abstract = {This habilitation thesis presents some of my most recent work, which has been done in collaboration with several other people. In particular this thesis concentrates on our contributions to the study of incomplete information in the context of data interoperability. In this scenario data is heterogenous and decentralized, needs to be integrated from several sources and exchanged between different applications. Incompleteness, i.e. the presence of ``missing'' or ``unknown'' portions of data, is naturally generated in data exchange and integration, due to data heterogeneity. The management of incomplete information poses new challenges in this context.The focus of our study is the development of models of incomplete information suitable to data interoperability tasks, and the study of techniques for efficiently querying several forms of incompleteness.},
  langid = {english},
  school = {Ecole Normale Sup\'erieure de Cachan},
  keywords = {\#nosource},
  file = {C:\Users\nhiot\OneDrive\zotero\2014\Sirangelo - 2014 - Representing and Querying Incomplete Information .pdf}
}

@article{skavantzosNormalizingPropertyGraphs2023a,
  ids = {skavantzosNormalizingPropertyGraphs2023,skavantzosphilippNormalizingPropertyGraphs2023},
  title = {Normalizing {{Property Graphs}}},
  author = {Skavantzos, Philipp and Link, Sebastian},
  year = {2023},
  month = aug,
  journal = {Proc. VLDB Endow.},
  volume = {16},
  number = {11},
  pages = {3031--3043},
  publisher = {{VLDB Endowment}},
  issn = {2150-8097},
  doi = {10.14778/3611479.3611506},
  urldate = {2023-10-18},
  abstract = {Normalization aims at minimizing sources of potential data inconsistency and costs of update maintenance incurred by data redundancy. For relational databases, different classes of dependencies cause data redundancy and have resulted in proposals such as Third, Boyce-Codd, Fourth and Fifth Normal Form. Features of more advanced data models make it challenging to extend achievements from the relational model to missing, non-atomic, or uncertain data. We initiate research on the normalization of graph data, starting with a class of functional dependencies tailored to property graphs. We show that this class captures important semantics of applications, constitutes a rich source of data redundancy, its implication problem can be decided in linear time, and facilitates the normalization of property graphs flexibly tailored to their labels and properties that are targeted by applications. We normalize property graphs into Boyce-Codd Normal Form without loss of data and dependencies whenever possible for the target labels and properties, but guarantee Third Normal Form in general. Experiments on real-world property graphs quantify and qualify various benefits of graph normalization: 1) removing redundant property values as sources of inconsistent data, 2) detecting inconsistency as violation of functional dependencies, 3) reducing update overheads by orders of magnitude, and 4) significant speed ups of aggregate queries.},
  langid = {english},
  file = {C:\Users\nhiot\Zotero\storage\WV55R54N\Skavantzos et Link - 2023 - Normalizing Property Graphs.pdf}
}

@article{sparckjonesStatisticalInterpretationTerm1972,
  ids = {sparckjonesStatisticalInterpretationTerm1972a,sparckjonesStatisticalInterpretationTerm2004},
  title = {A Statistical Interpretation of Term Specificity and Its Application in Retrieval},
  author = {Sparck Jones, Karen},
  year = {1972},
  month = jan,
  journal = {Journal of Documentation},
  volume = {28},
  number = {1},
  pages = {11--21},
  publisher = {{MCB UP Ltd}},
  issn = {0022-0418},
  doi = {10.1108/eb026526},
  urldate = {2023-09-22},
  abstract = {The exhaustivity of document descriptions and the specificity of index terms are usually regarded as independent. It is suggested that specificity should be interpreted statistically, as a function of term use rather than of term meaning. The effects on retrieval of variations in term specificity are examined, experiments with three test collections showing in particular that frequently-occurring terms are required for good overall performance. It is argued that terms should be weighted according to collection frequency, so that matches on less frequent, more specific, terms are of greater value than matches on frequent terms. Results for the test collections show that considerable improvements in performance are obtained with this very simple procedure.},
  keywords = {\#nosource},
  annotation = {QID: Q54296753},
  file = {C:\Users\nhiot\OneDrive\zotero\1972\Sparck Jones - 1972 - A statistical interpretation of term specificity a.pdf}
}

@inproceedings{strubellEnergyPolicyConsiderations2019,
  title = {Energy and {{Policy Considerations}} for {{Deep Learning}} in {{NLP}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  year = {2019},
  month = jun,
  pages = {3645--3650},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1355},
  urldate = {2023-10-23},
  abstract = {Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.},
  keywords = {Computer Science - Computation and Language},
  annotation = {QID: Q64512333},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2019\\Strubell et al. - 2019 - Energy and Policy Considerations for Deep Learning2.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\IYNS5HUA\\1906.html}
}

@inproceedings{tellierHowSplitRecursive2008,
  title = {How to Split Recursive Automata},
  booktitle = {Grammatical Inference: {{Algorithms}} and Applications. 9th International Colloquium, {{ICGI}} 2008 Saint-Malo, France, September 22-24, 2008 Proceedings},
  author = {Tellier, Isabelle},
  editor = {Clark, Alexander and Coste, Fran{\c c}ois and Miclet, Laurent},
  year = {2008},
  series = {{{LNAI}}},
  volume = {5278},
  pages = {200--212},
  publisher = {{Springer}},
  doi = {10.1007/978-3-540-88009-7_16},
  hal_id = {inria-00341770},
  hal_version = {v1},
  keywords = {categorial grammars,grammatical inference,recursive automata}
}

@book{vukoticNeo4jAction2015,
  title = {Neo4j in Action},
  author = {Vukotic, Aleksa and Watt, Nicki and Abedrabbo, Tareq and Fox, Dominic and Partner, Jonas},
  year = {2015},
  volume = {22},
  publisher = {{Manning Shelter Island}},
  file = {C:\Users\nhiot\OneDrive\zotero\2015\Vukotic et al. - 2015 - Neo4j in action.pdf}
}

@incollection{wehrliActesTALN20002000,
  title = {Actes de {{TALN}} 2000 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 2000 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Wehrli, Eric and Rajman, Martin},
  year = {2000},
  month = oct,
  publisher = {{EPFL / ATALA}},
  address = {{Lausanne}},
  keywords = {\#nosource}
}

@article{wengMedicalSubdomainClassification2017,
  ids = {weng2017medical},
  title = {Medical Subdomain Classification of Clinical Notes Using a Machine Learning-Based Natural Language Processing Approach},
  author = {Weng, Wei-Hung and Wagholikar, Kavishwar B. and McCray, Alexa T. and Szolovits, Peter and Chueh, Henry C.},
  year = {2017},
  month = dec,
  journal = {BMC Medical Informatics and Decision Making},
  volume = {17},
  number = {1},
  pages = {1--13},
  publisher = {{BioMed Central}},
  issn = {1472-6947},
  doi = {10/gg5fzg},
  urldate = {2021-05-28},
  abstract = {The medical subdomain of a clinical note, such as cardiology or neurology, is useful content-derived metadata for developing machine learning downstream applications. To classify the medical subdomain of a note accurately, we have constructed a machine learning-based natural language processing (NLP) pipeline and developed medical subdomain classifiers based on the content of the note.},
  keywords = {Deep Learning,Distributed Representation,Machine Learning,{Medical Decision Making, Computer-assisted},Natural Language Processing,Unified Medical Language System},
  annotation = {QID: Q45943393},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\2017\\Weng et al. - 2017 - Medical subdomain classification of clinical notes.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\SHS3H2R3\\s12911-017-0556-8.html}
}

@article{whiteGraphSemigroupHomomorphisms1983,
  title = {Graph and Semigroup Homomorphisms on Networks of Relations},
  author = {White, Douglas R. and Reitz, Karl P.},
  year = {1983},
  month = jun,
  journal = {Social Networks},
  volume = {5},
  number = {2},
  pages = {193--234},
  issn = {0378-8733},
  doi = {10.1016/0378-8733(83)90025-4},
  urldate = {2023-10-31},
  abstract = {The algebraic definitions presented here are motivated by our search for an adequate formalization of the concepts of social roles as regularities in social network patterns. The theorems represent significant homomorphic reductions of social networks which are possible using these definitions to capture the role structure of a network. The concepts build directly on the pioneering work of S.F. Nadel (1957) and the pathbreaking approach to blockmodeling introduced by Lorrain and White (1971) and refined in subsequent years (White, Boorman and Breiger 1976;Boorman and White 1976; Arabie, Boorman and Levitt, 1978; Sailer, 1978). Blockmodeling is one of the predominant techniques for deriving structural models of social networks. When a network is represented by a directed multigraph, a blockmodel of the multigraph can be characterized as mapping points and edges onto their images in a reduced multigraph. The relations in a network or multigraph can also be composed to form a semigroup. In the first part of the paper we examine ``graph'' homomorphisms, or homomorphic mappings of the points or actors in a network. A family of basic concepts of role equivalence are introduced, and theorems presented to show the structure preserving properties of their various induced homomorphisms. This extends the ``classic'' approach to blockmodeling via the equivalence of positions. Lorrain and White (1971), Pattison (1980), Boyd, 1980, Boyd, 1982, and most recently Bonacich (1982) have explored the topic taken up in the second part of this paper, namely the homomorphic reduction of the semigroup of relations on a network, and the relation between semigroup and graph homomorphisms. Our approach allows us a significant beginning in reducing the complexity of a multigraph by collapsing relations which play a similar ``role'' in the network.},
  file = {C\:\\Users\\nhiot\\Zotero\\storage\\IGQAVQVA\\White et Reitz - 1983 - Graph and semigroup homomorphisms on networks of r.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\9HE8RHYA\\0378873383900254.html}
}

@article{winslettModelbasedApproachUpdating1988,
  title = {A Model-Based Approach to Updating Databases with Incomplete Information},
  author = {Winslett, Marianne},
  year = {1988},
  month = jun,
  journal = {ACM Trans. Database Syst.},
  volume = {13},
  number = {2},
  pages = {167--196},
  publisher = {{ACM New York, NY, USA}},
  issn = {0362-5915},
  doi = {10.1145/42338.42386},
  urldate = {2023-08-03},
  abstract = {Suppose one wishes to construct, use, and maintain a database of facts about the real world, even though the state of that world is only partially known. In the artificial intelligence domain, this problem arises when an agent has a base set of beliefs that reflect partial knowledge about the world, and then tries to incorporate new, possibly contradictory knowledge into this set of beliefs. In the database domain, one facet of this situation is the well-known null values problem. We choose to represent such a database as a logical theory, and view the models of the theory as representing possible states of the world that are consistent with all known information. How can new information be incorporated into the database? For example, given the new information that ``b or c is true,'' how can one get rid of all outdated information about b and c, add the new information, and yet in the process not disturb any other information in the database? In current-day database management systems, the difficult and tedious burden of determining exactly what to add and remove from the database is placed on the user. The goal of our research was to relieve users of that burden, by equipping the database management system with update algorithms that can automatically determine what to add and remove from the database. Under our approach, new information about the state of the world is input to the database management system as a well-formed formula that the state of the world is now known to satisfy. We have constructed database update algorithms to interpret this update formula and incorporate the new information represented by the formula into the database without further assistance from the user. In this paper we show how to embed the incomplete database and the incoming information in the language of mathematical logic, explain the semantics of our update operators, and discuss the algorithms that implement these operators.},
  langid = {english},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1988\\Winslett - 1988 - A model-based approach to updating databases with .pdf;C\:\\Users\\nhiot\\Zotero\\storage\\IP27ELLK\\42338.html}
}

@book{winslettUpdatingLogicalDatabases1990,
  title = {Updating Logical Databases},
  author = {Winslett, Marianne},
  year = {1990},
  publisher = {{Cambridge University Press}},
  address = {{New York, NY, USA}},
  optisbn = {0-521-37371-9},
  keywords = {\#nosource}
}

@article{wishartDrugBankKnowledgebaseDrugs2008,
  title = {{{DrugBank}}: A Knowledgebase for Drugs, Drug Actions and Drug Targets},
  shorttitle = {{{DrugBank}}},
  author = {Wishart, David S. and Knox, Craig and Guo, An Chi and Cheng, Dean and Shrivastava, Savita and Tzur, Dan and Gautam, Bijaya and Hassanali, Murtaza},
  year = {2008},
  journal = {Nucleic acids research},
  volume = {36},
  number = {suppl\_1},
  pages = {D901--D906},
  publisher = {{Oxford University Press}},
  doi = {10.1093/nar/gkm958},
  annotation = {QID: Q24650300},
  file = {C\:\\Users\\nhiot\\Zotero\\storage\\ESEGG5Y5\\2508024.html;C\:\\Users\\nhiot\\Zotero\\storage\\PZDDAR83\\2508024.html}
}

@article{wishartDrugBankMajorUpdate2018,
  title = {{{DrugBank}} 5.0: A Major Update to the {{DrugBank}} Database for 2018},
  shorttitle = {{{DrugBank}} 5.0},
  author = {Wishart, David S. and Feunang, Yannick D. and Guo, An C. and Lo, Elvis J. and Marcu, Ana and Grant, Jason R. and Sajed, Tanvir and Johnson, Daniel and Li, Carin and Sayeeda, Zinat},
  year = {2018},
  journal = {Nucleic acids research},
  volume = {46},
  number = {D1},
  pages = {D1074--D1082},
  publisher = {{Oxford University Press}},
  doi = {10.1093/nar/gkx1037},
  annotation = {QID: Q47128239},
  file = {C:\Users\nhiot\Zotero\storage\6JDTJNUQ\4602867.html}
}

@article{zanioloDatabaseRelationsNull1984,
  title = {Database Relations with Null Values},
  author = {Zaniolo, Carlo},
  year = {1984},
  month = feb,
  journal = {Journal of Computer and System Sciences},
  volume = {28},
  number = {1},
  pages = {142--166},
  issn = {0022-0000},
  doi = {10.1016/0022-0000(84)90080-1},
  urldate = {2023-08-08},
  abstract = {A new formal approach is proposed for modeling incomplete database information by means of null values. The basis of our approach is an interpretation of nulls which obviates the need for more than one type of null. The conceptual soundness of this approach is demonstrated by generalizing the formal framework of the relational data model to include null values. In particular, the set-theoretical properties of relations with nulls are studied and the definitions of set inclusion, set union, and set difference are generalized. A simple and efficient strategy for evaluating queries in the presence of nulls is provided. The operators of relational algebra are then generalized accordingly. Finally, the deep-rooted logical and computational problems of previous approaches are reviewed to emphasize the superior practicability of the solution.},
  langid = {english},
  optbibsource = {dblp computer science bibliography, http://dblp.org},
  optbiburl = {http://dblp.org/rec/bib/journals/jcss/Zaniolo84},
  optdoi = {10.1016/0022-0000(84)90080-1},
  opttimestamp = {Sat, 20 May 2017 00:25:52 +0200},
  opturl = {https://doi.org/10.1016/0022-0000(84)90080-1},
  keywords = {\#nosource},
  file = {C\:\\Users\\nhiot\\OneDrive\\zotero\\1984\\Zaniolo - 1984 - Database relations with null values.pdf;C\:\\Users\\nhiot\\Zotero\\storage\\N6ACRI36\\0022000084900801.html}
}

@book{zipfPsychobiologyLanguage1935,
  title = {The Psycho-Biology of Language},
  author = {Zipf, G. K.},
  year = {1935},
  series = {The Psycho-Biology of Language},
  pages = {ix, 336},
  publisher = {{Houghton, Mifflin}},
  address = {{Oxford, England}},
  abstract = {Frequency counts of phonemes, morphemes, and words in samples of written discourse in diverse languages are presented in support of the generalization that the more complex any speech element, the less frequently does it occur. Thus, the greater the frequency of occurrence of words, the less tends to be their average length, and the smaller also is the number of different words. The relation between frequency and number of different words is said to be expressed by the formula ab2 = k, in which a represents the number of different words of a given frequency and b the frequency. The relationship between the magnitude of speech elements and their frequency is attributed to the operation of a "law" of linguistic change: that as the frequency of phonemes or of linguistic forms increases, their magnitude decreases. There is thus a tendency to "maintain an equilibrium" between length and frequency, and this tendency rests upon an "underlying law of economy." Human beings strive to maintain an "emotional equilibrium" between variety and repetitiveness of environmental factors and behavior. A speaker's discourse must represent a compromise between variety and repetitiveness adapted to the hearer's "tolerable limits of change in maintaining emotional equilibrium." This accounts for the maintenance of the relationship ab2 = k; the exponent of b expresses this "rate of variegation." (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C:\Users\nhiot\Zotero\storage\S2VNTYX9\1935-04756-000.html}
}

@incollection{zweigenbaumActesTALN19981998,
  title = {Actes de {{TALN}} 1998 ({{Traitement}} Automatique Des Langues Naturelles)},
  booktitle = {Actes de {{TALN}} 1998 ({{Traitement}} Automatique Des Langues Naturelles)},
  editor = {Zweigenbaum, Pierre},
  year = {1998},
  month = jun,
  publisher = {{ATALA}},
  address = {{Paris}},
  keywords = {\#nosource}
}
